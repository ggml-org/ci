Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.9s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.173s
user	0m1.036s
sys	0m1.458s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target xxhash
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-backend-ops
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-quantize-perf
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-eval-callback
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-embedding
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-infill
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-bench
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 82%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-passkey
[ 82%] Built target llama-parallel
[ 82%] Built target llama-perplexity
[ 82%] Built target llama-cli
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-quantize
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-gen-docs
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-run
[ 92%] Built target llama-gen-docs
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.197s
user	0m6.603s
sys	0m10.259s

main: quantize time =  3962.56 ms
main:    total time =  3962.56 ms

main: quantize time =  2641.01 ms
main:    total time =  2641.01 ms

main: quantize time =  2399.42 ms
main:    total time =  2399.42 ms

main: quantize time =  4230.96 ms
main:    total time =  4230.96 ms

main: quantize time =  2374.39 ms
main:    total time =  2374.39 ms

main: quantize time =  5350.32 ms
main:    total time =  5350.32 ms

main: quantize time =  5817.64 ms
main:    total time =  5817.64 ms

main: quantize time =  6950.67 ms
main:    total time =  6950.67 ms

main: quantize time =  6023.03 ms
main:    total time =  6023.03 ms

main: quantize time =  4393.73 ms
main:    total time =  4393.73 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.141 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.289 I main: llama backend init
0.00.000.294 I main: load the model and apply lora adapter, if any
0.00.058.605 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.070.972 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.070.991 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.070.995 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.070.996 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.070.997 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.070.997 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.070.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.071.000 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.071.001 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.071.001 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.071.002 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.071.003 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.071.003 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.071.004 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.071.011 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.071.011 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.071.011 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.077.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.080.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.761 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.086.769 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.771 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.772 I llama_model_loader: - type  f32:  194 tensors
0.00.086.772 I llama_model_loader: - type  f16:   98 tensors
0.00.086.774 I print_info: file format = GGUF V3 (latest)
0.00.086.776 I print_info: file type   = all F32 (guessed)
0.00.086.779 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.050 I load: special tokens cache size = 25
0.00.102.263 I load: token to piece cache size = 0.2984 MB
0.00.102.290 I print_info: arch             = gptneox
0.00.102.291 I print_info: vocab_only       = 0
0.00.102.291 I print_info: n_ctx_train      = 2048
0.00.102.291 I print_info: n_embd           = 2048
0.00.102.292 I print_info: n_layer          = 24
0.00.102.296 I print_info: n_head           = 16
0.00.102.297 I print_info: n_head_kv        = 16
0.00.102.297 I print_info: n_rot            = 32
0.00.102.297 I print_info: n_swa            = 0
0.00.102.297 I print_info: n_embd_head_k    = 128
0.00.102.297 I print_info: n_embd_head_v    = 128
0.00.102.298 I print_info: n_gqa            = 1
0.00.102.298 I print_info: n_embd_k_gqa     = 2048
0.00.102.299 I print_info: n_embd_v_gqa     = 2048
0.00.102.300 I print_info: f_norm_eps       = 1.0e-05
0.00.102.300 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.300 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.300 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.301 I print_info: f_logit_scale    = 0.0e+00
0.00.102.301 I print_info: n_ff             = 8192
0.00.102.301 I print_info: n_expert         = 0
0.00.102.302 I print_info: n_expert_used    = 0
0.00.102.302 I print_info: causal attn      = 1
0.00.102.302 I print_info: pooling type     = 0
0.00.102.302 I print_info: rope type        = 2
0.00.102.302 I print_info: rope scaling     = linear
0.00.102.303 I print_info: freq_base_train  = 10000.0
0.00.102.303 I print_info: freq_scale_train = 1
0.00.102.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.303 I print_info: rope_finetuned   = unknown
0.00.102.303 I print_info: ssm_d_conv       = 0
0.00.102.303 I print_info: ssm_d_inner      = 0
0.00.102.303 I print_info: ssm_d_state      = 0
0.00.102.304 I print_info: ssm_dt_rank      = 0
0.00.102.304 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.305 I print_info: model type       = 1.4B
0.00.102.305 I print_info: model params     = 1.41 B
0.00.102.305 I print_info: general.name     = 1.4B
0.00.102.306 I print_info: vocab type       = BPE
0.00.102.306 I print_info: n_vocab          = 50304
0.00.102.306 I print_info: n_merges         = 50009
0.00.102.306 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.307 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.307 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.307 I print_info: LF token         = 187 'Ċ'
0.00.102.307 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.308 I print_info: max token length = 1024
0.00.102.308 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.140.588 I load_tensors: offloading 24 repeating layers to GPU
0.00.140.591 I load_tensors: offloading output layer to GPU
0.00.140.591 I load_tensors: offloaded 25/25 layers to GPU
0.00.140.617 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.140.618 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.141.191 I llama_init_from_model: n_seq_max     = 1
0.00.141.192 I llama_init_from_model: n_ctx         = 2048
0.00.141.192 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.141.192 I llama_init_from_model: n_batch       = 2048
0.00.141.193 I llama_init_from_model: n_ubatch      = 512
0.00.141.193 I llama_init_from_model: flash_attn    = 0
0.00.141.194 I llama_init_from_model: freq_base     = 10000.0
0.00.141.194 I llama_init_from_model: freq_scale    = 1
0.00.141.195 I ggml_metal_init: allocating
0.00.141.235 I ggml_metal_init: found device: Apple M4
0.00.141.239 I ggml_metal_init: picking default device: Apple M4
0.00.157.357 I ggml_metal_init: using embedded metal library
0.00.166.467 I ggml_metal_init: GPU name:   Apple M4
0.00.166.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.166.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.166.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.166.471 I ggml_metal_init: simdgroup reduction   = true
0.00.166.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.166.471 I ggml_metal_init: has residency sets    = true
0.00.166.472 I ggml_metal_init: has bfloat            = true
0.00.166.472 I ggml_metal_init: use bfloat            = true
0.00.166.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.166.474 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.196.425 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.225.791 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.225.796 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.225.817 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.230.357 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.230.361 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.230.361 I llama_init_from_model: graph nodes  = 967
0.00.230.361 I llama_init_from_model: graph splits = 2
0.00.230.373 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.230.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.230.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.292.275 I main: llama threadpool init, n_threads = 4
0.00.292.313 I 
0.00.292.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.292.341 I 
0.00.292.483 I sampler seed: 1234
0.00.292.488 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.292.520 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.292.521 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.292.521 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.137.631 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.02.137.632 I llama_perf_context_print:        load time =     232.81 ms
0.02.137.634 I llama_perf_context_print: prompt eval time =      53.35 ms /     7 tokens (    7.62 ms per token,   131.22 tokens per second)
0.02.137.636 I llama_perf_context_print:        eval time =    1788.84 ms /    63 runs   (   28.39 ms per token,    35.22 tokens per second)
0.02.137.637 I llama_perf_context_print:       total time =    1846.21 ms /    70 tokens
0.02.137.834 I ggml_metal_free: deallocating

real	0m2.475s
user	0m0.119s
sys	0m0.128s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.453 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.463 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.466 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.468 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.342 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.364 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.365 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.366 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.366 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.366 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.367 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.368 I llama_model_loader: - type  f32:  194 tensors
0.00.038.368 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.369 I print_info: file format = GGUF V3 (latest)
0.00.038.369 I print_info: file type   = Q8_0
0.00.038.370 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.386 I load: special tokens cache size = 25
0.00.055.061 I load: token to piece cache size = 0.2984 MB
0.00.055.077 I print_info: arch             = gptneox
0.00.055.079 I print_info: vocab_only       = 0
0.00.055.079 I print_info: n_ctx_train      = 2048
0.00.055.079 I print_info: n_embd           = 2048
0.00.055.079 I print_info: n_layer          = 24
0.00.055.085 I print_info: n_head           = 16
0.00.055.087 I print_info: n_head_kv        = 16
0.00.055.088 I print_info: n_rot            = 32
0.00.055.088 I print_info: n_swa            = 0
0.00.055.088 I print_info: n_embd_head_k    = 128
0.00.055.088 I print_info: n_embd_head_v    = 128
0.00.055.089 I print_info: n_gqa            = 1
0.00.055.090 I print_info: n_embd_k_gqa     = 2048
0.00.055.091 I print_info: n_embd_v_gqa     = 2048
0.00.055.096 I print_info: f_norm_eps       = 1.0e-05
0.00.055.097 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.098 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.098 I print_info: f_logit_scale    = 0.0e+00
0.00.055.099 I print_info: n_ff             = 8192
0.00.055.099 I print_info: n_expert         = 0
0.00.055.099 I print_info: n_expert_used    = 0
0.00.055.099 I print_info: causal attn      = 1
0.00.055.100 I print_info: pooling type     = 0
0.00.055.101 I print_info: rope type        = 2
0.00.055.102 I print_info: rope scaling     = linear
0.00.055.102 I print_info: freq_base_train  = 10000.0
0.00.055.102 I print_info: freq_scale_train = 1
0.00.055.103 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.103 I print_info: rope_finetuned   = unknown
0.00.055.104 I print_info: ssm_d_conv       = 0
0.00.055.105 I print_info: ssm_d_inner      = 0
0.00.055.105 I print_info: ssm_d_state      = 0
0.00.055.105 I print_info: ssm_dt_rank      = 0
0.00.055.106 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.106 I print_info: model type       = 1.4B
0.00.055.106 I print_info: model params     = 1.41 B
0.00.055.106 I print_info: general.name     = 1.4B
0.00.055.107 I print_info: vocab type       = BPE
0.00.055.107 I print_info: n_vocab          = 50304
0.00.055.107 I print_info: n_merges         = 50009
0.00.055.108 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.108 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.109 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.110 I print_info: LF token         = 187 'Ċ'
0.00.055.110 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.110 I print_info: max token length = 1024
0.00.055.111 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.041.490 I load_tensors: offloading 24 repeating layers to GPU
0.01.041.496 I load_tensors: offloading output layer to GPU
0.01.041.497 I load_tensors: offloaded 25/25 layers to GPU
0.01.041.521 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.041.523 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.042.590 I llama_init_from_model: n_seq_max     = 1
0.01.042.592 I llama_init_from_model: n_ctx         = 2048
0.01.042.593 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.042.593 I llama_init_from_model: n_batch       = 2048
0.01.042.594 I llama_init_from_model: n_ubatch      = 512
0.01.042.594 I llama_init_from_model: flash_attn    = 0
0.01.042.595 I llama_init_from_model: freq_base     = 10000.0
0.01.042.595 I llama_init_from_model: freq_scale    = 1
0.01.042.596 I ggml_metal_init: allocating
0.01.042.611 I ggml_metal_init: found device: Apple M4
0.01.042.619 I ggml_metal_init: picking default device: Apple M4
0.01.044.043 I ggml_metal_init: using embedded metal library
0.01.049.401 I ggml_metal_init: GPU name:   Apple M4
0.01.049.404 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.049.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.049.405 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.049.405 I ggml_metal_init: simdgroup reduction   = true
0.01.049.405 I ggml_metal_init: simdgroup matrix mul. = true
0.01.049.406 I ggml_metal_init: has residency sets    = true
0.01.049.406 I ggml_metal_init: has bfloat            = true
0.01.049.406 I ggml_metal_init: use bfloat            = true
0.01.049.407 I ggml_metal_init: hasUnifiedMemory      = true
0.01.049.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.064.936 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.112.818 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.112.824 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.112.846 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.117.144 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.117.145 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.117.146 I llama_init_from_model: graph nodes  = 967
0.01.117.146 I llama_init_from_model: graph splits = 2
0.01.117.151 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.117.275 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.117.276 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.173.332 I main: llama threadpool init, n_threads = 4
0.01.173.381 I 
0.01.173.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.173.402 I 
0.01.173.582 I sampler seed: 1234
0.01.173.586 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.173.601 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.173.602 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.173.602 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.264.656 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.02.264.657 I llama_perf_context_print:        load time =    1162.72 ms
0.02.264.658 I llama_perf_context_print: prompt eval time =      48.97 ms /     7 tokens (    7.00 ms per token,   142.95 tokens per second)
0.02.264.658 I llama_perf_context_print:        eval time =    1039.16 ms /    63 runs   (   16.49 ms per token,    60.63 tokens per second)
0.02.264.659 I llama_perf_context_print:       total time =    1092.09 ms /    70 tokens
0.02.264.931 I ggml_metal_free: deallocating

real	0m2.283s
user	0m0.110s
sys	0m0.267s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.017.190 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.798 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.810 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.814 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.146 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.281 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.283 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.283 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.284 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.284 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.284 I llama_model_loader: - type  f32:  194 tensors
0.00.046.285 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.285 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.286 I print_info: file format = GGUF V3 (latest)
0.00.046.286 I print_info: file type   = Q4_0
0.00.046.287 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.055.906 I load: special tokens cache size = 25
0.00.063.828 I load: token to piece cache size = 0.2984 MB
0.00.063.843 I print_info: arch             = gptneox
0.00.063.845 I print_info: vocab_only       = 0
0.00.063.845 I print_info: n_ctx_train      = 2048
0.00.063.845 I print_info: n_embd           = 2048
0.00.063.846 I print_info: n_layer          = 24
0.00.063.850 I print_info: n_head           = 16
0.00.063.851 I print_info: n_head_kv        = 16
0.00.063.851 I print_info: n_rot            = 32
0.00.063.851 I print_info: n_swa            = 0
0.00.063.851 I print_info: n_embd_head_k    = 128
0.00.063.851 I print_info: n_embd_head_v    = 128
0.00.063.852 I print_info: n_gqa            = 1
0.00.063.853 I print_info: n_embd_k_gqa     = 2048
0.00.063.854 I print_info: n_embd_v_gqa     = 2048
0.00.063.854 I print_info: f_norm_eps       = 1.0e-05
0.00.063.857 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.858 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.858 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.858 I print_info: f_logit_scale    = 0.0e+00
0.00.063.859 I print_info: n_ff             = 8192
0.00.063.859 I print_info: n_expert         = 0
0.00.063.859 I print_info: n_expert_used    = 0
0.00.063.859 I print_info: causal attn      = 1
0.00.063.860 I print_info: pooling type     = 0
0.00.063.860 I print_info: rope type        = 2
0.00.063.860 I print_info: rope scaling     = linear
0.00.063.861 I print_info: freq_base_train  = 10000.0
0.00.063.861 I print_info: freq_scale_train = 1
0.00.063.861 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.861 I print_info: rope_finetuned   = unknown
0.00.063.862 I print_info: ssm_d_conv       = 0
0.00.063.862 I print_info: ssm_d_inner      = 0
0.00.063.867 I print_info: ssm_d_state      = 0
0.00.063.867 I print_info: ssm_dt_rank      = 0
0.00.063.867 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.867 I print_info: model type       = 1.4B
0.00.063.868 I print_info: model params     = 1.41 B
0.00.063.868 I print_info: general.name     = 1.4B
0.00.063.869 I print_info: vocab type       = BPE
0.00.063.873 I print_info: n_vocab          = 50304
0.00.063.873 I print_info: n_merges         = 50009
0.00.063.874 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.874 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.876 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.876 I print_info: LF token         = 187 'Ċ'
0.00.063.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.877 I print_info: max token length = 1024
0.00.063.878 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.585.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.177 I load_tensors: offloading output layer to GPU
0.00.585.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.212 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.585.213 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.587.031 I llama_init_from_model: n_seq_max     = 1
0.00.587.033 I llama_init_from_model: n_ctx         = 2048
0.00.587.034 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.587.035 I llama_init_from_model: n_batch       = 2048
0.00.587.035 I llama_init_from_model: n_ubatch      = 512
0.00.587.035 I llama_init_from_model: flash_attn    = 0
0.00.587.038 I llama_init_from_model: freq_base     = 10000.0
0.00.587.038 I llama_init_from_model: freq_scale    = 1
0.00.587.042 I ggml_metal_init: allocating
0.00.587.130 I ggml_metal_init: found device: Apple M4
0.00.587.143 I ggml_metal_init: picking default device: Apple M4
0.00.589.061 I ggml_metal_init: using embedded metal library
0.00.595.788 I ggml_metal_init: GPU name:   Apple M4
0.00.595.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.794 I ggml_metal_init: simdgroup reduction   = true
0.00.595.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.795 I ggml_metal_init: has residency sets    = true
0.00.595.795 I ggml_metal_init: has bfloat            = true
0.00.595.795 I ggml_metal_init: use bfloat            = true
0.00.595.796 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.230 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.906 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.667.913 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.667.938 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.930 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.672.933 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.672.933 I llama_init_from_model: graph nodes  = 967
0.00.672.933 I llama_init_from_model: graph splits = 2
0.00.672.942 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.673.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.673.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.189 I main: llama threadpool init, n_threads = 4
0.00.730.232 I 
0.00.730.254 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.256 I 
0.00.730.421 I sampler seed: 1234
0.00.730.425 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.440 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.440 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.440 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.411.134 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47843.67 tokens per second)
0.01.411.134 I llama_perf_context_print:        load time =     712.21 ms
0.01.411.135 I llama_perf_context_print: prompt eval time =      49.07 ms /     7 tokens (    7.01 ms per token,   142.66 tokens per second)
0.01.411.136 I llama_perf_context_print:        eval time =     628.73 ms /    63 runs   (    9.98 ms per token,   100.20 tokens per second)
0.01.411.137 I llama_perf_context_print:       total time =     681.74 ms /    70 tokens
0.01.411.369 I ggml_metal_free: deallocating

real	0m1.430s
user	0m0.115s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.010 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.711 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.715 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.717 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.718 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.719 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.719 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.720 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.449 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.449 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.450 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.450 I llama_model_loader: - type  f32:  194 tensors
0.00.035.451 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.451 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.451 I print_info: file format = GGUF V3 (latest)
0.00.035.452 I print_info: file type   = Q4_1
0.00.035.452 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.137 I load: special tokens cache size = 25
0.00.051.632 I load: token to piece cache size = 0.2984 MB
0.00.051.647 I print_info: arch             = gptneox
0.00.051.648 I print_info: vocab_only       = 0
0.00.051.648 I print_info: n_ctx_train      = 2048
0.00.051.648 I print_info: n_embd           = 2048
0.00.051.649 I print_info: n_layer          = 24
0.00.051.651 I print_info: n_head           = 16
0.00.051.652 I print_info: n_head_kv        = 16
0.00.051.652 I print_info: n_rot            = 32
0.00.051.652 I print_info: n_swa            = 0
0.00.051.653 I print_info: n_embd_head_k    = 128
0.00.051.653 I print_info: n_embd_head_v    = 128
0.00.051.654 I print_info: n_gqa            = 1
0.00.051.654 I print_info: n_embd_k_gqa     = 2048
0.00.051.655 I print_info: n_embd_v_gqa     = 2048
0.00.051.656 I print_info: f_norm_eps       = 1.0e-05
0.00.051.656 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.658 I print_info: f_logit_scale    = 0.0e+00
0.00.051.659 I print_info: n_ff             = 8192
0.00.051.659 I print_info: n_expert         = 0
0.00.051.660 I print_info: n_expert_used    = 0
0.00.051.661 I print_info: causal attn      = 1
0.00.051.661 I print_info: pooling type     = 0
0.00.051.663 I print_info: rope type        = 2
0.00.051.664 I print_info: rope scaling     = linear
0.00.051.664 I print_info: freq_base_train  = 10000.0
0.00.051.665 I print_info: freq_scale_train = 1
0.00.051.665 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.666 I print_info: rope_finetuned   = unknown
0.00.051.666 I print_info: ssm_d_conv       = 0
0.00.051.666 I print_info: ssm_d_inner      = 0
0.00.051.667 I print_info: ssm_d_state      = 0
0.00.051.667 I print_info: ssm_dt_rank      = 0
0.00.051.667 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.667 I print_info: model type       = 1.4B
0.00.051.667 I print_info: model params     = 1.41 B
0.00.051.667 I print_info: general.name     = 1.4B
0.00.051.668 I print_info: vocab type       = BPE
0.00.051.668 I print_info: n_vocab          = 50304
0.00.051.668 I print_info: n_merges         = 50009
0.00.051.669 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.669 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.669 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.669 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.669 I print_info: LF token         = 187 'Ċ'
0.00.051.669 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.670 I print_info: max token length = 1024
0.00.051.672 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.548.935 I load_tensors: offloading 24 repeating layers to GPU
0.00.548.946 I load_tensors: offloading output layer to GPU
0.00.548.947 I load_tensors: offloaded 25/25 layers to GPU
0.00.548.977 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.548.978 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.550.629 I llama_init_from_model: n_seq_max     = 1
0.00.550.632 I llama_init_from_model: n_ctx         = 2048
0.00.550.633 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.550.633 I llama_init_from_model: n_batch       = 2048
0.00.550.633 I llama_init_from_model: n_ubatch      = 512
0.00.550.634 I llama_init_from_model: flash_attn    = 0
0.00.550.636 I llama_init_from_model: freq_base     = 10000.0
0.00.550.636 I llama_init_from_model: freq_scale    = 1
0.00.550.640 I ggml_metal_init: allocating
0.00.550.693 I ggml_metal_init: found device: Apple M4
0.00.550.705 I ggml_metal_init: picking default device: Apple M4
0.00.552.498 I ggml_metal_init: using embedded metal library
0.00.558.440 I ggml_metal_init: GPU name:   Apple M4
0.00.558.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.449 I ggml_metal_init: simdgroup reduction   = true
0.00.558.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.449 I ggml_metal_init: has residency sets    = true
0.00.558.450 I ggml_metal_init: has bfloat            = true
0.00.558.450 I ggml_metal_init: use bfloat            = true
0.00.558.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.579.893 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.446 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.630.452 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.630.484 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.189 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.635.191 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.635.192 I llama_init_from_model: graph nodes  = 967
0.00.635.192 I llama_init_from_model: graph splits = 2
0.00.635.201 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.635.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.635.335 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.636 I main: llama threadpool init, n_threads = 4
0.00.689.680 I 
0.00.689.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.701 I 
0.00.689.863 I sampler seed: 1234
0.00.689.868 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.882 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.883 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.883 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.420.380 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.420.381 I llama_perf_context_print:        load time =     679.88 ms
0.01.420.382 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.12 tokens per second)
0.01.420.382 I llama_perf_context_print:        eval time =     678.87 ms /    63 runs   (   10.78 ms per token,    92.80 tokens per second)
0.01.420.383 I llama_perf_context_print:       total time =     731.49 ms /    70 tokens
0.01.420.612 I ggml_metal_free: deallocating

real	0m1.438s
user	0m0.114s
sys	0m0.189s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.015.332 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.809 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.814 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.815 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.815 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.816 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.817 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.818 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.818 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.818 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.819 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.819 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.821 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.035.336 I llama_model_loader: - type  f32:  194 tensors
0.00.035.337 I llama_model_loader: - type q5_0:   97 tensors
0.00.035.337 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.337 I print_info: file format = GGUF V3 (latest)
0.00.035.338 I print_info: file type   = Q5_0
0.00.035.339 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.898 I load: special tokens cache size = 25
0.00.050.233 I load: token to piece cache size = 0.2984 MB
0.00.050.247 I print_info: arch             = gptneox
0.00.050.248 I print_info: vocab_only       = 0
0.00.050.248 I print_info: n_ctx_train      = 2048
0.00.050.249 I print_info: n_embd           = 2048
0.00.050.249 I print_info: n_layer          = 24
0.00.050.252 I print_info: n_head           = 16
0.00.050.253 I print_info: n_head_kv        = 16
0.00.050.253 I print_info: n_rot            = 32
0.00.050.253 I print_info: n_swa            = 0
0.00.050.253 I print_info: n_embd_head_k    = 128
0.00.050.253 I print_info: n_embd_head_v    = 128
0.00.050.254 I print_info: n_gqa            = 1
0.00.050.255 I print_info: n_embd_k_gqa     = 2048
0.00.050.256 I print_info: n_embd_v_gqa     = 2048
0.00.050.256 I print_info: f_norm_eps       = 1.0e-05
0.00.050.257 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.257 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.257 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.257 I print_info: f_logit_scale    = 0.0e+00
0.00.050.259 I print_info: n_ff             = 8192
0.00.050.259 I print_info: n_expert         = 0
0.00.050.259 I print_info: n_expert_used    = 0
0.00.050.259 I print_info: causal attn      = 1
0.00.050.259 I print_info: pooling type     = 0
0.00.050.259 I print_info: rope type        = 2
0.00.050.262 I print_info: rope scaling     = linear
0.00.050.262 I print_info: freq_base_train  = 10000.0
0.00.050.262 I print_info: freq_scale_train = 1
0.00.050.262 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.267 I print_info: rope_finetuned   = unknown
0.00.050.269 I print_info: ssm_d_conv       = 0
0.00.050.269 I print_info: ssm_d_inner      = 0
0.00.050.270 I print_info: ssm_d_state      = 0
0.00.050.270 I print_info: ssm_dt_rank      = 0
0.00.050.270 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.270 I print_info: model type       = 1.4B
0.00.050.271 I print_info: model params     = 1.41 B
0.00.050.272 I print_info: general.name     = 1.4B
0.00.050.272 I print_info: vocab type       = BPE
0.00.050.272 I print_info: n_vocab          = 50304
0.00.050.272 I print_info: n_merges         = 50009
0.00.050.273 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.273 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.274 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.274 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.274 I print_info: LF token         = 187 'Ċ'
0.00.050.275 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.275 I print_info: max token length = 1024
0.00.050.275 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.839.661 I load_tensors: offloading 24 repeating layers to GPU
0.00.839.676 I load_tensors: offloading output layer to GPU
0.00.839.677 I load_tensors: offloaded 25/25 layers to GPU
0.00.839.711 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.839.713 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.841.325 I llama_init_from_model: n_seq_max     = 1
0.00.841.328 I llama_init_from_model: n_ctx         = 2048
0.00.841.328 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.841.329 I llama_init_from_model: n_batch       = 2048
0.00.841.329 I llama_init_from_model: n_ubatch      = 512
0.00.841.330 I llama_init_from_model: flash_attn    = 0
0.00.841.332 I llama_init_from_model: freq_base     = 10000.0
0.00.841.332 I llama_init_from_model: freq_scale    = 1
0.00.841.335 I ggml_metal_init: allocating
0.00.841.407 I ggml_metal_init: found device: Apple M4
0.00.841.420 I ggml_metal_init: picking default device: Apple M4
0.00.843.300 I ggml_metal_init: using embedded metal library
0.00.850.098 I ggml_metal_init: GPU name:   Apple M4
0.00.850.102 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.850.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.850.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.850.104 I ggml_metal_init: simdgroup reduction   = true
0.00.850.104 I ggml_metal_init: simdgroup matrix mul. = true
0.00.850.105 I ggml_metal_init: has residency sets    = true
0.00.850.105 I ggml_metal_init: has bfloat            = true
0.00.850.105 I ggml_metal_init: use bfloat            = true
0.00.850.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.850.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.869.093 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.926.557 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.926.564 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.926.588 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.930.720 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.930.722 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.930.723 I llama_init_from_model: graph nodes  = 967
0.00.930.723 I llama_init_from_model: graph splits = 2
0.00.930.730 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.930.865 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.930.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.987.591 I main: llama threadpool init, n_threads = 4
0.00.987.651 I 
0.00.987.671 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.987.672 I 
0.00.987.825 I sampler seed: 1234
0.00.987.829 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.987.843 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.987.845 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.987.845 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.778.667 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49100.97 tokens per second)
0.01.778.668 I llama_perf_context_print:        load time =     971.50 ms
0.01.778.669 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.52 tokens per second)
0.01.778.669 I llama_perf_context_print:        eval time =     745.05 ms /    63 runs   (   11.83 ms per token,    84.56 tokens per second)
0.01.778.672 I llama_perf_context_print:       total time =     791.84 ms /    70 tokens
0.01.778.887 I ggml_metal_free: deallocating

real	0m1.795s
user	0m0.111s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.012.154 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.020.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.942 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.943 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.943 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.944 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.944 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.945 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.945 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.946 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.946 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.946 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.949 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.952 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.952 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.952 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.607 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.298 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.299 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.029.299 I llama_model_loader: - type  f32:  194 tensors
0.00.029.300 I llama_model_loader: - type q5_1:   97 tensors
0.00.029.300 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.301 I print_info: file format = GGUF V3 (latest)
0.00.029.301 I print_info: file type   = Q5_1
0.00.029.302 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.037.005 I load: special tokens cache size = 25
0.00.043.378 I load: token to piece cache size = 0.2984 MB
0.00.043.391 I print_info: arch             = gptneox
0.00.043.392 I print_info: vocab_only       = 0
0.00.043.393 I print_info: n_ctx_train      = 2048
0.00.043.393 I print_info: n_embd           = 2048
0.00.043.393 I print_info: n_layer          = 24
0.00.043.396 I print_info: n_head           = 16
0.00.043.397 I print_info: n_head_kv        = 16
0.00.043.397 I print_info: n_rot            = 32
0.00.043.397 I print_info: n_swa            = 0
0.00.043.398 I print_info: n_embd_head_k    = 128
0.00.043.398 I print_info: n_embd_head_v    = 128
0.00.043.399 I print_info: n_gqa            = 1
0.00.043.400 I print_info: n_embd_k_gqa     = 2048
0.00.043.401 I print_info: n_embd_v_gqa     = 2048
0.00.043.401 I print_info: f_norm_eps       = 1.0e-05
0.00.043.402 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.404 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.404 I print_info: f_logit_scale    = 0.0e+00
0.00.043.405 I print_info: n_ff             = 8192
0.00.043.405 I print_info: n_expert         = 0
0.00.043.405 I print_info: n_expert_used    = 0
0.00.043.405 I print_info: causal attn      = 1
0.00.043.405 I print_info: pooling type     = 0
0.00.043.407 I print_info: rope type        = 2
0.00.043.408 I print_info: rope scaling     = linear
0.00.043.409 I print_info: freq_base_train  = 10000.0
0.00.043.410 I print_info: freq_scale_train = 1
0.00.043.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.412 I print_info: rope_finetuned   = unknown
0.00.043.412 I print_info: ssm_d_conv       = 0
0.00.043.412 I print_info: ssm_d_inner      = 0
0.00.043.412 I print_info: ssm_d_state      = 0
0.00.043.413 I print_info: ssm_dt_rank      = 0
0.00.043.413 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.413 I print_info: model type       = 1.4B
0.00.043.413 I print_info: model params     = 1.41 B
0.00.043.418 I print_info: general.name     = 1.4B
0.00.043.418 I print_info: vocab type       = BPE
0.00.043.419 I print_info: n_vocab          = 50304
0.00.043.419 I print_info: n_merges         = 50009
0.00.043.419 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.419 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.419 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.421 I print_info: LF token         = 187 'Ċ'
0.00.043.421 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.422 I print_info: max token length = 1024
0.00.043.422 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.725.178 I load_tensors: offloading 24 repeating layers to GPU
0.00.725.190 I load_tensors: offloading output layer to GPU
0.00.725.191 I load_tensors: offloaded 25/25 layers to GPU
0.00.725.222 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.725.224 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.726.845 I llama_init_from_model: n_seq_max     = 1
0.00.726.849 I llama_init_from_model: n_ctx         = 2048
0.00.726.849 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.726.849 I llama_init_from_model: n_batch       = 2048
0.00.726.850 I llama_init_from_model: n_ubatch      = 512
0.00.726.850 I llama_init_from_model: flash_attn    = 0
0.00.726.852 I llama_init_from_model: freq_base     = 10000.0
0.00.726.853 I llama_init_from_model: freq_scale    = 1
0.00.726.854 I ggml_metal_init: allocating
0.00.726.871 I ggml_metal_init: found device: Apple M4
0.00.726.882 I ggml_metal_init: picking default device: Apple M4
0.00.728.451 I ggml_metal_init: using embedded metal library
0.00.734.798 I ggml_metal_init: GPU name:   Apple M4
0.00.734.801 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.734.802 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.734.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.734.803 I ggml_metal_init: simdgroup reduction   = true
0.00.734.803 I ggml_metal_init: simdgroup matrix mul. = true
0.00.734.804 I ggml_metal_init: has residency sets    = true
0.00.734.804 I ggml_metal_init: has bfloat            = true
0.00.734.804 I ggml_metal_init: use bfloat            = true
0.00.734.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.734.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.751.872 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.801.574 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.801.580 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.801.606 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.806.035 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.806.037 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.806.038 I llama_init_from_model: graph nodes  = 967
0.00.806.038 I llama_init_from_model: graph splits = 2
0.00.806.044 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.806.168 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.806.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.866.155 I main: llama threadpool init, n_threads = 4
0.00.866.220 I 
0.00.866.241 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.866.243 I 
0.00.866.391 I sampler seed: 1234
0.00.866.395 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.866.410 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.866.410 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.866.410 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.719.299 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50141.24 tokens per second)
0.01.719.299 I llama_perf_context_print:        load time =     853.28 ms
0.01.719.300 I llama_perf_context_print: prompt eval time =      51.86 ms /     7 tokens (    7.41 ms per token,   134.99 tokens per second)
0.01.719.301 I llama_perf_context_print:        eval time =     798.00 ms /    63 runs   (   12.67 ms per token,    78.95 tokens per second)
0.01.719.301 I llama_perf_context_print:       total time =     853.86 ms /    70 tokens
0.01.719.510 I ggml_metal_free: deallocating

real	0m1.741s
user	0m0.108s
sys	0m0.214s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.962 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.552 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.557 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.561 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.255 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.903 I llama_model_loader: - type  f32:  194 tensors
0.00.023.903 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.903 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.904 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.904 I print_info: file format = GGUF V3 (latest)
0.00.023.905 I print_info: file type   = Q2_K - Medium
0.00.023.907 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.565 I load: special tokens cache size = 25
0.00.037.527 I load: token to piece cache size = 0.2984 MB
0.00.037.536 I print_info: arch             = gptneox
0.00.037.537 I print_info: vocab_only       = 0
0.00.037.538 I print_info: n_ctx_train      = 2048
0.00.037.538 I print_info: n_embd           = 2048
0.00.037.538 I print_info: n_layer          = 24
0.00.037.541 I print_info: n_head           = 16
0.00.037.541 I print_info: n_head_kv        = 16
0.00.037.541 I print_info: n_rot            = 32
0.00.037.542 I print_info: n_swa            = 0
0.00.037.542 I print_info: n_embd_head_k    = 128
0.00.037.542 I print_info: n_embd_head_v    = 128
0.00.037.543 I print_info: n_gqa            = 1
0.00.037.543 I print_info: n_embd_k_gqa     = 2048
0.00.037.544 I print_info: n_embd_v_gqa     = 2048
0.00.037.545 I print_info: f_norm_eps       = 1.0e-05
0.00.037.545 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.547 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.548 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.548 I print_info: f_logit_scale    = 0.0e+00
0.00.037.548 I print_info: n_ff             = 8192
0.00.037.549 I print_info: n_expert         = 0
0.00.037.549 I print_info: n_expert_used    = 0
0.00.037.549 I print_info: causal attn      = 1
0.00.037.550 I print_info: pooling type     = 0
0.00.037.551 I print_info: rope type        = 2
0.00.037.551 I print_info: rope scaling     = linear
0.00.037.551 I print_info: freq_base_train  = 10000.0
0.00.037.552 I print_info: freq_scale_train = 1
0.00.037.552 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.552 I print_info: rope_finetuned   = unknown
0.00.037.552 I print_info: ssm_d_conv       = 0
0.00.037.552 I print_info: ssm_d_inner      = 0
0.00.037.552 I print_info: ssm_d_state      = 0
0.00.037.552 I print_info: ssm_dt_rank      = 0
0.00.037.553 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.554 I print_info: model type       = 1.4B
0.00.037.554 I print_info: model params     = 1.41 B
0.00.037.554 I print_info: general.name     = 1.4B
0.00.037.554 I print_info: vocab type       = BPE
0.00.037.556 I print_info: n_vocab          = 50304
0.00.037.556 I print_info: n_merges         = 50009
0.00.037.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.558 I print_info: LF token         = 187 'Ċ'
0.00.037.558 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.558 I print_info: max token length = 1024
0.00.037.558 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.365.541 I load_tensors: offloading 24 repeating layers to GPU
0.00.365.557 I load_tensors: offloading output layer to GPU
0.00.365.558 I load_tensors: offloaded 25/25 layers to GPU
0.00.365.596 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.365.597 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.367.248 I llama_init_from_model: n_seq_max     = 1
0.00.367.250 I llama_init_from_model: n_ctx         = 2048
0.00.367.251 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.367.252 I llama_init_from_model: n_batch       = 2048
0.00.367.252 I llama_init_from_model: n_ubatch      = 512
0.00.367.253 I llama_init_from_model: flash_attn    = 0
0.00.367.255 I llama_init_from_model: freq_base     = 10000.0
0.00.367.256 I llama_init_from_model: freq_scale    = 1
0.00.367.258 I ggml_metal_init: allocating
0.00.367.336 I ggml_metal_init: found device: Apple M4
0.00.367.349 I ggml_metal_init: picking default device: Apple M4
0.00.369.269 I ggml_metal_init: using embedded metal library
0.00.374.867 I ggml_metal_init: GPU name:   Apple M4
0.00.374.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.374.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.374.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.374.889 I ggml_metal_init: simdgroup reduction   = true
0.00.374.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.374.890 I ggml_metal_init: has residency sets    = true
0.00.374.890 I ggml_metal_init: has bfloat            = true
0.00.374.890 I ggml_metal_init: use bfloat            = true
0.00.374.897 I ggml_metal_init: hasUnifiedMemory      = true
0.00.374.900 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.397.059 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.451.980 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.451.986 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.452.012 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.456.078 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.456.080 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.456.080 I llama_init_from_model: graph nodes  = 967
0.00.456.080 I llama_init_from_model: graph splits = 2
0.00.456.086 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.456.210 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.456.210 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.515.333 I main: llama threadpool init, n_threads = 4
0.00.515.384 I 
0.00.515.403 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.515.404 I 
0.00.515.580 I sampler seed: 1234
0.00.515.585 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.515.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.515.628 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.515.628 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.202.216 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.01.202.217 I llama_perf_context_print:        load time =     505.65 ms
0.01.202.218 I llama_perf_context_print: prompt eval time =      44.46 ms /     7 tokens (    6.35 ms per token,   157.45 tokens per second)
0.01.202.218 I llama_perf_context_print:        eval time =     639.24 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.202.219 I llama_perf_context_print:       total time =     687.61 ms /    70 tokens
0.01.202.429 I ggml_metal_free: deallocating

real	0m1.218s
user	0m0.111s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.746 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.162 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.934 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.585 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.585 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.586 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.586 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.587 I llama_model_loader: - type  f32:  194 tensors
0.00.024.587 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.587 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.588 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.588 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.588 I print_info: file format = GGUF V3 (latest)
0.00.024.589 I print_info: file type   = Q3_K - Medium
0.00.024.590 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.661 I load: special tokens cache size = 25
0.00.038.970 I load: token to piece cache size = 0.2984 MB
0.00.038.990 I print_info: arch             = gptneox
0.00.038.991 I print_info: vocab_only       = 0
0.00.038.992 I print_info: n_ctx_train      = 2048
0.00.038.992 I print_info: n_embd           = 2048
0.00.038.992 I print_info: n_layer          = 24
0.00.038.995 I print_info: n_head           = 16
0.00.038.995 I print_info: n_head_kv        = 16
0.00.038.996 I print_info: n_rot            = 32
0.00.038.996 I print_info: n_swa            = 0
0.00.038.996 I print_info: n_embd_head_k    = 128
0.00.038.996 I print_info: n_embd_head_v    = 128
0.00.038.999 I print_info: n_gqa            = 1
0.00.038.999 I print_info: n_embd_k_gqa     = 2048
0.00.039.000 I print_info: n_embd_v_gqa     = 2048
0.00.039.001 I print_info: f_norm_eps       = 1.0e-05
0.00.039.001 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.001 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.001 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.001 I print_info: f_logit_scale    = 0.0e+00
0.00.039.002 I print_info: n_ff             = 8192
0.00.039.002 I print_info: n_expert         = 0
0.00.039.002 I print_info: n_expert_used    = 0
0.00.039.002 I print_info: causal attn      = 1
0.00.039.002 I print_info: pooling type     = 0
0.00.039.004 I print_info: rope type        = 2
0.00.039.004 I print_info: rope scaling     = linear
0.00.039.004 I print_info: freq_base_train  = 10000.0
0.00.039.004 I print_info: freq_scale_train = 1
0.00.039.004 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.005 I print_info: rope_finetuned   = unknown
0.00.039.005 I print_info: ssm_d_conv       = 0
0.00.039.005 I print_info: ssm_d_inner      = 0
0.00.039.005 I print_info: ssm_d_state      = 0
0.00.039.005 I print_info: ssm_dt_rank      = 0
0.00.039.005 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.005 I print_info: model type       = 1.4B
0.00.039.006 I print_info: model params     = 1.41 B
0.00.039.006 I print_info: general.name     = 1.4B
0.00.039.006 I print_info: vocab type       = BPE
0.00.039.007 I print_info: n_vocab          = 50304
0.00.039.007 I print_info: n_merges         = 50009
0.00.039.007 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.007 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.007 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: LF token         = 187 'Ċ'
0.00.039.008 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.009 I print_info: max token length = 1024
0.00.039.010 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.455.702 I load_tensors: offloading 24 repeating layers to GPU
0.00.455.717 I load_tensors: offloading output layer to GPU
0.00.455.717 I load_tensors: offloaded 25/25 layers to GPU
0.00.455.746 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.455.748 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.457.159 I llama_init_from_model: n_seq_max     = 1
0.00.457.164 I llama_init_from_model: n_ctx         = 2048
0.00.457.165 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.457.165 I llama_init_from_model: n_batch       = 2048
0.00.457.165 I llama_init_from_model: n_ubatch      = 512
0.00.457.166 I llama_init_from_model: flash_attn    = 0
0.00.457.166 I llama_init_from_model: freq_base     = 10000.0
0.00.457.167 I llama_init_from_model: freq_scale    = 1
0.00.457.170 I ggml_metal_init: allocating
0.00.457.219 I ggml_metal_init: found device: Apple M4
0.00.457.231 I ggml_metal_init: picking default device: Apple M4
0.00.459.046 I ggml_metal_init: using embedded metal library
0.00.464.816 I ggml_metal_init: GPU name:   Apple M4
0.00.464.828 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.464.829 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.464.830 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.464.830 I ggml_metal_init: simdgroup reduction   = true
0.00.464.831 I ggml_metal_init: simdgroup matrix mul. = true
0.00.464.831 I ggml_metal_init: has residency sets    = true
0.00.464.831 I ggml_metal_init: has bfloat            = true
0.00.464.831 I ggml_metal_init: use bfloat            = true
0.00.464.833 I ggml_metal_init: hasUnifiedMemory      = true
0.00.464.837 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.486.408 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.544.575 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.544.585 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.544.610 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.549.807 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.549.809 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.549.809 I llama_init_from_model: graph nodes  = 967
0.00.549.810 I llama_init_from_model: graph splits = 2
0.00.549.815 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.549.949 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.549.950 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.310 I main: llama threadpool init, n_threads = 4
0.00.609.360 I 
0.00.609.379 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.382 I 
0.00.609.537 I sampler seed: 1234
0.00.609.541 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.585 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.588 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.588 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.353.837 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48037.89 tokens per second)
0.01.353.837 I llama_perf_context_print:        load time =     599.85 ms
0.01.353.838 I llama_perf_context_print: prompt eval time =      50.98 ms /     7 tokens (    7.28 ms per token,   137.30 tokens per second)
0.01.353.839 I llama_perf_context_print:        eval time =     690.74 ms /    63 runs   (   10.96 ms per token,    91.21 tokens per second)
0.01.353.839 I llama_perf_context_print:       total time =     745.24 ms /    70 tokens
0.01.354.067 I ggml_metal_free: deallocating

real	0m1.370s
user	0m0.111s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.936 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.948 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.949 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.949 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.949 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.950 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.951 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.951 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.952 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.952 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.953 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.953 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.585 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.259 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.260 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.261 I llama_model_loader: - type  f32:  194 tensors
0.00.025.261 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.261 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.262 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.262 I print_info: file format = GGUF V3 (latest)
0.00.025.262 I print_info: file type   = Q4_K - Medium
0.00.025.263 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.907 I load: special tokens cache size = 25
0.00.038.980 I load: token to piece cache size = 0.2984 MB
0.00.038.993 I print_info: arch             = gptneox
0.00.038.994 I print_info: vocab_only       = 0
0.00.038.995 I print_info: n_ctx_train      = 2048
0.00.038.995 I print_info: n_embd           = 2048
0.00.038.995 I print_info: n_layer          = 24
0.00.038.998 I print_info: n_head           = 16
0.00.038.999 I print_info: n_head_kv        = 16
0.00.038.999 I print_info: n_rot            = 32
0.00.038.999 I print_info: n_swa            = 0
0.00.038.999 I print_info: n_embd_head_k    = 128
0.00.038.999 I print_info: n_embd_head_v    = 128
0.00.039.000 I print_info: n_gqa            = 1
0.00.039.002 I print_info: n_embd_k_gqa     = 2048
0.00.039.003 I print_info: n_embd_v_gqa     = 2048
0.00.039.003 I print_info: f_norm_eps       = 1.0e-05
0.00.039.003 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.004 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.004 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.004 I print_info: f_logit_scale    = 0.0e+00
0.00.039.005 I print_info: n_ff             = 8192
0.00.039.005 I print_info: n_expert         = 0
0.00.039.005 I print_info: n_expert_used    = 0
0.00.039.005 I print_info: causal attn      = 1
0.00.039.005 I print_info: pooling type     = 0
0.00.039.005 I print_info: rope type        = 2
0.00.039.010 I print_info: rope scaling     = linear
0.00.039.010 I print_info: freq_base_train  = 10000.0
0.00.039.011 I print_info: freq_scale_train = 1
0.00.039.011 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.011 I print_info: rope_finetuned   = unknown
0.00.039.011 I print_info: ssm_d_conv       = 0
0.00.039.011 I print_info: ssm_d_inner      = 0
0.00.039.013 I print_info: ssm_d_state      = 0
0.00.039.013 I print_info: ssm_dt_rank      = 0
0.00.039.013 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.013 I print_info: model type       = 1.4B
0.00.039.014 I print_info: model params     = 1.41 B
0.00.039.014 I print_info: general.name     = 1.4B
0.00.039.014 I print_info: vocab type       = BPE
0.00.039.015 I print_info: n_vocab          = 50304
0.00.039.015 I print_info: n_merges         = 50009
0.00.039.015 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.016 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.016 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.016 I print_info: LF token         = 187 'Ċ'
0.00.039.016 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.017 I print_info: max token length = 1024
0.00.039.017 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.512.637 I load_tensors: offloading 24 repeating layers to GPU
0.00.512.652 I load_tensors: offloading output layer to GPU
0.00.512.653 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.687 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.512.698 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.514.407 I llama_init_from_model: n_seq_max     = 1
0.00.514.410 I llama_init_from_model: n_ctx         = 2048
0.00.514.410 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.514.411 I llama_init_from_model: n_batch       = 2048
0.00.514.411 I llama_init_from_model: n_ubatch      = 512
0.00.514.412 I llama_init_from_model: flash_attn    = 0
0.00.514.414 I llama_init_from_model: freq_base     = 10000.0
0.00.514.414 I llama_init_from_model: freq_scale    = 1
0.00.514.417 I ggml_metal_init: allocating
0.00.514.491 I ggml_metal_init: found device: Apple M4
0.00.514.504 I ggml_metal_init: picking default device: Apple M4
0.00.516.402 I ggml_metal_init: using embedded metal library
0.00.523.235 I ggml_metal_init: GPU name:   Apple M4
0.00.523.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.523.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.523.242 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.523.243 I ggml_metal_init: simdgroup reduction   = true
0.00.523.243 I ggml_metal_init: simdgroup matrix mul. = true
0.00.523.243 I ggml_metal_init: has residency sets    = true
0.00.523.244 I ggml_metal_init: has bfloat            = true
0.00.523.244 I ggml_metal_init: use bfloat            = true
0.00.523.245 I ggml_metal_init: hasUnifiedMemory      = true
0.00.523.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.541.630 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.593.527 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.593.533 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.593.558 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.177 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.598.179 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.598.180 I llama_init_from_model: graph nodes  = 967
0.00.598.180 I llama_init_from_model: graph splits = 2
0.00.598.186 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.598.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.598.298 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.372 I main: llama threadpool init, n_threads = 4
0.00.655.410 I 
0.00.655.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.428 I 
0.00.655.609 I sampler seed: 1234
0.00.655.613 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.655.628 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.655.629 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.655.630 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.427.214 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.427.215 I llama_perf_context_print:        load time =     644.70 ms
0.01.427.216 I llama_perf_context_print: prompt eval time =      57.30 ms /     7 tokens (    8.19 ms per token,   122.17 tokens per second)
0.01.427.217 I llama_perf_context_print:        eval time =     711.37 ms /    63 runs   (   11.29 ms per token,    88.56 tokens per second)
0.01.427.217 I llama_perf_context_print:       total time =     772.64 ms /    70 tokens
0.01.427.473 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.109s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.723 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.728 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.738 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.738 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.742 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.742 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.742 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.450 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.159 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.159 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.160 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.160 I llama_model_loader: - type  f32:  194 tensors
0.00.025.160 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.161 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.161 I print_info: file format = GGUF V3 (latest)
0.00.025.162 I print_info: file type   = Q5_K - Medium
0.00.025.162 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.274 I load: special tokens cache size = 25
0.00.039.562 I load: token to piece cache size = 0.2984 MB
0.00.039.577 I print_info: arch             = gptneox
0.00.039.578 I print_info: vocab_only       = 0
0.00.039.578 I print_info: n_ctx_train      = 2048
0.00.039.578 I print_info: n_embd           = 2048
0.00.039.578 I print_info: n_layer          = 24
0.00.039.581 I print_info: n_head           = 16
0.00.039.582 I print_info: n_head_kv        = 16
0.00.039.582 I print_info: n_rot            = 32
0.00.039.582 I print_info: n_swa            = 0
0.00.039.582 I print_info: n_embd_head_k    = 128
0.00.039.582 I print_info: n_embd_head_v    = 128
0.00.039.583 I print_info: n_gqa            = 1
0.00.039.584 I print_info: n_embd_k_gqa     = 2048
0.00.039.584 I print_info: n_embd_v_gqa     = 2048
0.00.039.585 I print_info: f_norm_eps       = 1.0e-05
0.00.039.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.587 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.587 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.587 I print_info: f_logit_scale    = 0.0e+00
0.00.039.588 I print_info: n_ff             = 8192
0.00.039.588 I print_info: n_expert         = 0
0.00.039.588 I print_info: n_expert_used    = 0
0.00.039.589 I print_info: causal attn      = 1
0.00.039.589 I print_info: pooling type     = 0
0.00.039.590 I print_info: rope type        = 2
0.00.039.592 I print_info: rope scaling     = linear
0.00.039.592 I print_info: freq_base_train  = 10000.0
0.00.039.592 I print_info: freq_scale_train = 1
0.00.039.594 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.594 I print_info: rope_finetuned   = unknown
0.00.039.594 I print_info: ssm_d_conv       = 0
0.00.039.594 I print_info: ssm_d_inner      = 0
0.00.039.594 I print_info: ssm_d_state      = 0
0.00.039.595 I print_info: ssm_dt_rank      = 0
0.00.039.595 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.595 I print_info: model type       = 1.4B
0.00.039.595 I print_info: model params     = 1.41 B
0.00.039.596 I print_info: general.name     = 1.4B
0.00.039.596 I print_info: vocab type       = BPE
0.00.039.597 I print_info: n_vocab          = 50304
0.00.039.598 I print_info: n_merges         = 50009
0.00.039.598 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.598 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.598 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.598 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.598 I print_info: LF token         = 187 'Ċ'
0.00.039.599 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.599 I print_info: max token length = 1024
0.00.039.599 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.710 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.727 I load_tensors: offloading output layer to GPU
0.00.593.728 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.763 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.593.765 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.595.231 I llama_init_from_model: n_seq_max     = 1
0.00.595.233 I llama_init_from_model: n_ctx         = 2048
0.00.595.233 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.595.234 I llama_init_from_model: n_batch       = 2048
0.00.595.235 I llama_init_from_model: n_ubatch      = 512
0.00.595.235 I llama_init_from_model: flash_attn    = 0
0.00.595.236 I llama_init_from_model: freq_base     = 10000.0
0.00.595.237 I llama_init_from_model: freq_scale    = 1
0.00.595.238 I ggml_metal_init: allocating
0.00.595.255 I ggml_metal_init: found device: Apple M4
0.00.595.264 I ggml_metal_init: picking default device: Apple M4
0.00.596.832 I ggml_metal_init: using embedded metal library
0.00.602.994 I ggml_metal_init: GPU name:   Apple M4
0.00.602.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.999 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.000 I ggml_metal_init: simdgroup reduction   = true
0.00.603.000 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.000 I ggml_metal_init: has residency sets    = true
0.00.603.001 I ggml_metal_init: has bfloat            = true
0.00.603.001 I ggml_metal_init: use bfloat            = true
0.00.603.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.524 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.150 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.672.160 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.672.183 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.672 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.676.675 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.676.675 I llama_init_from_model: graph nodes  = 967
0.00.676.675 I llama_init_from_model: graph splits = 2
0.00.676.680 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.676.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.676.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.816 I main: llama threadpool init, n_threads = 4
0.00.736.864 I 
0.00.736.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.885 I 
0.00.737.031 I sampler seed: 1234
0.00.737.036 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.737.051 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.737.052 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.737.052 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.584.052 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.584.052 I llama_perf_context_print:        load time =     727.31 ms
0.01.584.053 I llama_perf_context_print: prompt eval time =      52.67 ms /     7 tokens (    7.52 ms per token,   132.90 tokens per second)
0.01.584.054 I llama_perf_context_print:        eval time =     791.51 ms /    63 runs   (   12.56 ms per token,    79.59 tokens per second)
0.01.584.056 I llama_perf_context_print:       total time =     848.00 ms /    70 tokens
0.01.584.287 I ggml_metal_free: deallocating

real	0m1.600s
user	0m0.109s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.665 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.318 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.322 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.326 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.326 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.327 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.327 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.327 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.328 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.328 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.329 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.329 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.330 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.330 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.332 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.332 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.332 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.018 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.708 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.711 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.711 I llama_model_loader: - type  f32:  194 tensors
0.00.024.711 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.712 I print_info: file format = GGUF V3 (latest)
0.00.024.712 I print_info: file type   = Q6_K
0.00.024.713 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.401 I load: special tokens cache size = 25
0.00.038.731 I load: token to piece cache size = 0.2984 MB
0.00.038.740 I print_info: arch             = gptneox
0.00.038.741 I print_info: vocab_only       = 0
0.00.038.741 I print_info: n_ctx_train      = 2048
0.00.038.741 I print_info: n_embd           = 2048
0.00.038.742 I print_info: n_layer          = 24
0.00.038.744 I print_info: n_head           = 16
0.00.038.745 I print_info: n_head_kv        = 16
0.00.038.748 I print_info: n_rot            = 32
0.00.038.748 I print_info: n_swa            = 0
0.00.038.748 I print_info: n_embd_head_k    = 128
0.00.038.749 I print_info: n_embd_head_v    = 128
0.00.038.749 I print_info: n_gqa            = 1
0.00.038.750 I print_info: n_embd_k_gqa     = 2048
0.00.038.751 I print_info: n_embd_v_gqa     = 2048
0.00.038.751 I print_info: f_norm_eps       = 1.0e-05
0.00.038.752 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.752 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.752 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.752 I print_info: f_logit_scale    = 0.0e+00
0.00.038.753 I print_info: n_ff             = 8192
0.00.038.753 I print_info: n_expert         = 0
0.00.038.753 I print_info: n_expert_used    = 0
0.00.038.753 I print_info: causal attn      = 1
0.00.038.754 I print_info: pooling type     = 0
0.00.038.754 I print_info: rope type        = 2
0.00.038.754 I print_info: rope scaling     = linear
0.00.038.758 I print_info: freq_base_train  = 10000.0
0.00.038.759 I print_info: freq_scale_train = 1
0.00.038.761 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.763 I print_info: rope_finetuned   = unknown
0.00.038.763 I print_info: ssm_d_conv       = 0
0.00.038.763 I print_info: ssm_d_inner      = 0
0.00.038.763 I print_info: ssm_d_state      = 0
0.00.038.763 I print_info: ssm_dt_rank      = 0
0.00.038.763 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.763 I print_info: model type       = 1.4B
0.00.038.764 I print_info: model params     = 1.41 B
0.00.038.764 I print_info: general.name     = 1.4B
0.00.038.765 I print_info: vocab type       = BPE
0.00.038.765 I print_info: n_vocab          = 50304
0.00.038.765 I print_info: n_merges         = 50009
0.00.038.766 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.766 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.766 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.766 I print_info: LF token         = 187 'Ċ'
0.00.038.775 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.776 I print_info: max token length = 1024
0.00.038.777 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.939 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.942 I load_tensors: offloading output layer to GPU
0.00.628.942 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.962 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.628.964 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.630.363 I llama_init_from_model: n_seq_max     = 1
0.00.630.364 I llama_init_from_model: n_ctx         = 2048
0.00.630.365 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.630.365 I llama_init_from_model: n_batch       = 2048
0.00.630.366 I llama_init_from_model: n_ubatch      = 512
0.00.630.366 I llama_init_from_model: flash_attn    = 0
0.00.630.367 I llama_init_from_model: freq_base     = 10000.0
0.00.630.367 I llama_init_from_model: freq_scale    = 1
0.00.630.368 I ggml_metal_init: allocating
0.00.630.391 I ggml_metal_init: found device: Apple M4
0.00.630.400 I ggml_metal_init: picking default device: Apple M4
0.00.631.935 I ggml_metal_init: using embedded metal library
0.00.638.008 I ggml_metal_init: GPU name:   Apple M4
0.00.638.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.013 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.014 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.014 I ggml_metal_init: simdgroup reduction   = true
0.00.638.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.015 I ggml_metal_init: has residency sets    = true
0.00.638.015 I ggml_metal_init: has bfloat            = true
0.00.638.015 I ggml_metal_init: use bfloat            = true
0.00.638.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.017 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.017 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.706.463 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.706.468 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.706.491 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.711.585 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.711.587 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.711.588 I llama_init_from_model: graph nodes  = 967
0.00.711.588 I llama_init_from_model: graph splits = 2
0.00.711.594 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.711.719 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.711.719 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.323 I main: llama threadpool init, n_threads = 4
0.00.780.378 I 
0.00.780.399 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.399 I 
0.00.780.579 I sampler seed: 1234
0.00.780.584 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.599 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.599 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.599 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.653.705 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.653.705 I llama_perf_context_print:        load time =     769.93 ms
0.01.653.706 I llama_perf_context_print: prompt eval time =      57.48 ms /     7 tokens (    8.21 ms per token,   121.77 tokens per second)
0.01.653.707 I llama_perf_context_print:        eval time =     812.72 ms /    63 runs   (   12.90 ms per token,    77.52 tokens per second)
0.01.653.707 I llama_perf_context_print:       total time =     874.11 ms /    70 tokens
0.01.653.977 I ggml_metal_free: deallocating

real	0m1.672s
user	0m0.107s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.565 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.389 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.352 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.362 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.372 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.373 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.375 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.375 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.376 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.377 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.377 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.378 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.381 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.485 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.344 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.345 I llama_model_loader: - type  f32:  194 tensors
0.00.057.346 I llama_model_loader: - type  f16:   98 tensors
0.00.057.347 I print_info: file format = GGUF V3 (latest)
0.00.057.347 I print_info: file type   = all F32 (guessed)
0.00.057.348 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.256 I load: special tokens cache size = 25
0.00.077.810 I load: token to piece cache size = 0.2984 MB
0.00.077.819 I print_info: arch             = gptneox
0.00.077.820 I print_info: vocab_only       = 0
0.00.077.821 I print_info: n_ctx_train      = 2048
0.00.077.821 I print_info: n_embd           = 2048
0.00.077.821 I print_info: n_layer          = 24
0.00.077.824 I print_info: n_head           = 16
0.00.077.824 I print_info: n_head_kv        = 16
0.00.077.824 I print_info: n_rot            = 32
0.00.077.825 I print_info: n_swa            = 0
0.00.077.825 I print_info: n_embd_head_k    = 128
0.00.077.825 I print_info: n_embd_head_v    = 128
0.00.077.826 I print_info: n_gqa            = 1
0.00.077.827 I print_info: n_embd_k_gqa     = 2048
0.00.077.828 I print_info: n_embd_v_gqa     = 2048
0.00.077.829 I print_info: f_norm_eps       = 1.0e-05
0.00.077.829 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.829 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.829 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.830 I print_info: f_logit_scale    = 0.0e+00
0.00.077.830 I print_info: n_ff             = 8192
0.00.077.830 I print_info: n_expert         = 0
0.00.077.831 I print_info: n_expert_used    = 0
0.00.077.831 I print_info: causal attn      = 1
0.00.077.831 I print_info: pooling type     = 0
0.00.077.831 I print_info: rope type        = 2
0.00.077.831 I print_info: rope scaling     = linear
0.00.077.832 I print_info: freq_base_train  = 10000.0
0.00.077.832 I print_info: freq_scale_train = 1
0.00.077.832 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.832 I print_info: rope_finetuned   = unknown
0.00.077.833 I print_info: ssm_d_conv       = 0
0.00.077.833 I print_info: ssm_d_inner      = 0
0.00.077.833 I print_info: ssm_d_state      = 0
0.00.077.833 I print_info: ssm_dt_rank      = 0
0.00.077.833 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.833 I print_info: model type       = 1.4B
0.00.077.834 I print_info: model params     = 1.41 B
0.00.077.834 I print_info: general.name     = 1.4B
0.00.077.834 I print_info: vocab type       = BPE
0.00.077.835 I print_info: n_vocab          = 50304
0.00.077.835 I print_info: n_merges         = 50009
0.00.077.837 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.837 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.837 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.837 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.838 I print_info: LF token         = 187 'Ċ'
0.00.077.838 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.839 I print_info: max token length = 1024
0.00.077.840 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.433.487 I load_tensors: offloading 24 repeating layers to GPU
0.01.433.495 I load_tensors: offloading output layer to GPU
0.01.433.496 I load_tensors: offloaded 25/25 layers to GPU
0.01.433.520 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.433.522 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.434.478 I llama_init_from_model: n_seq_max     = 1
0.01.434.479 I llama_init_from_model: n_ctx         = 128
0.01.434.480 I llama_init_from_model: n_ctx_per_seq = 128
0.01.434.480 I llama_init_from_model: n_batch       = 128
0.01.434.480 I llama_init_from_model: n_ubatch      = 128
0.01.434.480 I llama_init_from_model: flash_attn    = 0
0.01.434.481 I llama_init_from_model: freq_base     = 10000.0
0.01.434.481 I llama_init_from_model: freq_scale    = 1
0.01.434.482 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.434.483 I ggml_metal_init: allocating
0.01.434.521 I ggml_metal_init: found device: Apple M4
0.01.434.528 I ggml_metal_init: picking default device: Apple M4
0.01.435.694 I ggml_metal_init: using embedded metal library
0.01.439.616 I ggml_metal_init: GPU name:   Apple M4
0.01.439.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.439.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.439.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.439.620 I ggml_metal_init: simdgroup reduction   = true
0.01.439.620 I ggml_metal_init: simdgroup matrix mul. = true
0.01.439.620 I ggml_metal_init: has residency sets    = true
0.01.439.620 I ggml_metal_init: has bfloat            = true
0.01.439.620 I ggml_metal_init: use bfloat            = true
0.01.439.621 I ggml_metal_init: hasUnifiedMemory      = true
0.01.439.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.450.755 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.452.513 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.452.515 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.452.571 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.454.263 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.454.264 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.454.265 I llama_init_from_model: graph nodes  = 967
0.01.454.265 I llama_init_from_model: graph splits = 2
0.01.454.266 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.454.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.491.176 I 
0.01.491.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.491.241 I perplexity: tokenizing the input ..
0.01.496.906 I perplexity: tokenization took 5.663 ms
0.01.496.912 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.615.703 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.616.987 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.617.018 I llama_perf_context_print:        load time =    1465.77 ms
0.01.617.019 I llama_perf_context_print: prompt eval time =     118.50 ms /   128 tokens (    0.93 ms per token,  1080.15 tokens per second)
0.01.617.019 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.617.020 I llama_perf_context_print:       total time =     125.84 ms /   129 tokens
0.01.617.321 I ggml_metal_free: deallocating

real	0m1.830s
user	0m0.100s
sys	0m0.256s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.160 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.306 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.317 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.317 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.320 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.320 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.321 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.323 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.324 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.324 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.077 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.809 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.810 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.810 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.810 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.811 I llama_model_loader: - type  f32:  194 tensors
0.00.024.811 I llama_model_loader: - type q8_0:   98 tensors
0.00.024.812 I print_info: file format = GGUF V3 (latest)
0.00.024.812 I print_info: file type   = Q8_0
0.00.024.818 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.032.751 I load: special tokens cache size = 25
0.00.039.114 I load: token to piece cache size = 0.2984 MB
0.00.039.131 I print_info: arch             = gptneox
0.00.039.132 I print_info: vocab_only       = 0
0.00.039.132 I print_info: n_ctx_train      = 2048
0.00.039.132 I print_info: n_embd           = 2048
0.00.039.133 I print_info: n_layer          = 24
0.00.039.137 I print_info: n_head           = 16
0.00.039.138 I print_info: n_head_kv        = 16
0.00.039.138 I print_info: n_rot            = 32
0.00.039.138 I print_info: n_swa            = 0
0.00.039.138 I print_info: n_embd_head_k    = 128
0.00.039.138 I print_info: n_embd_head_v    = 128
0.00.039.139 I print_info: n_gqa            = 1
0.00.039.140 I print_info: n_embd_k_gqa     = 2048
0.00.039.140 I print_info: n_embd_v_gqa     = 2048
0.00.039.141 I print_info: f_norm_eps       = 1.0e-05
0.00.039.141 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.141 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.142 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.143 I print_info: f_logit_scale    = 0.0e+00
0.00.039.143 I print_info: n_ff             = 8192
0.00.039.143 I print_info: n_expert         = 0
0.00.039.144 I print_info: n_expert_used    = 0
0.00.039.144 I print_info: causal attn      = 1
0.00.039.144 I print_info: pooling type     = 0
0.00.039.146 I print_info: rope type        = 2
0.00.039.147 I print_info: rope scaling     = linear
0.00.039.147 I print_info: freq_base_train  = 10000.0
0.00.039.147 I print_info: freq_scale_train = 1
0.00.039.147 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.152 I print_info: rope_finetuned   = unknown
0.00.039.152 I print_info: ssm_d_conv       = 0
0.00.039.152 I print_info: ssm_d_inner      = 0
0.00.039.152 I print_info: ssm_d_state      = 0
0.00.039.152 I print_info: ssm_dt_rank      = 0
0.00.039.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.152 I print_info: model type       = 1.4B
0.00.039.153 I print_info: model params     = 1.41 B
0.00.039.153 I print_info: general.name     = 1.4B
0.00.039.153 I print_info: vocab type       = BPE
0.00.039.154 I print_info: n_vocab          = 50304
0.00.039.154 I print_info: n_merges         = 50009
0.00.039.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.154 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.154 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.154 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.155 I print_info: LF token         = 187 'Ċ'
0.00.039.155 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.155 I print_info: max token length = 1024
0.00.039.156 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.794.096 I load_tensors: offloading 24 repeating layers to GPU
0.00.794.104 I load_tensors: offloading output layer to GPU
0.00.794.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.794.136 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.794.140 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.795.465 I llama_init_from_model: n_seq_max     = 1
0.00.795.466 I llama_init_from_model: n_ctx         = 128
0.00.795.467 I llama_init_from_model: n_ctx_per_seq = 128
0.00.795.467 I llama_init_from_model: n_batch       = 128
0.00.795.467 I llama_init_from_model: n_ubatch      = 128
0.00.795.468 I llama_init_from_model: flash_attn    = 0
0.00.795.468 I llama_init_from_model: freq_base     = 10000.0
0.00.795.469 I llama_init_from_model: freq_scale    = 1
0.00.795.469 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.795.471 I ggml_metal_init: allocating
0.00.795.541 I ggml_metal_init: found device: Apple M4
0.00.795.551 I ggml_metal_init: picking default device: Apple M4
0.00.796.971 I ggml_metal_init: using embedded metal library
0.00.802.399 I ggml_metal_init: GPU name:   Apple M4
0.00.802.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.802.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.802.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.802.405 I ggml_metal_init: simdgroup reduction   = true
0.00.802.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.802.405 I ggml_metal_init: has residency sets    = true
0.00.802.405 I ggml_metal_init: has bfloat            = true
0.00.802.405 I ggml_metal_init: use bfloat            = true
0.00.802.406 I ggml_metal_init: hasUnifiedMemory      = true
0.00.802.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.817.927 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.821.251 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.821.254 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.821.279 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.824.480 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.824.481 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.824.482 I llama_init_from_model: graph nodes  = 967
0.00.824.483 I llama_init_from_model: graph splits = 2
0.00.824.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.824.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.853.145 I 
0.00.853.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.853.247 I perplexity: tokenizing the input ..
0.00.860.110 I perplexity: tokenization took 6.862 ms
0.00.860.118 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.998.023 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.999.374 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.999.396 I llama_perf_context_print:        load time =     843.98 ms
0.00.999.400 I llama_perf_context_print: prompt eval time =     137.68 ms /   128 tokens (    1.08 ms per token,   929.73 tokens per second)
0.00.999.401 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.999.402 I llama_perf_context_print:       total time =     146.26 ms /   129 tokens
0.00.999.745 I ggml_metal_free: deallocating

real	0m1.025s
user	0m0.074s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.283 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.138 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.151 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.152 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.154 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.154 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.154 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.155 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.157 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.906 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.669 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.669 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.670 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.670 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.670 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.671 I llama_model_loader: - type  f32:  194 tensors
0.00.027.671 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.671 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.672 I print_info: file format = GGUF V3 (latest)
0.00.027.673 I print_info: file type   = Q4_0
0.00.027.674 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.789 I load: special tokens cache size = 25
0.00.042.205 I load: token to piece cache size = 0.2984 MB
0.00.042.222 I print_info: arch             = gptneox
0.00.042.223 I print_info: vocab_only       = 0
0.00.042.223 I print_info: n_ctx_train      = 2048
0.00.042.223 I print_info: n_embd           = 2048
0.00.042.223 I print_info: n_layer          = 24
0.00.042.228 I print_info: n_head           = 16
0.00.042.233 I print_info: n_head_kv        = 16
0.00.042.233 I print_info: n_rot            = 32
0.00.042.233 I print_info: n_swa            = 0
0.00.042.234 I print_info: n_embd_head_k    = 128
0.00.042.234 I print_info: n_embd_head_v    = 128
0.00.042.234 I print_info: n_gqa            = 1
0.00.042.236 I print_info: n_embd_k_gqa     = 2048
0.00.042.237 I print_info: n_embd_v_gqa     = 2048
0.00.042.237 I print_info: f_norm_eps       = 1.0e-05
0.00.042.238 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.238 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.238 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.238 I print_info: f_logit_scale    = 0.0e+00
0.00.042.239 I print_info: n_ff             = 8192
0.00.042.239 I print_info: n_expert         = 0
0.00.042.239 I print_info: n_expert_used    = 0
0.00.042.239 I print_info: causal attn      = 1
0.00.042.239 I print_info: pooling type     = 0
0.00.042.239 I print_info: rope type        = 2
0.00.042.240 I print_info: rope scaling     = linear
0.00.042.240 I print_info: freq_base_train  = 10000.0
0.00.042.242 I print_info: freq_scale_train = 1
0.00.042.242 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.242 I print_info: rope_finetuned   = unknown
0.00.042.242 I print_info: ssm_d_conv       = 0
0.00.042.242 I print_info: ssm_d_inner      = 0
0.00.042.242 I print_info: ssm_d_state      = 0
0.00.042.243 I print_info: ssm_dt_rank      = 0
0.00.042.243 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.243 I print_info: model type       = 1.4B
0.00.042.243 I print_info: model params     = 1.41 B
0.00.042.243 I print_info: general.name     = 1.4B
0.00.042.244 I print_info: vocab type       = BPE
0.00.042.244 I print_info: n_vocab          = 50304
0.00.042.244 I print_info: n_merges         = 50009
0.00.042.244 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.245 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.245 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.245 I print_info: LF token         = 187 'Ċ'
0.00.042.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.245 I print_info: max token length = 1024
0.00.042.246 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.559.999 I load_tensors: offloading 24 repeating layers to GPU
0.00.560.010 I load_tensors: offloading output layer to GPU
0.00.560.010 I load_tensors: offloaded 25/25 layers to GPU
0.00.560.044 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.560.046 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.561.519 I llama_init_from_model: n_seq_max     = 1
0.00.561.522 I llama_init_from_model: n_ctx         = 128
0.00.561.523 I llama_init_from_model: n_ctx_per_seq = 128
0.00.561.523 I llama_init_from_model: n_batch       = 128
0.00.561.524 I llama_init_from_model: n_ubatch      = 128
0.00.561.524 I llama_init_from_model: flash_attn    = 0
0.00.561.526 I llama_init_from_model: freq_base     = 10000.0
0.00.561.526 I llama_init_from_model: freq_scale    = 1
0.00.561.527 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.561.529 I ggml_metal_init: allocating
0.00.561.645 I ggml_metal_init: found device: Apple M4
0.00.561.660 I ggml_metal_init: picking default device: Apple M4
0.00.563.575 I ggml_metal_init: using embedded metal library
0.00.570.552 I ggml_metal_init: GPU name:   Apple M4
0.00.570.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.570.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.570.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.570.563 I ggml_metal_init: simdgroup reduction   = true
0.00.570.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.570.563 I ggml_metal_init: has residency sets    = true
0.00.570.563 I ggml_metal_init: has bfloat            = true
0.00.570.564 I ggml_metal_init: use bfloat            = true
0.00.570.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.570.572 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.589.001 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.592.593 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.592.597 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.592.624 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.595.941 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.595.942 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.595.943 I llama_init_from_model: graph nodes  = 967
0.00.595.944 I llama_init_from_model: graph splits = 2
0.00.595.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.595.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.427 I 
0.00.627.517 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.523 I perplexity: tokenizing the input ..
0.00.634.701 I perplexity: tokenization took 7.175 ms
0.00.634.714 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.771.414 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.772.755 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.772.779 I llama_perf_context_print:        load time =     617.13 ms
0.00.772.780 I llama_perf_context_print: prompt eval time =     135.78 ms /   128 tokens (    1.06 ms per token,   942.68 tokens per second)
0.00.772.781 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.772.781 I llama_perf_context_print:       total time =     145.36 ms /   129 tokens
0.00.773.160 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.081s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.173 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.173 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.174 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.176 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.176 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.176 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.179 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.924 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.935 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.513 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.514 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.515 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.515 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.516 I llama_model_loader: - type  f32:  194 tensors
0.00.024.516 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.516 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.517 I print_info: file format = GGUF V3 (latest)
0.00.024.517 I print_info: file type   = Q4_1
0.00.024.519 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.430 I load: special tokens cache size = 25
0.00.038.923 I load: token to piece cache size = 0.2984 MB
0.00.038.941 I print_info: arch             = gptneox
0.00.038.942 I print_info: vocab_only       = 0
0.00.038.942 I print_info: n_ctx_train      = 2048
0.00.038.942 I print_info: n_embd           = 2048
0.00.038.942 I print_info: n_layer          = 24
0.00.038.947 I print_info: n_head           = 16
0.00.038.948 I print_info: n_head_kv        = 16
0.00.038.948 I print_info: n_rot            = 32
0.00.038.948 I print_info: n_swa            = 0
0.00.038.948 I print_info: n_embd_head_k    = 128
0.00.038.948 I print_info: n_embd_head_v    = 128
0.00.038.949 I print_info: n_gqa            = 1
0.00.038.949 I print_info: n_embd_k_gqa     = 2048
0.00.038.950 I print_info: n_embd_v_gqa     = 2048
0.00.038.950 I print_info: f_norm_eps       = 1.0e-05
0.00.038.951 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.951 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.951 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.954 I print_info: f_logit_scale    = 0.0e+00
0.00.038.955 I print_info: n_ff             = 8192
0.00.038.955 I print_info: n_expert         = 0
0.00.038.955 I print_info: n_expert_used    = 0
0.00.038.955 I print_info: causal attn      = 1
0.00.038.955 I print_info: pooling type     = 0
0.00.038.955 I print_info: rope type        = 2
0.00.038.955 I print_info: rope scaling     = linear
0.00.038.956 I print_info: freq_base_train  = 10000.0
0.00.038.956 I print_info: freq_scale_train = 1
0.00.038.956 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.957 I print_info: rope_finetuned   = unknown
0.00.038.957 I print_info: ssm_d_conv       = 0
0.00.038.957 I print_info: ssm_d_inner      = 0
0.00.038.957 I print_info: ssm_d_state      = 0
0.00.038.958 I print_info: ssm_dt_rank      = 0
0.00.038.958 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.959 I print_info: model type       = 1.4B
0.00.038.959 I print_info: model params     = 1.41 B
0.00.038.959 I print_info: general.name     = 1.4B
0.00.038.960 I print_info: vocab type       = BPE
0.00.038.960 I print_info: n_vocab          = 50304
0.00.038.960 I print_info: n_merges         = 50009
0.00.038.962 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.962 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.962 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.963 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.963 I print_info: LF token         = 187 'Ċ'
0.00.038.963 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.963 I print_info: max token length = 1024
0.00.038.964 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.546.782 I load_tensors: offloading 24 repeating layers to GPU
0.00.546.792 I load_tensors: offloading output layer to GPU
0.00.546.793 I load_tensors: offloaded 25/25 layers to GPU
0.00.546.829 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.546.830 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.548.281 I llama_init_from_model: n_seq_max     = 1
0.00.548.285 I llama_init_from_model: n_ctx         = 128
0.00.548.286 I llama_init_from_model: n_ctx_per_seq = 128
0.00.548.286 I llama_init_from_model: n_batch       = 128
0.00.548.286 I llama_init_from_model: n_ubatch      = 128
0.00.548.287 I llama_init_from_model: flash_attn    = 0
0.00.548.288 I llama_init_from_model: freq_base     = 10000.0
0.00.548.289 I llama_init_from_model: freq_scale    = 1
0.00.548.289 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.548.292 I ggml_metal_init: allocating
0.00.548.423 I ggml_metal_init: found device: Apple M4
0.00.548.437 I ggml_metal_init: picking default device: Apple M4
0.00.550.352 I ggml_metal_init: using embedded metal library
0.00.556.395 I ggml_metal_init: GPU name:   Apple M4
0.00.556.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.556.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.556.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.556.405 I ggml_metal_init: simdgroup reduction   = true
0.00.556.406 I ggml_metal_init: simdgroup matrix mul. = true
0.00.556.406 I ggml_metal_init: has residency sets    = true
0.00.556.406 I ggml_metal_init: has bfloat            = true
0.00.556.406 I ggml_metal_init: use bfloat            = true
0.00.556.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.556.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.575.931 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.579.492 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.579.496 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.579.522 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.582.983 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.582.985 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.582.986 I llama_init_from_model: graph nodes  = 967
0.00.582.986 I llama_init_from_model: graph splits = 2
0.00.582.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.582.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.289 I 
0.00.610.378 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.387 I perplexity: tokenizing the input ..
0.00.617.562 I perplexity: tokenization took 7.172 ms
0.00.617.578 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.754.382 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.755.735 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.755.758 I llama_perf_context_print:        load time =     601.34 ms
0.00.755.759 I llama_perf_context_print: prompt eval time =     135.91 ms /   128 tokens (    1.06 ms per token,   941.80 tokens per second)
0.00.755.759 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.755.760 I llama_perf_context_print:       total time =     145.47 ms /   129 tokens
0.00.756.121 I ggml_metal_free: deallocating

real	0m0.770s
user	0m0.079s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.002 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.242 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.248 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.252 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.253 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.811 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.846 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.635 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.636 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.636 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.637 I llama_model_loader: - type  f32:  194 tensors
0.00.024.637 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.638 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.638 I print_info: file format = GGUF V3 (latest)
0.00.024.639 I print_info: file type   = Q5_0
0.00.024.640 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.773 I load: special tokens cache size = 25
0.00.039.235 I load: token to piece cache size = 0.2984 MB
0.00.039.251 I print_info: arch             = gptneox
0.00.039.252 I print_info: vocab_only       = 0
0.00.039.252 I print_info: n_ctx_train      = 2048
0.00.039.253 I print_info: n_embd           = 2048
0.00.039.253 I print_info: n_layer          = 24
0.00.039.257 I print_info: n_head           = 16
0.00.039.257 I print_info: n_head_kv        = 16
0.00.039.257 I print_info: n_rot            = 32
0.00.039.258 I print_info: n_swa            = 0
0.00.039.258 I print_info: n_embd_head_k    = 128
0.00.039.258 I print_info: n_embd_head_v    = 128
0.00.039.258 I print_info: n_gqa            = 1
0.00.039.259 I print_info: n_embd_k_gqa     = 2048
0.00.039.260 I print_info: n_embd_v_gqa     = 2048
0.00.039.260 I print_info: f_norm_eps       = 1.0e-05
0.00.039.261 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.261 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.261 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.261 I print_info: f_logit_scale    = 0.0e+00
0.00.039.262 I print_info: n_ff             = 8192
0.00.039.262 I print_info: n_expert         = 0
0.00.039.262 I print_info: n_expert_used    = 0
0.00.039.262 I print_info: causal attn      = 1
0.00.039.262 I print_info: pooling type     = 0
0.00.039.262 I print_info: rope type        = 2
0.00.039.263 I print_info: rope scaling     = linear
0.00.039.263 I print_info: freq_base_train  = 10000.0
0.00.039.263 I print_info: freq_scale_train = 1
0.00.039.263 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.264 I print_info: rope_finetuned   = unknown
0.00.039.264 I print_info: ssm_d_conv       = 0
0.00.039.264 I print_info: ssm_d_inner      = 0
0.00.039.264 I print_info: ssm_d_state      = 0
0.00.039.264 I print_info: ssm_dt_rank      = 0
0.00.039.264 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.265 I print_info: model type       = 1.4B
0.00.039.266 I print_info: model params     = 1.41 B
0.00.039.267 I print_info: general.name     = 1.4B
0.00.039.267 I print_info: vocab type       = BPE
0.00.039.267 I print_info: n_vocab          = 50304
0.00.039.267 I print_info: n_merges         = 50009
0.00.039.268 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: LF token         = 187 'Ċ'
0.00.039.269 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.269 I print_info: max token length = 1024
0.00.039.269 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.106 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.120 I load_tensors: offloading output layer to GPU
0.00.653.120 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.159 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.653.160 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.654.716 I llama_init_from_model: n_seq_max     = 1
0.00.654.718 I llama_init_from_model: n_ctx         = 128
0.00.654.719 I llama_init_from_model: n_ctx_per_seq = 128
0.00.654.719 I llama_init_from_model: n_batch       = 128
0.00.654.720 I llama_init_from_model: n_ubatch      = 128
0.00.654.720 I llama_init_from_model: flash_attn    = 0
0.00.654.722 I llama_init_from_model: freq_base     = 10000.0
0.00.654.723 I llama_init_from_model: freq_scale    = 1
0.00.654.723 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.654.726 I ggml_metal_init: allocating
0.00.654.854 I ggml_metal_init: found device: Apple M4
0.00.654.869 I ggml_metal_init: picking default device: Apple M4
0.00.656.775 I ggml_metal_init: using embedded metal library
0.00.663.855 I ggml_metal_init: GPU name:   Apple M4
0.00.663.863 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.864 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.865 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.866 I ggml_metal_init: simdgroup reduction   = true
0.00.663.866 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.866 I ggml_metal_init: has residency sets    = true
0.00.663.867 I ggml_metal_init: has bfloat            = true
0.00.663.867 I ggml_metal_init: use bfloat            = true
0.00.663.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.668 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.247 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.686.251 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.686.276 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.588 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.689.590 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.689.591 I llama_init_from_model: graph nodes  = 967
0.00.689.591 I llama_init_from_model: graph splits = 2
0.00.689.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.689.594 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.378 I 
0.00.722.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.472 I perplexity: tokenizing the input ..
0.00.729.678 I perplexity: tokenization took 7.202 ms
0.00.729.685 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.874.350 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.875.767 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.875.787 I llama_perf_context_print:        load time =     713.37 ms
0.00.875.788 I llama_perf_context_print: prompt eval time =     143.70 ms /   128 tokens (    1.12 ms per token,   890.75 tokens per second)
0.00.875.789 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.875.789 I llama_perf_context_print:       total time =     153.41 ms /   129 tokens
0.00.876.150 I ggml_metal_free: deallocating

real	0m0.890s
user	0m0.080s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.045 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.144 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.155 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.156 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.156 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.157 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.932 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.727 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.728 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.728 I llama_model_loader: - type  f32:  194 tensors
0.00.025.728 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.729 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.729 I print_info: file format = GGUF V3 (latest)
0.00.025.730 I print_info: file type   = Q5_1
0.00.025.731 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.931 I load: special tokens cache size = 25
0.00.040.124 I load: token to piece cache size = 0.2984 MB
0.00.040.141 I print_info: arch             = gptneox
0.00.040.142 I print_info: vocab_only       = 0
0.00.040.142 I print_info: n_ctx_train      = 2048
0.00.040.143 I print_info: n_embd           = 2048
0.00.040.143 I print_info: n_layer          = 24
0.00.040.147 I print_info: n_head           = 16
0.00.040.148 I print_info: n_head_kv        = 16
0.00.040.148 I print_info: n_rot            = 32
0.00.040.148 I print_info: n_swa            = 0
0.00.040.148 I print_info: n_embd_head_k    = 128
0.00.040.148 I print_info: n_embd_head_v    = 128
0.00.040.149 I print_info: n_gqa            = 1
0.00.040.150 I print_info: n_embd_k_gqa     = 2048
0.00.040.150 I print_info: n_embd_v_gqa     = 2048
0.00.040.151 I print_info: f_norm_eps       = 1.0e-05
0.00.040.151 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.151 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.152 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.152 I print_info: f_logit_scale    = 0.0e+00
0.00.040.152 I print_info: n_ff             = 8192
0.00.040.153 I print_info: n_expert         = 0
0.00.040.153 I print_info: n_expert_used    = 0
0.00.040.153 I print_info: causal attn      = 1
0.00.040.153 I print_info: pooling type     = 0
0.00.040.153 I print_info: rope type        = 2
0.00.040.153 I print_info: rope scaling     = linear
0.00.040.155 I print_info: freq_base_train  = 10000.0
0.00.040.155 I print_info: freq_scale_train = 1
0.00.040.155 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.156 I print_info: rope_finetuned   = unknown
0.00.040.156 I print_info: ssm_d_conv       = 0
0.00.040.156 I print_info: ssm_d_inner      = 0
0.00.040.156 I print_info: ssm_d_state      = 0
0.00.040.156 I print_info: ssm_dt_rank      = 0
0.00.040.156 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.156 I print_info: model type       = 1.4B
0.00.040.157 I print_info: model params     = 1.41 B
0.00.040.157 I print_info: general.name     = 1.4B
0.00.040.157 I print_info: vocab type       = BPE
0.00.040.158 I print_info: n_vocab          = 50304
0.00.040.158 I print_info: n_merges         = 50009
0.00.040.158 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.158 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.158 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.158 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.159 I print_info: LF token         = 187 'Ċ'
0.00.040.159 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.159 I print_info: max token length = 1024
0.00.040.159 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.708.347 I load_tensors: offloading 24 repeating layers to GPU
0.00.708.364 I load_tensors: offloading output layer to GPU
0.00.708.365 I load_tensors: offloaded 25/25 layers to GPU
0.00.708.403 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.708.404 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.709.677 I llama_init_from_model: n_seq_max     = 1
0.00.709.683 I llama_init_from_model: n_ctx         = 128
0.00.709.683 I llama_init_from_model: n_ctx_per_seq = 128
0.00.709.684 I llama_init_from_model: n_batch       = 128
0.00.709.685 I llama_init_from_model: n_ubatch      = 128
0.00.709.685 I llama_init_from_model: flash_attn    = 0
0.00.709.688 I llama_init_from_model: freq_base     = 10000.0
0.00.709.689 I llama_init_from_model: freq_scale    = 1
0.00.709.690 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.709.693 I ggml_metal_init: allocating
0.00.709.738 I ggml_metal_init: found device: Apple M4
0.00.709.747 I ggml_metal_init: picking default device: Apple M4
0.00.711.342 I ggml_metal_init: using embedded metal library
0.00.717.735 I ggml_metal_init: GPU name:   Apple M4
0.00.717.740 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.742 I ggml_metal_init: simdgroup reduction   = true
0.00.717.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.743 I ggml_metal_init: has residency sets    = true
0.00.717.743 I ggml_metal_init: has bfloat            = true
0.00.717.743 I ggml_metal_init: use bfloat            = true
0.00.717.744 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.734.948 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.738.435 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.738.439 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.738.470 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.605 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.741.607 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.741.608 I llama_init_from_model: graph nodes  = 967
0.00.741.608 I llama_init_from_model: graph splits = 2
0.00.741.611 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.741.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.756 I 
0.00.770.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.868 I perplexity: tokenizing the input ..
0.00.778.126 I perplexity: tokenization took 7.254 ms
0.00.778.142 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.913.595 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.914.932 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.914.957 I llama_perf_context_print:        load time =     760.70 ms
0.00.914.958 I llama_perf_context_print: prompt eval time =     134.53 ms /   128 tokens (    1.05 ms per token,   951.45 tokens per second)
0.00.914.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.914.959 I llama_perf_context_print:       total time =     144.21 ms /   129 tokens
0.00.915.362 I ggml_metal_free: deallocating

real	0m0.931s
user	0m0.079s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.109 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.117 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.118 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.118 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.118 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.119 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.120 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.120 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.121 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.121 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.122 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.124 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.875 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.562 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.563 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.564 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.564 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.564 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.565 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.565 I llama_model_loader: - type  f32:  194 tensors
0.00.024.566 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.566 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.566 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.567 I print_info: file format = GGUF V3 (latest)
0.00.024.568 I print_info: file type   = Q2_K - Medium
0.00.024.569 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.771 I load: special tokens cache size = 25
0.00.039.074 I load: token to piece cache size = 0.2984 MB
0.00.039.091 I print_info: arch             = gptneox
0.00.039.092 I print_info: vocab_only       = 0
0.00.039.093 I print_info: n_ctx_train      = 2048
0.00.039.093 I print_info: n_embd           = 2048
0.00.039.093 I print_info: n_layer          = 24
0.00.039.097 I print_info: n_head           = 16
0.00.039.098 I print_info: n_head_kv        = 16
0.00.039.098 I print_info: n_rot            = 32
0.00.039.098 I print_info: n_swa            = 0
0.00.039.099 I print_info: n_embd_head_k    = 128
0.00.039.099 I print_info: n_embd_head_v    = 128
0.00.039.099 I print_info: n_gqa            = 1
0.00.039.100 I print_info: n_embd_k_gqa     = 2048
0.00.039.101 I print_info: n_embd_v_gqa     = 2048
0.00.039.101 I print_info: f_norm_eps       = 1.0e-05
0.00.039.102 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.102 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.102 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.102 I print_info: f_logit_scale    = 0.0e+00
0.00.039.103 I print_info: n_ff             = 8192
0.00.039.104 I print_info: n_expert         = 0
0.00.039.104 I print_info: n_expert_used    = 0
0.00.039.104 I print_info: causal attn      = 1
0.00.039.104 I print_info: pooling type     = 0
0.00.039.104 I print_info: rope type        = 2
0.00.039.104 I print_info: rope scaling     = linear
0.00.039.105 I print_info: freq_base_train  = 10000.0
0.00.039.105 I print_info: freq_scale_train = 1
0.00.039.105 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.105 I print_info: rope_finetuned   = unknown
0.00.039.105 I print_info: ssm_d_conv       = 0
0.00.039.107 I print_info: ssm_d_inner      = 0
0.00.039.107 I print_info: ssm_d_state      = 0
0.00.039.107 I print_info: ssm_dt_rank      = 0
0.00.039.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.109 I print_info: model type       = 1.4B
0.00.039.109 I print_info: model params     = 1.41 B
0.00.039.109 I print_info: general.name     = 1.4B
0.00.039.110 I print_info: vocab type       = BPE
0.00.039.110 I print_info: n_vocab          = 50304
0.00.039.110 I print_info: n_merges         = 50009
0.00.039.111 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.112 I print_info: LF token         = 187 'Ċ'
0.00.039.113 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: max token length = 1024
0.00.039.114 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.341.862 I load_tensors: offloading 24 repeating layers to GPU
0.00.341.878 I load_tensors: offloading output layer to GPU
0.00.341.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.341.918 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.341.919 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.343.631 I llama_init_from_model: n_seq_max     = 1
0.00.343.635 I llama_init_from_model: n_ctx         = 128
0.00.343.636 I llama_init_from_model: n_ctx_per_seq = 128
0.00.343.636 I llama_init_from_model: n_batch       = 128
0.00.343.636 I llama_init_from_model: n_ubatch      = 128
0.00.343.637 I llama_init_from_model: flash_attn    = 0
0.00.343.639 I llama_init_from_model: freq_base     = 10000.0
0.00.343.639 I llama_init_from_model: freq_scale    = 1
0.00.343.640 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.343.643 I ggml_metal_init: allocating
0.00.343.765 I ggml_metal_init: found device: Apple M4
0.00.343.780 I ggml_metal_init: picking default device: Apple M4
0.00.345.635 I ggml_metal_init: using embedded metal library
0.00.351.117 I ggml_metal_init: GPU name:   Apple M4
0.00.351.132 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.134 I ggml_metal_init: simdgroup reduction   = true
0.00.351.135 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.135 I ggml_metal_init: has residency sets    = true
0.00.351.135 I ggml_metal_init: has bfloat            = true
0.00.351.135 I ggml_metal_init: use bfloat            = true
0.00.351.137 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.466 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.376.119 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.376.130 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.376.163 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.379.415 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.379.417 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.379.417 I llama_init_from_model: graph nodes  = 967
0.00.379.418 I llama_init_from_model: graph splits = 2
0.00.379.421 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.379.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.407.779 I 
0.00.407.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.407.874 I perplexity: tokenizing the input ..
0.00.414.820 I perplexity: tokenization took 6.944 ms
0.00.414.825 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.547.370 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.548.703 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.548.725 I llama_perf_context_print:        load time =     398.73 ms
0.00.548.726 I llama_perf_context_print: prompt eval time =     131.65 ms /   128 tokens (    1.03 ms per token,   972.26 tokens per second)
0.00.548.726 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.548.727 I llama_perf_context_print:       total time =     140.95 ms /   129 tokens
0.00.549.073 I ggml_metal_free: deallocating

real	0m0.563s
user	0m0.081s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.882 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.693 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.701 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.702 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.704 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.707 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.709 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.710 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.385 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.087 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.089 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.090 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.091 I llama_model_loader: - type  f32:  194 tensors
0.00.024.091 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.091 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.092 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.092 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.093 I print_info: file format = GGUF V3 (latest)
0.00.024.093 I print_info: file type   = Q3_K - Medium
0.00.024.095 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.884 I load: special tokens cache size = 25
0.00.038.196 I load: token to piece cache size = 0.2984 MB
0.00.038.213 I print_info: arch             = gptneox
0.00.038.213 I print_info: vocab_only       = 0
0.00.038.214 I print_info: n_ctx_train      = 2048
0.00.038.214 I print_info: n_embd           = 2048
0.00.038.214 I print_info: n_layer          = 24
0.00.038.218 I print_info: n_head           = 16
0.00.038.219 I print_info: n_head_kv        = 16
0.00.038.219 I print_info: n_rot            = 32
0.00.038.219 I print_info: n_swa            = 0
0.00.038.219 I print_info: n_embd_head_k    = 128
0.00.038.220 I print_info: n_embd_head_v    = 128
0.00.038.220 I print_info: n_gqa            = 1
0.00.038.221 I print_info: n_embd_k_gqa     = 2048
0.00.038.221 I print_info: n_embd_v_gqa     = 2048
0.00.038.222 I print_info: f_norm_eps       = 1.0e-05
0.00.038.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.222 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.223 I print_info: f_logit_scale    = 0.0e+00
0.00.038.223 I print_info: n_ff             = 8192
0.00.038.223 I print_info: n_expert         = 0
0.00.038.224 I print_info: n_expert_used    = 0
0.00.038.224 I print_info: causal attn      = 1
0.00.038.227 I print_info: pooling type     = 0
0.00.038.227 I print_info: rope type        = 2
0.00.038.227 I print_info: rope scaling     = linear
0.00.038.228 I print_info: freq_base_train  = 10000.0
0.00.038.228 I print_info: freq_scale_train = 1
0.00.038.228 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.228 I print_info: rope_finetuned   = unknown
0.00.038.228 I print_info: ssm_d_conv       = 0
0.00.038.228 I print_info: ssm_d_inner      = 0
0.00.038.229 I print_info: ssm_d_state      = 0
0.00.038.229 I print_info: ssm_dt_rank      = 0
0.00.038.229 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.229 I print_info: model type       = 1.4B
0.00.038.229 I print_info: model params     = 1.41 B
0.00.038.229 I print_info: general.name     = 1.4B
0.00.038.230 I print_info: vocab type       = BPE
0.00.038.230 I print_info: n_vocab          = 50304
0.00.038.230 I print_info: n_merges         = 50009
0.00.038.230 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.231 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.231 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.231 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.231 I print_info: LF token         = 187 'Ċ'
0.00.038.231 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.231 I print_info: max token length = 1024
0.00.038.232 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.801 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.816 I load_tensors: offloading output layer to GPU
0.00.435.817 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.849 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.851 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.455 I llama_init_from_model: n_seq_max     = 1
0.00.437.458 I llama_init_from_model: n_ctx         = 128
0.00.437.458 I llama_init_from_model: n_ctx_per_seq = 128
0.00.437.459 I llama_init_from_model: n_batch       = 128
0.00.437.459 I llama_init_from_model: n_ubatch      = 128
0.00.437.459 I llama_init_from_model: flash_attn    = 0
0.00.437.462 I llama_init_from_model: freq_base     = 10000.0
0.00.437.462 I llama_init_from_model: freq_scale    = 1
0.00.437.463 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.472 I ggml_metal_init: allocating
0.00.437.554 I ggml_metal_init: found device: Apple M4
0.00.437.568 I ggml_metal_init: picking default device: Apple M4
0.00.439.414 I ggml_metal_init: using embedded metal library
0.00.445.382 I ggml_metal_init: GPU name:   Apple M4
0.00.445.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.398 I ggml_metal_init: simdgroup reduction   = true
0.00.445.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.399 I ggml_metal_init: has residency sets    = true
0.00.445.399 I ggml_metal_init: has bfloat            = true
0.00.445.399 I ggml_metal_init: use bfloat            = true
0.00.445.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.372 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.468.888 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.468.892 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.468.919 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.472.418 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.472.420 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.472.421 I llama_init_from_model: graph nodes  = 967
0.00.472.421 I llama_init_from_model: graph splits = 2
0.00.472.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.472.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.636 I 
0.00.502.740 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.748 I perplexity: tokenizing the input ..
0.00.509.758 I perplexity: tokenization took 7.006 ms
0.00.509.766 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.263 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.654.609 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.654.637 I llama_perf_context_print:        load time =     493.75 ms
0.00.654.638 I llama_perf_context_print: prompt eval time =     142.55 ms /   128 tokens (    1.11 ms per token,   897.93 tokens per second)
0.00.654.639 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.654.640 I llama_perf_context_print:       total time =     152.00 ms /   129 tokens
0.00.655.023 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.079s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.783 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.776 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.776 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.777 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.779 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.575 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.629 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.411 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.413 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.413 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.414 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.414 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.414 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.415 I llama_model_loader: - type  f32:  194 tensors
0.00.024.415 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.415 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.416 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.416 I print_info: file format = GGUF V3 (latest)
0.00.024.417 I print_info: file type   = Q4_K - Medium
0.00.024.418 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.657 I load: special tokens cache size = 25
0.00.039.018 I load: token to piece cache size = 0.2984 MB
0.00.039.035 I print_info: arch             = gptneox
0.00.039.036 I print_info: vocab_only       = 0
0.00.039.036 I print_info: n_ctx_train      = 2048
0.00.039.036 I print_info: n_embd           = 2048
0.00.039.037 I print_info: n_layer          = 24
0.00.039.041 I print_info: n_head           = 16
0.00.039.041 I print_info: n_head_kv        = 16
0.00.039.042 I print_info: n_rot            = 32
0.00.039.042 I print_info: n_swa            = 0
0.00.039.042 I print_info: n_embd_head_k    = 128
0.00.039.042 I print_info: n_embd_head_v    = 128
0.00.039.043 I print_info: n_gqa            = 1
0.00.039.043 I print_info: n_embd_k_gqa     = 2048
0.00.039.044 I print_info: n_embd_v_gqa     = 2048
0.00.039.044 I print_info: f_norm_eps       = 1.0e-05
0.00.039.045 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.045 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.045 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.045 I print_info: f_logit_scale    = 0.0e+00
0.00.039.046 I print_info: n_ff             = 8192
0.00.039.046 I print_info: n_expert         = 0
0.00.039.046 I print_info: n_expert_used    = 0
0.00.039.046 I print_info: causal attn      = 1
0.00.039.046 I print_info: pooling type     = 0
0.00.039.046 I print_info: rope type        = 2
0.00.039.046 I print_info: rope scaling     = linear
0.00.039.047 I print_info: freq_base_train  = 10000.0
0.00.039.047 I print_info: freq_scale_train = 1
0.00.039.047 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.047 I print_info: rope_finetuned   = unknown
0.00.039.047 I print_info: ssm_d_conv       = 0
0.00.039.047 I print_info: ssm_d_inner      = 0
0.00.039.048 I print_info: ssm_d_state      = 0
0.00.039.048 I print_info: ssm_dt_rank      = 0
0.00.039.048 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.048 I print_info: model type       = 1.4B
0.00.039.048 I print_info: model params     = 1.41 B
0.00.039.048 I print_info: general.name     = 1.4B
0.00.039.049 I print_info: vocab type       = BPE
0.00.039.049 I print_info: n_vocab          = 50304
0.00.039.049 I print_info: n_merges         = 50009
0.00.039.049 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: LF token         = 187 'Ċ'
0.00.039.050 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.051 I print_info: max token length = 1024
0.00.039.051 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.535.928 I load_tensors: offloading 24 repeating layers to GPU
0.00.535.945 I load_tensors: offloading output layer to GPU
0.00.535.946 I load_tensors: offloaded 25/25 layers to GPU
0.00.535.981 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.535.983 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.537.532 I llama_init_from_model: n_seq_max     = 1
0.00.537.535 I llama_init_from_model: n_ctx         = 128
0.00.537.536 I llama_init_from_model: n_ctx_per_seq = 128
0.00.537.536 I llama_init_from_model: n_batch       = 128
0.00.537.537 I llama_init_from_model: n_ubatch      = 128
0.00.537.537 I llama_init_from_model: flash_attn    = 0
0.00.537.539 I llama_init_from_model: freq_base     = 10000.0
0.00.537.540 I llama_init_from_model: freq_scale    = 1
0.00.537.540 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.537.543 I ggml_metal_init: allocating
0.00.537.624 I ggml_metal_init: found device: Apple M4
0.00.537.637 I ggml_metal_init: picking default device: Apple M4
0.00.539.486 I ggml_metal_init: using embedded metal library
0.00.546.407 I ggml_metal_init: GPU name:   Apple M4
0.00.546.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.546.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.546.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.546.417 I ggml_metal_init: simdgroup reduction   = true
0.00.546.418 I ggml_metal_init: simdgroup matrix mul. = true
0.00.546.418 I ggml_metal_init: has residency sets    = true
0.00.546.418 I ggml_metal_init: has bfloat            = true
0.00.546.418 I ggml_metal_init: use bfloat            = true
0.00.546.420 I ggml_metal_init: hasUnifiedMemory      = true
0.00.546.426 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.564.375 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.567.841 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.567.847 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.567.883 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.571.081 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.571.083 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.571.083 I llama_init_from_model: graph nodes  = 967
0.00.571.084 I llama_init_from_model: graph splits = 2
0.00.571.086 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.571.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.089 I 
0.00.602.177 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.183 I perplexity: tokenizing the input ..
0.00.609.588 I perplexity: tokenization took 7.401 ms
0.00.609.601 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.308 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.757.660 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.757.684 I llama_perf_context_print:        load time =     593.30 ms
0.00.757.685 I llama_perf_context_print: prompt eval time =     145.81 ms /   128 tokens (    1.14 ms per token,   877.88 tokens per second)
0.00.757.686 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.757.686 I llama_perf_context_print:       total time =     155.60 ms /   129 tokens
0.00.758.071 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.080s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.018 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.969 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.977 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.978 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.980 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.983 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.984 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.984 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.984 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.985 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.987 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.362 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.362 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.363 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.363 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.363 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.364 I llama_model_loader: - type  f32:  194 tensors
0.00.025.364 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.365 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.365 I print_info: file format = GGUF V3 (latest)
0.00.025.366 I print_info: file type   = Q5_K - Medium
0.00.025.368 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.438 I load: special tokens cache size = 25
0.00.039.753 I load: token to piece cache size = 0.2984 MB
0.00.039.770 I print_info: arch             = gptneox
0.00.039.770 I print_info: vocab_only       = 0
0.00.039.771 I print_info: n_ctx_train      = 2048
0.00.039.771 I print_info: n_embd           = 2048
0.00.039.771 I print_info: n_layer          = 24
0.00.039.775 I print_info: n_head           = 16
0.00.039.775 I print_info: n_head_kv        = 16
0.00.039.776 I print_info: n_rot            = 32
0.00.039.776 I print_info: n_swa            = 0
0.00.039.776 I print_info: n_embd_head_k    = 128
0.00.039.776 I print_info: n_embd_head_v    = 128
0.00.039.777 I print_info: n_gqa            = 1
0.00.039.777 I print_info: n_embd_k_gqa     = 2048
0.00.039.778 I print_info: n_embd_v_gqa     = 2048
0.00.039.778 I print_info: f_norm_eps       = 1.0e-05
0.00.039.779 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.779 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.779 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.779 I print_info: f_logit_scale    = 0.0e+00
0.00.039.780 I print_info: n_ff             = 8192
0.00.039.780 I print_info: n_expert         = 0
0.00.039.780 I print_info: n_expert_used    = 0
0.00.039.780 I print_info: causal attn      = 1
0.00.039.780 I print_info: pooling type     = 0
0.00.039.781 I print_info: rope type        = 2
0.00.039.781 I print_info: rope scaling     = linear
0.00.039.781 I print_info: freq_base_train  = 10000.0
0.00.039.781 I print_info: freq_scale_train = 1
0.00.039.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.784 I print_info: rope_finetuned   = unknown
0.00.039.784 I print_info: ssm_d_conv       = 0
0.00.039.785 I print_info: ssm_d_inner      = 0
0.00.039.785 I print_info: ssm_d_state      = 0
0.00.039.785 I print_info: ssm_dt_rank      = 0
0.00.039.786 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.786 I print_info: model type       = 1.4B
0.00.039.786 I print_info: model params     = 1.41 B
0.00.039.786 I print_info: general.name     = 1.4B
0.00.039.787 I print_info: vocab type       = BPE
0.00.039.789 I print_info: n_vocab          = 50304
0.00.039.789 I print_info: n_merges         = 50009
0.00.039.789 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.789 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: LF token         = 187 'Ċ'
0.00.039.790 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: max token length = 1024
0.00.039.791 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.437 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.455 I load_tensors: offloading output layer to GPU
0.00.591.455 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.490 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.492 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.593.269 I llama_init_from_model: n_seq_max     = 1
0.00.593.273 I llama_init_from_model: n_ctx         = 128
0.00.593.274 I llama_init_from_model: n_ctx_per_seq = 128
0.00.593.274 I llama_init_from_model: n_batch       = 128
0.00.593.275 I llama_init_from_model: n_ubatch      = 128
0.00.593.275 I llama_init_from_model: flash_attn    = 0
0.00.593.277 I llama_init_from_model: freq_base     = 10000.0
0.00.593.278 I llama_init_from_model: freq_scale    = 1
0.00.593.279 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.593.281 I ggml_metal_init: allocating
0.00.593.337 I ggml_metal_init: found device: Apple M4
0.00.593.351 I ggml_metal_init: picking default device: Apple M4
0.00.594.960 I ggml_metal_init: using embedded metal library
0.00.601.509 I ggml_metal_init: GPU name:   Apple M4
0.00.601.513 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.514 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.515 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.516 I ggml_metal_init: simdgroup reduction   = true
0.00.601.516 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.516 I ggml_metal_init: has residency sets    = true
0.00.601.516 I ggml_metal_init: has bfloat            = true
0.00.601.517 I ggml_metal_init: use bfloat            = true
0.00.601.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.669 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.622.195 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.622.200 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.622.226 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.607 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.625.609 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.625.610 I llama_init_from_model: graph nodes  = 967
0.00.625.610 I llama_init_from_model: graph splits = 2
0.00.625.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.056 I 
0.00.662.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.162 I perplexity: tokenizing the input ..
0.00.669.272 I perplexity: tokenization took 7.107 ms
0.00.669.278 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.812 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.820.149 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.820.175 I llama_perf_context_print:        load time =     652.03 ms
0.00.820.177 I llama_perf_context_print: prompt eval time =     148.63 ms /   128 tokens (    1.16 ms per token,   861.17 tokens per second)
0.00.820.177 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.178 I llama_perf_context_print:       total time =     158.12 ms /   129 tokens
0.00.820.550 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.078s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.943 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.509 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.511 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.513 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.515 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.516 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.519 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.141 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.788 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.789 I llama_model_loader: - type  f32:  194 tensors
0.00.023.789 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.790 I print_info: file format = GGUF V3 (latest)
0.00.023.790 I print_info: file type   = Q6_K
0.00.023.792 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.666 I load: special tokens cache size = 25
0.00.037.945 I load: token to piece cache size = 0.2984 MB
0.00.037.970 I print_info: arch             = gptneox
0.00.037.971 I print_info: vocab_only       = 0
0.00.037.972 I print_info: n_ctx_train      = 2048
0.00.037.972 I print_info: n_embd           = 2048
0.00.037.972 I print_info: n_layer          = 24
0.00.037.976 I print_info: n_head           = 16
0.00.037.981 I print_info: n_head_kv        = 16
0.00.037.981 I print_info: n_rot            = 32
0.00.037.981 I print_info: n_swa            = 0
0.00.037.982 I print_info: n_embd_head_k    = 128
0.00.037.982 I print_info: n_embd_head_v    = 128
0.00.037.983 I print_info: n_gqa            = 1
0.00.037.983 I print_info: n_embd_k_gqa     = 2048
0.00.037.984 I print_info: n_embd_v_gqa     = 2048
0.00.037.986 I print_info: f_norm_eps       = 1.0e-05
0.00.037.986 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.986 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.986 I print_info: f_logit_scale    = 0.0e+00
0.00.037.987 I print_info: n_ff             = 8192
0.00.037.987 I print_info: n_expert         = 0
0.00.037.987 I print_info: n_expert_used    = 0
0.00.037.987 I print_info: causal attn      = 1
0.00.037.988 I print_info: pooling type     = 0
0.00.037.988 I print_info: rope type        = 2
0.00.038.013 I print_info: rope scaling     = linear
0.00.038.016 I print_info: freq_base_train  = 10000.0
0.00.038.016 I print_info: freq_scale_train = 1
0.00.038.016 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.018 I print_info: rope_finetuned   = unknown
0.00.038.018 I print_info: ssm_d_conv       = 0
0.00.038.018 I print_info: ssm_d_inner      = 0
0.00.038.018 I print_info: ssm_d_state      = 0
0.00.038.018 I print_info: ssm_dt_rank      = 0
0.00.038.018 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.019 I print_info: model type       = 1.4B
0.00.038.019 I print_info: model params     = 1.41 B
0.00.038.019 I print_info: general.name     = 1.4B
0.00.038.020 I print_info: vocab type       = BPE
0.00.038.020 I print_info: n_vocab          = 50304
0.00.038.020 I print_info: n_merges         = 50009
0.00.038.021 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.021 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.021 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.021 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.023 I print_info: LF token         = 187 'Ċ'
0.00.038.023 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.023 I print_info: max token length = 1024
0.00.038.024 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.602.858 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.864 I load_tensors: offloading output layer to GPU
0.00.602.865 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.891 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.602.894 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.604.476 I llama_init_from_model: n_seq_max     = 1
0.00.604.478 I llama_init_from_model: n_ctx         = 128
0.00.604.479 I llama_init_from_model: n_ctx_per_seq = 128
0.00.604.479 I llama_init_from_model: n_batch       = 128
0.00.604.479 I llama_init_from_model: n_ubatch      = 128
0.00.604.480 I llama_init_from_model: flash_attn    = 0
0.00.604.481 I llama_init_from_model: freq_base     = 10000.0
0.00.604.481 I llama_init_from_model: freq_scale    = 1
0.00.604.482 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.483 I ggml_metal_init: allocating
0.00.604.538 I ggml_metal_init: found device: Apple M4
0.00.604.549 I ggml_metal_init: picking default device: Apple M4
0.00.606.093 I ggml_metal_init: using embedded metal library
0.00.612.123 I ggml_metal_init: GPU name:   Apple M4
0.00.612.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.129 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.129 I ggml_metal_init: simdgroup reduction   = true
0.00.612.130 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.130 I ggml_metal_init: has residency sets    = true
0.00.612.130 I ggml_metal_init: has bfloat            = true
0.00.612.130 I ggml_metal_init: use bfloat            = true
0.00.612.132 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.347 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.785 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.788 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.819 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.010 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.636.012 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.636.013 I llama_init_from_model: graph nodes  = 967
0.00.636.013 I llama_init_from_model: graph splits = 2
0.00.636.016 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.636.016 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.873 I 
0.00.667.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.967 I perplexity: tokenizing the input ..
0.00.674.903 I perplexity: tokenization took 6.933 ms
0.00.674.909 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.293 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.807.627 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.807.651 I llama_perf_context_print:        load time =     658.92 ms
0.00.807.655 I llama_perf_context_print: prompt eval time =     130.57 ms /   128 tokens (    1.02 ms per token,   980.28 tokens per second)
0.00.807.658 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.658 I llama_perf_context_print:       total time =     139.78 ms /   129 tokens
0.00.808.031 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.077s
sys	0m0.132s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.398 I build: 4869 (2c9f833d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.223 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.141 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.149 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.153 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.157 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.158 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.158 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.159 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.159 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.160 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.162 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.341 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.342 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.343 I llama_model_loader: - type  f32:  194 tensors
0.00.054.343 I llama_model_loader: - type  f16:   98 tensors
0.00.054.344 I print_info: file format = GGUF V3 (latest)
0.00.054.350 I print_info: file type   = all F32 (guessed)
0.00.054.352 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.334 I load: special tokens cache size = 25
0.00.074.950 I load: token to piece cache size = 0.2984 MB
0.00.074.970 I print_info: arch             = gptneox
0.00.074.971 I print_info: vocab_only       = 0
0.00.074.971 I print_info: n_ctx_train      = 2048
0.00.074.972 I print_info: n_embd           = 2048
0.00.074.972 I print_info: n_layer          = 24
0.00.074.977 I print_info: n_head           = 16
0.00.074.977 I print_info: n_head_kv        = 16
0.00.074.977 I print_info: n_rot            = 32
0.00.074.978 I print_info: n_swa            = 0
0.00.074.978 I print_info: n_embd_head_k    = 128
0.00.074.978 I print_info: n_embd_head_v    = 128
0.00.074.979 I print_info: n_gqa            = 1
0.00.074.979 I print_info: n_embd_k_gqa     = 2048
0.00.075.001 I print_info: n_embd_v_gqa     = 2048
0.00.075.002 I print_info: f_norm_eps       = 1.0e-05
0.00.075.003 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.003 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.003 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.003 I print_info: f_logit_scale    = 0.0e+00
0.00.075.005 I print_info: n_ff             = 8192
0.00.075.006 I print_info: n_expert         = 0
0.00.075.006 I print_info: n_expert_used    = 0
0.00.075.006 I print_info: causal attn      = 1
0.00.075.006 I print_info: pooling type     = 0
0.00.075.006 I print_info: rope type        = 2
0.00.075.007 I print_info: rope scaling     = linear
0.00.075.007 I print_info: freq_base_train  = 10000.0
0.00.075.007 I print_info: freq_scale_train = 1
0.00.075.007 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.008 I print_info: rope_finetuned   = unknown
0.00.075.008 I print_info: ssm_d_conv       = 0
0.00.075.008 I print_info: ssm_d_inner      = 0
0.00.075.008 I print_info: ssm_d_state      = 0
0.00.075.008 I print_info: ssm_dt_rank      = 0
0.00.075.008 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.009 I print_info: model type       = 1.4B
0.00.075.009 I print_info: model params     = 1.41 B
0.00.075.009 I print_info: general.name     = 1.4B
0.00.075.010 I print_info: vocab type       = BPE
0.00.075.010 I print_info: n_vocab          = 50304
0.00.075.010 I print_info: n_merges         = 50009
0.00.075.011 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.011 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.011 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.011 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.011 I print_info: LF token         = 187 'Ċ'
0.00.075.012 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.012 I print_info: max token length = 1024
0.00.075.012 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.334.307 I load_tensors: offloading 24 repeating layers to GPU
0.01.334.310 I load_tensors: offloading output layer to GPU
0.01.334.311 I load_tensors: offloaded 25/25 layers to GPU
0.01.334.331 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.334.332 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.334.817 I llama_init_from_model: n_seq_max     = 1
0.01.334.818 I llama_init_from_model: n_ctx         = 128
0.01.334.818 I llama_init_from_model: n_ctx_per_seq = 128
0.01.334.818 I llama_init_from_model: n_batch       = 128
0.01.334.818 I llama_init_from_model: n_ubatch      = 128
0.01.334.818 I llama_init_from_model: flash_attn    = 0
0.01.334.819 I llama_init_from_model: freq_base     = 10000.0
0.01.334.819 I llama_init_from_model: freq_scale    = 1
0.01.334.820 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.334.822 I ggml_metal_init: allocating
0.01.334.866 I ggml_metal_init: found device: Apple M4
0.01.334.872 I ggml_metal_init: picking default device: Apple M4
0.01.335.637 I ggml_metal_init: using embedded metal library
0.01.338.108 I ggml_metal_init: GPU name:   Apple M4
0.01.338.110 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.338.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.338.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.338.111 I ggml_metal_init: simdgroup reduction   = true
0.01.338.111 I ggml_metal_init: simdgroup matrix mul. = true
0.01.338.111 I ggml_metal_init: has residency sets    = true
0.01.338.112 I ggml_metal_init: has bfloat            = true
0.01.338.112 I ggml_metal_init: use bfloat            = true
0.01.338.112 I ggml_metal_init: hasUnifiedMemory      = true
0.01.338.113 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.347.886 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.349.586 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.349.589 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.349.605 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.351.131 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.351.132 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.351.132 I llama_init_from_model: graph nodes  = 967
0.01.351.133 I llama_init_from_model: graph splits = 2
0.01.351.134 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.351.134 I 
0.01.351.177 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.351.179 I compute_imatrix: tokenizing the input ..
0.01.355.069 I compute_imatrix: tokenization took 3.89 ms
0.01.355.072 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.621.382 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.624.397 I llama_perf_context_print:        load time =    1597.16 ms
0.01.624.398 I llama_perf_context_print: prompt eval time =     264.42 ms /   128 tokens (    2.07 ms per token,   484.08 tokens per second)
0.01.624.398 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.624.399 I llama_perf_context_print:       total time =    1600.17 ms /   129 tokens
0.01.624.955 I ggml_metal_free: deallocating

real	0m1.839s
user	0m0.131s
sys	0m0.202s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4869 (2c9f833d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x114804a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x114805160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x114805710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114805cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x114806270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x114806820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x114806dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x114807380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x114807930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x114807e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x114808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x114808830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x114809350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x114809b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11480a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11480aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11480b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11480b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11480bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11480c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11480ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11480d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11480dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11480e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11480ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11480f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11480f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11480fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x114810100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1148105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x114810860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x114810f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x114811210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1148116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114811b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x114811ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114812490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x114812930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x114812dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x114813270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114813710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x114813bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114814050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1148144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1148147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x114814cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1148151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x114815bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x114816070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x114816510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1148169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x114816e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1148172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x114817790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x114817c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1148180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x114818570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x114818a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x114818f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x114819400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1148196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x114819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11481a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11481a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11481a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11481ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11481b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11481b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11481bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11481c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11481c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11481c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11481ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11481d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11481d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11481de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11481e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11481e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11481ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11481f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11481f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11481fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114820360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1148208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114820e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114821350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1148218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114821df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x114822340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x114822890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114822de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114823330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114823880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x114823dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114824320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x114824870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x114824dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1148156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114825230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1148259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x114825f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x114826480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1148269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x114826f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x114827470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1148279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x114827f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x114828460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1148289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x114828f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x114829450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1148299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x114829ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11482a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11482a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11482acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11482b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11482b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11482bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11482bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11482c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11482c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11482cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11482d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11482d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11482db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11482dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11482e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11482e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11482ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11482f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11482f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11482fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114830010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1148304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114830950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114830df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114831730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x114831bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x114832070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x114832510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1148329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x114832e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1148332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x114833790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x114833c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1148340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x114834570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x114834a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x114834eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x114835350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1148357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x114835c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x114836130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1148365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x114836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x114836f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1148373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x114837850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x114837cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x114838190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x114838630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x114838ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x114838f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x114839410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1148398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x114839d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11483a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11483a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11483ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11483afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11483b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11483b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11483bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11483c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11483c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11483cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11483d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11483d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11483d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11483de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11483e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11483e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11483ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11483f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11483f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11483f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11483fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x114840310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1148407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114840c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1148410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x114841640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x114841b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1148420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x114842630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x114842ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x114842f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x114843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1148438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x114843d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1148441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x114844740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x114844be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x114845080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x114845520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1148459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114845e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x114846300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x114846b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1148470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x114847360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1148478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114847e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114848440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1148489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114848fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114849550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x114849b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11484a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11484a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11484ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11484b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11484b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11484bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11484c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11484c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11484ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11484d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11484d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11484df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11484e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11484eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11484f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11484f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11484fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114850160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114850710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114850cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114851270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x114851820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114851dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x114852380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x114852930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114852ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114853490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x114853a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x114853ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1148545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x114854b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114855100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1148556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x114855c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x114856210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1148567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x114856d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x114857320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1148578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x114857e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x114858430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1148589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114858f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114859540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x114859af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11485a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11485a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11485ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11485b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11485b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11485bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11485c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11485c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11485ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11485cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11485d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11485d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11485de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11485e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11485e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11485ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11485f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11485f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11485fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x114860100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x114860600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x114860b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x114861000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x114861500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x114861a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x114861f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x114862400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114862900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114863310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114863a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114864150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114864870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114864b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1148652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114865760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114865c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.663.287 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.663.291 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x114e04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x114e05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x114e056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114e05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x114e05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x114e06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x114e06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x114e06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x114e07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x114e075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x114e07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x114e08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x114e08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x114e093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x114e09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x114e0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x114e0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x114e0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x114e0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x114e0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x114e0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x114e0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x114e0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x114e0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x114e0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x114e0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x114e0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x114e0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x114e0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x114e0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x114e0fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x114e100f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x114e107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x114e10c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114e11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x114e115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114e11a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x114e11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x114e12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x114e12830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114e12cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x114e13170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114e13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x114e13ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x114e13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x114e143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x114e14890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x114e14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x114e151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x114e15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x114e15b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x114e15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x114e16450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x114e168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x114e16d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x114e17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x114e176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x114e17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x114e17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x114e180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x114e18530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x114e189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x114e18e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x114e19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x114e196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x114e19b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x114e19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x114e1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x114e1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x114e1ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x114e1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x114e1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x114e1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x114e1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x114e1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x114e1c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x114e1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x114e1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x114e1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x114e1d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x114e1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x114e1e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114e1e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x114e1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114e1efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114e1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x114e1f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114e1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x114e20170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x114e205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114e20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114e20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114e21330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x114e217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114e21c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x114e22080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x114e224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x114e22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114e22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x114e23240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x114e236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x114e23b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x114e23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x114e24400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x114e24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x114e24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x114e25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x114e255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x114e25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x114e25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x114e26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x114e26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x114e26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x114e27060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x114e274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x114e27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x114e27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x114e28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x114e28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114e28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114e28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x114e293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114e29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114e29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x114e2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x114e2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x114e2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x114e2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x114e2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x114e2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x114e2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x114e2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x114e2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114e2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x114e2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114e2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114e2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114e2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114e2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x114e2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x114e2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x114e2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x114e2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x114e2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x114e2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x114e2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x114e302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x114e30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x114e30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x114e31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x114e31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x114e31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x114e31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x114e321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x114e32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x114e32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x114e32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x114e333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x114e33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x114e33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x114e340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x114e34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x114e349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x114e34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x114e352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x114e35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x114e35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x114e36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x114e365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x114e36a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x114e36eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x114e37320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x114e37790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x114e37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x114e38070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114e384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114e38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x114e38dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114e39230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114e396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x114e39b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x114e39f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114e3a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114e3a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x114e3acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x114e3b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x114e3b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x114e3ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x114e3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x114e3c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x114e3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114e3cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x114e3d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x114e3d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x114e3d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x114e3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x114e3e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x114e3e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x114e3eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x114e3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x114e3f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x114e3f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x114e40010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x114e402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x114e40880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x114e40d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x114e41460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x114e41900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114e41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x114e42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x114e42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x114e42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x114e43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x114e438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114e43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114e44410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x114e449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114e44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114e45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x114e45ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x114e46080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114e46630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x114e46be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x114e47190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x114e47740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x114e47cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x114e482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114e48850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x114e48e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x114e493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x114e49960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114e49f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x114e4a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114e4aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114e4b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x114e4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114e4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114e4c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114e4c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114e4cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114e4d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x114e4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114e4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x114e4e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x114e4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114e4eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114e4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x114e4fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x114e4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x114e50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x114e50b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114e510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x114e51680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x114e51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x114e521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x114e52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x114e52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x114e532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x114e538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x114e53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x114e54400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x114e549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114e54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114e55510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x114e55ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114e56070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x114e56620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x114e56bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x114e570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114e575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114e57ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x114e57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114e584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114e589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114e58ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114e593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114e598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114e59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x114e5a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114e5a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x114e5acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x114e5b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x114e5b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x114e5bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x114e5c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x114e5c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x114e5cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x114e5cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x114e5d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x114e5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x114e5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x114e5e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114e5e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114e5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114e5fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114e60120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114e60840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114e60b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x114e61290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114e61730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114e61bd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e07710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e07b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e07ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e08460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e08d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e09620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e0aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e0bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e0c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e0cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e0d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e0dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e0f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e0fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e10cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e11160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e11aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e12880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e13920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e13dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e15040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e15e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e18c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e19a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e1a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e1b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e1ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e1bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e1c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e1cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e1d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e1d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e1d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e1de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e1e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e1e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e1eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e1f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e20600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e20a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e20ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e22510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e22980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e23b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e24420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e24890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e24d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e25890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e25d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e26320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e26e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e27430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e27f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e28af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e2a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e2bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e2c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e2c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e2d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e2d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e30c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e31b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e32510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e32a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e33910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e33e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e37010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e3b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e3d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e3e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e41f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e42410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e42e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e43310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e43810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e44e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e45dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e4a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e4a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e4b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e4bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e4c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e4c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e4cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e4d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e4e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e4eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e4f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e50d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e51e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e52410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e52f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e53ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e54080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e54630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e54be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e55740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e55cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e56850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e56e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e57f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e59020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e5a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e5a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e5ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e5b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e5bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e5c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e5c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e5ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e5d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e5da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e5df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e5e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e5ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e5f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e5fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e60210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e60710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e60c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e61110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e61610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e62010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x128e62510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x128e62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x128e62f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x128e63410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x128e63910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x128e63e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x128e64310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x128e64810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x128e64d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x128e65210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e65710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e66120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e66f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e67940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e68570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e68a10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.738s
user	0m0.267s
sys	0m0.319s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4869 (2c9f833d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13160c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13160d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13160d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13160dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13160e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13160e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13160ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13160f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13160f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13160fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1316102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1316107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1316112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131612290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1316129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1316130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1316137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131613f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1316146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131614e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131615520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131615c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1316164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1316170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131617540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131618080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131618520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1316187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131618ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131619190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131619630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131619ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13161a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13161a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13161ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13161b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13161b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13161bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13161bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13161c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13161c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13161cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13161d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13161db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13161dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13161e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13161e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13161edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13161f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13161f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13161fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131620050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1316204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131620990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131620ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131621380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131621640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131621ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131621f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131622420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1316228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131623200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1316236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131623b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131623fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131624480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131624920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131624dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131625310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131625860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131625db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131626300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131626850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131626da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1316272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131627840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131627d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1316282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131628830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131628d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1316292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131629820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131629d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13162a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13162a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13162ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13162b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13162b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13162bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13162c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13162c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13162cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13161d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13162d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13162d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13162deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13162e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13162e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13162eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13162f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13162f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13162fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1316303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131630930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131630e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1316313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131631920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131631e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131632310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1316327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131632c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1316330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131633590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131633a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131633ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131634370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131634810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131634cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131635150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1316355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131635a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131635f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1316363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131636870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131636d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1316371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131637650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131637af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131637f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131638430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1316388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131638d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131639210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1316396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131639b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131639ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13163a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13163a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13163add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13163b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13163b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13163bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13163c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13163c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13163c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13163ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13163d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13163d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13163dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13163e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13163e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13163e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13163ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13163f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13163f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13163fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131640110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1316405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131640a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131640ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131641390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131641830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131641cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131642170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131642610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131642ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131642f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1316433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131643890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131643d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1316441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131644670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131644b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131644fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131645450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1316458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131645d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131646230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1316466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131646b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131647010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1316474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131647950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131647df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131648290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131648730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131648bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131649070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1316495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131649b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13164a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13164a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13164aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13164aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13164b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13164b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13164bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13164c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13164c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13164cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13164d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13164d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13164d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13164dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13164e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13164ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13164f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13164f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13164f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13164fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1316503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131650970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131650f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1316514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131651a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131652030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1316525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131652b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131653140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1316536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131653ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131654250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131654800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131655360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131655910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131656470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131656a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131656fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131657580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131657b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1316580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131658690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131658c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1316591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1316597a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131659d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13165a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13165a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13165ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13165b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13165b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13165bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13165c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13165cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13165d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13165d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13165dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13165e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13165e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13165ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13165f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13165f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13165fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1316603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131660960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131660f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1316614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131661a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131662020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1316625d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131662b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131663080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131663580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131663a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131663f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131664480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131664980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131664e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131665380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131665880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131665d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131666280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131666780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131666c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131667180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x131667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x131667b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x131668080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x131668580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x131668a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x131668f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x131669480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x131669980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x131669e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13166a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13166a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13166b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13166b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13166c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13166c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13166cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13166d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13166d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13166db80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.159.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.159.917 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131707f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1317083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131708810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131708c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1317090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131709560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1317099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131709e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13170a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13170a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13170ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13170b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13170bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13170c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13170cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13170d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13170db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13170e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13170e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13170f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13170f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13170ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1317106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131710e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131711530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1317117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131711ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131711f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131712390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131712800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131712dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1317132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1317139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131713e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1317142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131714790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131714c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1317150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131715570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131715a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131715eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131716350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1317167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131716c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131717130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1317175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131717a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131717f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1317183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131718850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131718cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131719190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131719630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131719ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131719f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13171a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13171a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13171ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13171ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13171b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13171b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13171bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13171bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13171c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13171c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13171cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13171d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13171d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13171da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13171df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13171e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13171e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13171ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13171f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13171f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13171f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13171fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131720280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1317206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131720b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131720fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131721440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1317218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131721d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131722190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131722600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131722a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131722ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131723350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1317237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131723c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1317240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131724510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131724980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131724df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131725260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1317256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131725b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131725fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131726420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131726890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131726d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131727170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1317275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131727a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131727ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131728330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1317287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131728c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131729080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1317294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131729960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131729dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13172a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13172a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13172ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13172af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13172b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13172b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13172bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13172c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13172c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13172ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13172cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13172d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13172d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13172dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13172e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13172e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13172e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13172edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13172f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13172f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13172fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13172ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1317303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131730850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131730cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131731130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1317315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131731a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131731e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1317322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131732760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131732bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131733040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1317334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131733920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131733d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131734200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131734670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131734ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131734f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1317353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131735830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131735ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131736110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131736580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1317369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131736e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1317372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131737740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131737bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131738020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131738490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131738900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131739080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131739340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1317397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131739c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13173a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13173a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13173a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13173ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13173b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13173b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13173bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13173bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13173c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13173c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13173ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13173d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13173d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13173da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13173deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13173e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13173e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13173ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13173f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13173f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13173f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13173fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131740230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1317406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131740b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131740f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1317413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131741860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131741cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1317425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131742a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1317431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1317434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131743a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131743f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131744640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131744ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131744f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131745420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131745c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1317464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131746a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131747040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1317475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131747ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131748150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131748700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131748cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131749260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131749810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131749dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13174a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13174a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13174aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13174b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13174ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13174bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13174c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13174cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13174d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13174d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13174dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13174e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13174e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13174ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13174f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13174f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13174fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131750420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1317509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131750f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131751530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131751ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131752090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131752640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131752bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1317531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131753750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131753d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1317542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131754860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131754e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1317553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131755970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131755f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1317564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131756a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131757030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1317575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131757b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131758140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1317586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131758ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131759250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131759800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131759db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13175a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13175a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13175acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13175b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13175b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13175bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13175c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13175c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13175cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13175cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13175d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13175d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13175deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13175e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13175e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13175edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13175f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13175f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13175fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1317601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1317606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x131760bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1317610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1317615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131761ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1317624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131762be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131763300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131763a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131763ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131764470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131764910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131764db0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1047044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1047056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1047063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1047078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1047083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x104708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x104709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10470a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10470a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10470b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10470b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10470bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10470c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10470cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10470d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10470db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10470de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10470e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10470e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10470e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10470ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10470f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10470f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10470fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104710490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104710930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104710dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104711270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104711710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104711bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104712050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1047124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104712990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104712e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1047132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104713770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104713c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1047140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104714550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1047149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104714e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104715330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1047157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104715c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104716110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1047165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104716a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104716ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1047171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104717470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1047178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104717d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1047181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104718630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104718aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104718f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104719380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1047197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104719c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10471a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10471a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10471a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10471ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10471b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10471b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10471bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10471bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10471c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10471c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10471cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10471d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10471d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10471da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10471def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10471e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10471e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10471ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10471f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10471f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10471f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10471fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104720270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1047206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104720b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104720fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104721430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1047218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104721d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104722180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1047228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104722d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104723330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1047238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104723e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1047249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104724fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104725550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x104725b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1047260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x104726660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104726c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1047271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104727770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104727d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104728220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104728c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104729120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104729620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104729b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10472a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10472a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10472aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10472af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10472b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10472b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10472be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10472c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10472c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10472cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10472d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10472d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10472dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10472e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10472e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10472eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10472f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10472f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10472fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10472ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104730420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104730920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104730e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104731820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104731d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104732220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104732720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104732c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104733120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104733620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104733b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104734020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104734520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104734a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104734f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104735420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104735920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104735e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104736320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104736820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104736d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x104737220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104737720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104737c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104738120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104738620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104738b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104739520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104739a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104739f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10473a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10473a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10473ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10473b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10473b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10473bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10473c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10473c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10473cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10473d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10473d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10473db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10473e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10473e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10473ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10473ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10473f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10473f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10473fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104740320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104740820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104740d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1047412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104741880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x104741e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1047423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1047428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104742de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1047432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1047439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104743e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104744120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1047446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104744bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1047452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104745750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104745bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104746090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1047468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104746ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104747150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104747700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104747cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104748260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104748810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104748dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104749370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104749920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104749ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10474a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10474aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10474afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10474b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10474bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10474c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10474c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10474cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10474d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10474d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10474dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10474e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10474e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10474ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10474f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10474f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10474ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104750ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104751090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104751640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104751bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1047521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104752750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104752d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1047532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104753860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104753e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1047543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104754970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104754f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1047554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104755a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104756030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1047565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104757140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1047576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104757ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104758250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104758800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104758db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104759360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104759910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104759ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10475a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10475aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10475af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10475b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10475b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10475be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10475c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10475c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10475cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10475d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10475d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10475dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10475e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10475e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10475eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10475f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10475f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10475fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10475ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x104760420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x104760920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x104760e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x104761320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x104761820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x104761d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x104762220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104762720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104763130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104763850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104763f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104764690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104764950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1047650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104765580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104765a20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m1.064s
user	0m0.262s
sys	0m0.194s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
