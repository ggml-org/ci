Requirement already satisfied: numpy~=1.24.4 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.36.2)
Requirement already satisfied: gguf>=0.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.6.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.2)
Requirement already satisfied: torch~=2.1.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.2)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.1)
Requirement already satisfied: filelock in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: safetensors>=0.3.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.1)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: packaging>=20.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)
Requirement already satisfied: regex!=2019.12.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)
Requirement already satisfied: requests in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: jinja2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: networkx in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)
Requirement already satisfied: fsspec in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.12.2)
Requirement already satisfied: typing-extensions in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: sympy in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from jinja2->torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.4)
Requirement already satisfied: certifi>=2017.4.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.11.17)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)
Requirement already satisfied: mpmath>=0.19 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from sympy->torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
ERROR: File "setup.py" or "setup.cfg" not found. Directory cannot be installed in editable mode: /Users/ggml/work/llama.cpp/gguf-py
(A "pyproject.toml" file was found, but editable mode currently requires a setuptools-based build.)
+ gg_run_ctest_debug
+ cd /Users/ggml/work/llama.cpp
+ tee /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/ctest_debug.log
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_METAL_SHADER_DEBUG=ON ..
-- The C compiler identification is AppleClang 15.0.0.15000100
-- The CXX compiler identification is AppleClang 15.0.0.15000100
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.3 (Apple Git-145)") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Accelerate framework found
-- Metal framework found
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done (0.7s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-debug

real	0m1.059s
user	0m0.384s
sys	0m0.437s
+ tee -a /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/ctest_debug-make.log
+ make -j
[  1%] Generating build details from Git
[  2%] Compiling Metal kernels
[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-metal.m.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
-- Found Git: /usr/bin/git (found version "2.39.3 (Apple Git-145)") 
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target build_info
[  6%] Built target ggml
[  8%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  8%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  9%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[ 10%] Linking C static library libggml_static.a
[ 10%] Built target ggml_static
[ 10%] Linking CXX executable ../../bin/gguf
[ 10%] Built target gguf
[ 10%] Built target ggml-metal
[ 10%] Linking CXX static library libllama.a
[ 10%] Built target llama
[ 11%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 13%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 15%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 16%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 17%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 18%] Linking CXX executable ../bin/test-c
[ 19%] Linking CXX executable ../../bin/benchmark
[ 20%] Linking CXX executable ../../bin/quantize
[ 20%] Built target llava
[ 21%] Linking CXX executable ../../bin/quantize-stats
[ 21%] Linking CXX static library libcommon.a
[ 22%] Linking CXX static library libllava_static.a
[ 22%] Built target llava_static
[ 22%] Built target common
[ 22%] Built target test-c
[ 22%] Built target benchmark
[ 22%] Built target quantize-stats
[ 22%] Built target quantize
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 25%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 25%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 30%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 35%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 36%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 39%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 40%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 43%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 44%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 45%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 46%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 47%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 48%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 49%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../../bin/baby-llama
[ 54%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-quantize-perf
[ 57%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 58%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 59%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 60%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 62%] Linking CXX executable ../../bin/batched-bench
[ 63%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 64%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 65%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 66%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 66%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/imatrix
[ 69%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 70%] Linking CXX executable ../bin/test-grad0
[ 70%] Linking CXX executable ../../bin/embedding
[ 70%] Linking CXX executable ../../bin/main
[ 70%] Linking CXX executable ../bin/test-backend-ops
[ 71%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 71%] Linking CXX executable ../bin/test-rope
[ 72%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 73%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 74%] Linking CXX executable ../bin/test-grammar-parser
[ 75%] Linking CXX executable ../../bin/beam-search
[ 76%] Linking CXX executable ../../bin/q8dot
[ 77%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 78%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 78%] Linking CXX executable ../bin/test-model-load-cancel
[ 79%] Linking CXX executable ../bin/test-quantize-fns
[ 80%] Linking CXX executable ../../bin/export-lora
[ 81%] Linking CXX executable ../../bin/save-load-state
[ 81%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 82%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 83%] Linking CXX executable ../../bin/parallel
[ 84%] Linking CXX executable ../bin/test-autorelease
[ 85%] Linking CXX executable ../../bin/infill
[ 86%] Linking CXX executable ../bin/test-chat-template
[ 87%] Linking CXX executable ../../bin/vdot
[ 88%] Linking CXX executable ../../bin/finetune
[ 90%] Linking CXX executable ../bin/test-sampling
[ 90%] Linking CXX executable ../../bin/gritlm
[ 91%] Linking CXX executable ../../bin/llama-bench
[ 92%] Linking CXX executable ../../bin/batched
[ 92%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 92%] Linking CXX executable ../../bin/tokenize
[ 93%] Linking CXX executable ../../bin/speculative
[ 93%] Linking CXX executable ../../bin/lookahead
[ 94%] Linking CXX executable ../../bin/llava-cli
[ 95%] Linking CXX executable ../../bin/lookup
[ 96%] Linking CXX executable ../../bin/server
[ 97%] Linking CXX executable ../../bin/train-text-from-scratch
[ 98%] Linking CXX executable ../../bin/simple
[ 99%] Linking CXX executable ../../bin/perplexity
[ 99%] Linking CXX executable ../../bin/passkey
[ 99%] Built target main
[ 99%] Built target test-tokenizer-1-bpe
[ 99%] Built target test-grad0
[ 99%] Built target batched-bench
[ 99%] Built target imatrix
[ 99%] Built target test-backend-ops
[ 99%] Built target q8dot
[ 99%] Built target test-grammar-parser
[ 99%] Built target test-quantize-perf
[ 99%] Built target vdot
[ 99%] Built target infill
[ 99%] Built target test-model-load-cancel
[ 99%] Built target test-tokenizer-0-falcon
[ 99%] Built target test-rope
[ 99%] Built target export-lora
[ 99%] Built target test-autorelease
[ 99%] Built target test-chat-template
[ 99%] Built target convert-llama2c-to-ggml
[ 99%] Built target test-sampling
[ 99%] Built target test-quantize-fns
[ 99%] Built target parallel
[ 99%] Built target embedding
[ 99%] Built target simple
[ 99%] Built target llama-bench
[ 99%] Built target test-tokenizer-1-llama
[ 99%] Built target train-text-from-scratch
[ 99%] Built target tokenize
[ 99%] Built target baby-llama
[ 99%] Built target beam-search
[ 99%] Built target perplexity
[ 99%] Built target save-load-state
[ 99%] Built target llava-cli
[ 99%] Built target finetune
[ 99%] Built target lookup
[ 99%] Built target gritlm
[ 99%] Built target batched
[ 99%] Built target test-tokenizer-0-llama
[ 99%] Built target speculative
[ 99%] Built target lookahead
[ 99%] Built target server
[ 99%] Built target passkey
[100%] Linking CXX executable ../bin/test-llama-grammar
[100%] Built target test-llama-grammar

real	0m7.623s
user	0m17.699s
sys	0m4.222s
+ tee -a /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-quantize-fns
 1/21 Test  #1: test-quantize-fns ...................   Passed   28.79 sec
      Start  2: test-quantize-perf
 2/21 Test  #2: test-quantize-perf ..................   Passed   10.30 sec
      Start  3: test-sampling
 3/21 Test  #3: test-sampling .......................   Passed    0.23 sec
      Start  4: test-chat-template
 4/21 Test  #4: test-chat-template ..................   Passed    0.21 sec
      Start  5: test-tokenizer-0-llama
 5/21 Test  #5: test-tokenizer-0-llama ..............   Passed    0.36 sec
      Start  6: test-tokenizer-0-falcon
 6/21 Test  #6: test-tokenizer-0-falcon .............   Passed    1.02 sec
      Start  7: test-tokenizer-1-llama
 7/21 Test  #7: test-tokenizer-1-llama ..............   Passed    3.20 sec
      Start  8: test-tokenizer-1-baichuan
 8/21 Test  #8: test-tokenizer-1-baichuan ...........   Passed    3.21 sec
      Start  9: test-tokenizer-1-falcon
 9/21 Test  #9: test-tokenizer-1-falcon .............   Passed    6.19 sec
      Start 10: test-tokenizer-1-aquila
10/21 Test #10: test-tokenizer-1-aquila .............   Passed    8.59 sec
      Start 11: test-tokenizer-1-mpt
11/21 Test #11: test-tokenizer-1-mpt ................   Passed    4.98 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/21 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    4.95 sec
      Start 13: test-tokenizer-1-gpt-neox
13/21 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    4.98 sec
      Start 14: test-tokenizer-1-refact
14/21 Test #14: test-tokenizer-1-refact .............   Passed    4.76 sec
      Start 15: test-tokenizer-1-starcoder
15/21 Test #15: test-tokenizer-1-starcoder ..........   Passed    4.76 sec
      Start 16: test-tokenizer-1-gpt2
16/21 Test #16: test-tokenizer-1-gpt2 ...............   Passed    5.02 sec
      Start 17: test-grammar-parser
17/21 Test #17: test-grammar-parser .................   Passed    0.18 sec
      Start 18: test-llama-grammar
18/21 Test #18: test-llama-grammar ..................   Passed    0.22 sec
      Start 19: test-grad0
19/21 Test #19: test-grad0 ..........................   Passed    0.84 sec
      Start 20: test-backend-ops
20/21 Test #20: test-backend-ops ....................   Passed  100.36 sec
      Start 21: test-rope
21/21 Test #21: test-rope ...........................   Passed    0.36 sec

100% tests passed, 0 tests failed out of 21

Label Time Summary:
main    = 193.51 sec*proc (21 tests)

Total Test time (real) = 193.53 sec

real	3m13.677s
user	5m19.275s
sys	0m4.449s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_METAL_SHADER_DEBUG=ON ..
-- The C compiler identification is AppleClang 15.0.0.15000100
-- The CXX compiler identification is AppleClang 15.0.0.15000100
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.3 (Apple Git-145)") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Accelerate framework found
-- Metal framework found
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done (0.6s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m0.853s
user	0m0.393s
sys	0m0.417s
+ tee -a /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/ctest_release-make.log
+ make -j
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-metal.m.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Compiling Metal kernels
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  5%] Built target ggml
[  6%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  7%] Linking C static library libggml_static.a
[  8%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  9%] Built target build_info
[ 10%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[ 10%] Linking CXX executable ../../bin/gguf
[ 10%] Built target ggml_static
[ 10%] Built target gguf
[ 10%] Built target ggml-metal
[ 10%] Linking CXX static library libllama.a
[ 10%] Built target llama
[ 10%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 11%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 11%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 11%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 17%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 16%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 18%] Linking CXX executable ../bin/test-c
[ 19%] Linking CXX executable ../../bin/benchmark
[ 21%] Linking CXX executable ../../bin/quantize
[ 21%] Linking CXX executable ../../bin/quantize-stats
[ 21%] Built target llava
[ 21%] Linking CXX static library libcommon.a
[ 22%] Linking CXX static library libllava_static.a
[ 22%] Built target common
[ 23%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 25%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 25%] Built target llava_static
[ 26%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 27%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 29%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 31%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 32%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 32%] Built target test-c
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 33%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 39%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 44%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 49%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 52%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 53%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 56%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 56%] Linking CXX executable ../../bin/lookahead
[ 57%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 58%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 59%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 60%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 60%] Linking CXX executable ../../bin/passkey
[ 61%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 63%] Linking CXX executable ../bin/test-sampling
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Linking CXX executable ../../bin/main
[ 64%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 64%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 65%] Linking CXX executable ../../bin/beam-search
[ 66%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 67%] Linking CXX executable ../bin/test-grad0
[ 68%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../bin/test-model-load-cancel
[ 70%] Linking CXX executable ../../bin/parallel
[ 71%] Linking CXX executable ../../bin/train-text-from-scratch
[ 72%] Linking CXX executable ../bin/test-quantize-fns
[ 73%] Linking CXX executable ../bin/test-grammar-parser
[ 74%] Linking CXX executable ../bin/test-quantize-perf
[ 75%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 76%] Linking CXX executable ../../bin/baby-llama
[ 77%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 78%] Linking CXX executable ../../bin/llava-cli
[ 79%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 79%] Built target benchmark
[ 80%] Linking CXX executable ../../bin/server
[ 81%] Linking CXX executable ../../bin/finetune
[ 82%] Linking CXX executable ../../bin/save-load-state
[ 82%] Linking CXX executable ../../bin/tokenize
[ 83%] Linking CXX executable ../bin/test-autorelease
[ 84%] Linking CXX executable ../../bin/lookup
[ 84%] Linking CXX executable ../../bin/embedding
[ 85%] Linking CXX executable ../../bin/batched-bench
[ 86%] Linking CXX executable ../../bin/llama-bench
[ 87%] Linking CXX executable ../bin/test-chat-template
[ 88%] Linking CXX executable ../../bin/simple
[ 89%] Linking CXX executable ../bin/test-backend-ops
[ 89%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 90%] Linking CXX executable ../../bin/gritlm
[ 91%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 92%] Linking CXX executable ../../bin/speculative
[ 93%] Linking CXX executable ../../bin/infill
[ 93%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 94%] Linking CXX executable ../../bin/q8dot
[ 95%] Linking CXX executable ../../bin/vdot
[ 96%] Linking CXX executable ../../bin/perplexity
[ 97%] Linking CXX executable ../../bin/imatrix
[ 98%] Linking CXX executable ../../bin/export-lora
[ 98%] Built target quantize
[ 98%] Built target quantize-stats
[ 99%] Linking CXX executable ../../bin/batched
[ 99%] Built target test-tokenizer-1-llama
[ 99%] Built target lookahead
[ 99%] Built target passkey
[ 99%] Built target test-grammar-parser
[ 99%] Built target test-quantize-perf
[ 99%] Built target convert-llama2c-to-ggml
[ 99%] Built target finetune
[ 99%] Built target test-quantize-fns
[ 99%] Built target main
[ 99%] Built target test-sampling
[ 99%] Built target beam-search
[ 99%] Built target lookup
[ 99%] Built target infill
[ 99%] Built target test-model-load-cancel
[ 99%] Built target test-chat-template
[ 99%] Built target test-autorelease
[ 99%] Built target test-rope
[ 99%] Built target test-grad0
[ 99%] Built target simple
[ 99%] Built target train-text-from-scratch
[ 99%] Built target speculative
[ 99%] Built target export-lora
[ 99%] Built target imatrix
[ 99%] Built target llama-bench
[ 99%] Built target tokenize
[ 99%] Built target gritlm
[ 99%] Built target baby-llama
[ 99%] Built target parallel
[ 99%] Built target llava-cli
[ 99%] Built target test-backend-ops
[ 99%] Built target test-tokenizer-0-falcon
[ 99%] Built target q8dot
[ 99%] Built target save-load-state
[ 99%] Built target perplexity
[ 99%] Built target test-tokenizer-1-bpe
[ 99%] Built target vdot
[ 99%] Built target batched
[ 99%] Built target server
[ 99%] Built target batched-bench
[ 99%] Built target embedding
[ 99%] Built target test-tokenizer-0-llama
[100%] Linking CXX executable ../bin/test-llama-grammar
[100%] Built target test-llama-grammar

real	0m13.975s
user	0m23.188s
sys	0m3.972s
+ '[' -z ']'
+ tee -a /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-quantize-fns
 1/21 Test  #1: test-quantize-fns ...................   Passed   16.62 sec
      Start  2: test-quantize-perf
 2/21 Test  #2: test-quantize-perf ..................   Passed    5.73 sec
      Start  3: test-sampling
 3/21 Test  #3: test-sampling .......................   Passed    0.19 sec
      Start  4: test-chat-template
 4/21 Test  #4: test-chat-template ..................   Passed    0.18 sec
      Start  5: test-tokenizer-0-llama
 5/21 Test  #5: test-tokenizer-0-llama ..............   Passed    0.24 sec
      Start  6: test-tokenizer-0-falcon
 6/21 Test  #6: test-tokenizer-0-falcon .............   Passed    0.35 sec
      Start  7: test-tokenizer-1-llama
 7/21 Test  #7: test-tokenizer-1-llama ..............   Passed    0.55 sec
      Start  8: test-tokenizer-1-baichuan
 8/21 Test  #8: test-tokenizer-1-baichuan ...........   Passed    0.40 sec
      Start  9: test-tokenizer-1-falcon
 9/21 Test  #9: test-tokenizer-1-falcon .............   Passed    0.80 sec
      Start 10: test-tokenizer-1-aquila
10/21 Test #10: test-tokenizer-1-aquila .............   Passed    0.87 sec
      Start 11: test-tokenizer-1-mpt
11/21 Test #11: test-tokenizer-1-mpt ................   Passed    0.48 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/21 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    0.49 sec
      Start 13: test-tokenizer-1-gpt-neox
13/21 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    0.48 sec
      Start 14: test-tokenizer-1-refact
14/21 Test #14: test-tokenizer-1-refact .............   Passed    0.47 sec
      Start 15: test-tokenizer-1-starcoder
15/21 Test #15: test-tokenizer-1-starcoder ..........   Passed    0.47 sec
      Start 16: test-tokenizer-1-gpt2
16/21 Test #16: test-tokenizer-1-gpt2 ...............   Passed    0.49 sec
      Start 17: test-grammar-parser
17/21 Test #17: test-grammar-parser .................   Passed    0.18 sec
      Start 18: test-llama-grammar
18/21 Test #18: test-llama-grammar ..................   Passed    0.18 sec
      Start 19: test-grad0
19/21 Test #19: test-grad0 ..........................   Passed    0.83 sec
      Start 20: test-backend-ops
20/21 Test #20: test-backend-ops ....................   Passed   31.55 sec
      Start 21: test-rope
21/21 Test #21: test-rope ...........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 21

Label Time Summary:
main    =  61.76 sec*proc (21 tests)

Total Test time (real) =  61.78 sec

real	1m1.782s
user	1m8.351s
sys	0m3.527s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/embd_bge_small.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-14 13:01:36 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model:
2024-03-14 13:01:36 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-14 13:01:36 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-03-14 13:01:37 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-14 13:01:38 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2024-03-14 13:01:38 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2024-03-14 13:01:38 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-14 13:01:38 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-14 13:01:39 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_METAL_SHADER_DEBUG=ON ..
-- The C compiler identification is AppleClang 15.0.0.15000100
-- The CXX compiler identification is AppleClang 15.0.0.15000100
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.3 (Apple Git-145)") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Accelerate framework found
-- Metal framework found
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done (0.7s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m0.878s
user	0m0.389s
sys	0m0.435s
+ tee -a /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/embd_bge_small-make.log
+ make -j
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Compiling Metal kernels
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-metal.m.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  6%] Built target build_info
[  6%] Built target ggml
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  9%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  9%] Linking C static library libggml_static.a
[ 10%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[ 10%] Linking CXX executable ../../bin/gguf
[ 10%] Linking CXX static library libllama.a
[ 10%] Built target ggml_static
[ 10%] Built target llama
[ 10%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 11%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 11%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 12%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 17%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 17%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 18%] Linking CXX executable ../bin/test-c
[ 18%] Built target llava
[ 19%] Linking CXX executable ../../bin/quantize-stats
[ 20%] Linking CXX executable ../../bin/benchmark
[ 21%] Linking CXX executable ../../bin/quantize
[ 21%] Built target gguf
[ 21%] Linking CXX static library libcommon.a
[ 22%] Linking CXX static library libllava_static.a
[ 22%] Built target common
[ 22%] Built target llava_static
[ 23%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 35%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 35%] Built target test-c
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Built target benchmark
[ 39%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 39%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 40%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 41%] Linking CXX executable ../bin/test-sampling
[ 42%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 43%] Built target quantize
[ 44%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 45%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target quantize-stats
[ 47%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 47%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-chat-template
[ 50%] Linking CXX executable ../bin/test-quantize-fns
[ 51%] Linking CXX executable ../../bin/beam-search
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-quantize-perf
[ 53%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 54%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 55%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 56%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 57%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 58%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 58%] Linking CXX executable ../../bin/embedding
[ 59%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 60%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 61%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 62%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 63%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 64%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 65%] Linking CXX executable ../bin/test-grad0
[ 65%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 66%] Linking CXX executable ../../bin/batched
[ 67%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 68%] Linking CXX executable ../../bin/simple
[ 69%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 70%] Linking CXX executable ../bin/test-autorelease
[ 71%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 72%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 73%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 74%] Linking CXX executable ../bin/test-llama-grammar
[ 75%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 76%] Linking CXX executable ../bin/test-grammar-parser
[ 77%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/server
[ 79%] Linking CXX executable ../../bin/finetune
[ 80%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 81%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 81%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 82%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 83%] Linking CXX executable ../../bin/export-lora
[ 83%] Linking CXX executable ../../bin/passkey
[ 84%] Linking CXX executable ../../bin/batched-bench
[ 84%] Linking CXX executable ../bin/test-backend-ops
[ 85%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 86%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 86%] Linking CXX executable ../bin/test-model-load-cancel
[ 87%] Linking CXX executable ../../bin/baby-llama
[ 88%] Linking CXX executable ../../bin/llama-bench
[ 89%] Linking CXX executable ../../bin/train-text-from-scratch
[ 90%] Linking CXX executable ../../bin/save-load-state
[ 91%] Linking CXX executable ../../bin/parallel
[ 92%] Linking CXX executable ../../bin/speculative
[ 93%] Linking CXX executable ../../bin/q8dot
[ 94%] Linking CXX executable ../../bin/llava-cli
[ 95%] Linking CXX executable ../../bin/imatrix
[ 96%] Linking CXX executable ../../bin/infill
[ 97%] Linking CXX executable ../../bin/perplexity
[ 98%] Linking CXX executable ../../bin/gritlm
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Linking CXX executable ../../bin/lookahead
[ 99%] Linking CXX executable ../bin/test-rope
[100%] Linking CXX executable ../../bin/lookup
[100%] Linking CXX executable ../../bin/main
[100%] Linking CXX executable ../../bin/tokenize
[100%] Built target test-sampling
[100%] Built target test-tokenizer-1-bpe
[100%] Built target test-chat-template
[100%] Built target test-llama-grammar
[100%] Built target test-quantize-perf
[100%] Built target test-autorelease
[100%] Built target test-quantize-fns
[100%] Built target embedding
[100%] Built target batched
[100%] Built target beam-search
[100%] Built target test-grad0
[100%] Built target test-tokenizer-1-llama
[100%] Built target train-text-from-scratch
[100%] Built target test-grammar-parser
[100%] Built target server
[100%] Built target test-tokenizer-0-llama
[100%] Built target test-model-load-cancel
[100%] Built target vdot
[100%] Built target batched-bench
[100%] Built target convert-llama2c-to-ggml
[100%] Built target speculative
[100%] Built target export-lora
[100%] Built target finetune
[100%] Built target parallel
[100%] Built target infill
[100%] Built target q8dot
[100%] Built target lookup
[100%] Built target simple
[100%] Built target main
[100%] Built target test-rope
[100%] Built target save-load-state
[100%] Built target passkey
[100%] Built target tokenize
[100%] Built target perplexity
[100%] Built target llama-bench
[100%] Built target lookahead
[100%] Built target gritlm
[100%] Built target test-backend-ops
[100%] Built target baby-llama
[100%] Built target test-tokenizer-0-falcon
[100%] Built target llava-cli
[100%] Built target imatrix
[100%] Built target ggml-metal

real	0m3.058s
user	0m11.293s
sys	0m3.758s
+ python3 ../convert-hf-to-gguf.py ../models-mnt/bge-small
/Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Traceback (most recent call last):
Loading model: bge-small
gguf: This GGUF file is for Little Endian only
Set model parameters
Set model tokenizer
fname_tokenizer: ../models-mnt/bge-small
gguf: Setting special token type pad to 0
Exporting model to '../models-mnt/bge-small/ggml-model-f16.gguf'
gguf: loading model part 'pytorch_model.bin'
token_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
position_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
token_types.weight, n_dims = 2, torch.float32 --> <class 'numpy.float32'>
  File "/Users/ggml/work/llama.cpp/build-ci-release/../convert-hf-to-gguf.py", line 2056, in <module>
    main()
  File "/Users/ggml/work/llama.cpp/build-ci-release/../convert-hf-to-gguf.py", line 2050, in main
    model_instance.write()
  File "/Users/ggml/work/llama.cpp/build-ci-release/../convert-hf-to-gguf.py", line 164, in write
    self.write_tensors()
  File "/Users/ggml/work/llama.cpp/build-ci-release/../convert-hf-to-gguf.py", line 1743, in write_tensors
    self.gguf_writer.add_tensor(new_name, data)
  File "/Users/ggml/work/llama.cpp/build-ci-release/../gguf-py/gguf/gguf_writer.py", line 238, in add_tensor
    self.add_tensor_info(name, shape, tensor.dtype, tensor.nbytes, raw_dtype = raw_dtype)
  File "/Users/ggml/work/llama.cpp/build-ci-release/../gguf-py/gguf/gguf_writer.py", line 218, in add_tensor_info
    raise ValueError("Only F32, F16, I8, I16, I32 tensors are supported for now")
ValueError: Only F32, F16, I8, I16, I32 tensors are supported for now
+ cur=1
+ echo 1
+ set +x
cat: /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/embd_bge_small-tg-f16.log: No such file or directory
cat: /Users/ggml/results/llama.cpp/2c/4fb69246834503db7b78bcbedcef506bbc60c4/ggml-100-m1/embd_bge_small-tg-q8_0.log: No such file or directory
