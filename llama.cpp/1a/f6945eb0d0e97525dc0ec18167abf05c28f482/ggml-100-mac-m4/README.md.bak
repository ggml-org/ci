### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.61 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.62 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.39 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.24 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.30 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.81 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.98 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  197.12 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.09 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.35 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 242.62 sec*proc (28 tests)

Total Test time (real) = 242.63 sec

real	4m2.658s
user	8m28.712s
sys	0m7.072s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.44 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.39 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.78 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.48 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.06 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  53.18 sec*proc (28 tests)

Total Test time (real) =  53.20 sec

real	0m53.208s
user	1m16.633s
sys	0m6.057s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.157 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.287 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.934 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.946 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.948 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.948 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.948 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.950 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.950 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.952 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.953 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.953 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.956 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.956 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.957 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.957 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.957 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.958 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.958 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.972 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.973 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.974 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.974 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.974 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.975 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.026.975 I llama_model_loader: - type  f32:  124 tensors
0.00.026.975 I llama_model_loader: - type  f16:   73 tensors
0.00.026.976 I print_info: file format = GGUF V3 (latest)
0.00.026.977 I print_info: file type   = F16
0.00.026.978 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.030.030 I load: special tokens cache size = 5
0.00.031.592 I load: token to piece cache size = 0.2032 MB
0.00.031.595 I print_info: arch             = bert
0.00.031.595 I print_info: vocab_only       = 0
0.00.031.595 I print_info: n_ctx_train      = 512
0.00.031.596 I print_info: n_embd           = 384
0.00.031.596 I print_info: n_layer          = 12
0.00.031.599 I print_info: n_head           = 12
0.00.031.599 I print_info: n_head_kv        = 12
0.00.031.600 I print_info: n_rot            = 32
0.00.031.600 I print_info: n_swa            = 0
0.00.031.600 I print_info: n_embd_head_k    = 32
0.00.031.600 I print_info: n_embd_head_v    = 32
0.00.031.601 I print_info: n_gqa            = 1
0.00.031.601 I print_info: n_embd_k_gqa     = 384
0.00.031.602 I print_info: n_embd_v_gqa     = 384
0.00.031.604 I print_info: f_norm_eps       = 1.0e-12
0.00.031.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.605 I print_info: f_logit_scale    = 0.0e+00
0.00.031.607 I print_info: n_ff             = 1536
0.00.031.607 I print_info: n_expert         = 0
0.00.031.610 I print_info: n_expert_used    = 0
0.00.031.610 I print_info: causal attn      = 0
0.00.031.610 I print_info: pooling type     = 2
0.00.031.610 I print_info: rope type        = 2
0.00.031.610 I print_info: rope scaling     = linear
0.00.031.611 I print_info: freq_base_train  = 10000.0
0.00.031.611 I print_info: freq_scale_train = 1
0.00.031.612 I print_info: n_ctx_orig_yarn  = 512
0.00.031.613 I print_info: rope_finetuned   = unknown
0.00.031.613 I print_info: ssm_d_conv       = 0
0.00.031.613 I print_info: ssm_d_inner      = 0
0.00.031.613 I print_info: ssm_d_state      = 0
0.00.031.613 I print_info: ssm_dt_rank      = 0
0.00.031.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.614 I print_info: model type       = 33M
0.00.031.614 I print_info: model params     = 33.21 M
0.00.031.614 I print_info: general.name     = Bge Small
0.00.031.615 I print_info: vocab type       = WPM
0.00.031.615 I print_info: n_vocab          = 30522
0.00.031.615 I print_info: n_merges         = 0
0.00.031.615 I print_info: BOS token        = 101 '[CLS]'
0.00.031.615 I print_info: UNK token        = 100 '[UNK]'
0.00.031.616 I print_info: SEP token        = 102 '[SEP]'
0.00.031.617 I print_info: PAD token        = 0 '[PAD]'
0.00.031.617 I print_info: MASK token       = 103 '[MASK]'
0.00.031.617 I print_info: LF token         = 0 '[PAD]'
0.00.031.618 I print_info: max token length = 21
0.00.033.038 I load_tensors: offloading 12 repeating layers to GPU
0.00.033.039 I load_tensors: offloading output layer to GPU
0.00.033.040 I load_tensors: offloaded 13/13 layers to GPU
0.00.033.061 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.062 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.033.260 I llama_init_from_model: n_seq_max     = 1
0.00.033.261 I llama_init_from_model: n_ctx         = 512
0.00.033.262 I llama_init_from_model: n_ctx_per_seq = 512
0.00.033.262 I llama_init_from_model: n_batch       = 2048
0.00.033.262 I llama_init_from_model: n_ubatch      = 2048
0.00.033.262 I llama_init_from_model: flash_attn    = 0
0.00.033.263 I llama_init_from_model: freq_base     = 10000.0
0.00.033.263 I llama_init_from_model: freq_scale    = 1
0.00.033.263 I ggml_metal_init: allocating
0.00.033.267 I ggml_metal_init: found device: Apple M4
0.00.033.269 I ggml_metal_init: picking default device: Apple M4
0.00.033.992 I ggml_metal_init: using embedded metal library
0.00.037.082 I ggml_metal_init: GPU name:   Apple M4
0.00.037.085 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.037.085 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.037.085 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.037.086 I ggml_metal_init: simdgroup reduction   = true
0.00.037.086 I ggml_metal_init: simdgroup matrix mul. = true
0.00.037.086 I ggml_metal_init: has bfloat            = true
0.00.037.086 I ggml_metal_init: use bfloat            = true
0.00.037.087 I ggml_metal_init: hasUnifiedMemory      = true
0.00.037.088 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.046.947 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.047.479 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.047.482 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.047.483 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.048.142 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.048.143 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.048.143 I llama_init_from_model: graph nodes  = 429
0.00.048.144 I llama_init_from_model: graph splits = 2
0.00.048.145 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.048.145 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.053.191 I 
0.00.053.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.053.769 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.057.953 I llama_perf_context_print:        load time =      33.90 ms
0.00.057.954 I llama_perf_context_print: prompt eval time =       4.04 ms /     9 tokens (    0.45 ms per token,  2229.38 tokens per second)
0.00.057.955 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.057.955 I llama_perf_context_print:       total time =       4.76 ms /    10 tokens
0.00.058.161 I ggml_metal_free: deallocating

real	0m0.239s
user	0m0.041s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.759 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.125 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.129 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.130 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.133 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.133 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.133 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.134 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.135 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.135 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.136 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.136 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.136 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.139 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.139 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.140 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.140 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.140 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.141 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.392 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.027 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.028 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.028 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.028 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.029 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.029 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.029 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.030 I llama_model_loader: - type  f32:  124 tensors
0.00.014.030 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.031 I print_info: file format = GGUF V3 (latest)
0.00.014.031 I print_info: file type   = Q8_0
0.00.014.033 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.326 I load: special tokens cache size = 5
0.00.017.540 I load: token to piece cache size = 0.2032 MB
0.00.017.543 I print_info: arch             = bert
0.00.017.543 I print_info: vocab_only       = 0
0.00.017.543 I print_info: n_ctx_train      = 512
0.00.017.544 I print_info: n_embd           = 384
0.00.017.544 I print_info: n_layer          = 12
0.00.017.547 I print_info: n_head           = 12
0.00.017.548 I print_info: n_head_kv        = 12
0.00.017.548 I print_info: n_rot            = 32
0.00.017.548 I print_info: n_swa            = 0
0.00.017.548 I print_info: n_embd_head_k    = 32
0.00.017.548 I print_info: n_embd_head_v    = 32
0.00.017.549 I print_info: n_gqa            = 1
0.00.017.550 I print_info: n_embd_k_gqa     = 384
0.00.017.550 I print_info: n_embd_v_gqa     = 384
0.00.017.551 I print_info: f_norm_eps       = 1.0e-12
0.00.017.551 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.551 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.552 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.552 I print_info: f_logit_scale    = 0.0e+00
0.00.017.552 I print_info: n_ff             = 1536
0.00.017.553 I print_info: n_expert         = 0
0.00.017.553 I print_info: n_expert_used    = 0
0.00.017.553 I print_info: causal attn      = 0
0.00.017.553 I print_info: pooling type     = 2
0.00.017.553 I print_info: rope type        = 2
0.00.017.553 I print_info: rope scaling     = linear
0.00.017.554 I print_info: freq_base_train  = 10000.0
0.00.017.555 I print_info: freq_scale_train = 1
0.00.017.556 I print_info: n_ctx_orig_yarn  = 512
0.00.017.556 I print_info: rope_finetuned   = unknown
0.00.017.556 I print_info: ssm_d_conv       = 0
0.00.017.556 I print_info: ssm_d_inner      = 0
0.00.017.556 I print_info: ssm_d_state      = 0
0.00.017.557 I print_info: ssm_dt_rank      = 0
0.00.017.557 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.557 I print_info: model type       = 33M
0.00.017.557 I print_info: model params     = 33.21 M
0.00.017.558 I print_info: general.name     = Bge Small
0.00.017.558 I print_info: vocab type       = WPM
0.00.017.558 I print_info: n_vocab          = 30522
0.00.017.558 I print_info: n_merges         = 0
0.00.017.559 I print_info: BOS token        = 101 '[CLS]'
0.00.017.559 I print_info: UNK token        = 100 '[UNK]'
0.00.017.559 I print_info: SEP token        = 102 '[SEP]'
0.00.017.560 I print_info: PAD token        = 0 '[PAD]'
0.00.017.560 I print_info: MASK token       = 103 '[MASK]'
0.00.017.561 I print_info: LF token         = 0 '[PAD]'
0.00.017.561 I print_info: max token length = 21
0.00.018.777 I load_tensors: offloading 12 repeating layers to GPU
0.00.018.778 I load_tensors: offloading output layer to GPU
0.00.018.779 I load_tensors: offloaded 13/13 layers to GPU
0.00.018.786 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.787 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.018.933 I llama_init_from_model: n_seq_max     = 1
0.00.018.933 I llama_init_from_model: n_ctx         = 512
0.00.018.934 I llama_init_from_model: n_ctx_per_seq = 512
0.00.018.934 I llama_init_from_model: n_batch       = 2048
0.00.018.934 I llama_init_from_model: n_ubatch      = 2048
0.00.018.934 I llama_init_from_model: flash_attn    = 0
0.00.018.934 I llama_init_from_model: freq_base     = 10000.0
0.00.018.935 I llama_init_from_model: freq_scale    = 1
0.00.018.935 I ggml_metal_init: allocating
0.00.018.938 I ggml_metal_init: found device: Apple M4
0.00.018.943 I ggml_metal_init: picking default device: Apple M4
0.00.019.534 I ggml_metal_init: using embedded metal library
0.00.021.906 I ggml_metal_init: GPU name:   Apple M4
0.00.021.908 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.909 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.909 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.909 I ggml_metal_init: simdgroup reduction   = true
0.00.021.909 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.910 I ggml_metal_init: has bfloat            = true
0.00.021.910 I ggml_metal_init: use bfloat            = true
0.00.021.910 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.911 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.129 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.032.599 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.601 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.602 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.033.192 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.033.193 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.033.193 I llama_init_from_model: graph nodes  = 429
0.00.033.193 I llama_init_from_model: graph splits = 2
0.00.033.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.595 I 
0.00.037.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.135 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.538 I llama_perf_context_print:        load time =      28.83 ms
0.00.042.540 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2105.26 tokens per second)
0.00.042.541 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.542 I llama_perf_context_print:       total time =       4.94 ms /    10 tokens
0.00.042.740 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.014s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.188 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.698 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.739 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.744 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.747 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.749 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.749 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.750 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.752 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.753 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.753 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.754 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.754 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.758 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.759 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.759 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.046.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.049.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.053.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.825 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.053.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.053.826 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.053.826 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.053.826 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.053.827 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.053.827 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.053.828 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.053.828 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.053.829 I llama_model_loader: - type  f32:   40 tensors
0.00.053.829 I llama_model_loader: - type  f16:   30 tensors
0.00.053.829 I print_info: file format = GGUF V3 (latest)
0.00.053.830 I print_info: file type   = F16
0.00.053.831 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.070.567 W load: empty token at index 5
0.00.075.095 W load: model vocab missing newline token, using special_pad_id instead
0.00.076.473 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.076.504 I load: special tokens cache size = 5
0.00.332.121 I load: token to piece cache size = 1.5060 MB
0.00.332.127 I print_info: arch             = jina-bert-v2
0.00.332.127 I print_info: vocab_only       = 0
0.00.332.128 I print_info: n_ctx_train      = 8192
0.00.332.128 I print_info: n_embd           = 384
0.00.332.128 I print_info: n_layer          = 4
0.00.332.135 I print_info: n_head           = 12
0.00.332.135 I print_info: n_head_kv        = 12
0.00.332.135 I print_info: n_rot            = 32
0.00.332.136 I print_info: n_swa            = 0
0.00.332.136 I print_info: n_embd_head_k    = 32
0.00.332.136 I print_info: n_embd_head_v    = 32
0.00.332.136 I print_info: n_gqa            = 1
0.00.332.137 I print_info: n_embd_k_gqa     = 384
0.00.332.137 I print_info: n_embd_v_gqa     = 384
0.00.332.138 I print_info: f_norm_eps       = 1.0e-12
0.00.332.139 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.332.139 I print_info: f_clamp_kqv      = 0.0e+00
0.00.332.140 I print_info: f_max_alibi_bias = 8.0e+00
0.00.332.140 I print_info: f_logit_scale    = 0.0e+00
0.00.332.141 I print_info: n_ff             = 1536
0.00.332.141 I print_info: n_expert         = 0
0.00.332.141 I print_info: n_expert_used    = 0
0.00.332.142 I print_info: causal attn      = 0
0.00.332.143 I print_info: pooling type     = -1
0.00.332.144 I print_info: rope type        = -1
0.00.332.144 I print_info: rope scaling     = linear
0.00.332.144 I print_info: freq_base_train  = 10000.0
0.00.332.145 I print_info: freq_scale_train = 1
0.00.332.145 I print_info: n_ctx_orig_yarn  = 8192
0.00.332.145 I print_info: rope_finetuned   = unknown
0.00.332.146 I print_info: ssm_d_conv       = 0
0.00.332.146 I print_info: ssm_d_inner      = 0
0.00.332.146 I print_info: ssm_d_state      = 0
0.00.332.146 I print_info: ssm_dt_rank      = 0
0.00.332.146 I print_info: ssm_dt_b_c_rms   = 0
0.00.332.146 I print_info: model type       = 33M
0.00.332.147 I print_info: model params     = 32.90 M
0.00.332.147 I print_info: general.name     = Jina Bert Implementation
0.00.332.148 I print_info: vocab type       = BPE
0.00.332.148 I print_info: n_vocab          = 61056
0.00.332.148 I print_info: n_merges         = 39382
0.00.332.148 I print_info: BOS token        = 0 '<s>'
0.00.332.148 I print_info: EOS token        = 2 '</s>'
0.00.332.149 I print_info: UNK token        = 3 '<unk>'
0.00.332.149 I print_info: SEP token        = 2 '</s>'
0.00.332.149 I print_info: PAD token        = 1 '<pad>'
0.00.332.149 I print_info: MASK token       = 4 '<mask>'
0.00.332.150 I print_info: EOG token        = 2 '</s>'
0.00.332.150 I print_info: max token length = 45
0.00.333.438 I load_tensors: offloading 4 repeating layers to GPU
0.00.333.439 I load_tensors: offloading output layer to GPU
0.00.333.439 I load_tensors: offloaded 5/5 layers to GPU
0.00.333.466 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.467 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.333.778 I llama_init_from_model: n_seq_max     = 1
0.00.333.779 I llama_init_from_model: n_ctx         = 8192
0.00.333.779 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.333.779 I llama_init_from_model: n_batch       = 2048
0.00.333.779 I llama_init_from_model: n_ubatch      = 2048
0.00.333.780 I llama_init_from_model: flash_attn    = 0
0.00.333.780 I llama_init_from_model: freq_base     = 10000.0
0.00.333.780 I llama_init_from_model: freq_scale    = 1
0.00.333.781 I ggml_metal_init: allocating
0.00.333.784 I ggml_metal_init: found device: Apple M4
0.00.333.786 I ggml_metal_init: picking default device: Apple M4
0.00.334.854 I ggml_metal_init: using embedded metal library
0.00.337.509 I ggml_metal_init: GPU name:   Apple M4
0.00.337.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.512 I ggml_metal_init: simdgroup reduction   = true
0.00.337.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.512 I ggml_metal_init: has bfloat            = true
0.00.337.512 I ggml_metal_init: use bfloat            = true
0.00.337.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.346.990 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.349.379 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.381 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.383 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.350.045 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.350.046 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.350.046 I llama_init_from_model: graph nodes  = 154
0.00.350.046 I llama_init_from_model: graph splits = 2
0.00.350.047 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.350.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.567 I 
0.00.361.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.750 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.750 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.753 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.753 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.758 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.758 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.362.240 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.959 I llama_perf_context_print:        load time =     336.86 ms
0.00.365.960 I llama_perf_context_print: prompt eval time =       3.70 ms /    62 tokens (    0.06 ms per token, 16774.89 tokens per second)
0.00.365.961 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.966 I llama_perf_context_print:       total time =       4.39 ms /    63 tokens
0.00.366.194 I ggml_metal_free: deallocating

real	0m1.092s
user	0m0.338s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.131 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.311 I main: llama backend init
0.00.000.317 I main: load the model and apply lora adapter, if any
0.00.065.625 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.078.029 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.078.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.078.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.078.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.078.050 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.078.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.078.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.078.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.078.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.078.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.078.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.078.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.078.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.078.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.078.073 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.078.074 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.078.075 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.084.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.086.995 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.093.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.093.926 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.093.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.093.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.093.929 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.093.930 I llama_model_loader: - type  f32:  194 tensors
0.00.093.931 I llama_model_loader: - type  f16:   98 tensors
0.00.093.932 I print_info: file format = GGUF V3 (latest)
0.00.093.934 I print_info: file type   = all F32 (guessed)
0.00.093.936 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.130.190 I load: special tokens cache size = 25
0.00.138.024 I load: token to piece cache size = 0.2984 MB
0.00.138.028 I print_info: arch             = gptneox
0.00.138.028 I print_info: vocab_only       = 0
0.00.138.028 I print_info: n_ctx_train      = 2048
0.00.138.028 I print_info: n_embd           = 2048
0.00.138.029 I print_info: n_layer          = 24
0.00.138.033 I print_info: n_head           = 16
0.00.138.033 I print_info: n_head_kv        = 16
0.00.138.034 I print_info: n_rot            = 32
0.00.138.034 I print_info: n_swa            = 0
0.00.138.034 I print_info: n_embd_head_k    = 128
0.00.138.034 I print_info: n_embd_head_v    = 128
0.00.138.035 I print_info: n_gqa            = 1
0.00.138.036 I print_info: n_embd_k_gqa     = 2048
0.00.138.036 I print_info: n_embd_v_gqa     = 2048
0.00.138.037 I print_info: f_norm_eps       = 1.0e-05
0.00.138.037 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.138.037 I print_info: f_clamp_kqv      = 0.0e+00
0.00.138.037 I print_info: f_max_alibi_bias = 0.0e+00
0.00.138.037 I print_info: f_logit_scale    = 0.0e+00
0.00.138.038 I print_info: n_ff             = 8192
0.00.138.038 I print_info: n_expert         = 0
0.00.138.039 I print_info: n_expert_used    = 0
0.00.138.039 I print_info: causal attn      = 1
0.00.138.039 I print_info: pooling type     = 0
0.00.138.039 I print_info: rope type        = 2
0.00.138.039 I print_info: rope scaling     = linear
0.00.138.042 I print_info: freq_base_train  = 10000.0
0.00.138.042 I print_info: freq_scale_train = 1
0.00.138.042 I print_info: n_ctx_orig_yarn  = 2048
0.00.138.043 I print_info: rope_finetuned   = unknown
0.00.138.043 I print_info: ssm_d_conv       = 0
0.00.138.043 I print_info: ssm_d_inner      = 0
0.00.138.043 I print_info: ssm_d_state      = 0
0.00.138.043 I print_info: ssm_dt_rank      = 0
0.00.138.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.138.043 I print_info: model type       = 1.4B
0.00.138.044 I print_info: model params     = 1.41 B
0.00.138.044 I print_info: general.name     = 1.4B
0.00.138.045 I print_info: vocab type       = BPE
0.00.138.045 I print_info: n_vocab          = 50304
0.00.138.045 I print_info: n_merges         = 50009
0.00.138.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.138.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.138.046 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.138.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.138.046 I print_info: LF token         = 128 'Ä'
0.00.138.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.138.047 I print_info: max token length = 1024
0.00.140.810 I load_tensors: offloading 24 repeating layers to GPU
0.00.140.810 I load_tensors: offloading output layer to GPU
0.00.140.810 I load_tensors: offloaded 25/25 layers to GPU
0.00.140.829 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.140.830 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.141.164 I llama_init_from_model: n_seq_max     = 1
0.00.141.165 I llama_init_from_model: n_ctx         = 2048
0.00.141.165 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.141.165 I llama_init_from_model: n_batch       = 2048
0.00.141.165 I llama_init_from_model: n_ubatch      = 512
0.00.141.166 I llama_init_from_model: flash_attn    = 0
0.00.141.166 I llama_init_from_model: freq_base     = 10000.0
0.00.141.166 I llama_init_from_model: freq_scale    = 1
0.00.141.167 I ggml_metal_init: allocating
0.00.141.170 I ggml_metal_init: found device: Apple M4
0.00.141.173 I ggml_metal_init: picking default device: Apple M4
0.00.141.892 I ggml_metal_init: using embedded metal library
0.00.151.555 I ggml_metal_init: GPU name:   Apple M4
0.00.151.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.151.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.151.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.151.558 I ggml_metal_init: simdgroup reduction   = true
0.00.151.558 I ggml_metal_init: simdgroup matrix mul. = true
0.00.151.558 I ggml_metal_init: has bfloat            = true
0.00.151.559 I ggml_metal_init: use bfloat            = true
0.00.151.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.151.560 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.179.819 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.200.408 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.200.413 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.200.432 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.201.381 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.201.382 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.201.382 I llama_init_from_model: graph nodes  = 967
0.00.201.383 I llama_init_from_model: graph splits = 2
0.00.201.386 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.201.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.201.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.282.533 I main: llama threadpool init, n_threads = 4
0.00.282.570 I 
0.00.282.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.282.598 I 
0.00.282.664 I sampler seed: 1234
0.00.282.668 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.282.694 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.282.695 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.282.695 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.130.290 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.02.130.291 I llama_perf_context_print:        load time =     215.87 ms
0.02.130.293 I llama_perf_context_print: prompt eval time =      53.80 ms /     7 tokens (    7.69 ms per token,   130.10 tokens per second)
0.02.130.294 I llama_perf_context_print:        eval time =    1791.00 ms /    63 runs   (   28.43 ms per token,    35.18 tokens per second)
0.02.130.294 I llama_perf_context_print:       total time =    1848.79 ms /    70 tokens
0.02.130.529 I ggml_metal_free: deallocating

real	0m2.419s
user	0m0.150s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.001 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.110 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.106 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.111 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.113 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.116 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.117 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.117 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.120 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.120 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.120 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.121 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.121 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.122 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.124 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.278 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.364 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.365 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.366 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.366 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.367 I llama_model_loader: - type  f32:  194 tensors
0.00.042.368 I llama_model_loader: - type  f16:   98 tensors
0.00.042.368 I print_info: file format = GGUF V3 (latest)
0.00.042.369 I print_info: file type   = all F32 (guessed)
0.00.042.370 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.117 I load: special tokens cache size = 25
0.00.068.230 I load: token to piece cache size = 0.2984 MB
0.00.068.236 I print_info: arch             = gptneox
0.00.068.236 I print_info: vocab_only       = 0
0.00.068.236 I print_info: n_ctx_train      = 2048
0.00.068.236 I print_info: n_embd           = 2048
0.00.068.237 I print_info: n_layer          = 24
0.00.068.241 I print_info: n_head           = 16
0.00.068.242 I print_info: n_head_kv        = 16
0.00.068.242 I print_info: n_rot            = 32
0.00.068.242 I print_info: n_swa            = 0
0.00.068.242 I print_info: n_embd_head_k    = 128
0.00.068.242 I print_info: n_embd_head_v    = 128
0.00.068.243 I print_info: n_gqa            = 1
0.00.068.244 I print_info: n_embd_k_gqa     = 2048
0.00.068.244 I print_info: n_embd_v_gqa     = 2048
0.00.068.245 I print_info: f_norm_eps       = 1.0e-05
0.00.068.245 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.245 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.245 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.245 I print_info: f_logit_scale    = 0.0e+00
0.00.068.246 I print_info: n_ff             = 8192
0.00.068.246 I print_info: n_expert         = 0
0.00.068.246 I print_info: n_expert_used    = 0
0.00.068.246 I print_info: causal attn      = 1
0.00.068.246 I print_info: pooling type     = 0
0.00.068.247 I print_info: rope type        = 2
0.00.068.247 I print_info: rope scaling     = linear
0.00.068.247 I print_info: freq_base_train  = 10000.0
0.00.068.247 I print_info: freq_scale_train = 1
0.00.068.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.248 I print_info: rope_finetuned   = unknown
0.00.068.248 I print_info: ssm_d_conv       = 0
0.00.068.248 I print_info: ssm_d_inner      = 0
0.00.068.248 I print_info: ssm_d_state      = 0
0.00.068.248 I print_info: ssm_dt_rank      = 0
0.00.068.248 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.249 I print_info: model type       = 1.4B
0.00.068.249 I print_info: model params     = 1.41 B
0.00.068.251 I print_info: general.name     = 1.4B
0.00.068.252 I print_info: vocab type       = BPE
0.00.068.252 I print_info: n_vocab          = 50304
0.00.068.252 I print_info: n_merges         = 50009
0.00.068.252 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.252 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.252 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.253 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.253 I print_info: LF token         = 128 'Ä'
0.00.068.253 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.253 I print_info: max token length = 1024
0.00.070.684 I load_tensors: offloading 24 repeating layers to GPU
0.00.070.684 I load_tensors: offloading output layer to GPU
0.00.070.684 I load_tensors: offloaded 25/25 layers to GPU
0.00.070.695 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.070.696 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.070.959 I llama_init_from_model: n_seq_max     = 1
0.00.070.960 I llama_init_from_model: n_ctx         = 128
0.00.070.960 I llama_init_from_model: n_ctx_per_seq = 128
0.00.070.960 I llama_init_from_model: n_batch       = 128
0.00.070.960 I llama_init_from_model: n_ubatch      = 128
0.00.070.961 I llama_init_from_model: flash_attn    = 0
0.00.070.961 I llama_init_from_model: freq_base     = 10000.0
0.00.070.961 I llama_init_from_model: freq_scale    = 1
0.00.070.962 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.962 I ggml_metal_init: allocating
0.00.070.966 I ggml_metal_init: found device: Apple M4
0.00.070.968 I ggml_metal_init: picking default device: Apple M4
0.00.071.594 I ggml_metal_init: using embedded metal library
0.00.074.001 I ggml_metal_init: GPU name:   Apple M4
0.00.074.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.003 I ggml_metal_init: simdgroup reduction   = true
0.00.074.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.004 I ggml_metal_init: has bfloat            = true
0.00.074.004 I ggml_metal_init: use bfloat            = true
0.00.074.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.417 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.756 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.758 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.773 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.638 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.086.639 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.086.639 I llama_init_from_model: graph nodes  = 967
0.00.086.639 I llama_init_from_model: graph splits = 2
0.00.086.641 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.641 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.987.231 I 
0.00.987.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.987.354 I perplexity: tokenizing the input ..
0.01.001.388 I perplexity: tokenization took 14.033 ms
0.01.001.417 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.122.626 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.124.324 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.124.350 I llama_perf_context_print:        load time =     967.11 ms
0.01.124.353 I llama_perf_context_print: prompt eval time =     120.27 ms /   128 tokens (    0.94 ms per token,  1064.32 tokens per second)
0.01.124.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.124.357 I llama_perf_context_print:       total time =     137.12 ms /   129 tokens
0.01.125.032 I ggml_metal_free: deallocating

real	0m1.314s
user	0m0.103s
sys	0m0.185s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.919 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.780 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.785 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.787 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.787 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.788 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.788 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.789 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.790 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.790 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.790 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.791 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.791 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.792 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.792 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.794 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.794 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.795 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.068 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.069 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.069 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.070 I llama_model_loader: - type  f32:  194 tensors
0.00.036.070 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.071 I print_info: file format = GGUF V3 (latest)
0.00.036.071 I print_info: file type   = Q8_0
0.00.036.072 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.318 I load: special tokens cache size = 25
0.00.064.560 I load: token to piece cache size = 0.2984 MB
0.00.064.565 I print_info: arch             = gptneox
0.00.064.565 I print_info: vocab_only       = 0
0.00.064.565 I print_info: n_ctx_train      = 2048
0.00.064.565 I print_info: n_embd           = 2048
0.00.064.566 I print_info: n_layer          = 24
0.00.064.571 I print_info: n_head           = 16
0.00.064.572 I print_info: n_head_kv        = 16
0.00.064.572 I print_info: n_rot            = 32
0.00.064.572 I print_info: n_swa            = 0
0.00.064.573 I print_info: n_embd_head_k    = 128
0.00.064.573 I print_info: n_embd_head_v    = 128
0.00.064.573 I print_info: n_gqa            = 1
0.00.064.574 I print_info: n_embd_k_gqa     = 2048
0.00.064.576 I print_info: n_embd_v_gqa     = 2048
0.00.064.577 I print_info: f_norm_eps       = 1.0e-05
0.00.064.577 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.577 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.577 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.578 I print_info: f_logit_scale    = 0.0e+00
0.00.064.578 I print_info: n_ff             = 8192
0.00.064.578 I print_info: n_expert         = 0
0.00.064.579 I print_info: n_expert_used    = 0
0.00.064.579 I print_info: causal attn      = 1
0.00.064.579 I print_info: pooling type     = 0
0.00.064.579 I print_info: rope type        = 2
0.00.064.579 I print_info: rope scaling     = linear
0.00.064.580 I print_info: freq_base_train  = 10000.0
0.00.064.580 I print_info: freq_scale_train = 1
0.00.064.581 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.581 I print_info: rope_finetuned   = unknown
0.00.064.581 I print_info: ssm_d_conv       = 0
0.00.064.581 I print_info: ssm_d_inner      = 0
0.00.064.582 I print_info: ssm_d_state      = 0
0.00.064.582 I print_info: ssm_dt_rank      = 0
0.00.064.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.582 I print_info: model type       = 1.4B
0.00.064.583 I print_info: model params     = 1.41 B
0.00.064.583 I print_info: general.name     = 1.4B
0.00.064.584 I print_info: vocab type       = BPE
0.00.064.584 I print_info: n_vocab          = 50304
0.00.064.584 I print_info: n_merges         = 50009
0.00.064.585 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.585 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.585 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.585 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.585 I print_info: LF token         = 128 'Ä'
0.00.064.586 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.586 I print_info: max token length = 1024
0.00.067.058 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.058 I load_tensors: offloading output layer to GPU
0.00.067.058 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.070 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.071 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.401 I llama_init_from_model: n_seq_max     = 1
0.00.067.402 I llama_init_from_model: n_ctx         = 2048
0.00.067.402 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.402 I llama_init_from_model: n_batch       = 2048
0.00.067.402 I llama_init_from_model: n_ubatch      = 512
0.00.067.403 I llama_init_from_model: flash_attn    = 0
0.00.067.403 I llama_init_from_model: freq_base     = 10000.0
0.00.067.403 I llama_init_from_model: freq_scale    = 1
0.00.067.404 I ggml_metal_init: allocating
0.00.067.407 I ggml_metal_init: found device: Apple M4
0.00.067.409 I ggml_metal_init: picking default device: Apple M4
0.00.068.180 I ggml_metal_init: using embedded metal library
0.00.070.937 I ggml_metal_init: GPU name:   Apple M4
0.00.070.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.940 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.940 I ggml_metal_init: simdgroup reduction   = true
0.00.070.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.940 I ggml_metal_init: has bfloat            = true
0.00.070.941 I ggml_metal_init: use bfloat            = true
0.00.070.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.942 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.473 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.305 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.313 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.338 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.632 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.109.635 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.109.635 I llama_init_from_model: graph nodes  = 967
0.00.109.635 I llama_init_from_model: graph splits = 2
0.00.109.640 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.755 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.410.194 I main: llama threadpool init, n_threads = 4
0.01.410.265 I 
0.01.410.312 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.410.317 I 
0.01.410.631 I sampler seed: 1234
0.01.410.638 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.410.689 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.410.694 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.410.695 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.565.089 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.02.565.090 I llama_perf_context_print:        load time =    1398.81 ms
0.02.565.091 I llama_perf_context_print: prompt eval time =      50.05 ms /     7 tokens (    7.15 ms per token,   139.86 tokens per second)
0.02.565.091 I llama_perf_context_print:        eval time =    1101.21 ms /    63 runs   (   17.48 ms per token,    57.21 tokens per second)
0.02.565.091 I llama_perf_context_print:       total time =    1156.36 ms /    70 tokens
0.02.565.339 I ggml_metal_free: deallocating

real	0m2.583s
user	0m0.130s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.126 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.174 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.695 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.701 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.703 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.704 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.704 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.706 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.706 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.707 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.707 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.708 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.710 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.044 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.045 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.045 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.045 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.046 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.046 I llama_model_loader: - type  f32:  194 tensors
0.00.035.047 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.048 I print_info: file format = GGUF V3 (latest)
0.00.035.051 I print_info: file type   = Q8_0
0.00.035.052 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.076 I load: special tokens cache size = 25
0.00.064.678 I load: token to piece cache size = 0.2984 MB
0.00.064.681 I print_info: arch             = gptneox
0.00.064.682 I print_info: vocab_only       = 0
0.00.064.682 I print_info: n_ctx_train      = 2048
0.00.064.682 I print_info: n_embd           = 2048
0.00.064.682 I print_info: n_layer          = 24
0.00.064.686 I print_info: n_head           = 16
0.00.064.687 I print_info: n_head_kv        = 16
0.00.064.687 I print_info: n_rot            = 32
0.00.064.687 I print_info: n_swa            = 0
0.00.064.688 I print_info: n_embd_head_k    = 128
0.00.064.688 I print_info: n_embd_head_v    = 128
0.00.064.689 I print_info: n_gqa            = 1
0.00.064.689 I print_info: n_embd_k_gqa     = 2048
0.00.064.690 I print_info: n_embd_v_gqa     = 2048
0.00.064.690 I print_info: f_norm_eps       = 1.0e-05
0.00.064.691 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.691 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.691 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.691 I print_info: f_logit_scale    = 0.0e+00
0.00.064.692 I print_info: n_ff             = 8192
0.00.064.692 I print_info: n_expert         = 0
0.00.064.692 I print_info: n_expert_used    = 0
0.00.064.692 I print_info: causal attn      = 1
0.00.064.693 I print_info: pooling type     = 0
0.00.064.693 I print_info: rope type        = 2
0.00.064.693 I print_info: rope scaling     = linear
0.00.064.693 I print_info: freq_base_train  = 10000.0
0.00.064.696 I print_info: freq_scale_train = 1
0.00.064.696 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.696 I print_info: rope_finetuned   = unknown
0.00.064.696 I print_info: ssm_d_conv       = 0
0.00.064.696 I print_info: ssm_d_inner      = 0
0.00.064.696 I print_info: ssm_d_state      = 0
0.00.064.696 I print_info: ssm_dt_rank      = 0
0.00.064.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.697 I print_info: model type       = 1.4B
0.00.064.697 I print_info: model params     = 1.41 B
0.00.064.697 I print_info: general.name     = 1.4B
0.00.064.698 I print_info: vocab type       = BPE
0.00.064.699 I print_info: n_vocab          = 50304
0.00.064.699 I print_info: n_merges         = 50009
0.00.064.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.700 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.700 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.700 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.704 I print_info: LF token         = 128 'Ä'
0.00.064.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.705 I print_info: max token length = 1024
0.00.067.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.103 I load_tensors: offloading output layer to GPU
0.00.067.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.115 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.117 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.545 I llama_init_from_model: n_seq_max     = 1
0.00.067.546 I llama_init_from_model: n_ctx         = 128
0.00.067.546 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.547 I llama_init_from_model: n_batch       = 128
0.00.067.547 I llama_init_from_model: n_ubatch      = 128
0.00.067.547 I llama_init_from_model: flash_attn    = 0
0.00.067.547 I llama_init_from_model: freq_base     = 10000.0
0.00.067.548 I llama_init_from_model: freq_scale    = 1
0.00.067.548 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.549 I ggml_metal_init: allocating
0.00.067.552 I ggml_metal_init: found device: Apple M4
0.00.067.554 I ggml_metal_init: picking default device: Apple M4
0.00.068.293 I ggml_metal_init: using embedded metal library
0.00.071.042 I ggml_metal_init: GPU name:   Apple M4
0.00.071.044 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.045 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.045 I ggml_metal_init: simdgroup reduction   = true
0.00.071.046 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.046 I ggml_metal_init: has bfloat            = true
0.00.071.046 I ggml_metal_init: use bfloat            = true
0.00.071.046 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.047 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.974 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.580 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.583 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.599 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.668 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.083.669 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.083.669 I llama_init_from_model: graph nodes  = 967
0.00.083.670 I llama_init_from_model: graph splits = 2
0.00.083.671 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.671 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.871.858 I 
0.00.871.903 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.871.916 I perplexity: tokenizing the input ..
0.00.879.931 I perplexity: tokenization took 8.013 ms
0.00.879.942 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.004.130 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.005.302 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.005.321 I llama_perf_context_print:        load time =     859.68 ms
0.01.005.322 I llama_perf_context_print: prompt eval time =     123.96 ms /   128 tokens (    0.97 ms per token,  1032.57 tokens per second)
0.01.005.326 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.005.327 I llama_perf_context_print:       total time =     133.46 ms /   129 tokens
0.01.005.863 I ggml_metal_free: deallocating

real	0m1.024s
user	0m0.092s
sys	0m0.137s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.015.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.859 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.041.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.869 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.870 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.872 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.872 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.873 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.873 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.874 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.876 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.280 I llama_model_loader: - type  f32:  194 tensors
0.00.051.280 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.280 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.281 I print_info: file format = GGUF V3 (latest)
0.00.051.281 I print_info: file type   = Q4_0
0.00.051.282 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.076.900 I load: special tokens cache size = 25
0.00.086.471 I load: token to piece cache size = 0.2984 MB
0.00.086.477 I print_info: arch             = gptneox
0.00.086.477 I print_info: vocab_only       = 0
0.00.086.478 I print_info: n_ctx_train      = 2048
0.00.086.478 I print_info: n_embd           = 2048
0.00.086.478 I print_info: n_layer          = 24
0.00.086.485 I print_info: n_head           = 16
0.00.086.486 I print_info: n_head_kv        = 16
0.00.086.486 I print_info: n_rot            = 32
0.00.086.486 I print_info: n_swa            = 0
0.00.086.486 I print_info: n_embd_head_k    = 128
0.00.086.488 I print_info: n_embd_head_v    = 128
0.00.086.489 I print_info: n_gqa            = 1
0.00.086.490 I print_info: n_embd_k_gqa     = 2048
0.00.086.491 I print_info: n_embd_v_gqa     = 2048
0.00.086.492 I print_info: f_norm_eps       = 1.0e-05
0.00.086.493 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.493 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.493 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.493 I print_info: f_logit_scale    = 0.0e+00
0.00.086.494 I print_info: n_ff             = 8192
0.00.086.495 I print_info: n_expert         = 0
0.00.086.495 I print_info: n_expert_used    = 0
0.00.086.495 I print_info: causal attn      = 1
0.00.086.495 I print_info: pooling type     = 0
0.00.086.495 I print_info: rope type        = 2
0.00.086.496 I print_info: rope scaling     = linear
0.00.086.496 I print_info: freq_base_train  = 10000.0
0.00.086.497 I print_info: freq_scale_train = 1
0.00.086.497 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.497 I print_info: rope_finetuned   = unknown
0.00.086.497 I print_info: ssm_d_conv       = 0
0.00.086.498 I print_info: ssm_d_inner      = 0
0.00.086.498 I print_info: ssm_d_state      = 0
0.00.086.498 I print_info: ssm_dt_rank      = 0
0.00.086.498 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.499 I print_info: model type       = 1.4B
0.00.086.499 I print_info: model params     = 1.41 B
0.00.086.500 I print_info: general.name     = 1.4B
0.00.086.501 I print_info: vocab type       = BPE
0.00.086.501 I print_info: n_vocab          = 50304
0.00.086.501 I print_info: n_merges         = 50009
0.00.086.502 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.502 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.502 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.503 I print_info: LF token         = 128 'Ä'
0.00.086.503 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.503 I print_info: max token length = 1024
0.00.089.443 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.444 I load_tensors: offloading output layer to GPU
0.00.089.444 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.457 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.089.458 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.089.921 I llama_init_from_model: n_seq_max     = 1
0.00.089.923 I llama_init_from_model: n_ctx         = 2048
0.00.089.923 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.089.923 I llama_init_from_model: n_batch       = 2048
0.00.089.924 I llama_init_from_model: n_ubatch      = 512
0.00.089.924 I llama_init_from_model: flash_attn    = 0
0.00.089.925 I llama_init_from_model: freq_base     = 10000.0
0.00.089.925 I llama_init_from_model: freq_scale    = 1
0.00.089.926 I ggml_metal_init: allocating
0.00.089.931 I ggml_metal_init: found device: Apple M4
0.00.089.934 I ggml_metal_init: picking default device: Apple M4
0.00.090.995 I ggml_metal_init: using embedded metal library
0.00.095.048 I ggml_metal_init: GPU name:   Apple M4
0.00.095.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.052 I ggml_metal_init: simdgroup reduction   = true
0.00.095.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.053 I ggml_metal_init: has bfloat            = true
0.00.095.053 I ggml_metal_init: use bfloat            = true
0.00.095.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.306 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.135.985 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.136.007 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.136.043 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.137.189 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.137.191 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.137.192 I llama_init_from_model: graph nodes  = 967
0.00.137.192 I llama_init_from_model: graph splits = 2
0.00.137.197 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.137.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.408 I main: llama threadpool init, n_threads = 4
0.00.828.490 I 
0.00.828.534 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.534 I 
0.00.828.875 I sampler seed: 1234
0.00.828.886 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.828.948 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.828.950 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.828.950 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.546.268 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.01.546.268 I llama_perf_context_print:        load time =     811.40 ms
0.01.546.270 I llama_perf_context_print: prompt eval time =      50.50 ms /     7 tokens (    7.21 ms per token,   138.63 tokens per second)
0.01.546.270 I llama_perf_context_print:        eval time =     663.71 ms /    63 runs   (   10.54 ms per token,    94.92 tokens per second)
0.01.546.270 I llama_perf_context_print:       total time =     719.07 ms /    70 tokens
0.01.546.497 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.135s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.661 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.124 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.125 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.126 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.127 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.127 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.128 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.128 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.129 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.129 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.130 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.132 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.132 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.093 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.169 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.127 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.127 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.127 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.128 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.129 I llama_model_loader: - type  f32:  194 tensors
0.00.027.129 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.129 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.130 I print_info: file format = GGUF V3 (latest)
0.00.027.130 I print_info: file type   = Q4_0
0.00.027.131 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.740 I load: special tokens cache size = 25
0.00.051.882 I load: token to piece cache size = 0.2984 MB
0.00.051.885 I print_info: arch             = gptneox
0.00.051.885 I print_info: vocab_only       = 0
0.00.051.885 I print_info: n_ctx_train      = 2048
0.00.051.885 I print_info: n_embd           = 2048
0.00.051.885 I print_info: n_layer          = 24
0.00.051.889 I print_info: n_head           = 16
0.00.051.889 I print_info: n_head_kv        = 16
0.00.051.890 I print_info: n_rot            = 32
0.00.051.890 I print_info: n_swa            = 0
0.00.051.890 I print_info: n_embd_head_k    = 128
0.00.051.890 I print_info: n_embd_head_v    = 128
0.00.051.891 I print_info: n_gqa            = 1
0.00.051.892 I print_info: n_embd_k_gqa     = 2048
0.00.051.892 I print_info: n_embd_v_gqa     = 2048
0.00.051.893 I print_info: f_norm_eps       = 1.0e-05
0.00.051.893 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.895 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.895 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.895 I print_info: f_logit_scale    = 0.0e+00
0.00.051.896 I print_info: n_ff             = 8192
0.00.051.896 I print_info: n_expert         = 0
0.00.051.896 I print_info: n_expert_used    = 0
0.00.051.896 I print_info: causal attn      = 1
0.00.051.897 I print_info: pooling type     = 0
0.00.051.897 I print_info: rope type        = 2
0.00.051.897 I print_info: rope scaling     = linear
0.00.051.897 I print_info: freq_base_train  = 10000.0
0.00.051.898 I print_info: freq_scale_train = 1
0.00.051.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.898 I print_info: rope_finetuned   = unknown
0.00.051.898 I print_info: ssm_d_conv       = 0
0.00.051.898 I print_info: ssm_d_inner      = 0
0.00.051.899 I print_info: ssm_d_state      = 0
0.00.051.899 I print_info: ssm_dt_rank      = 0
0.00.051.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.899 I print_info: model type       = 1.4B
0.00.051.900 I print_info: model params     = 1.41 B
0.00.051.900 I print_info: general.name     = 1.4B
0.00.051.900 I print_info: vocab type       = BPE
0.00.051.901 I print_info: n_vocab          = 50304
0.00.051.901 I print_info: n_merges         = 50009
0.00.051.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.904 I print_info: LF token         = 128 'Ä'
0.00.051.904 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.904 I print_info: max token length = 1024
0.00.053.821 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.821 I load_tensors: offloading output layer to GPU
0.00.053.821 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.831 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.832 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.106 I llama_init_from_model: n_seq_max     = 1
0.00.054.107 I llama_init_from_model: n_ctx         = 128
0.00.054.107 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.107 I llama_init_from_model: n_batch       = 128
0.00.054.107 I llama_init_from_model: n_ubatch      = 128
0.00.054.108 I llama_init_from_model: flash_attn    = 0
0.00.054.108 I llama_init_from_model: freq_base     = 10000.0
0.00.054.108 I llama_init_from_model: freq_scale    = 1
0.00.054.109 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.109 I ggml_metal_init: allocating
0.00.054.112 I ggml_metal_init: found device: Apple M4
0.00.054.114 I ggml_metal_init: picking default device: Apple M4
0.00.054.671 I ggml_metal_init: using embedded metal library
0.00.056.993 I ggml_metal_init: GPU name:   Apple M4
0.00.056.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.994 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.995 I ggml_metal_init: simdgroup reduction   = true
0.00.056.995 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.995 I ggml_metal_init: has bfloat            = true
0.00.056.995 I ggml_metal_init: use bfloat            = true
0.00.056.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.996 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.829 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.082 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.086 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.102 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.987 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.988 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.989 I llama_init_from_model: graph nodes  = 967
0.00.068.989 I llama_init_from_model: graph splits = 2
0.00.068.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.990 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.164 I 
0.00.616.206 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.216 I perplexity: tokenizing the input ..
0.00.624.498 I perplexity: tokenization took 8.28 ms
0.00.624.509 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.273 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.748.498 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.748.521 I llama_perf_context_print:        load time =     606.50 ms
0.00.748.522 I llama_perf_context_print: prompt eval time =     122.53 ms /   128 tokens (    0.96 ms per token,  1044.61 tokens per second)
0.00.748.523 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.524 I llama_perf_context_print:       total time =     132.36 ms /   129 tokens
0.00.749.030 I ggml_metal_free: deallocating

real	0m0.764s
user	0m0.077s
sys	0m0.090s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.151 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.923 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.934 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.903 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.916 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.849 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.850 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.851 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.851 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.851 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.852 I llama_model_loader: - type  f32:  194 tensors
0.00.028.852 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.852 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.853 I print_info: file format = GGUF V3 (latest)
0.00.028.853 I print_info: file type   = Q4_1
0.00.028.854 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.601 I load: special tokens cache size = 25
0.00.053.816 I load: token to piece cache size = 0.2984 MB
0.00.053.819 I print_info: arch             = gptneox
0.00.053.820 I print_info: vocab_only       = 0
0.00.053.820 I print_info: n_ctx_train      = 2048
0.00.053.820 I print_info: n_embd           = 2048
0.00.053.820 I print_info: n_layer          = 24
0.00.053.824 I print_info: n_head           = 16
0.00.053.824 I print_info: n_head_kv        = 16
0.00.053.824 I print_info: n_rot            = 32
0.00.053.824 I print_info: n_swa            = 0
0.00.053.825 I print_info: n_embd_head_k    = 128
0.00.053.825 I print_info: n_embd_head_v    = 128
0.00.053.826 I print_info: n_gqa            = 1
0.00.053.826 I print_info: n_embd_k_gqa     = 2048
0.00.053.827 I print_info: n_embd_v_gqa     = 2048
0.00.053.830 I print_info: f_norm_eps       = 1.0e-05
0.00.053.830 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.830 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.831 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.831 I print_info: f_logit_scale    = 0.0e+00
0.00.053.832 I print_info: n_ff             = 8192
0.00.053.832 I print_info: n_expert         = 0
0.00.053.832 I print_info: n_expert_used    = 0
0.00.053.832 I print_info: causal attn      = 1
0.00.053.832 I print_info: pooling type     = 0
0.00.053.832 I print_info: rope type        = 2
0.00.053.834 I print_info: rope scaling     = linear
0.00.053.834 I print_info: freq_base_train  = 10000.0
0.00.053.835 I print_info: freq_scale_train = 1
0.00.053.835 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.835 I print_info: rope_finetuned   = unknown
0.00.053.835 I print_info: ssm_d_conv       = 0
0.00.053.835 I print_info: ssm_d_inner      = 0
0.00.053.835 I print_info: ssm_d_state      = 0
0.00.053.835 I print_info: ssm_dt_rank      = 0
0.00.053.836 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.836 I print_info: model type       = 1.4B
0.00.053.836 I print_info: model params     = 1.41 B
0.00.053.836 I print_info: general.name     = 1.4B
0.00.053.837 I print_info: vocab type       = BPE
0.00.053.837 I print_info: n_vocab          = 50304
0.00.053.837 I print_info: n_merges         = 50009
0.00.053.838 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.838 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.838 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.838 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.839 I print_info: LF token         = 128 'Ä'
0.00.053.842 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.842 I print_info: max token length = 1024
0.00.055.431 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.432 I load_tensors: offloading output layer to GPU
0.00.055.432 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.442 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.443 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.055.743 I llama_init_from_model: n_seq_max     = 1
0.00.055.744 I llama_init_from_model: n_ctx         = 2048
0.00.055.744 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.744 I llama_init_from_model: n_batch       = 2048
0.00.055.744 I llama_init_from_model: n_ubatch      = 512
0.00.055.744 I llama_init_from_model: flash_attn    = 0
0.00.055.745 I llama_init_from_model: freq_base     = 10000.0
0.00.055.745 I llama_init_from_model: freq_scale    = 1
0.00.055.746 I ggml_metal_init: allocating
0.00.055.749 I ggml_metal_init: found device: Apple M4
0.00.055.751 I ggml_metal_init: picking default device: Apple M4
0.00.056.357 I ggml_metal_init: using embedded metal library
0.00.058.693 I ggml_metal_init: GPU name:   Apple M4
0.00.058.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.696 I ggml_metal_init: simdgroup reduction   = true
0.00.058.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.696 I ggml_metal_init: has bfloat            = true
0.00.058.696 I ggml_metal_init: use bfloat            = true
0.00.058.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.819 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.664 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.671 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.693 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.752 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.754 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.754 I llama_init_from_model: graph nodes  = 967
0.00.091.755 I llama_init_from_model: graph splits = 2
0.00.091.758 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.903 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.904 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.880 I main: llama threadpool init, n_threads = 4
0.00.751.920 I 
0.00.751.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.942 I 
0.00.752.109 I sampler seed: 1234
0.00.752.116 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.127 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.128 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.128 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.509.401 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.509.401 I llama_perf_context_print:        load time =     739.85 ms
0.01.509.402 I llama_perf_context_print: prompt eval time =      39.64 ms /     7 tokens (    5.66 ms per token,   176.57 tokens per second)
0.01.509.403 I llama_perf_context_print:        eval time =     714.70 ms /    63 runs   (   11.34 ms per token,    88.15 tokens per second)
0.01.509.403 I llama_perf_context_print:       total time =     758.40 ms /    70 tokens
0.01.509.615 I ggml_metal_free: deallocating

real	0m1.528s
user	0m0.112s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.868 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.869 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.869 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.809 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.836 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.838 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.838 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.839 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.839 I llama_model_loader: - type  f32:  194 tensors
0.00.024.840 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.840 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.841 I print_info: file format = GGUF V3 (latest)
0.00.024.841 I print_info: file type   = Q4_1
0.00.024.842 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.432 I load: special tokens cache size = 25
0.00.049.389 I load: token to piece cache size = 0.2984 MB
0.00.049.392 I print_info: arch             = gptneox
0.00.049.392 I print_info: vocab_only       = 0
0.00.049.392 I print_info: n_ctx_train      = 2048
0.00.049.392 I print_info: n_embd           = 2048
0.00.049.393 I print_info: n_layer          = 24
0.00.049.396 I print_info: n_head           = 16
0.00.049.396 I print_info: n_head_kv        = 16
0.00.049.396 I print_info: n_rot            = 32
0.00.049.397 I print_info: n_swa            = 0
0.00.049.399 I print_info: n_embd_head_k    = 128
0.00.049.399 I print_info: n_embd_head_v    = 128
0.00.049.400 I print_info: n_gqa            = 1
0.00.049.401 I print_info: n_embd_k_gqa     = 2048
0.00.049.401 I print_info: n_embd_v_gqa     = 2048
0.00.049.402 I print_info: f_norm_eps       = 1.0e-05
0.00.049.402 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.402 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.402 I print_info: f_logit_scale    = 0.0e+00
0.00.049.403 I print_info: n_ff             = 8192
0.00.049.403 I print_info: n_expert         = 0
0.00.049.403 I print_info: n_expert_used    = 0
0.00.049.404 I print_info: causal attn      = 1
0.00.049.404 I print_info: pooling type     = 0
0.00.049.404 I print_info: rope type        = 2
0.00.049.404 I print_info: rope scaling     = linear
0.00.049.410 I print_info: freq_base_train  = 10000.0
0.00.049.411 I print_info: freq_scale_train = 1
0.00.049.412 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.412 I print_info: rope_finetuned   = unknown
0.00.049.412 I print_info: ssm_d_conv       = 0
0.00.049.414 I print_info: ssm_d_inner      = 0
0.00.049.414 I print_info: ssm_d_state      = 0
0.00.049.414 I print_info: ssm_dt_rank      = 0
0.00.049.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.414 I print_info: model type       = 1.4B
0.00.049.415 I print_info: model params     = 1.41 B
0.00.049.415 I print_info: general.name     = 1.4B
0.00.049.415 I print_info: vocab type       = BPE
0.00.049.416 I print_info: n_vocab          = 50304
0.00.049.416 I print_info: n_merges         = 50009
0.00.049.416 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.417 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.417 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.418 I print_info: LF token         = 128 'Ä'
0.00.049.419 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.419 I print_info: max token length = 1024
0.00.051.406 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.406 I load_tensors: offloading output layer to GPU
0.00.051.406 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.417 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.418 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.728 I llama_init_from_model: n_seq_max     = 1
0.00.051.729 I llama_init_from_model: n_ctx         = 128
0.00.051.729 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.729 I llama_init_from_model: n_batch       = 128
0.00.051.729 I llama_init_from_model: n_ubatch      = 128
0.00.051.730 I llama_init_from_model: flash_attn    = 0
0.00.051.730 I llama_init_from_model: freq_base     = 10000.0
0.00.051.730 I llama_init_from_model: freq_scale    = 1
0.00.051.731 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.731 I ggml_metal_init: allocating
0.00.051.734 I ggml_metal_init: found device: Apple M4
0.00.051.736 I ggml_metal_init: picking default device: Apple M4
0.00.052.299 I ggml_metal_init: using embedded metal library
0.00.054.641 I ggml_metal_init: GPU name:   Apple M4
0.00.054.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.643 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.644 I ggml_metal_init: simdgroup reduction   = true
0.00.054.644 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.644 I ggml_metal_init: has bfloat            = true
0.00.054.644 I ggml_metal_init: use bfloat            = true
0.00.054.645 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.251 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.497 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.502 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.517 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.433 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.434 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.434 I llama_init_from_model: graph nodes  = 967
0.00.066.434 I llama_init_from_model: graph splits = 2
0.00.066.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.641 I 
0.00.659.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.785 I perplexity: tokenizing the input ..
0.00.667.283 I perplexity: tokenization took 7.495 ms
0.00.667.293 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.955 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.791.150 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.791.165 I llama_perf_context_print:        load time =     650.89 ms
0.00.791.167 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.45 tokens per second)
0.00.791.167 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.168 I llama_perf_context_print:       total time =     131.53 ms /   129 tokens
0.00.791.648 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.077s
sys	0m0.099s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.013.610 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.021.136 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.138 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.138 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.139 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.139 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.140 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.141 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.141 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.142 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.142 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.145 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.146 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.146 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.189 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.196 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.030.199 I llama_model_loader: - type  f32:  194 tensors
0.00.030.199 I llama_model_loader: - type q5_0:   97 tensors
0.00.030.199 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.200 I print_info: file format = GGUF V3 (latest)
0.00.030.201 I print_info: file type   = Q5_0
0.00.030.202 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.048.973 I load: special tokens cache size = 25
0.00.055.045 I load: token to piece cache size = 0.2984 MB
0.00.055.047 I print_info: arch             = gptneox
0.00.055.047 I print_info: vocab_only       = 0
0.00.055.048 I print_info: n_ctx_train      = 2048
0.00.055.048 I print_info: n_embd           = 2048
0.00.055.048 I print_info: n_layer          = 24
0.00.055.051 I print_info: n_head           = 16
0.00.055.052 I print_info: n_head_kv        = 16
0.00.055.052 I print_info: n_rot            = 32
0.00.055.052 I print_info: n_swa            = 0
0.00.055.053 I print_info: n_embd_head_k    = 128
0.00.055.053 I print_info: n_embd_head_v    = 128
0.00.055.053 I print_info: n_gqa            = 1
0.00.055.054 I print_info: n_embd_k_gqa     = 2048
0.00.055.055 I print_info: n_embd_v_gqa     = 2048
0.00.055.055 I print_info: f_norm_eps       = 1.0e-05
0.00.055.056 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.058 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.058 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.058 I print_info: f_logit_scale    = 0.0e+00
0.00.055.059 I print_info: n_ff             = 8192
0.00.055.059 I print_info: n_expert         = 0
0.00.055.059 I print_info: n_expert_used    = 0
0.00.055.059 I print_info: causal attn      = 1
0.00.055.059 I print_info: pooling type     = 0
0.00.055.060 I print_info: rope type        = 2
0.00.055.062 I print_info: rope scaling     = linear
0.00.055.063 I print_info: freq_base_train  = 10000.0
0.00.055.063 I print_info: freq_scale_train = 1
0.00.055.063 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.063 I print_info: rope_finetuned   = unknown
0.00.055.063 I print_info: ssm_d_conv       = 0
0.00.055.063 I print_info: ssm_d_inner      = 0
0.00.055.064 I print_info: ssm_d_state      = 0
0.00.055.064 I print_info: ssm_dt_rank      = 0
0.00.055.064 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.064 I print_info: model type       = 1.4B
0.00.055.065 I print_info: model params     = 1.41 B
0.00.055.065 I print_info: general.name     = 1.4B
0.00.055.065 I print_info: vocab type       = BPE
0.00.055.065 I print_info: n_vocab          = 50304
0.00.055.066 I print_info: n_merges         = 50009
0.00.055.067 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.067 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.073 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.073 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.074 I print_info: LF token         = 128 'Ä'
0.00.055.074 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.074 I print_info: max token length = 1024
0.00.056.823 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.823 I load_tensors: offloading output layer to GPU
0.00.056.823 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.833 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.056.835 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.057.111 I llama_init_from_model: n_seq_max     = 1
0.00.057.111 I llama_init_from_model: n_ctx         = 2048
0.00.057.112 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.057.112 I llama_init_from_model: n_batch       = 2048
0.00.057.112 I llama_init_from_model: n_ubatch      = 512
0.00.057.112 I llama_init_from_model: flash_attn    = 0
0.00.057.113 I llama_init_from_model: freq_base     = 10000.0
0.00.057.113 I llama_init_from_model: freq_scale    = 1
0.00.057.113 I ggml_metal_init: allocating
0.00.057.116 I ggml_metal_init: found device: Apple M4
0.00.057.118 I ggml_metal_init: picking default device: Apple M4
0.00.057.730 I ggml_metal_init: using embedded metal library
0.00.060.094 I ggml_metal_init: GPU name:   Apple M4
0.00.060.096 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.096 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.096 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.097 I ggml_metal_init: simdgroup reduction   = true
0.00.060.097 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.097 I ggml_metal_init: has bfloat            = true
0.00.060.097 I ggml_metal_init: use bfloat            = true
0.00.060.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.098 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.824 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.232 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.241 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.264 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.093.370 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.093.371 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.093.372 I llama_init_from_model: graph nodes  = 967
0.00.093.372 I llama_init_from_model: graph splits = 2
0.00.093.375 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.502 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.759 I main: llama threadpool init, n_threads = 4
0.00.847.794 I 
0.00.847.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.817 I 
0.00.848.050 I sampler seed: 1234
0.00.848.055 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.848.064 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.848.065 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.848.065 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.632.175 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.632.176 I llama_perf_context_print:        load time =     833.28 ms
0.01.632.176 I llama_perf_context_print: prompt eval time =      43.17 ms /     7 tokens (    6.17 ms per token,   162.13 tokens per second)
0.01.632.177 I llama_perf_context_print:        eval time =     737.89 ms /    63 runs   (   11.71 ms per token,    85.38 tokens per second)
0.01.632.177 I llama_perf_context_print:       total time =     785.28 ms /    70 tokens
0.01.632.434 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.110s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.658 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.462 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.469 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.469 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.470 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.470 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.471 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.471 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.472 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.472 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.473 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.473 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.473 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.475 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.534 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.512 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.512 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.513 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.513 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.513 I llama_model_loader: - type  f32:  194 tensors
0.00.026.514 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.514 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.514 I print_info: file format = GGUF V3 (latest)
0.00.026.515 I print_info: file type   = Q5_0
0.00.026.515 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.167 I load: special tokens cache size = 25
0.00.050.919 I load: token to piece cache size = 0.2984 MB
0.00.050.922 I print_info: arch             = gptneox
0.00.050.922 I print_info: vocab_only       = 0
0.00.050.922 I print_info: n_ctx_train      = 2048
0.00.050.922 I print_info: n_embd           = 2048
0.00.050.923 I print_info: n_layer          = 24
0.00.050.925 I print_info: n_head           = 16
0.00.050.926 I print_info: n_head_kv        = 16
0.00.050.926 I print_info: n_rot            = 32
0.00.050.927 I print_info: n_swa            = 0
0.00.050.929 I print_info: n_embd_head_k    = 128
0.00.050.929 I print_info: n_embd_head_v    = 128
0.00.050.930 I print_info: n_gqa            = 1
0.00.050.931 I print_info: n_embd_k_gqa     = 2048
0.00.050.932 I print_info: n_embd_v_gqa     = 2048
0.00.050.932 I print_info: f_norm_eps       = 1.0e-05
0.00.050.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.933 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.933 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.933 I print_info: f_logit_scale    = 0.0e+00
0.00.050.934 I print_info: n_ff             = 8192
0.00.050.934 I print_info: n_expert         = 0
0.00.050.934 I print_info: n_expert_used    = 0
0.00.050.934 I print_info: causal attn      = 1
0.00.050.934 I print_info: pooling type     = 0
0.00.050.934 I print_info: rope type        = 2
0.00.050.934 I print_info: rope scaling     = linear
0.00.050.935 I print_info: freq_base_train  = 10000.0
0.00.050.936 I print_info: freq_scale_train = 1
0.00.050.940 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.940 I print_info: rope_finetuned   = unknown
0.00.050.940 I print_info: ssm_d_conv       = 0
0.00.050.941 I print_info: ssm_d_inner      = 0
0.00.050.941 I print_info: ssm_d_state      = 0
0.00.050.941 I print_info: ssm_dt_rank      = 0
0.00.050.941 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.941 I print_info: model type       = 1.4B
0.00.050.942 I print_info: model params     = 1.41 B
0.00.050.942 I print_info: general.name     = 1.4B
0.00.050.943 I print_info: vocab type       = BPE
0.00.050.943 I print_info: n_vocab          = 50304
0.00.050.943 I print_info: n_merges         = 50009
0.00.050.949 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.951 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.951 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.951 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.952 I print_info: LF token         = 128 'Ä'
0.00.050.952 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.952 I print_info: max token length = 1024
0.00.052.856 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.856 I load_tensors: offloading output layer to GPU
0.00.052.857 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.867 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.868 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.159 I llama_init_from_model: n_seq_max     = 1
0.00.053.160 I llama_init_from_model: n_ctx         = 128
0.00.053.160 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.160 I llama_init_from_model: n_batch       = 128
0.00.053.160 I llama_init_from_model: n_ubatch      = 128
0.00.053.160 I llama_init_from_model: flash_attn    = 0
0.00.053.161 I llama_init_from_model: freq_base     = 10000.0
0.00.053.161 I llama_init_from_model: freq_scale    = 1
0.00.053.161 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.162 I ggml_metal_init: allocating
0.00.053.164 I ggml_metal_init: found device: Apple M4
0.00.053.166 I ggml_metal_init: picking default device: Apple M4
0.00.053.731 I ggml_metal_init: using embedded metal library
0.00.056.049 I ggml_metal_init: GPU name:   Apple M4
0.00.056.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.051 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.052 I ggml_metal_init: simdgroup reduction   = true
0.00.056.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.052 I ggml_metal_init: has bfloat            = true
0.00.056.052 I ggml_metal_init: use bfloat            = true
0.00.056.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.470 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.726 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.730 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.747 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.629 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.630 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.630 I llama_init_from_model: graph nodes  = 967
0.00.067.630 I llama_init_from_model: graph splits = 2
0.00.067.631 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.631 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.052 I 
0.00.643.093 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.102 I perplexity: tokenizing the input ..
0.00.651.180 I perplexity: tokenization took 8.076 ms
0.00.651.193 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.494 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.787.794 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.787.805 I llama_perf_context_print:        load time =     632.39 ms
0.00.787.806 I llama_perf_context_print: prompt eval time =     135.07 ms /   128 tokens (    1.06 ms per token,   947.64 tokens per second)
0.00.787.807 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.807 I llama_perf_context_print:       total time =     144.76 ms /   129 tokens
0.00.788.126 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.077s
sys	0m0.098s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.483 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.979 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.983 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.986 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.986 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.986 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.987 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.988 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.994 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.978 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.053 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.988 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.989 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.989 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.990 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.990 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.990 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.991 I llama_model_loader: - type  f32:  194 tensors
0.00.026.991 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.992 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.992 I print_info: file format = GGUF V3 (latest)
0.00.026.993 I print_info: file type   = Q5_1
0.00.026.995 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.729 I load: special tokens cache size = 25
0.00.051.990 I load: token to piece cache size = 0.2984 MB
0.00.051.993 I print_info: arch             = gptneox
0.00.051.994 I print_info: vocab_only       = 0
0.00.051.994 I print_info: n_ctx_train      = 2048
0.00.051.994 I print_info: n_embd           = 2048
0.00.051.994 I print_info: n_layer          = 24
0.00.051.998 I print_info: n_head           = 16
0.00.051.998 I print_info: n_head_kv        = 16
0.00.051.999 I print_info: n_rot            = 32
0.00.051.999 I print_info: n_swa            = 0
0.00.051.999 I print_info: n_embd_head_k    = 128
0.00.052.001 I print_info: n_embd_head_v    = 128
0.00.052.002 I print_info: n_gqa            = 1
0.00.052.003 I print_info: n_embd_k_gqa     = 2048
0.00.052.004 I print_info: n_embd_v_gqa     = 2048
0.00.052.004 I print_info: f_norm_eps       = 1.0e-05
0.00.052.005 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.005 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.005 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.005 I print_info: f_logit_scale    = 0.0e+00
0.00.052.006 I print_info: n_ff             = 8192
0.00.052.006 I print_info: n_expert         = 0
0.00.052.006 I print_info: n_expert_used    = 0
0.00.052.006 I print_info: causal attn      = 1
0.00.052.006 I print_info: pooling type     = 0
0.00.052.008 I print_info: rope type        = 2
0.00.052.010 I print_info: rope scaling     = linear
0.00.052.010 I print_info: freq_base_train  = 10000.0
0.00.052.010 I print_info: freq_scale_train = 1
0.00.052.011 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.011 I print_info: rope_finetuned   = unknown
0.00.052.011 I print_info: ssm_d_conv       = 0
0.00.052.011 I print_info: ssm_d_inner      = 0
0.00.052.011 I print_info: ssm_d_state      = 0
0.00.052.016 I print_info: ssm_dt_rank      = 0
0.00.052.016 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.016 I print_info: model type       = 1.4B
0.00.052.016 I print_info: model params     = 1.41 B
0.00.052.016 I print_info: general.name     = 1.4B
0.00.052.017 I print_info: vocab type       = BPE
0.00.052.017 I print_info: n_vocab          = 50304
0.00.052.017 I print_info: n_merges         = 50009
0.00.052.017 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.018 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.018 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.018 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.018 I print_info: LF token         = 128 'Ä'
0.00.052.018 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.024 I print_info: max token length = 1024
0.00.053.964 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.965 I load_tensors: offloading output layer to GPU
0.00.053.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.975 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.976 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.054.249 I llama_init_from_model: n_seq_max     = 1
0.00.054.250 I llama_init_from_model: n_ctx         = 2048
0.00.054.250 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.250 I llama_init_from_model: n_batch       = 2048
0.00.054.250 I llama_init_from_model: n_ubatch      = 512
0.00.054.251 I llama_init_from_model: flash_attn    = 0
0.00.054.251 I llama_init_from_model: freq_base     = 10000.0
0.00.054.251 I llama_init_from_model: freq_scale    = 1
0.00.054.252 I ggml_metal_init: allocating
0.00.054.255 I ggml_metal_init: found device: Apple M4
0.00.054.257 I ggml_metal_init: picking default device: Apple M4
0.00.054.861 I ggml_metal_init: using embedded metal library
0.00.057.231 I ggml_metal_init: GPU name:   Apple M4
0.00.057.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.233 I ggml_metal_init: simdgroup reduction   = true
0.00.057.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.233 I ggml_metal_init: has bfloat            = true
0.00.057.233 I ggml_metal_init: use bfloat            = true
0.00.057.234 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.938 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.462 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.469 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.487 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.652 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.653 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.654 I llama_init_from_model: graph nodes  = 967
0.00.087.654 I llama_init_from_model: graph splits = 2
0.00.087.657 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.783 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.570 I main: llama threadpool init, n_threads = 4
0.00.709.604 I 
0.00.709.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.631 I 
0.00.709.866 I sampler seed: 1234
0.00.709.871 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.709.881 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.709.882 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.709.882 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.538.751 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49685.09 tokens per second)
0.01.538.752 I llama_perf_context_print:        load time =     698.21 ms
0.01.538.753 I llama_perf_context_print: prompt eval time =      42.20 ms /     7 tokens (    6.03 ms per token,   165.87 tokens per second)
0.01.538.754 I llama_perf_context_print:        eval time =     784.23 ms /    63 runs   (   12.45 ms per token,    80.33 tokens per second)
0.01.538.754 I llama_perf_context_print:       total time =     830.06 ms /    70 tokens
0.01.539.060 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.078 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.002 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.008 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.012 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.014 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.014 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.015 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.017 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.018 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.016 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.017 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.018 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.018 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.019 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.019 I llama_model_loader: - type  f32:  194 tensors
0.00.025.020 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.020 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.021 I print_info: file format = GGUF V3 (latest)
0.00.025.021 I print_info: file type   = Q5_1
0.00.025.022 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.425 I load: special tokens cache size = 25
0.00.050.713 I load: token to piece cache size = 0.2984 MB
0.00.050.717 I print_info: arch             = gptneox
0.00.050.718 I print_info: vocab_only       = 0
0.00.050.718 I print_info: n_ctx_train      = 2048
0.00.050.718 I print_info: n_embd           = 2048
0.00.050.718 I print_info: n_layer          = 24
0.00.050.721 I print_info: n_head           = 16
0.00.050.722 I print_info: n_head_kv        = 16
0.00.050.722 I print_info: n_rot            = 32
0.00.050.722 I print_info: n_swa            = 0
0.00.050.723 I print_info: n_embd_head_k    = 128
0.00.050.723 I print_info: n_embd_head_v    = 128
0.00.050.724 I print_info: n_gqa            = 1
0.00.050.724 I print_info: n_embd_k_gqa     = 2048
0.00.050.725 I print_info: n_embd_v_gqa     = 2048
0.00.050.725 I print_info: f_norm_eps       = 1.0e-05
0.00.050.726 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.726 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.726 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.726 I print_info: f_logit_scale    = 0.0e+00
0.00.050.729 I print_info: n_ff             = 8192
0.00.050.729 I print_info: n_expert         = 0
0.00.050.729 I print_info: n_expert_used    = 0
0.00.050.729 I print_info: causal attn      = 1
0.00.050.729 I print_info: pooling type     = 0
0.00.050.730 I print_info: rope type        = 2
0.00.050.730 I print_info: rope scaling     = linear
0.00.050.730 I print_info: freq_base_train  = 10000.0
0.00.050.731 I print_info: freq_scale_train = 1
0.00.050.731 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.731 I print_info: rope_finetuned   = unknown
0.00.050.731 I print_info: ssm_d_conv       = 0
0.00.050.732 I print_info: ssm_d_inner      = 0
0.00.050.732 I print_info: ssm_d_state      = 0
0.00.050.732 I print_info: ssm_dt_rank      = 0
0.00.050.732 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.732 I print_info: model type       = 1.4B
0.00.050.733 I print_info: model params     = 1.41 B
0.00.050.733 I print_info: general.name     = 1.4B
0.00.050.733 I print_info: vocab type       = BPE
0.00.050.734 I print_info: n_vocab          = 50304
0.00.050.734 I print_info: n_merges         = 50009
0.00.050.734 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.734 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.734 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.735 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.735 I print_info: LF token         = 128 'Ä'
0.00.050.736 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.736 I print_info: max token length = 1024
0.00.052.896 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.897 I load_tensors: offloading output layer to GPU
0.00.052.897 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.908 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.909 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.208 I llama_init_from_model: n_seq_max     = 1
0.00.053.209 I llama_init_from_model: n_ctx         = 128
0.00.053.209 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.210 I llama_init_from_model: n_batch       = 128
0.00.053.210 I llama_init_from_model: n_ubatch      = 128
0.00.053.210 I llama_init_from_model: flash_attn    = 0
0.00.053.210 I llama_init_from_model: freq_base     = 10000.0
0.00.053.211 I llama_init_from_model: freq_scale    = 1
0.00.053.211 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.211 I ggml_metal_init: allocating
0.00.053.215 I ggml_metal_init: found device: Apple M4
0.00.053.217 I ggml_metal_init: picking default device: Apple M4
0.00.053.829 I ggml_metal_init: using embedded metal library
0.00.056.212 I ggml_metal_init: GPU name:   Apple M4
0.00.056.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.215 I ggml_metal_init: simdgroup reduction   = true
0.00.056.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.215 I ggml_metal_init: has bfloat            = true
0.00.056.215 I ggml_metal_init: use bfloat            = true
0.00.056.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.216 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.067 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.318 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.320 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.336 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.280 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.281 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.281 I llama_init_from_model: graph nodes  = 967
0.00.068.281 I llama_init_from_model: graph splits = 2
0.00.068.283 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.634.989 I 
0.00.635.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.089 I perplexity: tokenizing the input ..
0.00.643.217 I perplexity: tokenization took 8.126 ms
0.00.643.233 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.149 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.779.319 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.779.345 I llama_perf_context_print:        load time =     625.91 ms
0.00.779.345 I llama_perf_context_print: prompt eval time =     134.69 ms /   128 tokens (    1.05 ms per token,   950.34 tokens per second)
0.00.779.346 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.346 I llama_perf_context_print:       total time =     144.36 ms /   129 tokens
0.00.779.786 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.079s
sys	0m0.110s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.489 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.265 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.272 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.274 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.275 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.276 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.277 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.281 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.559 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.624 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.626 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.627 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.627 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.628 I llama_model_loader: - type  f32:  194 tensors
0.00.027.628 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.629 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.629 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.629 I print_info: file format = GGUF V3 (latest)
0.00.027.632 I print_info: file type   = Q2_K - Medium
0.00.027.633 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.047.762 I load: special tokens cache size = 25
0.00.053.967 I load: token to piece cache size = 0.2984 MB
0.00.053.971 I print_info: arch             = gptneox
0.00.053.972 I print_info: vocab_only       = 0
0.00.053.972 I print_info: n_ctx_train      = 2048
0.00.053.972 I print_info: n_embd           = 2048
0.00.053.972 I print_info: n_layer          = 24
0.00.053.977 I print_info: n_head           = 16
0.00.053.978 I print_info: n_head_kv        = 16
0.00.053.978 I print_info: n_rot            = 32
0.00.053.978 I print_info: n_swa            = 0
0.00.053.980 I print_info: n_embd_head_k    = 128
0.00.053.980 I print_info: n_embd_head_v    = 128
0.00.053.982 I print_info: n_gqa            = 1
0.00.053.983 I print_info: n_embd_k_gqa     = 2048
0.00.053.984 I print_info: n_embd_v_gqa     = 2048
0.00.053.984 I print_info: f_norm_eps       = 1.0e-05
0.00.053.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.985 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.985 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.985 I print_info: f_logit_scale    = 0.0e+00
0.00.053.986 I print_info: n_ff             = 8192
0.00.053.986 I print_info: n_expert         = 0
0.00.053.986 I print_info: n_expert_used    = 0
0.00.053.987 I print_info: causal attn      = 1
0.00.053.987 I print_info: pooling type     = 0
0.00.053.987 I print_info: rope type        = 2
0.00.053.987 I print_info: rope scaling     = linear
0.00.053.988 I print_info: freq_base_train  = 10000.0
0.00.053.988 I print_info: freq_scale_train = 1
0.00.053.988 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.989 I print_info: rope_finetuned   = unknown
0.00.053.989 I print_info: ssm_d_conv       = 0
0.00.053.989 I print_info: ssm_d_inner      = 0
0.00.053.989 I print_info: ssm_d_state      = 0
0.00.053.989 I print_info: ssm_dt_rank      = 0
0.00.053.989 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.990 I print_info: model type       = 1.4B
0.00.053.990 I print_info: model params     = 1.41 B
0.00.053.990 I print_info: general.name     = 1.4B
0.00.053.991 I print_info: vocab type       = BPE
0.00.053.991 I print_info: n_vocab          = 50304
0.00.053.991 I print_info: n_merges         = 50009
0.00.053.991 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.991 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.992 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.992 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.992 I print_info: LF token         = 128 'Ä'
0.00.053.992 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.992 I print_info: max token length = 1024
0.00.055.838 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.838 I load_tensors: offloading output layer to GPU
0.00.055.838 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.849 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.851 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.056.225 I llama_init_from_model: n_seq_max     = 1
0.00.056.226 I llama_init_from_model: n_ctx         = 2048
0.00.056.226 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.226 I llama_init_from_model: n_batch       = 2048
0.00.056.227 I llama_init_from_model: n_ubatch      = 512
0.00.056.227 I llama_init_from_model: flash_attn    = 0
0.00.056.227 I llama_init_from_model: freq_base     = 10000.0
0.00.056.229 I llama_init_from_model: freq_scale    = 1
0.00.056.230 I ggml_metal_init: allocating
0.00.056.234 I ggml_metal_init: found device: Apple M4
0.00.056.236 I ggml_metal_init: picking default device: Apple M4
0.00.056.882 I ggml_metal_init: using embedded metal library
0.00.059.305 I ggml_metal_init: GPU name:   Apple M4
0.00.059.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.308 I ggml_metal_init: simdgroup reduction   = true
0.00.059.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.308 I ggml_metal_init: has bfloat            = true
0.00.059.308 I ggml_metal_init: use bfloat            = true
0.00.059.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.561 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.020 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.035 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.074 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.023 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.025 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.025 I llama_init_from_model: graph nodes  = 967
0.00.090.026 I llama_init_from_model: graph splits = 2
0.00.090.029 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.157 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.017 I main: llama threadpool init, n_threads = 4
0.00.438.069 I 
0.00.438.097 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.438.099 I 
0.00.438.307 I sampler seed: 1234
0.00.438.314 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.438.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.438.349 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.438.349 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.111.644 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.111.645 I llama_perf_context_print:        load time =     425.60 ms
0.01.111.645 I llama_perf_context_print: prompt eval time =      36.00 ms /     7 tokens (    5.14 ms per token,   194.42 tokens per second)
0.01.111.646 I llama_perf_context_print:        eval time =     634.28 ms /    63 runs   (   10.07 ms per token,    99.32 tokens per second)
0.01.111.650 I llama_perf_context_print:       total time =     674.55 ms /    70 tokens
0.01.111.846 I ggml_metal_free: deallocating

real	0m1.132s
user	0m0.115s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.652 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.667 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.667 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.381 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.381 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.382 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.383 I llama_model_loader: - type  f32:  194 tensors
0.00.026.383 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.383 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.383 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.384 I print_info: file format = GGUF V3 (latest)
0.00.026.384 I print_info: file type   = Q2_K - Medium
0.00.026.385 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.799 I load: special tokens cache size = 25
0.00.052.070 I load: token to piece cache size = 0.2984 MB
0.00.052.073 I print_info: arch             = gptneox
0.00.052.073 I print_info: vocab_only       = 0
0.00.052.074 I print_info: n_ctx_train      = 2048
0.00.052.074 I print_info: n_embd           = 2048
0.00.052.074 I print_info: n_layer          = 24
0.00.052.077 I print_info: n_head           = 16
0.00.052.078 I print_info: n_head_kv        = 16
0.00.052.079 I print_info: n_rot            = 32
0.00.052.079 I print_info: n_swa            = 0
0.00.052.079 I print_info: n_embd_head_k    = 128
0.00.052.081 I print_info: n_embd_head_v    = 128
0.00.052.082 I print_info: n_gqa            = 1
0.00.052.083 I print_info: n_embd_k_gqa     = 2048
0.00.052.083 I print_info: n_embd_v_gqa     = 2048
0.00.052.085 I print_info: f_norm_eps       = 1.0e-05
0.00.052.086 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.086 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.086 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.086 I print_info: f_logit_scale    = 0.0e+00
0.00.052.087 I print_info: n_ff             = 8192
0.00.052.087 I print_info: n_expert         = 0
0.00.052.087 I print_info: n_expert_used    = 0
0.00.052.087 I print_info: causal attn      = 1
0.00.052.087 I print_info: pooling type     = 0
0.00.052.088 I print_info: rope type        = 2
0.00.052.088 I print_info: rope scaling     = linear
0.00.052.088 I print_info: freq_base_train  = 10000.0
0.00.052.088 I print_info: freq_scale_train = 1
0.00.052.089 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.089 I print_info: rope_finetuned   = unknown
0.00.052.090 I print_info: ssm_d_conv       = 0
0.00.052.090 I print_info: ssm_d_inner      = 0
0.00.052.091 I print_info: ssm_d_state      = 0
0.00.052.091 I print_info: ssm_dt_rank      = 0
0.00.052.091 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.091 I print_info: model type       = 1.4B
0.00.052.091 I print_info: model params     = 1.41 B
0.00.052.091 I print_info: general.name     = 1.4B
0.00.052.092 I print_info: vocab type       = BPE
0.00.052.092 I print_info: n_vocab          = 50304
0.00.052.092 I print_info: n_merges         = 50009
0.00.052.093 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.093 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.093 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.093 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.093 I print_info: LF token         = 128 'Ä'
0.00.052.094 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.094 I print_info: max token length = 1024
0.00.053.895 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.895 I load_tensors: offloading output layer to GPU
0.00.053.895 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.906 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.907 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.054.173 I llama_init_from_model: n_seq_max     = 1
0.00.054.174 I llama_init_from_model: n_ctx         = 128
0.00.054.174 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.174 I llama_init_from_model: n_batch       = 128
0.00.054.174 I llama_init_from_model: n_ubatch      = 128
0.00.054.174 I llama_init_from_model: flash_attn    = 0
0.00.054.175 I llama_init_from_model: freq_base     = 10000.0
0.00.054.175 I llama_init_from_model: freq_scale    = 1
0.00.054.175 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.176 I ggml_metal_init: allocating
0.00.054.178 I ggml_metal_init: found device: Apple M4
0.00.054.180 I ggml_metal_init: picking default device: Apple M4
0.00.054.739 I ggml_metal_init: using embedded metal library
0.00.057.080 I ggml_metal_init: GPU name:   Apple M4
0.00.057.082 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.082 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.083 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.083 I ggml_metal_init: simdgroup reduction   = true
0.00.057.083 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.083 I ggml_metal_init: has bfloat            = true
0.00.057.083 I ggml_metal_init: use bfloat            = true
0.00.057.084 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.372 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.578 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.595 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.431 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.432 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.432 I llama_init_from_model: graph nodes  = 967
0.00.068.432 I llama_init_from_model: graph splits = 2
0.00.068.434 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.376.664 I 
0.00.376.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.376.720 I perplexity: tokenizing the input ..
0.00.384.107 I perplexity: tokenization took 7.384 ms
0.00.384.118 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.516.866 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.518.121 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.518.129 I llama_perf_context_print:        load time =     365.85 ms
0.00.518.130 I llama_perf_context_print: prompt eval time =     132.52 ms /   128 tokens (    1.04 ms per token,   965.89 tokens per second)
0.00.518.130 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.518.131 I llama_perf_context_print:       total time =     141.47 ms /   129 tokens
0.00.518.479 I ggml_metal_free: deallocating

real	0m0.533s
user	0m0.077s
sys	0m0.065s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.354 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.356 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.357 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.357 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.365 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.836 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.836 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.837 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.837 I llama_model_loader: - type  f32:  194 tensors
0.00.024.838 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.838 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.838 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.838 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.840 I print_info: file format = GGUF V3 (latest)
0.00.024.841 I print_info: file type   = Q3_K - Medium
0.00.024.842 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.499 I load: special tokens cache size = 25
0.00.049.602 I load: token to piece cache size = 0.2984 MB
0.00.049.605 I print_info: arch             = gptneox
0.00.049.605 I print_info: vocab_only       = 0
0.00.049.606 I print_info: n_ctx_train      = 2048
0.00.049.606 I print_info: n_embd           = 2048
0.00.049.606 I print_info: n_layer          = 24
0.00.049.609 I print_info: n_head           = 16
0.00.049.609 I print_info: n_head_kv        = 16
0.00.049.609 I print_info: n_rot            = 32
0.00.049.610 I print_info: n_swa            = 0
0.00.049.610 I print_info: n_embd_head_k    = 128
0.00.049.610 I print_info: n_embd_head_v    = 128
0.00.049.611 I print_info: n_gqa            = 1
0.00.049.612 I print_info: n_embd_k_gqa     = 2048
0.00.049.612 I print_info: n_embd_v_gqa     = 2048
0.00.049.613 I print_info: f_norm_eps       = 1.0e-05
0.00.049.613 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.614 I print_info: f_logit_scale    = 0.0e+00
0.00.049.615 I print_info: n_ff             = 8192
0.00.049.615 I print_info: n_expert         = 0
0.00.049.615 I print_info: n_expert_used    = 0
0.00.049.617 I print_info: causal attn      = 1
0.00.049.619 I print_info: pooling type     = 0
0.00.049.619 I print_info: rope type        = 2
0.00.049.619 I print_info: rope scaling     = linear
0.00.049.619 I print_info: freq_base_train  = 10000.0
0.00.049.620 I print_info: freq_scale_train = 1
0.00.049.620 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.620 I print_info: rope_finetuned   = unknown
0.00.049.620 I print_info: ssm_d_conv       = 0
0.00.049.620 I print_info: ssm_d_inner      = 0
0.00.049.621 I print_info: ssm_d_state      = 0
0.00.049.621 I print_info: ssm_dt_rank      = 0
0.00.049.621 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.621 I print_info: model type       = 1.4B
0.00.049.621 I print_info: model params     = 1.41 B
0.00.049.622 I print_info: general.name     = 1.4B
0.00.049.622 I print_info: vocab type       = BPE
0.00.049.623 I print_info: n_vocab          = 50304
0.00.049.623 I print_info: n_merges         = 50009
0.00.049.623 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.625 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.625 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.625 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.625 I print_info: LF token         = 128 'Ä'
0.00.049.625 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.626 I print_info: max token length = 1024
0.00.051.488 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.488 I load_tensors: offloading output layer to GPU
0.00.051.488 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.499 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.500 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.780 I llama_init_from_model: n_seq_max     = 1
0.00.051.781 I llama_init_from_model: n_ctx         = 2048
0.00.051.781 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.781 I llama_init_from_model: n_batch       = 2048
0.00.051.781 I llama_init_from_model: n_ubatch      = 512
0.00.051.782 I llama_init_from_model: flash_attn    = 0
0.00.051.782 I llama_init_from_model: freq_base     = 10000.0
0.00.051.782 I llama_init_from_model: freq_scale    = 1
0.00.051.783 I ggml_metal_init: allocating
0.00.051.786 I ggml_metal_init: found device: Apple M4
0.00.051.788 I ggml_metal_init: picking default device: Apple M4
0.00.052.391 I ggml_metal_init: using embedded metal library
0.00.054.730 I ggml_metal_init: GPU name:   Apple M4
0.00.054.732 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.732 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.733 I ggml_metal_init: simdgroup reduction   = true
0.00.054.733 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.733 I ggml_metal_init: has bfloat            = true
0.00.054.733 I ggml_metal_init: use bfloat            = true
0.00.054.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.418 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.952 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.960 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.982 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.981 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.982 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.982 I llama_init_from_model: graph nodes  = 967
0.00.083.982 I llama_init_from_model: graph splits = 2
0.00.083.985 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.104 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.104 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.146 I main: llama threadpool init, n_threads = 4
0.00.547.181 I 
0.00.547.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.547.207 I 
0.00.547.436 I sampler seed: 1234
0.00.547.440 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.547.451 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.547.451 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.547.451 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.289.866 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.289.867 I llama_perf_context_print:        load time =     537.51 ms
0.01.289.868 I llama_perf_context_print: prompt eval time =      47.02 ms /     7 tokens (    6.72 ms per token,   148.86 tokens per second)
0.01.289.869 I llama_perf_context_print:        eval time =     692.57 ms /    63 runs   (   10.99 ms per token,    90.97 tokens per second)
0.01.289.870 I llama_perf_context_print:       total time =     743.61 ms /    70 tokens
0.01.290.155 I ggml_metal_free: deallocating

real	0m1.309s
user	0m0.109s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.922 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.934 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.939 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.062 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.064 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.064 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.065 I llama_model_loader: - type  f32:  194 tensors
0.00.025.065 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.065 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.066 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.066 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.066 I print_info: file format = GGUF V3 (latest)
0.00.025.067 I print_info: file type   = Q3_K - Medium
0.00.025.068 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.592 I load: special tokens cache size = 25
0.00.050.662 I load: token to piece cache size = 0.2984 MB
0.00.050.666 I print_info: arch             = gptneox
0.00.050.666 I print_info: vocab_only       = 0
0.00.050.666 I print_info: n_ctx_train      = 2048
0.00.050.666 I print_info: n_embd           = 2048
0.00.050.667 I print_info: n_layer          = 24
0.00.050.669 I print_info: n_head           = 16
0.00.050.670 I print_info: n_head_kv        = 16
0.00.050.671 I print_info: n_rot            = 32
0.00.050.671 I print_info: n_swa            = 0
0.00.050.671 I print_info: n_embd_head_k    = 128
0.00.050.671 I print_info: n_embd_head_v    = 128
0.00.050.674 I print_info: n_gqa            = 1
0.00.050.675 I print_info: n_embd_k_gqa     = 2048
0.00.050.675 I print_info: n_embd_v_gqa     = 2048
0.00.050.676 I print_info: f_norm_eps       = 1.0e-05
0.00.050.676 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.678 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.678 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.678 I print_info: f_logit_scale    = 0.0e+00
0.00.050.679 I print_info: n_ff             = 8192
0.00.050.679 I print_info: n_expert         = 0
0.00.050.679 I print_info: n_expert_used    = 0
0.00.050.679 I print_info: causal attn      = 1
0.00.050.679 I print_info: pooling type     = 0
0.00.050.679 I print_info: rope type        = 2
0.00.050.680 I print_info: rope scaling     = linear
0.00.050.680 I print_info: freq_base_train  = 10000.0
0.00.050.680 I print_info: freq_scale_train = 1
0.00.050.680 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.681 I print_info: rope_finetuned   = unknown
0.00.050.681 I print_info: ssm_d_conv       = 0
0.00.050.681 I print_info: ssm_d_inner      = 0
0.00.050.681 I print_info: ssm_d_state      = 0
0.00.050.681 I print_info: ssm_dt_rank      = 0
0.00.050.681 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.682 I print_info: model type       = 1.4B
0.00.050.682 I print_info: model params     = 1.41 B
0.00.050.686 I print_info: general.name     = 1.4B
0.00.050.687 I print_info: vocab type       = BPE
0.00.050.687 I print_info: n_vocab          = 50304
0.00.050.687 I print_info: n_merges         = 50009
0.00.050.688 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.688 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.688 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.688 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.688 I print_info: LF token         = 128 'Ä'
0.00.050.689 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.690 I print_info: max token length = 1024
0.00.053.435 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.435 I load_tensors: offloading output layer to GPU
0.00.053.436 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.441 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.442 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.844 I llama_init_from_model: n_seq_max     = 1
0.00.053.845 I llama_init_from_model: n_ctx         = 128
0.00.053.845 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.845 I llama_init_from_model: n_batch       = 128
0.00.053.845 I llama_init_from_model: n_ubatch      = 128
0.00.053.846 I llama_init_from_model: flash_attn    = 0
0.00.053.846 I llama_init_from_model: freq_base     = 10000.0
0.00.053.846 I llama_init_from_model: freq_scale    = 1
0.00.053.846 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.847 I ggml_metal_init: allocating
0.00.053.850 I ggml_metal_init: found device: Apple M4
0.00.053.852 I ggml_metal_init: picking default device: Apple M4
0.00.054.423 I ggml_metal_init: using embedded metal library
0.00.056.802 I ggml_metal_init: GPU name:   Apple M4
0.00.056.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.804 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.805 I ggml_metal_init: simdgroup reduction   = true
0.00.056.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.805 I ggml_metal_init: has bfloat            = true
0.00.056.805 I ggml_metal_init: use bfloat            = true
0.00.056.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.654 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.923 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.925 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.941 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.891 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.892 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.892 I llama_init_from_model: graph nodes  = 967
0.00.068.892 I llama_init_from_model: graph splits = 2
0.00.068.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.511 I 
0.00.477.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.580 I perplexity: tokenizing the input ..
0.00.485.064 I perplexity: tokenization took 7.483 ms
0.00.485.075 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.849 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.618.053 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.618.074 I llama_perf_context_print:        load time =     468.65 ms
0.00.618.075 I llama_perf_context_print: prompt eval time =     131.55 ms /   128 tokens (    1.03 ms per token,   973.04 tokens per second)
0.00.618.076 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.618.076 I llama_perf_context_print:       total time =     140.56 ms /   129 tokens
0.00.618.592 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.079s
sys	0m0.083s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.622 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.276 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.278 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.278 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.279 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.279 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.280 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.280 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.280 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.281 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.283 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.287 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.287 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.287 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.144 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.920 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.920 I llama_model_loader: - type  f32:  194 tensors
0.00.025.921 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.921 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.921 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.922 I print_info: file format = GGUF V3 (latest)
0.00.025.922 I print_info: file type   = Q4_K - Medium
0.00.025.923 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.669 I load: special tokens cache size = 25
0.00.050.714 I load: token to piece cache size = 0.2984 MB
0.00.050.717 I print_info: arch             = gptneox
0.00.050.717 I print_info: vocab_only       = 0
0.00.050.717 I print_info: n_ctx_train      = 2048
0.00.050.717 I print_info: n_embd           = 2048
0.00.050.717 I print_info: n_layer          = 24
0.00.050.720 I print_info: n_head           = 16
0.00.050.721 I print_info: n_head_kv        = 16
0.00.050.721 I print_info: n_rot            = 32
0.00.050.721 I print_info: n_swa            = 0
0.00.050.722 I print_info: n_embd_head_k    = 128
0.00.050.722 I print_info: n_embd_head_v    = 128
0.00.050.723 I print_info: n_gqa            = 1
0.00.050.724 I print_info: n_embd_k_gqa     = 2048
0.00.050.724 I print_info: n_embd_v_gqa     = 2048
0.00.050.725 I print_info: f_norm_eps       = 1.0e-05
0.00.050.725 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.725 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.726 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.726 I print_info: f_logit_scale    = 0.0e+00
0.00.050.726 I print_info: n_ff             = 8192
0.00.050.727 I print_info: n_expert         = 0
0.00.050.727 I print_info: n_expert_used    = 0
0.00.050.727 I print_info: causal attn      = 1
0.00.050.728 I print_info: pooling type     = 0
0.00.050.730 I print_info: rope type        = 2
0.00.050.730 I print_info: rope scaling     = linear
0.00.050.731 I print_info: freq_base_train  = 10000.0
0.00.050.731 I print_info: freq_scale_train = 1
0.00.050.731 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.732 I print_info: rope_finetuned   = unknown
0.00.050.732 I print_info: ssm_d_conv       = 0
0.00.050.732 I print_info: ssm_d_inner      = 0
0.00.050.732 I print_info: ssm_d_state      = 0
0.00.050.732 I print_info: ssm_dt_rank      = 0
0.00.050.732 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.732 I print_info: model type       = 1.4B
0.00.050.733 I print_info: model params     = 1.41 B
0.00.050.733 I print_info: general.name     = 1.4B
0.00.050.734 I print_info: vocab type       = BPE
0.00.050.734 I print_info: n_vocab          = 50304
0.00.050.734 I print_info: n_merges         = 50009
0.00.050.734 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.734 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.734 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.735 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.735 I print_info: LF token         = 128 'Ä'
0.00.050.735 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.740 I print_info: max token length = 1024
0.00.052.702 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.702 I load_tensors: offloading output layer to GPU
0.00.052.702 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.713 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.714 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.990 I llama_init_from_model: n_seq_max     = 1
0.00.052.990 I llama_init_from_model: n_ctx         = 2048
0.00.052.990 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.991 I llama_init_from_model: n_batch       = 2048
0.00.052.991 I llama_init_from_model: n_ubatch      = 512
0.00.052.991 I llama_init_from_model: flash_attn    = 0
0.00.052.991 I llama_init_from_model: freq_base     = 10000.0
0.00.052.992 I llama_init_from_model: freq_scale    = 1
0.00.052.992 I ggml_metal_init: allocating
0.00.052.995 I ggml_metal_init: found device: Apple M4
0.00.052.997 I ggml_metal_init: picking default device: Apple M4
0.00.053.587 I ggml_metal_init: using embedded metal library
0.00.055.931 I ggml_metal_init: GPU name:   Apple M4
0.00.055.932 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.933 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.933 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.933 I ggml_metal_init: simdgroup reduction   = true
0.00.055.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.934 I ggml_metal_init: has bfloat            = true
0.00.055.934 I ggml_metal_init: use bfloat            = true
0.00.055.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.935 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.586 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.570 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.576 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.599 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.631 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.633 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.633 I llama_init_from_model: graph nodes  = 967
0.00.086.633 I llama_init_from_model: graph splits = 2
0.00.086.636 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.771 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.760 I main: llama threadpool init, n_threads = 4
0.00.621.795 I 
0.00.621.819 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.819 I 
0.00.622.080 I sampler seed: 1234
0.00.622.084 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.095 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.096 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.096 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.378.480 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.378.480 I llama_perf_context_print:        load time =     611.24 ms
0.01.378.481 I llama_perf_context_print: prompt eval time =      51.04 ms /     7 tokens (    7.29 ms per token,   137.15 tokens per second)
0.01.378.482 I llama_perf_context_print:        eval time =     702.21 ms /    63 runs   (   11.15 ms per token,    89.72 tokens per second)
0.01.378.482 I llama_perf_context_print:       total time =     757.62 ms /    70 tokens
0.01.378.674 I ggml_metal_free: deallocating

real	0m1.397s
user	0m0.110s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.625 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.543 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.544 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.544 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.546 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.549 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.549 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.555 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.556 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.370 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.184 I llama_model_loader: - type  f32:  194 tensors
0.00.025.184 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.184 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.184 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.185 I print_info: file format = GGUF V3 (latest)
0.00.025.185 I print_info: file type   = Q4_K - Medium
0.00.025.186 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.772 I load: special tokens cache size = 25
0.00.049.835 I load: token to piece cache size = 0.2984 MB
0.00.049.838 I print_info: arch             = gptneox
0.00.049.838 I print_info: vocab_only       = 0
0.00.049.839 I print_info: n_ctx_train      = 2048
0.00.049.839 I print_info: n_embd           = 2048
0.00.049.839 I print_info: n_layer          = 24
0.00.049.842 I print_info: n_head           = 16
0.00.049.843 I print_info: n_head_kv        = 16
0.00.049.843 I print_info: n_rot            = 32
0.00.049.843 I print_info: n_swa            = 0
0.00.049.843 I print_info: n_embd_head_k    = 128
0.00.049.843 I print_info: n_embd_head_v    = 128
0.00.049.844 I print_info: n_gqa            = 1
0.00.049.845 I print_info: n_embd_k_gqa     = 2048
0.00.049.846 I print_info: n_embd_v_gqa     = 2048
0.00.049.846 I print_info: f_norm_eps       = 1.0e-05
0.00.049.846 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.847 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.848 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.849 I print_info: f_logit_scale    = 0.0e+00
0.00.049.850 I print_info: n_ff             = 8192
0.00.049.850 I print_info: n_expert         = 0
0.00.049.850 I print_info: n_expert_used    = 0
0.00.049.850 I print_info: causal attn      = 1
0.00.049.850 I print_info: pooling type     = 0
0.00.049.851 I print_info: rope type        = 2
0.00.049.852 I print_info: rope scaling     = linear
0.00.049.853 I print_info: freq_base_train  = 10000.0
0.00.049.853 I print_info: freq_scale_train = 1
0.00.049.853 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.853 I print_info: rope_finetuned   = unknown
0.00.049.853 I print_info: ssm_d_conv       = 0
0.00.049.854 I print_info: ssm_d_inner      = 0
0.00.049.854 I print_info: ssm_d_state      = 0
0.00.049.854 I print_info: ssm_dt_rank      = 0
0.00.049.854 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.854 I print_info: model type       = 1.4B
0.00.049.855 I print_info: model params     = 1.41 B
0.00.049.855 I print_info: general.name     = 1.4B
0.00.049.856 I print_info: vocab type       = BPE
0.00.049.856 I print_info: n_vocab          = 50304
0.00.049.856 I print_info: n_merges         = 50009
0.00.049.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: LF token         = 128 'Ä'
0.00.049.858 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.858 I print_info: max token length = 1024
0.00.051.789 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.789 I load_tensors: offloading output layer to GPU
0.00.051.789 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.800 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.801 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.070 I llama_init_from_model: n_seq_max     = 1
0.00.052.071 I llama_init_from_model: n_ctx         = 128
0.00.052.071 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.071 I llama_init_from_model: n_batch       = 128
0.00.052.071 I llama_init_from_model: n_ubatch      = 128
0.00.052.072 I llama_init_from_model: flash_attn    = 0
0.00.052.072 I llama_init_from_model: freq_base     = 10000.0
0.00.052.072 I llama_init_from_model: freq_scale    = 1
0.00.052.073 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.073 I ggml_metal_init: allocating
0.00.052.076 I ggml_metal_init: found device: Apple M4
0.00.052.078 I ggml_metal_init: picking default device: Apple M4
0.00.052.668 I ggml_metal_init: using embedded metal library
0.00.055.005 I ggml_metal_init: GPU name:   Apple M4
0.00.055.006 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.007 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.007 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.007 I ggml_metal_init: simdgroup reduction   = true
0.00.055.007 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.008 I ggml_metal_init: has bfloat            = true
0.00.055.008 I ggml_metal_init: use bfloat            = true
0.00.055.008 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.009 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.503 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.730 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.735 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.748 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.646 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.647 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.647 I llama_init_from_model: graph nodes  = 967
0.00.066.647 I llama_init_from_model: graph splits = 2
0.00.066.649 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.106 I 
0.00.618.146 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.154 I perplexity: tokenizing the input ..
0.00.625.927 I perplexity: tokenization took 7.77 ms
0.00.625.939 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.882 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.761.026 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.761.041 I llama_perf_context_print:        load time =     608.48 ms
0.00.761.041 I llama_perf_context_print: prompt eval time =     133.72 ms /   128 tokens (    1.04 ms per token,   957.25 tokens per second)
0.00.761.042 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.043 I llama_perf_context_print:       total time =     142.94 ms /   129 tokens
0.00.761.492 I ggml_metal_free: deallocating

real	0m0.776s
user	0m0.077s
sys	0m0.096s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.744 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.554 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.555 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.556 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.557 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.557 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.557 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.558 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.558 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.560 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.560 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.560 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.601 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.657 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.658 I llama_model_loader: - type  f32:  194 tensors
0.00.027.658 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.659 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.659 I print_info: file format = GGUF V3 (latest)
0.00.027.660 I print_info: file type   = Q5_K - Medium
0.00.027.660 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.151 I load: special tokens cache size = 25
0.00.053.236 I load: token to piece cache size = 0.2984 MB
0.00.053.239 I print_info: arch             = gptneox
0.00.053.239 I print_info: vocab_only       = 0
0.00.053.240 I print_info: n_ctx_train      = 2048
0.00.053.240 I print_info: n_embd           = 2048
0.00.053.240 I print_info: n_layer          = 24
0.00.053.243 I print_info: n_head           = 16
0.00.053.244 I print_info: n_head_kv        = 16
0.00.053.245 I print_info: n_rot            = 32
0.00.053.245 I print_info: n_swa            = 0
0.00.053.245 I print_info: n_embd_head_k    = 128
0.00.053.245 I print_info: n_embd_head_v    = 128
0.00.053.246 I print_info: n_gqa            = 1
0.00.053.246 I print_info: n_embd_k_gqa     = 2048
0.00.053.247 I print_info: n_embd_v_gqa     = 2048
0.00.053.248 I print_info: f_norm_eps       = 1.0e-05
0.00.053.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.249 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.249 I print_info: f_logit_scale    = 0.0e+00
0.00.053.249 I print_info: n_ff             = 8192
0.00.053.250 I print_info: n_expert         = 0
0.00.053.250 I print_info: n_expert_used    = 0
0.00.053.250 I print_info: causal attn      = 1
0.00.053.250 I print_info: pooling type     = 0
0.00.053.250 I print_info: rope type        = 2
0.00.053.250 I print_info: rope scaling     = linear
0.00.053.251 I print_info: freq_base_train  = 10000.0
0.00.053.251 I print_info: freq_scale_train = 1
0.00.053.251 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.251 I print_info: rope_finetuned   = unknown
0.00.053.252 I print_info: ssm_d_conv       = 0
0.00.053.252 I print_info: ssm_d_inner      = 0
0.00.053.252 I print_info: ssm_d_state      = 0
0.00.053.252 I print_info: ssm_dt_rank      = 0
0.00.053.252 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.253 I print_info: model type       = 1.4B
0.00.053.253 I print_info: model params     = 1.41 B
0.00.053.253 I print_info: general.name     = 1.4B
0.00.053.254 I print_info: vocab type       = BPE
0.00.053.254 I print_info: n_vocab          = 50304
0.00.053.254 I print_info: n_merges         = 50009
0.00.053.254 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.254 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.255 I print_info: LF token         = 128 'Ä'
0.00.053.255 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.255 I print_info: max token length = 1024
0.00.055.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.245 I load_tensors: offloading output layer to GPU
0.00.055.245 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.256 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.257 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.536 I llama_init_from_model: n_seq_max     = 1
0.00.055.536 I llama_init_from_model: n_ctx         = 2048
0.00.055.536 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.537 I llama_init_from_model: n_batch       = 2048
0.00.055.537 I llama_init_from_model: n_ubatch      = 512
0.00.055.537 I llama_init_from_model: flash_attn    = 0
0.00.055.537 I llama_init_from_model: freq_base     = 10000.0
0.00.055.538 I llama_init_from_model: freq_scale    = 1
0.00.055.538 I ggml_metal_init: allocating
0.00.055.541 I ggml_metal_init: found device: Apple M4
0.00.055.543 I ggml_metal_init: picking default device: Apple M4
0.00.056.142 I ggml_metal_init: using embedded metal library
0.00.058.464 I ggml_metal_init: GPU name:   Apple M4
0.00.058.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.467 I ggml_metal_init: simdgroup reduction   = true
0.00.058.467 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.467 I ggml_metal_init: has bfloat            = true
0.00.058.467 I ggml_metal_init: use bfloat            = true
0.00.058.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.424 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.588 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.596 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.622 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.681 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.683 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.683 I llama_init_from_model: graph nodes  = 967
0.00.088.683 I llama_init_from_model: graph splits = 2
0.00.088.686 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.820 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.821 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.819 I main: llama threadpool init, n_threads = 4
0.00.707.854 I 
0.00.707.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.886 I 
0.00.708.103 I sampler seed: 1234
0.00.708.109 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.708.119 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.708.119 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.708.120 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.550.438 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.550.439 I llama_perf_context_print:        load time =     696.20 ms
0.01.550.440 I llama_perf_context_print: prompt eval time =      51.60 ms /     7 tokens (    7.37 ms per token,   135.66 tokens per second)
0.01.550.440 I llama_perf_context_print:        eval time =     787.61 ms /    63 runs   (   12.50 ms per token,    79.99 tokens per second)
0.01.550.444 I llama_perf_context_print:       total time =     843.49 ms /    70 tokens
0.01.550.694 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.111s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.797 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.804 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.816 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.816 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.813 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.764 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.766 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.766 I llama_model_loader: - type  f32:  194 tensors
0.00.024.766 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.767 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.767 I print_info: file format = GGUF V3 (latest)
0.00.024.768 I print_info: file type   = Q5_K - Medium
0.00.024.772 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.385 I load: special tokens cache size = 25
0.00.049.533 I load: token to piece cache size = 0.2984 MB
0.00.049.537 I print_info: arch             = gptneox
0.00.049.537 I print_info: vocab_only       = 0
0.00.049.537 I print_info: n_ctx_train      = 2048
0.00.049.537 I print_info: n_embd           = 2048
0.00.049.538 I print_info: n_layer          = 24
0.00.049.540 I print_info: n_head           = 16
0.00.049.541 I print_info: n_head_kv        = 16
0.00.049.541 I print_info: n_rot            = 32
0.00.049.541 I print_info: n_swa            = 0
0.00.049.541 I print_info: n_embd_head_k    = 128
0.00.049.541 I print_info: n_embd_head_v    = 128
0.00.049.542 I print_info: n_gqa            = 1
0.00.049.543 I print_info: n_embd_k_gqa     = 2048
0.00.049.544 I print_info: n_embd_v_gqa     = 2048
0.00.049.544 I print_info: f_norm_eps       = 1.0e-05
0.00.049.548 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.549 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.549 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.549 I print_info: f_logit_scale    = 0.0e+00
0.00.049.550 I print_info: n_ff             = 8192
0.00.049.550 I print_info: n_expert         = 0
0.00.049.550 I print_info: n_expert_used    = 0
0.00.049.550 I print_info: causal attn      = 1
0.00.049.551 I print_info: pooling type     = 0
0.00.049.551 I print_info: rope type        = 2
0.00.049.551 I print_info: rope scaling     = linear
0.00.049.553 I print_info: freq_base_train  = 10000.0
0.00.049.555 I print_info: freq_scale_train = 1
0.00.049.555 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.555 I print_info: rope_finetuned   = unknown
0.00.049.555 I print_info: ssm_d_conv       = 0
0.00.049.555 I print_info: ssm_d_inner      = 0
0.00.049.555 I print_info: ssm_d_state      = 0
0.00.049.555 I print_info: ssm_dt_rank      = 0
0.00.049.556 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.556 I print_info: model type       = 1.4B
0.00.049.559 I print_info: model params     = 1.41 B
0.00.049.559 I print_info: general.name     = 1.4B
0.00.049.559 I print_info: vocab type       = BPE
0.00.049.560 I print_info: n_vocab          = 50304
0.00.049.560 I print_info: n_merges         = 50009
0.00.049.560 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.562 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.562 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.562 I print_info: LF token         = 128 'Ä'
0.00.049.562 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.562 I print_info: max token length = 1024
0.00.051.330 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.330 I load_tensors: offloading output layer to GPU
0.00.051.330 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.336 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.337 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.615 I llama_init_from_model: n_seq_max     = 1
0.00.051.616 I llama_init_from_model: n_ctx         = 128
0.00.051.616 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.616 I llama_init_from_model: n_batch       = 128
0.00.051.616 I llama_init_from_model: n_ubatch      = 128
0.00.051.616 I llama_init_from_model: flash_attn    = 0
0.00.051.617 I llama_init_from_model: freq_base     = 10000.0
0.00.051.617 I llama_init_from_model: freq_scale    = 1
0.00.051.617 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.618 I ggml_metal_init: allocating
0.00.051.621 I ggml_metal_init: found device: Apple M4
0.00.051.623 I ggml_metal_init: picking default device: Apple M4
0.00.052.182 I ggml_metal_init: using embedded metal library
0.00.054.560 I ggml_metal_init: GPU name:   Apple M4
0.00.054.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.562 I ggml_metal_init: simdgroup reduction   = true
0.00.054.562 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.562 I ggml_metal_init: has bfloat            = true
0.00.054.562 I ggml_metal_init: use bfloat            = true
0.00.054.563 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.077 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.313 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.317 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.330 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.269 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.270 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.270 I llama_init_from_model: graph nodes  = 967
0.00.066.270 I llama_init_from_model: graph splits = 2
0.00.066.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.986 I 
0.00.639.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.035 I perplexity: tokenizing the input ..
0.00.647.167 I perplexity: tokenization took 8.13 ms
0.00.647.178 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.023 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.789.186 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.789.203 I llama_perf_context_print:        load time =     630.21 ms
0.00.789.204 I llama_perf_context_print: prompt eval time =     140.62 ms /   128 tokens (    1.10 ms per token,   910.27 tokens per second)
0.00.789.204 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.205 I llama_perf_context_print:       total time =     150.22 ms /   129 tokens
0.00.789.737 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.078s
sys	0m0.119s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.739 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.431 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.436 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.438 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.439 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.439 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.440 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.440 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.441 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.441 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.441 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.442 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.445 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.445 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.445 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.509 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.467 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.470 I llama_model_loader: - type  f32:  194 tensors
0.00.026.471 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.471 I print_info: file format = GGUF V3 (latest)
0.00.026.472 I print_info: file type   = Q6_K
0.00.026.472 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.133 I load: special tokens cache size = 25
0.00.052.404 I load: token to piece cache size = 0.2984 MB
0.00.052.407 I print_info: arch             = gptneox
0.00.052.408 I print_info: vocab_only       = 0
0.00.052.408 I print_info: n_ctx_train      = 2048
0.00.052.408 I print_info: n_embd           = 2048
0.00.052.408 I print_info: n_layer          = 24
0.00.052.411 I print_info: n_head           = 16
0.00.052.412 I print_info: n_head_kv        = 16
0.00.052.412 I print_info: n_rot            = 32
0.00.052.412 I print_info: n_swa            = 0
0.00.052.413 I print_info: n_embd_head_k    = 128
0.00.052.413 I print_info: n_embd_head_v    = 128
0.00.052.413 I print_info: n_gqa            = 1
0.00.052.414 I print_info: n_embd_k_gqa     = 2048
0.00.052.417 I print_info: n_embd_v_gqa     = 2048
0.00.052.417 I print_info: f_norm_eps       = 1.0e-05
0.00.052.418 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.418 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.418 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.418 I print_info: f_logit_scale    = 0.0e+00
0.00.052.419 I print_info: n_ff             = 8192
0.00.052.419 I print_info: n_expert         = 0
0.00.052.420 I print_info: n_expert_used    = 0
0.00.052.420 I print_info: causal attn      = 1
0.00.052.420 I print_info: pooling type     = 0
0.00.052.420 I print_info: rope type        = 2
0.00.052.422 I print_info: rope scaling     = linear
0.00.052.423 I print_info: freq_base_train  = 10000.0
0.00.052.424 I print_info: freq_scale_train = 1
0.00.052.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.424 I print_info: rope_finetuned   = unknown
0.00.052.424 I print_info: ssm_d_conv       = 0
0.00.052.424 I print_info: ssm_d_inner      = 0
0.00.052.424 I print_info: ssm_d_state      = 0
0.00.052.425 I print_info: ssm_dt_rank      = 0
0.00.052.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.425 I print_info: model type       = 1.4B
0.00.052.425 I print_info: model params     = 1.41 B
0.00.052.426 I print_info: general.name     = 1.4B
0.00.052.430 I print_info: vocab type       = BPE
0.00.052.430 I print_info: n_vocab          = 50304
0.00.052.431 I print_info: n_merges         = 50009
0.00.052.431 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.431 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.431 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.431 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.432 I print_info: LF token         = 128 'Ä'
0.00.052.432 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.433 I print_info: max token length = 1024
0.00.054.448 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.448 I load_tensors: offloading output layer to GPU
0.00.054.449 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.459 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.461 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.739 I llama_init_from_model: n_seq_max     = 1
0.00.054.740 I llama_init_from_model: n_ctx         = 2048
0.00.054.740 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.740 I llama_init_from_model: n_batch       = 2048
0.00.054.740 I llama_init_from_model: n_ubatch      = 512
0.00.054.740 I llama_init_from_model: flash_attn    = 0
0.00.054.741 I llama_init_from_model: freq_base     = 10000.0
0.00.054.741 I llama_init_from_model: freq_scale    = 1
0.00.054.741 I ggml_metal_init: allocating
0.00.054.744 I ggml_metal_init: found device: Apple M4
0.00.054.747 I ggml_metal_init: picking default device: Apple M4
0.00.055.361 I ggml_metal_init: using embedded metal library
0.00.057.668 I ggml_metal_init: GPU name:   Apple M4
0.00.057.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.671 I ggml_metal_init: simdgroup reduction   = true
0.00.057.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.671 I ggml_metal_init: has bfloat            = true
0.00.057.671 I ggml_metal_init: use bfloat            = true
0.00.057.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.340 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.576 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.586 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.611 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.644 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.645 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.646 I llama_init_from_model: graph nodes  = 967
0.00.087.646 I llama_init_from_model: graph splits = 2
0.00.087.649 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.764 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.765 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.275 I main: llama threadpool init, n_threads = 4
0.00.758.309 I 
0.00.758.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.340 I 
0.00.758.570 I sampler seed: 1234
0.00.758.577 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.629 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.629 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.637.592 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.01.637.592 I llama_perf_context_print:        load time =     747.66 ms
0.01.637.593 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.75 tokens per second)
0.01.637.594 I llama_perf_context_print:        eval time =     821.60 ms /    63 runs   (   13.04 ms per token,    76.68 tokens per second)
0.01.637.595 I llama_perf_context_print:       total time =     880.19 ms /    70 tokens
0.01.637.840 I ggml_metal_free: deallocating

real	0m1.656s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4542 (1af6945e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.826 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.542 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.543 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.544 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.545 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.547 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.547 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.548 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.548 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.550 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.553 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.442 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.444 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.444 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.444 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.445 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.445 I llama_model_loader: - type  f32:  194 tensors
0.00.025.446 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.446 I print_info: file format = GGUF V3 (latest)
0.00.025.447 I print_info: file type   = Q6_K
0.00.025.447 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.984 I load: special tokens cache size = 25
0.00.049.947 I load: token to piece cache size = 0.2984 MB
0.00.049.950 I print_info: arch             = gptneox
0.00.049.950 I print_info: vocab_only       = 0
0.00.049.950 I print_info: n_ctx_train      = 2048
0.00.049.950 I print_info: n_embd           = 2048
0.00.049.950 I print_info: n_layer          = 24
0.00.049.953 I print_info: n_head           = 16
0.00.049.955 I print_info: n_head_kv        = 16
0.00.049.956 I print_info: n_rot            = 32
0.00.049.956 I print_info: n_swa            = 0
0.00.049.956 I print_info: n_embd_head_k    = 128
0.00.049.956 I print_info: n_embd_head_v    = 128
0.00.049.957 I print_info: n_gqa            = 1
0.00.049.958 I print_info: n_embd_k_gqa     = 2048
0.00.049.958 I print_info: n_embd_v_gqa     = 2048
0.00.049.960 I print_info: f_norm_eps       = 1.0e-05
0.00.049.960 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.960 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.961 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.961 I print_info: f_logit_scale    = 0.0e+00
0.00.049.962 I print_info: n_ff             = 8192
0.00.049.962 I print_info: n_expert         = 0
0.00.049.962 I print_info: n_expert_used    = 0
0.00.049.962 I print_info: causal attn      = 1
0.00.049.962 I print_info: pooling type     = 0
0.00.049.964 I print_info: rope type        = 2
0.00.049.964 I print_info: rope scaling     = linear
0.00.049.965 I print_info: freq_base_train  = 10000.0
0.00.049.965 I print_info: freq_scale_train = 1
0.00.049.965 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.965 I print_info: rope_finetuned   = unknown
0.00.049.966 I print_info: ssm_d_conv       = 0
0.00.049.966 I print_info: ssm_d_inner      = 0
0.00.049.966 I print_info: ssm_d_state      = 0
0.00.049.966 I print_info: ssm_dt_rank      = 0
0.00.049.966 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.967 I print_info: model type       = 1.4B
0.00.049.970 I print_info: model params     = 1.41 B
0.00.049.971 I print_info: general.name     = 1.4B
0.00.049.971 I print_info: vocab type       = BPE
0.00.049.971 I print_info: n_vocab          = 50304
0.00.049.971 I print_info: n_merges         = 50009
0.00.049.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.972 I print_info: LF token         = 128 'Ä'
0.00.049.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.973 I print_info: max token length = 1024
0.00.051.933 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.933 I load_tensors: offloading output layer to GPU
0.00.051.933 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.944 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.945 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.215 I llama_init_from_model: n_seq_max     = 1
0.00.052.216 I llama_init_from_model: n_ctx         = 128
0.00.052.216 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.216 I llama_init_from_model: n_batch       = 128
0.00.052.216 I llama_init_from_model: n_ubatch      = 128
0.00.052.216 I llama_init_from_model: flash_attn    = 0
0.00.052.217 I llama_init_from_model: freq_base     = 10000.0
0.00.052.217 I llama_init_from_model: freq_scale    = 1
0.00.052.217 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.218 I ggml_metal_init: allocating
0.00.052.221 I ggml_metal_init: found device: Apple M4
0.00.052.222 I ggml_metal_init: picking default device: Apple M4
0.00.052.772 I ggml_metal_init: using embedded metal library
0.00.055.105 I ggml_metal_init: GPU name:   Apple M4
0.00.055.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.107 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.107 I ggml_metal_init: simdgroup reduction   = true
0.00.055.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.108 I ggml_metal_init: has bfloat            = true
0.00.055.108 I ggml_metal_init: use bfloat            = true
0.00.055.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.534 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.849 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.854 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.870 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.724 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.725 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.725 I llama_init_from_model: graph nodes  = 967
0.00.066.725 I llama_init_from_model: graph splits = 2
0.00.066.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.295.605 I 
0.00.295.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.295.651 I perplexity: tokenizing the input ..
0.00.302.737 I perplexity: tokenization took 7.084 ms
0.00.302.748 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.443.067 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.444.222 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.444.236 I llama_perf_context_print:        load time =     285.78 ms
0.00.444.237 I llama_perf_context_print: prompt eval time =     140.09 ms /   128 tokens (    1.09 ms per token,   913.69 tokens per second)
0.00.444.238 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.444.238 I llama_perf_context_print:       total time =     148.63 ms /   129 tokens
0.00.444.679 I ggml_metal_free: deallocating

real	0m0.462s
user	0m0.076s
sys	0m0.053s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4542 (1af6945e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106104280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106104a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106104e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1061052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106105750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106105bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106106030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1061064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106106910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106106d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1061071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106107890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1061083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106108b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106109370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106109a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10610a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10610a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10610aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10610b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10610bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10610c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10610cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10610d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10610dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10610dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10610e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10610e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10610ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10610f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10610f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10610fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106110060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106110320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106110790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106111040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106111300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106111770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106111be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106112050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1061124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106112930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106112da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106113210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106113680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106113af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106113f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106114990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106114c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1061150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106115530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1061159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106115e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106116280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1061166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106116da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106117240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106117500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106117970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106118040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106118440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106118700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106118c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106119100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106119600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106119b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10611a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10611a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10611aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10611af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10611b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10611b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10611be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10611c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10611c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10611ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10611d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10611d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10611df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10611e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10611ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10611f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10611f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10611fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106120190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106120740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106120cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1061212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106121850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106121e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1061223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106122960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106122f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1061234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106123a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106124020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1061245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106114580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106124d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1061251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106125610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106125bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106126170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106126720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106126cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106127280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106127830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106127de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106128390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106128940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106128ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1061294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106129a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10612a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10612a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10612aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10612af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10612b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10612b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10612be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10612c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10612c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10612cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10612d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10612d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10612dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10612e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10612e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10612eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10612f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10612f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10612fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10612ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106130400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106130900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106130e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106131300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106131800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106131d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106132200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106132700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106132c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106133100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106133600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106133b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106134000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106134500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106134a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106134f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106135400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106135900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106135e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106136300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106136800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106136d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106137200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106137700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106137c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106138100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106138600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106138b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106139000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106139500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106139a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106139f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10613a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10613a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10613ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10613b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10613b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10613bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10613c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10613c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10613cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10613d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10613d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10613db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10613e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10613e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10613ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10613ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10613f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10613f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10613fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106140300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106140800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106140d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106141200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106141700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106141c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106142100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106142600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106142b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106143000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1061435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106143b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106144110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1061446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106144cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1061452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1061458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1061460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106146580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106146840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106146e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106147460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106147c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1061480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106148590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106148a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1061491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106149730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106149c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10614a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10614a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10614ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10614b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10614b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10614bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10614c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10614c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10614cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10614d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10614d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10614dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10614e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10614e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10614ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10614f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10614f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10614fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106150170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1061506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106150c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106151160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1061516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106151c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106152150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1061526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106152bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106153140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106153690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106153be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106154130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106154680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106154bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106155120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106155670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106155bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106156110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106156660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106156bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106157100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106157650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106157ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1061580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106158640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106158b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1061590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106159630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106159b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10615a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10615a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10615ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10615b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10615b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10615bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10615c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10615c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10615c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10615cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10615d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10615d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10615dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10615e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10615e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10615e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10615ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10615f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10615f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10615fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1061600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106160610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106160d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106161450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106161b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106162290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106162550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106162d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106163000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106163610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.178.980 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.178.983 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12660a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12660aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12660aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12660b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12660b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12660bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12660c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12660c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12660c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12660ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12660d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12660d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12660e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12660ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12660f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12660fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1266102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1266109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1266110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1266118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126611fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1266126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126612e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126613530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126613c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126613f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1266141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126614640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126614ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126614f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126615390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1266158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126615d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126615ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126616460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1266168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126616d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1266171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126617620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126617f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126618370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1266187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1266190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1266199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12661a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12661a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12661ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12661afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12661b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12661b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12661bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12661c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12661c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12661cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12661d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12661d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12661d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12661ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12661e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12661e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12661eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12661ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12661f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12661f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12661fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126620140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1266205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126620a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126620e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126621300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126621770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126621be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126622050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1266224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126622930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126622da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126623210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126623680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126623af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126623f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1266243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126625120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126625590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126625a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126625e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1266262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126626750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126626bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126627030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1266274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126627910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126627d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1266281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126628660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126628ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126628f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1266293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126629820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126629c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12662a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12662a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12662a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12662ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12662b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12662b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12662bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12662c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12662c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12662c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12662cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12662d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12662d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12662dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12662df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12662e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12662e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12662ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12662f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12662f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12662f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12662fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1266302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126630710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126630b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126630ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126631460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1266318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126631d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1266321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126632620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126632a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126632f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126633370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1266337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126633c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1266340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126634530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1266349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126634e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126635280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1266356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126635b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126635fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126636440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1266368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126636d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126637600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126637a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126637ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126638350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1266387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126638c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1266390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126639510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126639980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126639df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12663a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12663a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12663ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12663b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12663ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12663bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106104540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10610f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10611d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10611cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1061220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10611c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1061242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106121b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1061291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106128c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106128650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106123d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10611e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1061269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106143870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106123780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10611e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106121560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10611fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106126430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1061432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1061280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1061231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10611dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106120fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10611f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106125e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106127af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106122c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10611d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106120a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1061258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106127540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106122670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106120450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106126f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106146b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106144f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1061632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106144980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1061455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10610d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106114220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106107b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106110a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1061169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106117c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106162810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106124890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106147720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106145bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106163a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106163d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106163ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1061642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106164570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106164830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106164af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106164db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106165070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106165330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1061655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1061658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106165b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106165e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1061660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1061663b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106166670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106166930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106166bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106166eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106167170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106167430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1061676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1061679b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106167c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106167f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1061681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1061684b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106168770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106168a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106168cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106168fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106169270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106169530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1061697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106169ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106169d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10616a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10616a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10616a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10616a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10616ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10616adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10616b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10616b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10616b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10616b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10616bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10616be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10616c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10616c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10616c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10616c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10616cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10616cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10616d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10616d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10616d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10616d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10616dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10616df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10616e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10616e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10616e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10616ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10616ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10616eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10616f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10616f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10616f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10616faf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106170060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106170320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106170760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106170a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106170ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106170fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106171260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106171520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1061717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106171aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106171d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106172020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1061725f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106172bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1061731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1061734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106173770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106173a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106173cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106173fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106174270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106174530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1061747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106174ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106174d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106175030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1061752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1061755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106175870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106175b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106175df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1061760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106176370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106176630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1061768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106176bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106176e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106177130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1061773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1061776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106177970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106177c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106177ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1061781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106178470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106178730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1061789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106178cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106178f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106179230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1061794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1061797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106179a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106179d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106179ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10617a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10617a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10617a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10617aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10617adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10617b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10617b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10617b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10617b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10617bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10617be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10617c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10617c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10617c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10617c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10617cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10617ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10617d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10617d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10617d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10617d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10617dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10617df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10617e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10617e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10617e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10617ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10617ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10617efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10617f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10617f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10617f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10617fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10617fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106180030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1061802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1061805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106180870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106180b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106180df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1061810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106181370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106181630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1061818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106181bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106181e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106182130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1061823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1061826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106182970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106182c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106182ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1061831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106183470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106183730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1061839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106183cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106183f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106184230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1061844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1061847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106184a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106184d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106184ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1061852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106185570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106185830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106185af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106185db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106186070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106186330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1061865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1061868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106186b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106186e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1061870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1061873b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106187670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106187930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106187bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106187eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106188170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106188430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1061886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1061889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106188c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106188f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1061891f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1061894b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106189770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106189a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106189cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106189fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10618a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10618a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10618a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10618aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10618ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10618b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10618b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10618b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10618b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10618bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10618bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10618c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10618c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10618c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10618c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10618cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10618ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10618d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10618d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10618d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10618d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10618dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10618def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10618e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10618e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10618e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10618e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10618ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10618ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10618f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10618f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10618f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10618fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10618fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106190130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1061903f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1061908f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106190df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1061912f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1061917f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106191cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1061921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1061926f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106192bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1061930f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1061935f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106193ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106194150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106194700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106194cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1061952c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1061958d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106195ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1061966d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106196b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106196e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106197440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106197a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106198240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1061986e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106198b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106199020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1061997d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106199d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10619a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10619a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10619ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10619b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10619b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10619bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10619c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10619c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10619ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10619d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10619d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10619dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10619e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10619e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10619ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10619f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10619f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10619fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1061a0210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1061a0760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1061a0cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1061a1200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1061a1750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1061a1ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1061a21f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1061a2740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1061a2c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1061a31e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1061a3730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1061a3c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1061a41d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1061a4720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1061a4c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1061a51c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1061a5710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1061a5c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1061a61b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1061a6700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1061a6c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1061a71a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1061a76f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1061a7c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1061a8190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1061a86e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1061a8c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1061a9180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1061a96d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1061a9c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1061aa170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1061aa6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1061aac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1061ab160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1061ab6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1061abc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1061ac150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1061ac5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1061aca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1061acf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1061ad3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1061ad870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1061add10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1061ae1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1061ae650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1061aeaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1061aef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1061af430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1061af8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1061afd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1061b0210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1061b06b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1061b0c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1061b1320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1061b1a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1061b2160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1061b2880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1061b2b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1061b3330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1061b35f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1061b3c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.479s
user	0m0.296s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4542 (1af6945e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132f0d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x132f0da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132f0dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132f0e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132f0eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132f0f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132f0f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132f0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132f101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132f106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132f10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132f110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132f11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x132f123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132f12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132f132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132f14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132f14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132f15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132f15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132f15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132f16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132f16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132f17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132f177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132f17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132f18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132f18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132f19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132f19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132f199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x132f1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x132f1a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132f1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132f1af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x132f1b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x132f1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132f1bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x132f1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132f1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132f1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x132f1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x132f1d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132f1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132f1dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x132f1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132f1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132f1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132f1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132f1fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132f20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132f20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132f21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132f21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132f21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x132f22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132f22450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132f22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132f23250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132f23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132f239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132f23e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132f242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x132f24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132f24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132f250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132f25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132f25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132f25eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132f26350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132f267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132f26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132f271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132f27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132f27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132f28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132f29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132f29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x132f2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132f2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132f2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x132f2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132f2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132f2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132f2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132f2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132f2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x132f2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x132f2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132f2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132f2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132f1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132f2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132f2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132f302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132f30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132f30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132f312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132f31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132f31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132f322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132f32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132f32d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x132f332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132f337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132f33d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132f341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x132f34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132f34fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x132f35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132f35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132f35da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132f36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132f366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132f36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132f37020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132f374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132f37960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132f37e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x132f382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132f38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132f39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132f39520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132f399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132f39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132f3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x132f3a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132f3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x132f3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x132f3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132f3ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132f3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x132f3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132f3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132f3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x132f3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x132f3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x132f3da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x132f3df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132f3e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132f3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x132f3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132f3f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132f3f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x132f3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132f3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132f40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132f408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132f40d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132f41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x132f416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132f41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132f41fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132f42480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132f42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132f42dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132f43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132f43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132f43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132f44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132f444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132f44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132f44e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132f452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132f45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132f460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132f46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132f469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132f46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132f47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132f477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132f47c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132f48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x132f485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132f48a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132f48ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x132f49380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132f49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132f49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x132f4a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132f4a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132f4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132f4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132f4b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x132f4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132f4bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132f4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132f4c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132f4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132f4d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x132f4d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x132f4e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x132f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x132f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x132f4eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x132f4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x132f4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132f50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132f50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132f50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132f51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132f517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132f51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132f52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x132f527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132f52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132f53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x132f53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132f53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132f54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132f54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132f54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132f55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132f55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132f55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132f56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132f56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132f56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132f57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132f57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132f57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132f581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132f58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132f58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132f591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132f59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132f59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132f5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132f5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132f5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132f5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132f5b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132f5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132f5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132f5c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132f5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132f5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132f5d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132f5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x132f5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132f5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132f5ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132f5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x132f5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x132f5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132f60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132f606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132f60c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132f61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x132f616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132f61c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132f62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x132f626a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132f62bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132f63140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x132f63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132f63be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132f64080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132f64520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132f649c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132f64e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132f65300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x132f657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132f65c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132f660e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132f66580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132f66a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132f66ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132f67360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132f67800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132f67ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132f68140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x132f68690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132f68db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132f694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132f69bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132f6a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132f6a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132f6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132f6b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132f6b690 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132e05310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x132e05780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132e05bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132e06060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132e064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132e06940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132e06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132e07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132e07690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132e07b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132e07f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132e085d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132e090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x132e098a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132e0a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132e0a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132e0aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132e0b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132e0bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132e0c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132e0cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132e0d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132e0da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132e0e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132e0e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132e0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132e0ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132e0f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132e0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132e0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132e0ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132e10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x132e10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x132e10c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132e110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132e11520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x132e11990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x132e11e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132e12270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x132e126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132e12b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132e12fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x132e13430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x132e138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132e13d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132e14180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x132e145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132e14a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132e14ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132e15340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132e157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132e15c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132e16090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132e16500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132e16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132e16de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x132e17350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132e17850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132e17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132e18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132e185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132e18a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132e18e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132e192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x132e19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132e19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132e1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132e1a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132e1a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132e1ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132e1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132e1b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132e1bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132e1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132e1c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132e1c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132e1cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132e1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132e1d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132e1d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132e1de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132e1e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x132e1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132e1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132e1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x132e1f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132e1f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132e1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132e201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132e20650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132e20ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132e20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132e213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x132e21810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x132e21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132e220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132e22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132e229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132e22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132e232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132e23720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132e23b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132e24000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132e24470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132e248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132e24d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132e251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132e25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132e25aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132e25f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x132e26380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132e267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132e26c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132e270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x132e27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132e279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132e27e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x132e28290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132e28700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132e28b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132e28fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132e29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132e298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132e29d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132e2a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132e2a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132e2aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x132e2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132e2b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132e2b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132e2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132e2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132e2c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132e2c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132e2ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x132e2d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132e2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x132e2db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x132e2dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132e2e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132e2e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x132e2ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132e2f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132e2f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x132e2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x132e2fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x132e30340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x132e307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132e30c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132e31090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x132e31500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132e31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132e31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x132e32250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132e326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132e32b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132e32fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132e33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132e33880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x132e33cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132e34160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132e345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132e34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132e34eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132e35320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132e35790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132e363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132e36680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132e36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132e36db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132e37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132e37690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132e37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132e37f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132e383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132e38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132e38cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132e39130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132e395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132e39a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132e39e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132e3a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132e3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x132e3abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132e3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132e3b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x132e3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132e3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132e3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x132e3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132e3cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132e3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132e3d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132e3d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x132e3dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132e3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132e3e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132e3e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132e3ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132e3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x132e3f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x132e3fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x132e40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x132e40620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x132e40a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x132e40f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x132e41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132e417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132e41d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132e42210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132e42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132e43040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132e43600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132e43bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x132e44180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132e44740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132e44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x132e452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132e45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132e45e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132e46400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132e469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132e46f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132e47540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132e47b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132e480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132e48680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132e48c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132e49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132e497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132e49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132e4a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132e4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132e4aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132e4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132e4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132e4c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132e4c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132e4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132e4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132e4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132e4dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132e4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132e4e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132e4ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132e4f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132e4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132e4ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132e50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x132e50ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132e51080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132e51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132e51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x132e521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x132e52780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132e52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132e53300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132e538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132e53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x132e54440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132e54a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132e54fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x132e55580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132e55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132e56100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x132e566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132e56c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132e57240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132e57740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132e57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132e58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132e58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x132e58b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132e59040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132e59540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132e59a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132e59f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132e5a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132e5a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132e5ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132e5b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132e5b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x132e5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132e5c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132e5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132e5d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132e5dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132e5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132e5e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132e5ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132e5f030 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133808010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133808480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1338088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133808d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1338091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133809640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133809ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133809f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13380a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13380a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13380ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13380b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13380bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13380c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13380ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13380d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13380dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13380e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13380eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13380f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13380fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133810120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133810840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133810f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133811680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133811940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133811c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133812070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1338124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133812950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133812e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133813360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1338137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133813a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133813f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133814370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1338148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133814dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1338152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1338157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133815cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1338161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1338166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133816bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1338170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133817540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1338179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133817e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133818290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133818700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133818b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133818fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133819450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1338198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133819d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13381a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13381a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13381ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13381b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13381ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13381bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13381c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13381c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13381cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13381d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13381d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13381dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13381df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13381e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13381e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13381ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13381f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13381f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13381fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133820120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133820670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133820bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133821110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133821660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133821bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133822100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133822650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133822ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1338230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133823640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133823b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1338240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133824630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133824b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1338250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133825620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133825b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1338260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133826610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133826b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1338270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133827600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133827b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1338280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1338285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133828b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133829090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1338295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133829b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13382a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13382a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13382ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13382b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13382b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13382bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13382c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13382c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13382cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13382cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13382d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13382d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13382dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13382e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13382e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13382eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13382f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13382f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13382f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13382fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133830280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133830720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133830bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133831060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133831500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1338319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133831e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1338322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133832780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133832c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1338330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133833560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133833a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133833ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133834340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1338347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133834c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133835120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1338355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133835a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133835f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1338363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133836840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133836ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133837180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133837620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133837ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133837f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133838400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1338388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133838d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1338391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133839680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133839b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133839fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13383a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13383a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13383ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13383b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13383b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13383bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13383c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13383c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13383c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13383ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13383d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13383d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13383dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13383e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13383e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13383e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13383ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13383f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13383f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13383fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1338400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133840580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133840a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133840ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133841360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133841800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133841ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133842140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1338425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133842a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133842f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1338433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133843860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133843d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133844250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1338447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133844cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133845240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133845500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133845b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133846120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133846730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133846f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1338473c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133847680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133847c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1338482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133848a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133848f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1338493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133849870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13384a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13384a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13384aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13384b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13384b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13384bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13384c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13384c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13384caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13384cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13384d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13384da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13384dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13384e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13384ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13384efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13384f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13384fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13384ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133850510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133850a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133850fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133851500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133851a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133851fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1338524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133852a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133852f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1338534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133853a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133853f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1338544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133854a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133854f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1338554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133855a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133855f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1338564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133856a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133856f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1338574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1338579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133857f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133858490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1338589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133858f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133859480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1338599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133859f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13385a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13385a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13385af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13385b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13385b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13385bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13385c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13385c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13385ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13385d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13385d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13385dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13385e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13385e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13385ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13385eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13385f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13385f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13385fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133860120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1338605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133860a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133860f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133861450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133861b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133862290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1338629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1338630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133863390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133863b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133863e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133864450 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.928s
user	0m0.242s
sys	0m0.138s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.12 sec
        1.15 real         0.70 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.58 real         0.15 user         0.05 sys
```
