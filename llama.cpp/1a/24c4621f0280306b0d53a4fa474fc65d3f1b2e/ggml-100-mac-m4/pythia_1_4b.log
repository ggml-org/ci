Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.036s
user	0m1.046s
sys	0m1.461s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Built target llava
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Built target test-log
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target test-quantize-perf
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-rope
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-embedding
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-bench
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-lookup
[ 79%] Built target llama-lookahead
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-parallel
[ 80%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-passkey
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-quantize
[ 87%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-qwen2vl-cli
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.235s
user	0m6.484s
sys	0m10.292s

main: quantize time =  6326.59 ms
main:    total time =  6326.59 ms

main: quantize time =  3907.97 ms
main:    total time =  3907.97 ms

main: quantize time =  2176.42 ms
main:    total time =  2176.42 ms

main: quantize time =  3688.72 ms
main:    total time =  3688.72 ms

main: quantize time =  2279.74 ms
main:    total time =  2279.74 ms

main: quantize time =  4952.46 ms
main:    total time =  4952.47 ms

main: quantize time =  6214.70 ms
main:    total time =  6214.70 ms

main: quantize time =  6883.98 ms
main:    total time =  6883.98 ms

main: quantize time =  6219.67 ms
main:    total time =  6219.67 ms

main: quantize time =  4571.67 ms
main:    total time =  4571.67 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.177 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.365 I main: llama backend init
0.00.000.376 I main: load the model and apply lora adapter, if any
0.00.056.803 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.069.029 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.069.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.069.050 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.069.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.069.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.069.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.069.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.069.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.069.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.069.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.069.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.069.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.069.059 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.069.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.069.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.069.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.069.067 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.078.057 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.087.397 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.087.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.087.407 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.087.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.087.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.087.409 I llama_model_loader: - type  f32:  194 tensors
0.00.087.410 I llama_model_loader: - type  f16:   98 tensors
0.00.087.412 I print_info: file format = GGUF V3 (latest)
0.00.087.422 I print_info: file type   = all F32 (guessed)
0.00.087.423 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.104.543 I load: special tokens cache size = 25
0.00.114.334 I load: token to piece cache size = 0.2984 MB
0.00.114.337 I print_info: arch             = gptneox
0.00.114.338 I print_info: vocab_only       = 0
0.00.114.338 I print_info: n_ctx_train      = 2048
0.00.114.338 I print_info: n_embd           = 2048
0.00.114.338 I print_info: n_layer          = 24
0.00.114.342 I print_info: n_head           = 16
0.00.114.344 I print_info: n_head_kv        = 16
0.00.114.344 I print_info: n_rot            = 32
0.00.114.344 I print_info: n_swa            = 0
0.00.114.344 I print_info: n_embd_head_k    = 128
0.00.114.344 I print_info: n_embd_head_v    = 128
0.00.114.346 I print_info: n_gqa            = 1
0.00.114.347 I print_info: n_embd_k_gqa     = 2048
0.00.114.348 I print_info: n_embd_v_gqa     = 2048
0.00.114.349 I print_info: f_norm_eps       = 1.0e-05
0.00.114.350 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.114.350 I print_info: f_clamp_kqv      = 0.0e+00
0.00.114.350 I print_info: f_max_alibi_bias = 0.0e+00
0.00.114.350 I print_info: f_logit_scale    = 0.0e+00
0.00.114.351 I print_info: n_ff             = 8192
0.00.114.351 I print_info: n_expert         = 0
0.00.114.351 I print_info: n_expert_used    = 0
0.00.114.352 I print_info: causal attn      = 1
0.00.114.352 I print_info: pooling type     = 0
0.00.114.352 I print_info: rope type        = 2
0.00.114.354 I print_info: rope scaling     = linear
0.00.114.355 I print_info: freq_base_train  = 10000.0
0.00.114.355 I print_info: freq_scale_train = 1
0.00.114.355 I print_info: n_ctx_orig_yarn  = 2048
0.00.114.355 I print_info: rope_finetuned   = unknown
0.00.114.356 I print_info: ssm_d_conv       = 0
0.00.114.356 I print_info: ssm_d_inner      = 0
0.00.114.356 I print_info: ssm_d_state      = 0
0.00.114.356 I print_info: ssm_dt_rank      = 0
0.00.114.356 I print_info: ssm_dt_b_c_rms   = 0
0.00.114.356 I print_info: model type       = 1.4B
0.00.114.357 I print_info: model params     = 1.41 B
0.00.114.357 I print_info: general.name     = 1.4B
0.00.114.358 I print_info: vocab type       = BPE
0.00.114.358 I print_info: n_vocab          = 50304
0.00.114.358 I print_info: n_merges         = 50009
0.00.114.358 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.114.359 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.114.359 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.114.360 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.114.364 I print_info: LF token         = 187 'Ċ'
0.00.114.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.114.365 I print_info: max token length = 1024
0.00.114.365 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.230.172 I load_tensors: offloading 24 repeating layers to GPU
0.00.230.176 I load_tensors: offloading output layer to GPU
0.00.230.177 I load_tensors: offloaded 25/25 layers to GPU
0.00.230.208 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.230.210 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.230.845 I llama_init_from_model: n_seq_max     = 1
0.00.230.847 I llama_init_from_model: n_ctx         = 2048
0.00.230.847 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.230.847 I llama_init_from_model: n_batch       = 2048
0.00.230.847 I llama_init_from_model: n_ubatch      = 512
0.00.230.847 I llama_init_from_model: flash_attn    = 0
0.00.230.848 I llama_init_from_model: freq_base     = 10000.0
0.00.230.848 I llama_init_from_model: freq_scale    = 1
0.00.230.851 I ggml_metal_init: allocating
0.00.231.067 I ggml_metal_init: found device: Apple M4
0.00.231.073 I ggml_metal_init: picking default device: Apple M4
0.00.232.523 I ggml_metal_init: using embedded metal library
0.00.250.702 I ggml_metal_init: GPU name:   Apple M4
0.00.250.704 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.250.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.250.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.250.705 I ggml_metal_init: simdgroup reduction   = true
0.00.250.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.250.705 I ggml_metal_init: has residency sets    = true
0.00.250.705 I ggml_metal_init: has bfloat            = true
0.00.250.706 I ggml_metal_init: use bfloat            = true
0.00.250.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.250.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.318.773 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.354.703 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.354.709 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.354.754 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.358.713 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.358.715 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.358.715 I llama_init_from_model: graph nodes  = 967
0.00.358.715 I llama_init_from_model: graph splits = 2
0.00.358.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.358.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.358.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.426.302 I main: llama threadpool init, n_threads = 4
0.00.426.343 I 
0.00.426.375 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.376 I 
0.00.426.546 I sampler seed: 1234
0.00.426.550 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.426.575 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.426.576 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.426.576 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.255.010 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.02.255.011 I llama_perf_context_print:        load time =     368.62 ms
0.02.255.012 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.30 tokens per second)
0.02.255.013 I llama_perf_context_print:        eval time =    1781.79 ms /    63 runs   (   28.28 ms per token,    35.36 tokens per second)
0.02.255.013 I llama_perf_context_print:       total time =    1829.58 ms /    70 tokens
0.02.255.197 I ggml_metal_free: deallocating

real	0m2.589s
user	0m0.136s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.791 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.870 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.870 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.871 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.871 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.872 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.874 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.874 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.938 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.781 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.783 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.784 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.784 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.785 I llama_model_loader: - type  f32:  194 tensors
0.00.035.785 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.786 I print_info: file format = GGUF V3 (latest)
0.00.035.787 I print_info: file type   = Q8_0
0.00.035.788 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.377 I load: special tokens cache size = 25
0.00.052.790 I load: token to piece cache size = 0.2984 MB
0.00.052.794 I print_info: arch             = gptneox
0.00.052.794 I print_info: vocab_only       = 0
0.00.052.794 I print_info: n_ctx_train      = 2048
0.00.052.795 I print_info: n_embd           = 2048
0.00.052.795 I print_info: n_layer          = 24
0.00.052.799 I print_info: n_head           = 16
0.00.052.799 I print_info: n_head_kv        = 16
0.00.052.800 I print_info: n_rot            = 32
0.00.052.800 I print_info: n_swa            = 0
0.00.052.800 I print_info: n_embd_head_k    = 128
0.00.052.800 I print_info: n_embd_head_v    = 128
0.00.052.801 I print_info: n_gqa            = 1
0.00.052.802 I print_info: n_embd_k_gqa     = 2048
0.00.052.803 I print_info: n_embd_v_gqa     = 2048
0.00.052.803 I print_info: f_norm_eps       = 1.0e-05
0.00.052.804 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.804 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.804 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.804 I print_info: f_logit_scale    = 0.0e+00
0.00.052.805 I print_info: n_ff             = 8192
0.00.052.805 I print_info: n_expert         = 0
0.00.052.805 I print_info: n_expert_used    = 0
0.00.052.805 I print_info: causal attn      = 1
0.00.052.806 I print_info: pooling type     = 0
0.00.052.806 I print_info: rope type        = 2
0.00.052.806 I print_info: rope scaling     = linear
0.00.052.807 I print_info: freq_base_train  = 10000.0
0.00.052.807 I print_info: freq_scale_train = 1
0.00.052.807 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.807 I print_info: rope_finetuned   = unknown
0.00.052.808 I print_info: ssm_d_conv       = 0
0.00.052.808 I print_info: ssm_d_inner      = 0
0.00.052.808 I print_info: ssm_d_state      = 0
0.00.052.808 I print_info: ssm_dt_rank      = 0
0.00.052.808 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.808 I print_info: model type       = 1.4B
0.00.052.809 I print_info: model params     = 1.41 B
0.00.052.809 I print_info: general.name     = 1.4B
0.00.052.810 I print_info: vocab type       = BPE
0.00.052.810 I print_info: n_vocab          = 50304
0.00.052.810 I print_info: n_merges         = 50009
0.00.052.811 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.811 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.811 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.814 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.814 I print_info: LF token         = 187 'Ċ'
0.00.052.814 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.815 I print_info: max token length = 1024
0.00.052.815 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.239.980 I load_tensors: offloading 24 repeating layers to GPU
0.01.239.990 I load_tensors: offloading output layer to GPU
0.01.239.991 I load_tensors: offloaded 25/25 layers to GPU
0.01.240.024 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.240.025 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.241.508 I llama_init_from_model: n_seq_max     = 1
0.01.241.511 I llama_init_from_model: n_ctx         = 2048
0.01.241.511 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.241.511 I llama_init_from_model: n_batch       = 2048
0.01.241.512 I llama_init_from_model: n_ubatch      = 512
0.01.241.512 I llama_init_from_model: flash_attn    = 0
0.01.241.513 I llama_init_from_model: freq_base     = 10000.0
0.01.241.514 I llama_init_from_model: freq_scale    = 1
0.01.241.515 I ggml_metal_init: allocating
0.01.241.527 I ggml_metal_init: found device: Apple M4
0.01.241.535 I ggml_metal_init: picking default device: Apple M4
0.01.242.930 I ggml_metal_init: using embedded metal library
0.01.249.352 I ggml_metal_init: GPU name:   Apple M4
0.01.249.355 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.249.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.249.357 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.249.358 I ggml_metal_init: simdgroup reduction   = true
0.01.249.358 I ggml_metal_init: simdgroup matrix mul. = true
0.01.249.358 I ggml_metal_init: has residency sets    = true
0.01.249.359 I ggml_metal_init: has bfloat            = true
0.01.249.359 I ggml_metal_init: use bfloat            = true
0.01.249.360 I ggml_metal_init: hasUnifiedMemory      = true
0.01.249.361 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.265.891 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.301.136 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.301.142 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.301.173 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.305.508 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.305.512 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.305.512 I llama_init_from_model: graph nodes  = 967
0.01.305.512 I llama_init_from_model: graph splits = 2
0.01.305.522 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.305.651 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.305.652 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.366.510 I main: llama threadpool init, n_threads = 4
0.01.366.556 I 
0.01.366.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.366.579 I 
0.01.366.741 I sampler seed: 1234
0.01.366.746 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.366.757 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.366.757 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.366.757 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.461.373 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.02.461.374 I llama_perf_context_print:        load time =    1354.96 ms
0.02.461.375 I llama_perf_context_print: prompt eval time =      48.92 ms /     7 tokens (    6.99 ms per token,   143.09 tokens per second)
0.02.461.376 I llama_perf_context_print:        eval time =    1042.88 ms /    63 runs   (   16.55 ms per token,    60.41 tokens per second)
0.02.461.376 I llama_perf_context_print:       total time =    1095.62 ms /    70 tokens
0.02.461.607 I ggml_metal_free: deallocating

real	0m2.485s
user	0m0.114s
sys	0m0.274s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.011.448 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.305 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.314 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.315 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.316 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.317 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.317 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.318 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.318 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.320 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.204 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.049 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.051 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.052 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.053 I llama_model_loader: - type  f32:  194 tensors
0.00.028.053 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.053 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.054 I print_info: file format = GGUF V3 (latest)
0.00.028.055 I print_info: file type   = Q4_0
0.00.028.056 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.263 I load: special tokens cache size = 25
0.00.042.348 I load: token to piece cache size = 0.2984 MB
0.00.042.351 I print_info: arch             = gptneox
0.00.042.351 I print_info: vocab_only       = 0
0.00.042.352 I print_info: n_ctx_train      = 2048
0.00.042.352 I print_info: n_embd           = 2048
0.00.042.352 I print_info: n_layer          = 24
0.00.042.357 I print_info: n_head           = 16
0.00.042.358 I print_info: n_head_kv        = 16
0.00.042.361 I print_info: n_rot            = 32
0.00.042.361 I print_info: n_swa            = 0
0.00.042.361 I print_info: n_embd_head_k    = 128
0.00.042.361 I print_info: n_embd_head_v    = 128
0.00.042.362 I print_info: n_gqa            = 1
0.00.042.363 I print_info: n_embd_k_gqa     = 2048
0.00.042.364 I print_info: n_embd_v_gqa     = 2048
0.00.042.364 I print_info: f_norm_eps       = 1.0e-05
0.00.042.365 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.365 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.365 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.365 I print_info: f_logit_scale    = 0.0e+00
0.00.042.366 I print_info: n_ff             = 8192
0.00.042.366 I print_info: n_expert         = 0
0.00.042.367 I print_info: n_expert_used    = 0
0.00.042.367 I print_info: causal attn      = 1
0.00.042.367 I print_info: pooling type     = 0
0.00.042.367 I print_info: rope type        = 2
0.00.042.367 I print_info: rope scaling     = linear
0.00.042.368 I print_info: freq_base_train  = 10000.0
0.00.042.368 I print_info: freq_scale_train = 1
0.00.042.368 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.369 I print_info: rope_finetuned   = unknown
0.00.042.369 I print_info: ssm_d_conv       = 0
0.00.042.369 I print_info: ssm_d_inner      = 0
0.00.042.369 I print_info: ssm_d_state      = 0
0.00.042.369 I print_info: ssm_dt_rank      = 0
0.00.042.369 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.370 I print_info: model type       = 1.4B
0.00.042.370 I print_info: model params     = 1.41 B
0.00.042.371 I print_info: general.name     = 1.4B
0.00.042.371 I print_info: vocab type       = BPE
0.00.042.371 I print_info: n_vocab          = 50304
0.00.042.372 I print_info: n_merges         = 50009
0.00.042.372 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.372 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.372 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.373 I print_info: LF token         = 187 'Ċ'
0.00.042.373 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.373 I print_info: max token length = 1024
0.00.042.374 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.683 I load_tensors: offloading output layer to GPU
0.00.587.684 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.719 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.587.721 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.589.442 I llama_init_from_model: n_seq_max     = 1
0.00.589.445 I llama_init_from_model: n_ctx         = 2048
0.00.589.445 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.589.446 I llama_init_from_model: n_batch       = 2048
0.00.589.446 I llama_init_from_model: n_ubatch      = 512
0.00.589.447 I llama_init_from_model: flash_attn    = 0
0.00.589.449 I llama_init_from_model: freq_base     = 10000.0
0.00.589.449 I llama_init_from_model: freq_scale    = 1
0.00.589.451 I ggml_metal_init: allocating
0.00.589.531 I ggml_metal_init: found device: Apple M4
0.00.589.543 I ggml_metal_init: picking default device: Apple M4
0.00.591.396 I ggml_metal_init: using embedded metal library
0.00.597.209 I ggml_metal_init: GPU name:   Apple M4
0.00.597.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.216 I ggml_metal_init: simdgroup reduction   = true
0.00.597.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.216 I ggml_metal_init: has residency sets    = true
0.00.597.217 I ggml_metal_init: has bfloat            = true
0.00.597.217 I ggml_metal_init: use bfloat            = true
0.00.597.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.600 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.145 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.673.154 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.673.189 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.415 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.677.417 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.677.418 I llama_init_from_model: graph nodes  = 967
0.00.677.418 I llama_init_from_model: graph splits = 2
0.00.677.421 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.677.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.292 I main: llama threadpool init, n_threads = 4
0.00.732.337 I 
0.00.732.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.361 I 
0.00.732.509 I sampler seed: 1234
0.00.732.513 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.524 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.526 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.526 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.410.717 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48630.14 tokens per second)
0.01.410.718 I llama_perf_context_print:        load time =     720.10 ms
0.01.410.719 I llama_perf_context_print: prompt eval time =      48.99 ms /     7 tokens (    7.00 ms per token,   142.87 tokens per second)
0.01.410.719 I llama_perf_context_print:        eval time =     626.28 ms /    63 runs   (    9.94 ms per token,   100.59 tokens per second)
0.01.410.720 I llama_perf_context_print:       total time =     679.17 ms /    70 tokens
0.01.410.944 I ggml_metal_free: deallocating

real	0m1.428s
user	0m0.111s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.375 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.962 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.967 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.974 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.974 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.974 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.975 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.976 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.976 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.977 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.977 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.977 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.978 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.979 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.980 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.305 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.305 I llama_model_loader: - type  f32:  194 tensors
0.00.026.306 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.306 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.306 I print_info: file format = GGUF V3 (latest)
0.00.026.307 I print_info: file type   = Q4_1
0.00.026.308 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.103 I load: special tokens cache size = 25
0.00.040.081 I load: token to piece cache size = 0.2984 MB
0.00.040.083 I print_info: arch             = gptneox
0.00.040.084 I print_info: vocab_only       = 0
0.00.040.084 I print_info: n_ctx_train      = 2048
0.00.040.084 I print_info: n_embd           = 2048
0.00.040.084 I print_info: n_layer          = 24
0.00.040.087 I print_info: n_head           = 16
0.00.040.088 I print_info: n_head_kv        = 16
0.00.040.088 I print_info: n_rot            = 32
0.00.040.088 I print_info: n_swa            = 0
0.00.040.088 I print_info: n_embd_head_k    = 128
0.00.040.090 I print_info: n_embd_head_v    = 128
0.00.040.091 I print_info: n_gqa            = 1
0.00.040.092 I print_info: n_embd_k_gqa     = 2048
0.00.040.092 I print_info: n_embd_v_gqa     = 2048
0.00.040.093 I print_info: f_norm_eps       = 1.0e-05
0.00.040.094 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.094 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.094 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.094 I print_info: f_logit_scale    = 0.0e+00
0.00.040.095 I print_info: n_ff             = 8192
0.00.040.095 I print_info: n_expert         = 0
0.00.040.095 I print_info: n_expert_used    = 0
0.00.040.095 I print_info: causal attn      = 1
0.00.040.095 I print_info: pooling type     = 0
0.00.040.096 I print_info: rope type        = 2
0.00.040.096 I print_info: rope scaling     = linear
0.00.040.096 I print_info: freq_base_train  = 10000.0
0.00.040.097 I print_info: freq_scale_train = 1
0.00.040.097 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.097 I print_info: rope_finetuned   = unknown
0.00.040.097 I print_info: ssm_d_conv       = 0
0.00.040.097 I print_info: ssm_d_inner      = 0
0.00.040.097 I print_info: ssm_d_state      = 0
0.00.040.098 I print_info: ssm_dt_rank      = 0
0.00.040.098 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.098 I print_info: model type       = 1.4B
0.00.040.098 I print_info: model params     = 1.41 B
0.00.040.099 I print_info: general.name     = 1.4B
0.00.040.099 I print_info: vocab type       = BPE
0.00.040.099 I print_info: n_vocab          = 50304
0.00.040.100 I print_info: n_merges         = 50009
0.00.040.100 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.100 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.100 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.101 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.101 I print_info: LF token         = 187 'Ċ'
0.00.040.101 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.101 I print_info: max token length = 1024
0.00.040.102 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.296 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.308 I load_tensors: offloading output layer to GPU
0.00.628.309 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.345 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.628.346 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.629.708 I llama_init_from_model: n_seq_max     = 1
0.00.629.711 I llama_init_from_model: n_ctx         = 2048
0.00.629.712 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.629.712 I llama_init_from_model: n_batch       = 2048
0.00.629.713 I llama_init_from_model: n_ubatch      = 512
0.00.629.713 I llama_init_from_model: flash_attn    = 0
0.00.629.716 I llama_init_from_model: freq_base     = 10000.0
0.00.629.716 I llama_init_from_model: freq_scale    = 1
0.00.629.719 I ggml_metal_init: allocating
0.00.629.779 I ggml_metal_init: found device: Apple M4
0.00.629.793 I ggml_metal_init: picking default device: Apple M4
0.00.631.573 I ggml_metal_init: using embedded metal library
0.00.637.145 I ggml_metal_init: GPU name:   Apple M4
0.00.637.156 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.157 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.158 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.159 I ggml_metal_init: simdgroup reduction   = true
0.00.637.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.159 I ggml_metal_init: has residency sets    = true
0.00.637.160 I ggml_metal_init: has bfloat            = true
0.00.637.160 I ggml_metal_init: use bfloat            = true
0.00.637.165 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.005 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.234 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.712.240 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.712.285 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.916 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.717.918 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.717.919 I llama_init_from_model: graph nodes  = 967
0.00.717.919 I llama_init_from_model: graph splits = 2
0.00.717.929 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.718.053 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.053 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.098 I main: llama threadpool init, n_threads = 4
0.00.772.143 I 
0.00.772.164 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.165 I 
0.00.772.318 I sampler seed: 1234
0.00.772.322 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.359 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.362 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.362 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.501.803 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.501.804 I llama_perf_context_print:        load time =     761.00 ms
0.01.501.805 I llama_perf_context_print: prompt eval time =      48.79 ms /     7 tokens (    6.97 ms per token,   143.47 tokens per second)
0.01.501.806 I llama_perf_context_print:        eval time =     677.87 ms /    63 runs   (   10.76 ms per token,    92.94 tokens per second)
0.01.501.806 I llama_perf_context_print:       total time =     730.42 ms /    70 tokens
0.01.501.993 I ggml_metal_free: deallocating

real	0m1.520s
user	0m0.110s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.077 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.495 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.506 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.506 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.507 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.261 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.276 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.017 I llama_model_loader: - type  f32:  194 tensors
0.00.026.018 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.018 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.018 I print_info: file format = GGUF V3 (latest)
0.00.026.019 I print_info: file type   = Q5_0
0.00.026.020 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.766 I load: special tokens cache size = 25
0.00.039.795 I load: token to piece cache size = 0.2984 MB
0.00.039.798 I print_info: arch             = gptneox
0.00.039.798 I print_info: vocab_only       = 0
0.00.039.798 I print_info: n_ctx_train      = 2048
0.00.039.798 I print_info: n_embd           = 2048
0.00.039.799 I print_info: n_layer          = 24
0.00.039.801 I print_info: n_head           = 16
0.00.039.802 I print_info: n_head_kv        = 16
0.00.039.802 I print_info: n_rot            = 32
0.00.039.803 I print_info: n_swa            = 0
0.00.039.803 I print_info: n_embd_head_k    = 128
0.00.039.803 I print_info: n_embd_head_v    = 128
0.00.039.804 I print_info: n_gqa            = 1
0.00.039.805 I print_info: n_embd_k_gqa     = 2048
0.00.039.805 I print_info: n_embd_v_gqa     = 2048
0.00.039.806 I print_info: f_norm_eps       = 1.0e-05
0.00.039.806 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.806 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.808 I print_info: f_logit_scale    = 0.0e+00
0.00.039.809 I print_info: n_ff             = 8192
0.00.039.809 I print_info: n_expert         = 0
0.00.039.809 I print_info: n_expert_used    = 0
0.00.039.809 I print_info: causal attn      = 1
0.00.039.809 I print_info: pooling type     = 0
0.00.039.810 I print_info: rope type        = 2
0.00.039.810 I print_info: rope scaling     = linear
0.00.039.810 I print_info: freq_base_train  = 10000.0
0.00.039.811 I print_info: freq_scale_train = 1
0.00.039.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.811 I print_info: rope_finetuned   = unknown
0.00.039.811 I print_info: ssm_d_conv       = 0
0.00.039.811 I print_info: ssm_d_inner      = 0
0.00.039.812 I print_info: ssm_d_state      = 0
0.00.039.812 I print_info: ssm_dt_rank      = 0
0.00.039.812 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.812 I print_info: model type       = 1.4B
0.00.039.813 I print_info: model params     = 1.41 B
0.00.039.813 I print_info: general.name     = 1.4B
0.00.039.813 I print_info: vocab type       = BPE
0.00.039.813 I print_info: n_vocab          = 50304
0.00.039.814 I print_info: n_merges         = 50009
0.00.039.814 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.814 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.814 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.815 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.815 I print_info: LF token         = 187 'Ċ'
0.00.039.815 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: max token length = 1024
0.00.039.817 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.144 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.155 I load_tensors: offloading output layer to GPU
0.00.634.155 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.191 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.634.193 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.635.724 I llama_init_from_model: n_seq_max     = 1
0.00.635.726 I llama_init_from_model: n_ctx         = 2048
0.00.635.727 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.728 I llama_init_from_model: n_batch       = 2048
0.00.635.728 I llama_init_from_model: n_ubatch      = 512
0.00.635.728 I llama_init_from_model: flash_attn    = 0
0.00.635.731 I llama_init_from_model: freq_base     = 10000.0
0.00.635.731 I llama_init_from_model: freq_scale    = 1
0.00.635.733 I ggml_metal_init: allocating
0.00.635.807 I ggml_metal_init: found device: Apple M4
0.00.635.820 I ggml_metal_init: picking default device: Apple M4
0.00.637.704 I ggml_metal_init: using embedded metal library
0.00.644.453 I ggml_metal_init: GPU name:   Apple M4
0.00.644.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.459 I ggml_metal_init: simdgroup reduction   = true
0.00.644.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.460 I ggml_metal_init: has residency sets    = true
0.00.644.460 I ggml_metal_init: has bfloat            = true
0.00.644.460 I ggml_metal_init: use bfloat            = true
0.00.644.461 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.463 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.698 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.757 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.731.763 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.731.797 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.736.185 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.736.187 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.736.188 I llama_init_from_model: graph nodes  = 967
0.00.736.188 I llama_init_from_model: graph splits = 2
0.00.736.194 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.736.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.561 I main: llama threadpool init, n_threads = 4
0.00.796.606 I 
0.00.796.628 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.629 I 
0.00.796.783 I sampler seed: 1234
0.00.796.788 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.809 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.809 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.809 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.599.222 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.599.222 I llama_perf_context_print:        load time =     785.77 ms
0.01.599.223 I llama_perf_context_print: prompt eval time =      52.97 ms /     7 tokens (    7.57 ms per token,   132.15 tokens per second)
0.01.599.224 I llama_perf_context_print:        eval time =     746.51 ms /    63 runs   (   11.85 ms per token,    84.39 tokens per second)
0.01.599.225 I llama_perf_context_print:       total time =     803.38 ms /    70 tokens
0.01.599.474 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.111s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.800 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.432 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.433 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.435 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.437 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.437 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.445 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.446 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.446 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.598 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.630 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.669 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.672 I llama_model_loader: - type  f32:  194 tensors
0.00.025.673 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.673 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.673 I print_info: file format = GGUF V3 (latest)
0.00.025.674 I print_info: file type   = Q5_1
0.00.025.676 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.778 I load: special tokens cache size = 25
0.00.039.698 I load: token to piece cache size = 0.2984 MB
0.00.039.701 I print_info: arch             = gptneox
0.00.039.701 I print_info: vocab_only       = 0
0.00.039.701 I print_info: n_ctx_train      = 2048
0.00.039.702 I print_info: n_embd           = 2048
0.00.039.702 I print_info: n_layer          = 24
0.00.039.705 I print_info: n_head           = 16
0.00.039.705 I print_info: n_head_kv        = 16
0.00.039.706 I print_info: n_rot            = 32
0.00.039.707 I print_info: n_swa            = 0
0.00.039.708 I print_info: n_embd_head_k    = 128
0.00.039.708 I print_info: n_embd_head_v    = 128
0.00.039.708 I print_info: n_gqa            = 1
0.00.039.709 I print_info: n_embd_k_gqa     = 2048
0.00.039.715 I print_info: n_embd_v_gqa     = 2048
0.00.039.716 I print_info: f_norm_eps       = 1.0e-05
0.00.039.720 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.720 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.720 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.721 I print_info: f_logit_scale    = 0.0e+00
0.00.039.722 I print_info: n_ff             = 8192
0.00.039.722 I print_info: n_expert         = 0
0.00.039.722 I print_info: n_expert_used    = 0
0.00.039.722 I print_info: causal attn      = 1
0.00.039.722 I print_info: pooling type     = 0
0.00.039.724 I print_info: rope type        = 2
0.00.039.725 I print_info: rope scaling     = linear
0.00.039.725 I print_info: freq_base_train  = 10000.0
0.00.039.725 I print_info: freq_scale_train = 1
0.00.039.725 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.726 I print_info: rope_finetuned   = unknown
0.00.039.726 I print_info: ssm_d_conv       = 0
0.00.039.726 I print_info: ssm_d_inner      = 0
0.00.039.726 I print_info: ssm_d_state      = 0
0.00.039.726 I print_info: ssm_dt_rank      = 0
0.00.039.726 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.726 I print_info: model type       = 1.4B
0.00.039.727 I print_info: model params     = 1.41 B
0.00.039.727 I print_info: general.name     = 1.4B
0.00.039.727 I print_info: vocab type       = BPE
0.00.039.730 I print_info: n_vocab          = 50304
0.00.039.730 I print_info: n_merges         = 50009
0.00.039.730 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.730 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.731 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.731 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.731 I print_info: LF token         = 187 'Ċ'
0.00.039.731 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.733 I print_info: max token length = 1024
0.00.039.733 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.507 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.521 I load_tensors: offloading output layer to GPU
0.00.601.522 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.554 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.601.555 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.603.226 I llama_init_from_model: n_seq_max     = 1
0.00.603.229 I llama_init_from_model: n_ctx         = 2048
0.00.603.229 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.603.230 I llama_init_from_model: n_batch       = 2048
0.00.603.230 I llama_init_from_model: n_ubatch      = 512
0.00.603.231 I llama_init_from_model: flash_attn    = 0
0.00.603.231 I llama_init_from_model: freq_base     = 10000.0
0.00.603.232 I llama_init_from_model: freq_scale    = 1
0.00.603.233 I ggml_metal_init: allocating
0.00.603.249 I ggml_metal_init: found device: Apple M4
0.00.603.257 I ggml_metal_init: picking default device: Apple M4
0.00.604.755 I ggml_metal_init: using embedded metal library
0.00.611.537 I ggml_metal_init: GPU name:   Apple M4
0.00.611.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.542 I ggml_metal_init: simdgroup reduction   = true
0.00.611.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.543 I ggml_metal_init: has residency sets    = true
0.00.611.543 I ggml_metal_init: has bfloat            = true
0.00.611.543 I ggml_metal_init: use bfloat            = true
0.00.611.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.465 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.225 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.233 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.267 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.566 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.568 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.568 I llama_init_from_model: graph nodes  = 967
0.00.685.569 I llama_init_from_model: graph splits = 2
0.00.685.576 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.688 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.613 I main: llama threadpool init, n_threads = 4
0.00.746.668 I 
0.00.746.690 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.690 I 
0.00.746.857 I sampler seed: 1234
0.00.746.862 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.883 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.884 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.884 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.595.444 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.595.445 I llama_perf_context_print:        load time =     737.09 ms
0.01.595.445 I llama_perf_context_print: prompt eval time =      52.08 ms /     7 tokens (    7.44 ms per token,   134.42 tokens per second)
0.01.595.447 I llama_perf_context_print:        eval time =     793.52 ms /    63 runs   (   12.60 ms per token,    79.39 tokens per second)
0.01.595.447 I llama_perf_context_print:       total time =     849.55 ms /    70 tokens
0.01.595.726 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.110s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.001 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.472 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.478 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.479 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.479 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.480 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.480 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.481 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.483 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.484 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.485 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.485 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.289 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.342 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.114 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.114 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.114 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.114 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.115 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.115 I llama_model_loader: - type  f32:  194 tensors
0.00.025.115 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.116 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.116 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.116 I print_info: file format = GGUF V3 (latest)
0.00.025.117 I print_info: file type   = Q2_K - Medium
0.00.025.117 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.917 I load: special tokens cache size = 25
0.00.038.659 I load: token to piece cache size = 0.2984 MB
0.00.038.662 I print_info: arch             = gptneox
0.00.038.662 I print_info: vocab_only       = 0
0.00.038.662 I print_info: n_ctx_train      = 2048
0.00.038.662 I print_info: n_embd           = 2048
0.00.038.662 I print_info: n_layer          = 24
0.00.038.665 I print_info: n_head           = 16
0.00.038.666 I print_info: n_head_kv        = 16
0.00.038.666 I print_info: n_rot            = 32
0.00.038.667 I print_info: n_swa            = 0
0.00.038.667 I print_info: n_embd_head_k    = 128
0.00.038.667 I print_info: n_embd_head_v    = 128
0.00.038.668 I print_info: n_gqa            = 1
0.00.038.669 I print_info: n_embd_k_gqa     = 2048
0.00.038.669 I print_info: n_embd_v_gqa     = 2048
0.00.038.670 I print_info: f_norm_eps       = 1.0e-05
0.00.038.670 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.671 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.671 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.671 I print_info: f_logit_scale    = 0.0e+00
0.00.038.671 I print_info: n_ff             = 8192
0.00.038.672 I print_info: n_expert         = 0
0.00.038.672 I print_info: n_expert_used    = 0
0.00.038.672 I print_info: causal attn      = 1
0.00.038.672 I print_info: pooling type     = 0
0.00.038.673 I print_info: rope type        = 2
0.00.038.673 I print_info: rope scaling     = linear
0.00.038.674 I print_info: freq_base_train  = 10000.0
0.00.038.674 I print_info: freq_scale_train = 1
0.00.038.675 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.675 I print_info: rope_finetuned   = unknown
0.00.038.675 I print_info: ssm_d_conv       = 0
0.00.038.675 I print_info: ssm_d_inner      = 0
0.00.038.677 I print_info: ssm_d_state      = 0
0.00.038.677 I print_info: ssm_dt_rank      = 0
0.00.038.677 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.677 I print_info: model type       = 1.4B
0.00.038.678 I print_info: model params     = 1.41 B
0.00.038.678 I print_info: general.name     = 1.4B
0.00.038.679 I print_info: vocab type       = BPE
0.00.038.679 I print_info: n_vocab          = 50304
0.00.038.679 I print_info: n_merges         = 50009
0.00.038.679 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.679 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.679 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.680 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.684 I print_info: LF token         = 187 'Ċ'
0.00.038.684 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.684 I print_info: max token length = 1024
0.00.038.685 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.335.628 I load_tensors: offloading 24 repeating layers to GPU
0.00.335.643 I load_tensors: offloading output layer to GPU
0.00.335.644 I load_tensors: offloaded 25/25 layers to GPU
0.00.335.675 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.335.677 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.348 I llama_init_from_model: n_seq_max     = 1
0.00.337.351 I llama_init_from_model: n_ctx         = 2048
0.00.337.351 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.337.352 I llama_init_from_model: n_batch       = 2048
0.00.337.352 I llama_init_from_model: n_ubatch      = 512
0.00.337.352 I llama_init_from_model: flash_attn    = 0
0.00.337.354 I llama_init_from_model: freq_base     = 10000.0
0.00.337.354 I llama_init_from_model: freq_scale    = 1
0.00.337.357 I ggml_metal_init: allocating
0.00.337.454 I ggml_metal_init: found device: Apple M4
0.00.337.467 I ggml_metal_init: picking default device: Apple M4
0.00.339.447 I ggml_metal_init: using embedded metal library
0.00.345.022 I ggml_metal_init: GPU name:   Apple M4
0.00.345.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.037 I ggml_metal_init: simdgroup reduction   = true
0.00.345.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.038 I ggml_metal_init: has residency sets    = true
0.00.345.038 I ggml_metal_init: has bfloat            = true
0.00.345.039 I ggml_metal_init: use bfloat            = true
0.00.345.043 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.367.438 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.437.084 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.437.091 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.437.131 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.441.547 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.441.549 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.441.550 I llama_init_from_model: graph nodes  = 967
0.00.441.550 I llama_init_from_model: graph splits = 2
0.00.441.556 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.441.689 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.441.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.463 I main: llama threadpool init, n_threads = 4
0.00.502.510 I 
0.00.502.534 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.535 I 
0.00.502.716 I sampler seed: 1234
0.00.502.720 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.502.731 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.502.732 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.502.733 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.185.078 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.185.081 I llama_perf_context_print:        load time =     491.74 ms
0.01.185.083 I llama_perf_context_print: prompt eval time =      44.31 ms /     7 tokens (    6.33 ms per token,   157.99 tokens per second)
0.01.185.084 I llama_perf_context_print:        eval time =     635.12 ms /    63 runs   (   10.08 ms per token,    99.19 tokens per second)
0.01.185.084 I llama_perf_context_print:       total time =     683.34 ms /    70 tokens
0.01.185.340 I ggml_metal_free: deallocating

real	0m1.203s
user	0m0.112s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.269 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.674 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.678 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.678 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.678 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.679 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.530 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.512 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.318 I llama_model_loader: - type  f32:  194 tensors
0.00.025.319 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.319 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.319 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.319 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.320 I print_info: file format = GGUF V3 (latest)
0.00.025.320 I print_info: file type   = Q3_K - Medium
0.00.025.321 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.072 I load: special tokens cache size = 25
0.00.038.975 I load: token to piece cache size = 0.2984 MB
0.00.038.977 I print_info: arch             = gptneox
0.00.038.978 I print_info: vocab_only       = 0
0.00.038.978 I print_info: n_ctx_train      = 2048
0.00.038.978 I print_info: n_embd           = 2048
0.00.038.978 I print_info: n_layer          = 24
0.00.038.981 I print_info: n_head           = 16
0.00.038.982 I print_info: n_head_kv        = 16
0.00.038.982 I print_info: n_rot            = 32
0.00.038.983 I print_info: n_swa            = 0
0.00.038.984 I print_info: n_embd_head_k    = 128
0.00.038.984 I print_info: n_embd_head_v    = 128
0.00.038.985 I print_info: n_gqa            = 1
0.00.038.986 I print_info: n_embd_k_gqa     = 2048
0.00.038.987 I print_info: n_embd_v_gqa     = 2048
0.00.038.987 I print_info: f_norm_eps       = 1.0e-05
0.00.038.988 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.988 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.988 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.988 I print_info: f_logit_scale    = 0.0e+00
0.00.038.989 I print_info: n_ff             = 8192
0.00.038.989 I print_info: n_expert         = 0
0.00.038.989 I print_info: n_expert_used    = 0
0.00.038.991 I print_info: causal attn      = 1
0.00.038.992 I print_info: pooling type     = 0
0.00.038.992 I print_info: rope type        = 2
0.00.038.993 I print_info: rope scaling     = linear
0.00.038.993 I print_info: freq_base_train  = 10000.0
0.00.038.993 I print_info: freq_scale_train = 1
0.00.038.993 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.994 I print_info: rope_finetuned   = unknown
0.00.038.994 I print_info: ssm_d_conv       = 0
0.00.038.994 I print_info: ssm_d_inner      = 0
0.00.038.994 I print_info: ssm_d_state      = 0
0.00.038.994 I print_info: ssm_dt_rank      = 0
0.00.038.994 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.995 I print_info: model type       = 1.4B
0.00.038.995 I print_info: model params     = 1.41 B
0.00.038.995 I print_info: general.name     = 1.4B
0.00.038.996 I print_info: vocab type       = BPE
0.00.038.996 I print_info: n_vocab          = 50304
0.00.038.996 I print_info: n_merges         = 50009
0.00.038.996 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.996 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.996 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.997 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.997 I print_info: LF token         = 187 'Ċ'
0.00.038.997 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.997 I print_info: max token length = 1024
0.00.039.001 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.952 I load_tensors: offloading output layer to GPU
0.00.457.953 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.985 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.457.986 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.459.632 I llama_init_from_model: n_seq_max     = 1
0.00.459.638 I llama_init_from_model: n_ctx         = 2048
0.00.459.638 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.459.639 I llama_init_from_model: n_batch       = 2048
0.00.459.639 I llama_init_from_model: n_ubatch      = 512
0.00.459.639 I llama_init_from_model: flash_attn    = 0
0.00.459.640 I llama_init_from_model: freq_base     = 10000.0
0.00.459.641 I llama_init_from_model: freq_scale    = 1
0.00.459.644 I ggml_metal_init: allocating
0.00.459.691 I ggml_metal_init: found device: Apple M4
0.00.459.704 I ggml_metal_init: picking default device: Apple M4
0.00.461.529 I ggml_metal_init: using embedded metal library
0.00.467.317 I ggml_metal_init: GPU name:   Apple M4
0.00.467.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.467.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.467.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.467.331 I ggml_metal_init: simdgroup reduction   = true
0.00.467.331 I ggml_metal_init: simdgroup matrix mul. = true
0.00.467.331 I ggml_metal_init: has residency sets    = true
0.00.467.332 I ggml_metal_init: has bfloat            = true
0.00.467.332 I ggml_metal_init: use bfloat            = true
0.00.467.354 I ggml_metal_init: hasUnifiedMemory      = true
0.00.467.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.488.178 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.987 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.542.993 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.543.026 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.547.484 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.547.485 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.547.486 I llama_init_from_model: graph nodes  = 967
0.00.547.486 I llama_init_from_model: graph splits = 2
0.00.547.491 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.547.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.547.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.605.377 I main: llama threadpool init, n_threads = 4
0.00.605.417 I 
0.00.605.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.605.439 I 
0.00.605.601 I sampler seed: 1234
0.00.605.606 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.605.616 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.605.617 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.605.617 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.348.331 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49754.73 tokens per second)
0.01.348.331 I llama_perf_context_print:        load time =     595.33 ms
0.01.348.332 I llama_perf_context_print: prompt eval time =      49.83 ms /     7 tokens (    7.12 ms per token,   140.48 tokens per second)
0.01.348.334 I llama_perf_context_print:        eval time =     690.00 ms /    63 runs   (   10.95 ms per token,    91.30 tokens per second)
0.01.348.334 I llama_perf_context_print:       total time =     743.73 ms /    70 tokens
0.01.348.535 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.110s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.265 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.273 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.274 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.274 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.276 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.151 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.903 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.903 I llama_model_loader: - type  f32:  194 tensors
0.00.024.904 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.904 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.904 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.905 I print_info: file format = GGUF V3 (latest)
0.00.024.905 I print_info: file type   = Q4_K - Medium
0.00.024.906 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.707 I load: special tokens cache size = 25
0.00.038.717 I load: token to piece cache size = 0.2984 MB
0.00.038.719 I print_info: arch             = gptneox
0.00.038.719 I print_info: vocab_only       = 0
0.00.038.720 I print_info: n_ctx_train      = 2048
0.00.038.720 I print_info: n_embd           = 2048
0.00.038.720 I print_info: n_layer          = 24
0.00.038.723 I print_info: n_head           = 16
0.00.038.723 I print_info: n_head_kv        = 16
0.00.038.723 I print_info: n_rot            = 32
0.00.038.724 I print_info: n_swa            = 0
0.00.038.724 I print_info: n_embd_head_k    = 128
0.00.038.724 I print_info: n_embd_head_v    = 128
0.00.038.724 I print_info: n_gqa            = 1
0.00.038.725 I print_info: n_embd_k_gqa     = 2048
0.00.038.726 I print_info: n_embd_v_gqa     = 2048
0.00.038.726 I print_info: f_norm_eps       = 1.0e-05
0.00.038.727 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.727 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.727 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.727 I print_info: f_logit_scale    = 0.0e+00
0.00.038.728 I print_info: n_ff             = 8192
0.00.038.728 I print_info: n_expert         = 0
0.00.038.728 I print_info: n_expert_used    = 0
0.00.038.728 I print_info: causal attn      = 1
0.00.038.730 I print_info: pooling type     = 0
0.00.038.732 I print_info: rope type        = 2
0.00.038.732 I print_info: rope scaling     = linear
0.00.038.733 I print_info: freq_base_train  = 10000.0
0.00.038.733 I print_info: freq_scale_train = 1
0.00.038.733 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.734 I print_info: rope_finetuned   = unknown
0.00.038.734 I print_info: ssm_d_conv       = 0
0.00.038.735 I print_info: ssm_d_inner      = 0
0.00.038.735 I print_info: ssm_d_state      = 0
0.00.038.735 I print_info: ssm_dt_rank      = 0
0.00.038.735 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.735 I print_info: model type       = 1.4B
0.00.038.736 I print_info: model params     = 1.41 B
0.00.038.736 I print_info: general.name     = 1.4B
0.00.038.736 I print_info: vocab type       = BPE
0.00.038.737 I print_info: n_vocab          = 50304
0.00.038.737 I print_info: n_merges         = 50009
0.00.038.737 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.739 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.739 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.739 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.739 I print_info: LF token         = 187 'Ċ'
0.00.038.739 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.740 I print_info: max token length = 1024
0.00.038.740 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.512.095 I load_tensors: offloading 24 repeating layers to GPU
0.00.512.105 I load_tensors: offloading output layer to GPU
0.00.512.106 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.142 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.512.143 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.513.658 I llama_init_from_model: n_seq_max     = 1
0.00.513.662 I llama_init_from_model: n_ctx         = 2048
0.00.513.663 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.513.663 I llama_init_from_model: n_batch       = 2048
0.00.513.663 I llama_init_from_model: n_ubatch      = 512
0.00.513.664 I llama_init_from_model: flash_attn    = 0
0.00.513.666 I llama_init_from_model: freq_base     = 10000.0
0.00.513.667 I llama_init_from_model: freq_scale    = 1
0.00.513.669 I ggml_metal_init: allocating
0.00.513.742 I ggml_metal_init: found device: Apple M4
0.00.513.755 I ggml_metal_init: picking default device: Apple M4
0.00.515.670 I ggml_metal_init: using embedded metal library
0.00.521.971 I ggml_metal_init: GPU name:   Apple M4
0.00.521.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.979 I ggml_metal_init: simdgroup reduction   = true
0.00.521.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.979 I ggml_metal_init: has residency sets    = true
0.00.521.980 I ggml_metal_init: has bfloat            = true
0.00.521.980 I ggml_metal_init: use bfloat            = true
0.00.521.981 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.541.050 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.582 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.611.589 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.611.625 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.616.206 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.616.209 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.616.209 I llama_init_from_model: graph nodes  = 967
0.00.616.209 I llama_init_from_model: graph splits = 2
0.00.616.214 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.616.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.616.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.939 I main: llama threadpool init, n_threads = 4
0.00.671.979 I 
0.00.671.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.001 I 
0.00.672.140 I sampler seed: 1234
0.00.672.145 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.672.173 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.672.174 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.672.174 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.435.612 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46557.38 tokens per second)
0.01.435.612 I llama_perf_context_print:        load time =     662.32 ms
0.01.435.613 I llama_perf_context_print: prompt eval time =      47.29 ms /     7 tokens (    6.76 ms per token,   148.01 tokens per second)
0.01.435.614 I llama_perf_context_print:        eval time =     713.37 ms /    63 runs   (   11.32 ms per token,    88.31 tokens per second)
0.01.435.614 I llama_perf_context_print:       total time =     764.47 ms /    70 tokens
0.01.435.919 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.110s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.362 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.206 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.212 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.214 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.215 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.216 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.079 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.943 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.946 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.946 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.947 I llama_model_loader: - type  f32:  194 tensors
0.00.026.947 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.947 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.948 I print_info: file format = GGUF V3 (latest)
0.00.026.949 I print_info: file type   = Q5_K - Medium
0.00.026.950 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.140 I load: special tokens cache size = 25
0.00.041.224 I load: token to piece cache size = 0.2984 MB
0.00.041.229 I print_info: arch             = gptneox
0.00.041.229 I print_info: vocab_only       = 0
0.00.041.229 I print_info: n_ctx_train      = 2048
0.00.041.229 I print_info: n_embd           = 2048
0.00.041.229 I print_info: n_layer          = 24
0.00.041.233 I print_info: n_head           = 16
0.00.041.234 I print_info: n_head_kv        = 16
0.00.041.234 I print_info: n_rot            = 32
0.00.041.234 I print_info: n_swa            = 0
0.00.041.234 I print_info: n_embd_head_k    = 128
0.00.041.236 I print_info: n_embd_head_v    = 128
0.00.041.237 I print_info: n_gqa            = 1
0.00.041.238 I print_info: n_embd_k_gqa     = 2048
0.00.041.238 I print_info: n_embd_v_gqa     = 2048
0.00.041.239 I print_info: f_norm_eps       = 1.0e-05
0.00.041.239 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.239 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.239 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.240 I print_info: f_logit_scale    = 0.0e+00
0.00.041.240 I print_info: n_ff             = 8192
0.00.041.240 I print_info: n_expert         = 0
0.00.041.241 I print_info: n_expert_used    = 0
0.00.041.241 I print_info: causal attn      = 1
0.00.041.241 I print_info: pooling type     = 0
0.00.041.241 I print_info: rope type        = 2
0.00.041.241 I print_info: rope scaling     = linear
0.00.041.241 I print_info: freq_base_train  = 10000.0
0.00.041.242 I print_info: freq_scale_train = 1
0.00.041.242 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.242 I print_info: rope_finetuned   = unknown
0.00.041.242 I print_info: ssm_d_conv       = 0
0.00.041.242 I print_info: ssm_d_inner      = 0
0.00.041.242 I print_info: ssm_d_state      = 0
0.00.041.243 I print_info: ssm_dt_rank      = 0
0.00.041.243 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.243 I print_info: model type       = 1.4B
0.00.041.243 I print_info: model params     = 1.41 B
0.00.041.243 I print_info: general.name     = 1.4B
0.00.041.244 I print_info: vocab type       = BPE
0.00.041.244 I print_info: n_vocab          = 50304
0.00.041.244 I print_info: n_merges         = 50009
0.00.041.244 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.245 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.245 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.245 I print_info: LF token         = 187 'Ċ'
0.00.041.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.246 I print_info: max token length = 1024
0.00.041.246 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.570.314 I load_tensors: offloading 24 repeating layers to GPU
0.00.570.320 I load_tensors: offloading output layer to GPU
0.00.570.320 I load_tensors: offloaded 25/25 layers to GPU
0.00.570.340 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.570.342 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.571.196 I llama_init_from_model: n_seq_max     = 1
0.00.571.199 I llama_init_from_model: n_ctx         = 2048
0.00.571.199 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.571.200 I llama_init_from_model: n_batch       = 2048
0.00.571.200 I llama_init_from_model: n_ubatch      = 512
0.00.571.200 I llama_init_from_model: flash_attn    = 0
0.00.571.202 I llama_init_from_model: freq_base     = 10000.0
0.00.571.202 I llama_init_from_model: freq_scale    = 1
0.00.571.203 I ggml_metal_init: allocating
0.00.571.239 I ggml_metal_init: found device: Apple M4
0.00.571.250 I ggml_metal_init: picking default device: Apple M4
0.00.572.326 I ggml_metal_init: using embedded metal library
0.00.576.635 I ggml_metal_init: GPU name:   Apple M4
0.00.576.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.576.641 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.576.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.576.642 I ggml_metal_init: simdgroup reduction   = true
0.00.576.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.576.642 I ggml_metal_init: has residency sets    = true
0.00.576.642 I ggml_metal_init: has bfloat            = true
0.00.576.643 I ggml_metal_init: use bfloat            = true
0.00.576.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.576.646 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.593.722 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.626.288 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.626.295 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.626.333 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.820 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.630.822 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.630.822 I llama_init_from_model: graph nodes  = 967
0.00.630.823 I llama_init_from_model: graph splits = 2
0.00.630.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.630.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.630.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.887 I main: llama threadpool init, n_threads = 4
0.00.691.928 I 
0.00.691.948 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.951 I 
0.00.692.075 I sampler seed: 1234
0.00.692.079 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.088 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.089 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.089 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.551.887 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.551.887 I llama_perf_context_print:        load time =     680.73 ms
0.01.551.888 I llama_perf_context_print: prompt eval time =      61.94 ms /     7 tokens (    8.85 ms per token,   113.01 tokens per second)
0.01.551.889 I llama_perf_context_print:        eval time =     795.07 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.551.890 I llama_perf_context_print:       total time =     860.79 ms /    70 tokens
0.01.552.114 I ggml_metal_free: deallocating

real	0m1.572s
user	0m0.106s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.776 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.920 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.934 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.941 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.749 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.757 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.507 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.509 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.509 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.509 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.510 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.510 I llama_model_loader: - type  f32:  194 tensors
0.00.025.511 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.511 I print_info: file format = GGUF V3 (latest)
0.00.025.512 I print_info: file type   = Q6_K
0.00.025.513 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.761 I load: special tokens cache size = 25
0.00.039.716 I load: token to piece cache size = 0.2984 MB
0.00.039.718 I print_info: arch             = gptneox
0.00.039.719 I print_info: vocab_only       = 0
0.00.039.719 I print_info: n_ctx_train      = 2048
0.00.039.719 I print_info: n_embd           = 2048
0.00.039.719 I print_info: n_layer          = 24
0.00.039.722 I print_info: n_head           = 16
0.00.039.723 I print_info: n_head_kv        = 16
0.00.039.723 I print_info: n_rot            = 32
0.00.039.723 I print_info: n_swa            = 0
0.00.039.723 I print_info: n_embd_head_k    = 128
0.00.039.723 I print_info: n_embd_head_v    = 128
0.00.039.724 I print_info: n_gqa            = 1
0.00.039.725 I print_info: n_embd_k_gqa     = 2048
0.00.039.727 I print_info: n_embd_v_gqa     = 2048
0.00.039.728 I print_info: f_norm_eps       = 1.0e-05
0.00.039.728 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.729 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.729 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.729 I print_info: f_logit_scale    = 0.0e+00
0.00.039.730 I print_info: n_ff             = 8192
0.00.039.730 I print_info: n_expert         = 0
0.00.039.730 I print_info: n_expert_used    = 0
0.00.039.730 I print_info: causal attn      = 1
0.00.039.730 I print_info: pooling type     = 0
0.00.039.731 I print_info: rope type        = 2
0.00.039.731 I print_info: rope scaling     = linear
0.00.039.731 I print_info: freq_base_train  = 10000.0
0.00.039.733 I print_info: freq_scale_train = 1
0.00.039.733 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.733 I print_info: rope_finetuned   = unknown
0.00.039.734 I print_info: ssm_d_conv       = 0
0.00.039.734 I print_info: ssm_d_inner      = 0
0.00.039.734 I print_info: ssm_d_state      = 0
0.00.039.734 I print_info: ssm_dt_rank      = 0
0.00.039.734 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.734 I print_info: model type       = 1.4B
0.00.039.735 I print_info: model params     = 1.41 B
0.00.039.735 I print_info: general.name     = 1.4B
0.00.039.735 I print_info: vocab type       = BPE
0.00.039.736 I print_info: n_vocab          = 50304
0.00.039.736 I print_info: n_merges         = 50009
0.00.039.736 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.736 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.736 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.736 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.740 I print_info: LF token         = 187 'Ċ'
0.00.039.740 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.741 I print_info: max token length = 1024
0.00.039.743 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.094 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.099 I load_tensors: offloading output layer to GPU
0.00.638.100 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.125 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.638.129 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.446 I llama_init_from_model: n_seq_max     = 1
0.00.639.448 I llama_init_from_model: n_ctx         = 2048
0.00.639.448 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.448 I llama_init_from_model: n_batch       = 2048
0.00.639.449 I llama_init_from_model: n_ubatch      = 512
0.00.639.449 I llama_init_from_model: flash_attn    = 0
0.00.639.450 I llama_init_from_model: freq_base     = 10000.0
0.00.639.451 I llama_init_from_model: freq_scale    = 1
0.00.639.451 I ggml_metal_init: allocating
0.00.639.466 I ggml_metal_init: found device: Apple M4
0.00.639.472 I ggml_metal_init: picking default device: Apple M4
0.00.640.872 I ggml_metal_init: using embedded metal library
0.00.647.093 I ggml_metal_init: GPU name:   Apple M4
0.00.647.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.099 I ggml_metal_init: simdgroup reduction   = true
0.00.647.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.100 I ggml_metal_init: has residency sets    = true
0.00.647.100 I ggml_metal_init: has bfloat            = true
0.00.647.100 I ggml_metal_init: use bfloat            = true
0.00.647.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.054 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.114 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.714.121 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.714.155 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.347 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.718.349 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.718.349 I llama_init_from_model: graph nodes  = 967
0.00.718.350 I llama_init_from_model: graph splits = 2
0.00.718.356 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.718.490 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.697 I main: llama threadpool init, n_threads = 4
0.00.784.738 I 
0.00.784.758 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.760 I 
0.00.784.910 I sampler seed: 1234
0.00.784.916 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.945 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.946 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.700.906 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.700.907 I llama_perf_context_print:        load time =     775.13 ms
0.01.700.908 I llama_perf_context_print: prompt eval time =      57.56 ms /     7 tokens (    8.22 ms per token,   121.62 tokens per second)
0.01.700.909 I llama_perf_context_print:        eval time =     855.52 ms /    63 runs   (   13.58 ms per token,    73.64 tokens per second)
0.01.700.910 I llama_perf_context_print:       total time =     917.00 ms /    70 tokens
0.01.701.203 I ggml_metal_free: deallocating

real	0m1.716s
user	0m0.107s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.641 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.238 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.792 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.811 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.818 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.268 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.269 I llama_model_loader: - type  f32:  194 tensors
0.00.057.269 I llama_model_loader: - type  f16:   98 tensors
0.00.057.270 I print_info: file format = GGUF V3 (latest)
0.00.057.271 I print_info: file type   = all F32 (guessed)
0.00.057.273 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.299 I load: special tokens cache size = 25
0.00.077.501 I load: token to piece cache size = 0.2984 MB
0.00.077.505 I print_info: arch             = gptneox
0.00.077.505 I print_info: vocab_only       = 0
0.00.077.506 I print_info: n_ctx_train      = 2048
0.00.077.506 I print_info: n_embd           = 2048
0.00.077.506 I print_info: n_layer          = 24
0.00.077.510 I print_info: n_head           = 16
0.00.077.511 I print_info: n_head_kv        = 16
0.00.077.511 I print_info: n_rot            = 32
0.00.077.511 I print_info: n_swa            = 0
0.00.077.511 I print_info: n_embd_head_k    = 128
0.00.077.512 I print_info: n_embd_head_v    = 128
0.00.077.512 I print_info: n_gqa            = 1
0.00.077.513 I print_info: n_embd_k_gqa     = 2048
0.00.077.514 I print_info: n_embd_v_gqa     = 2048
0.00.077.516 I print_info: f_norm_eps       = 1.0e-05
0.00.077.516 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.516 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.516 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.517 I print_info: f_logit_scale    = 0.0e+00
0.00.077.518 I print_info: n_ff             = 8192
0.00.077.518 I print_info: n_expert         = 0
0.00.077.518 I print_info: n_expert_used    = 0
0.00.077.519 I print_info: causal attn      = 1
0.00.077.519 I print_info: pooling type     = 0
0.00.077.519 I print_info: rope type        = 2
0.00.077.520 I print_info: rope scaling     = linear
0.00.077.520 I print_info: freq_base_train  = 10000.0
0.00.077.520 I print_info: freq_scale_train = 1
0.00.077.520 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.521 I print_info: rope_finetuned   = unknown
0.00.077.523 I print_info: ssm_d_conv       = 0
0.00.077.523 I print_info: ssm_d_inner      = 0
0.00.077.523 I print_info: ssm_d_state      = 0
0.00.077.523 I print_info: ssm_dt_rank      = 0
0.00.077.523 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.523 I print_info: model type       = 1.4B
0.00.077.524 I print_info: model params     = 1.41 B
0.00.077.524 I print_info: general.name     = 1.4B
0.00.077.524 I print_info: vocab type       = BPE
0.00.077.525 I print_info: n_vocab          = 50304
0.00.077.525 I print_info: n_merges         = 50009
0.00.077.525 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.525 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.525 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.526 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.527 I print_info: LF token         = 187 'Ċ'
0.00.077.531 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.531 I print_info: max token length = 1024
0.00.077.532 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.452.726 I load_tensors: offloading 24 repeating layers to GPU
0.01.452.731 I load_tensors: offloading output layer to GPU
0.01.452.732 I load_tensors: offloaded 25/25 layers to GPU
0.01.452.754 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.452.756 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.453.472 I llama_init_from_model: n_seq_max     = 1
0.01.453.473 I llama_init_from_model: n_ctx         = 128
0.01.453.474 I llama_init_from_model: n_ctx_per_seq = 128
0.01.453.474 I llama_init_from_model: n_batch       = 128
0.01.453.474 I llama_init_from_model: n_ubatch      = 128
0.01.453.474 I llama_init_from_model: flash_attn    = 0
0.01.453.475 I llama_init_from_model: freq_base     = 10000.0
0.01.453.475 I llama_init_from_model: freq_scale    = 1
0.01.453.476 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.453.476 I ggml_metal_init: allocating
0.01.453.534 I ggml_metal_init: found device: Apple M4
0.01.453.541 I ggml_metal_init: picking default device: Apple M4
0.01.454.664 I ggml_metal_init: using embedded metal library
0.01.458.869 I ggml_metal_init: GPU name:   Apple M4
0.01.458.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.458.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.458.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.458.873 I ggml_metal_init: simdgroup reduction   = true
0.01.458.873 I ggml_metal_init: simdgroup matrix mul. = true
0.01.458.873 I ggml_metal_init: has residency sets    = true
0.01.458.873 I ggml_metal_init: has bfloat            = true
0.01.458.873 I ggml_metal_init: use bfloat            = true
0.01.458.874 I ggml_metal_init: hasUnifiedMemory      = true
0.01.458.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.470.304 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.472.137 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.472.141 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.472.166 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.473.914 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.473.915 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.473.916 I llama_init_from_model: graph nodes  = 967
0.01.473.916 I llama_init_from_model: graph splits = 2
0.01.473.917 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.473.917 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.509.165 I 
0.01.509.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.509.230 I perplexity: tokenizing the input ..
0.01.514.199 I perplexity: tokenization took 4.968 ms
0.01.514.220 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.633.073 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.634.387 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.634.400 I llama_perf_context_print:        load time =    1483.91 ms
0.01.634.401 I llama_perf_context_print: prompt eval time =     118.57 ms /   128 tokens (    0.93 ms per token,  1079.51 tokens per second)
0.01.634.401 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.634.403 I llama_perf_context_print:       total time =     125.24 ms /   129 tokens
0.01.634.736 I ggml_metal_free: deallocating

real	0m1.818s
user	0m0.099s
sys	0m0.269s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.284 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.764 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.771 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.772 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.772 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.775 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.776 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.776 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.777 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.779 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.779 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.779 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.286 I llama_model_loader: - type  f32:  194 tensors
0.00.026.286 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.287 I print_info: file format = GGUF V3 (latest)
0.00.026.288 I print_info: file type   = Q8_0
0.00.026.289 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.617 I load: special tokens cache size = 25
0.00.040.717 I load: token to piece cache size = 0.2984 MB
0.00.040.722 I print_info: arch             = gptneox
0.00.040.722 I print_info: vocab_only       = 0
0.00.040.722 I print_info: n_ctx_train      = 2048
0.00.040.722 I print_info: n_embd           = 2048
0.00.040.722 I print_info: n_layer          = 24
0.00.040.727 I print_info: n_head           = 16
0.00.040.728 I print_info: n_head_kv        = 16
0.00.040.728 I print_info: n_rot            = 32
0.00.040.729 I print_info: n_swa            = 0
0.00.040.729 I print_info: n_embd_head_k    = 128
0.00.040.729 I print_info: n_embd_head_v    = 128
0.00.040.730 I print_info: n_gqa            = 1
0.00.040.730 I print_info: n_embd_k_gqa     = 2048
0.00.040.731 I print_info: n_embd_v_gqa     = 2048
0.00.040.732 I print_info: f_norm_eps       = 1.0e-05
0.00.040.732 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.732 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.732 I print_info: f_logit_scale    = 0.0e+00
0.00.040.733 I print_info: n_ff             = 8192
0.00.040.733 I print_info: n_expert         = 0
0.00.040.733 I print_info: n_expert_used    = 0
0.00.040.733 I print_info: causal attn      = 1
0.00.040.734 I print_info: pooling type     = 0
0.00.040.734 I print_info: rope type        = 2
0.00.040.734 I print_info: rope scaling     = linear
0.00.040.739 I print_info: freq_base_train  = 10000.0
0.00.040.739 I print_info: freq_scale_train = 1
0.00.040.739 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.740 I print_info: rope_finetuned   = unknown
0.00.040.740 I print_info: ssm_d_conv       = 0
0.00.040.740 I print_info: ssm_d_inner      = 0
0.00.040.740 I print_info: ssm_d_state      = 0
0.00.040.740 I print_info: ssm_dt_rank      = 0
0.00.040.741 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.742 I print_info: model type       = 1.4B
0.00.040.743 I print_info: model params     = 1.41 B
0.00.040.743 I print_info: general.name     = 1.4B
0.00.040.743 I print_info: vocab type       = BPE
0.00.040.744 I print_info: n_vocab          = 50304
0.00.040.744 I print_info: n_merges         = 50009
0.00.040.744 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.744 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.744 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.744 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: LF token         = 187 'Ċ'
0.00.040.745 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: max token length = 1024
0.00.040.746 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.905.725 I load_tensors: offloading 24 repeating layers to GPU
0.00.905.734 I load_tensors: offloading output layer to GPU
0.00.905.734 I load_tensors: offloaded 25/25 layers to GPU
0.00.905.765 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.905.768 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.906.915 I llama_init_from_model: n_seq_max     = 1
0.00.906.917 I llama_init_from_model: n_ctx         = 128
0.00.906.917 I llama_init_from_model: n_ctx_per_seq = 128
0.00.906.917 I llama_init_from_model: n_batch       = 128
0.00.906.918 I llama_init_from_model: n_ubatch      = 128
0.00.906.918 I llama_init_from_model: flash_attn    = 0
0.00.906.919 I llama_init_from_model: freq_base     = 10000.0
0.00.906.920 I llama_init_from_model: freq_scale    = 1
0.00.906.920 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.906.921 I ggml_metal_init: allocating
0.00.907.005 I ggml_metal_init: found device: Apple M4
0.00.907.019 I ggml_metal_init: picking default device: Apple M4
0.00.908.481 I ggml_metal_init: using embedded metal library
0.00.914.178 I ggml_metal_init: GPU name:   Apple M4
0.00.914.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.914.182 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.914.183 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.914.183 I ggml_metal_init: simdgroup reduction   = true
0.00.914.183 I ggml_metal_init: simdgroup matrix mul. = true
0.00.914.183 I ggml_metal_init: has residency sets    = true
0.00.914.184 I ggml_metal_init: has bfloat            = true
0.00.914.184 I ggml_metal_init: use bfloat            = true
0.00.914.185 I ggml_metal_init: hasUnifiedMemory      = true
0.00.914.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.931.258 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.934.901 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.934.905 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.934.946 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.938.015 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.938.016 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.938.017 I llama_init_from_model: graph nodes  = 967
0.00.938.017 I llama_init_from_model: graph splits = 2
0.00.938.021 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.938.021 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.962.681 I 
0.00.962.742 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.962.759 I perplexity: tokenizing the input ..
0.00.969.650 I perplexity: tokenization took 6.886 ms
0.00.969.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.095.255 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.096.572 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.096.586 I llama_perf_context_print:        load time =     951.85 ms
0.01.096.588 I llama_perf_context_print: prompt eval time =     124.61 ms /   128 tokens (    0.97 ms per token,  1027.25 tokens per second)
0.01.096.589 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.096.589 I llama_perf_context_print:       total time =     133.91 ms /   129 tokens
0.01.097.060 I ggml_metal_free: deallocating

real	0m1.113s
user	0m0.079s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.377 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.608 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.615 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.617 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.618 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.618 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.620 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.620 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.620 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.621 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.624 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.624 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.563 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.617 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.501 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.503 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.503 I llama_model_loader: - type  f32:  194 tensors
0.00.026.504 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.504 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.505 I print_info: file format = GGUF V3 (latest)
0.00.026.505 I print_info: file type   = Q4_0
0.00.026.506 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.846 I load: special tokens cache size = 25
0.00.040.874 I load: token to piece cache size = 0.2984 MB
0.00.040.878 I print_info: arch             = gptneox
0.00.040.879 I print_info: vocab_only       = 0
0.00.040.879 I print_info: n_ctx_train      = 2048
0.00.040.879 I print_info: n_embd           = 2048
0.00.040.879 I print_info: n_layer          = 24
0.00.040.883 I print_info: n_head           = 16
0.00.040.884 I print_info: n_head_kv        = 16
0.00.040.884 I print_info: n_rot            = 32
0.00.040.884 I print_info: n_swa            = 0
0.00.040.885 I print_info: n_embd_head_k    = 128
0.00.040.885 I print_info: n_embd_head_v    = 128
0.00.040.885 I print_info: n_gqa            = 1
0.00.040.886 I print_info: n_embd_k_gqa     = 2048
0.00.040.887 I print_info: n_embd_v_gqa     = 2048
0.00.040.888 I print_info: f_norm_eps       = 1.0e-05
0.00.040.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.888 I print_info: f_logit_scale    = 0.0e+00
0.00.040.889 I print_info: n_ff             = 8192
0.00.040.889 I print_info: n_expert         = 0
0.00.040.889 I print_info: n_expert_used    = 0
0.00.040.889 I print_info: causal attn      = 1
0.00.040.890 I print_info: pooling type     = 0
0.00.040.890 I print_info: rope type        = 2
0.00.040.890 I print_info: rope scaling     = linear
0.00.040.890 I print_info: freq_base_train  = 10000.0
0.00.040.891 I print_info: freq_scale_train = 1
0.00.040.891 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.891 I print_info: rope_finetuned   = unknown
0.00.040.891 I print_info: ssm_d_conv       = 0
0.00.040.891 I print_info: ssm_d_inner      = 0
0.00.040.891 I print_info: ssm_d_state      = 0
0.00.040.891 I print_info: ssm_dt_rank      = 0
0.00.040.895 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.895 I print_info: model type       = 1.4B
0.00.040.895 I print_info: model params     = 1.41 B
0.00.040.895 I print_info: general.name     = 1.4B
0.00.040.896 I print_info: vocab type       = BPE
0.00.040.897 I print_info: n_vocab          = 50304
0.00.040.897 I print_info: n_merges         = 50009
0.00.040.897 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.898 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.898 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.898 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.899 I print_info: LF token         = 187 'Ċ'
0.00.040.900 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.900 I print_info: max token length = 1024
0.00.040.900 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.572.852 I load_tensors: offloading 24 repeating layers to GPU
0.00.572.887 I load_tensors: offloading output layer to GPU
0.00.572.887 I load_tensors: offloaded 25/25 layers to GPU
0.00.572.925 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.572.927 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.574.207 I llama_init_from_model: n_seq_max     = 1
0.00.574.210 I llama_init_from_model: n_ctx         = 128
0.00.574.211 I llama_init_from_model: n_ctx_per_seq = 128
0.00.574.212 I llama_init_from_model: n_batch       = 128
0.00.574.212 I llama_init_from_model: n_ubatch      = 128
0.00.574.212 I llama_init_from_model: flash_attn    = 0
0.00.574.217 I llama_init_from_model: freq_base     = 10000.0
0.00.574.218 I llama_init_from_model: freq_scale    = 1
0.00.574.218 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.574.221 I ggml_metal_init: allocating
0.00.574.312 I ggml_metal_init: found device: Apple M4
0.00.574.328 I ggml_metal_init: picking default device: Apple M4
0.00.576.147 I ggml_metal_init: using embedded metal library
0.00.581.910 I ggml_metal_init: GPU name:   Apple M4
0.00.581.925 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.581.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.581.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.581.928 I ggml_metal_init: simdgroup reduction   = true
0.00.581.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.581.929 I ggml_metal_init: has residency sets    = true
0.00.581.929 I ggml_metal_init: has bfloat            = true
0.00.581.929 I ggml_metal_init: use bfloat            = true
0.00.581.932 I ggml_metal_init: hasUnifiedMemory      = true
0.00.581.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.602.884 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.606.562 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.606.567 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.606.614 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.609.815 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.609.817 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.609.818 I llama_init_from_model: graph nodes  = 967
0.00.609.818 I llama_init_from_model: graph splits = 2
0.00.609.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.609.821 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.774 I 
0.00.638.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.859 I perplexity: tokenizing the input ..
0.00.645.189 I perplexity: tokenization took 6.328 ms
0.00.645.204 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.849 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.782.181 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.782.197 I llama_perf_context_print:        load time =     628.39 ms
0.00.782.198 I llama_perf_context_print: prompt eval time =     135.26 ms /   128 tokens (    1.06 ms per token,   946.30 tokens per second)
0.00.782.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.199 I llama_perf_context_print:       total time =     143.43 ms /   129 tokens
0.00.782.640 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.081s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.264 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.844 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.890 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.895 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.902 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.902 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.903 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.903 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.903 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.905 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.905 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.906 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.906 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.907 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.909 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.909 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.910 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.691 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.749 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.702 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.703 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.704 I llama_model_loader: - type  f32:  194 tensors
0.00.025.704 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.704 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.705 I print_info: file format = GGUF V3 (latest)
0.00.025.706 I print_info: file type   = Q4_1
0.00.025.707 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.034 I load: special tokens cache size = 25
0.00.040.208 I load: token to piece cache size = 0.2984 MB
0.00.040.213 I print_info: arch             = gptneox
0.00.040.213 I print_info: vocab_only       = 0
0.00.040.213 I print_info: n_ctx_train      = 2048
0.00.040.214 I print_info: n_embd           = 2048
0.00.040.214 I print_info: n_layer          = 24
0.00.040.218 I print_info: n_head           = 16
0.00.040.219 I print_info: n_head_kv        = 16
0.00.040.219 I print_info: n_rot            = 32
0.00.040.219 I print_info: n_swa            = 0
0.00.040.220 I print_info: n_embd_head_k    = 128
0.00.040.220 I print_info: n_embd_head_v    = 128
0.00.040.220 I print_info: n_gqa            = 1
0.00.040.221 I print_info: n_embd_k_gqa     = 2048
0.00.040.222 I print_info: n_embd_v_gqa     = 2048
0.00.040.222 I print_info: f_norm_eps       = 1.0e-05
0.00.040.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.225 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.226 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.226 I print_info: f_logit_scale    = 0.0e+00
0.00.040.227 I print_info: n_ff             = 8192
0.00.040.227 I print_info: n_expert         = 0
0.00.040.227 I print_info: n_expert_used    = 0
0.00.040.227 I print_info: causal attn      = 1
0.00.040.227 I print_info: pooling type     = 0
0.00.040.227 I print_info: rope type        = 2
0.00.040.228 I print_info: rope scaling     = linear
0.00.040.228 I print_info: freq_base_train  = 10000.0
0.00.040.228 I print_info: freq_scale_train = 1
0.00.040.228 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.229 I print_info: rope_finetuned   = unknown
0.00.040.229 I print_info: ssm_d_conv       = 0
0.00.040.229 I print_info: ssm_d_inner      = 0
0.00.040.229 I print_info: ssm_d_state      = 0
0.00.040.229 I print_info: ssm_dt_rank      = 0
0.00.040.229 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.230 I print_info: model type       = 1.4B
0.00.040.230 I print_info: model params     = 1.41 B
0.00.040.230 I print_info: general.name     = 1.4B
0.00.040.231 I print_info: vocab type       = BPE
0.00.040.231 I print_info: n_vocab          = 50304
0.00.040.231 I print_info: n_merges         = 50009
0.00.040.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.231 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.232 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.232 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.232 I print_info: LF token         = 187 'Ċ'
0.00.040.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.232 I print_info: max token length = 1024
0.00.040.233 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.576 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.582 I load_tensors: offloading output layer to GPU
0.00.660.583 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.612 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.660.614 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.661.847 I llama_init_from_model: n_seq_max     = 1
0.00.661.850 I llama_init_from_model: n_ctx         = 128
0.00.661.850 I llama_init_from_model: n_ctx_per_seq = 128
0.00.661.851 I llama_init_from_model: n_batch       = 128
0.00.661.851 I llama_init_from_model: n_ubatch      = 128
0.00.661.851 I llama_init_from_model: flash_attn    = 0
0.00.661.853 I llama_init_from_model: freq_base     = 10000.0
0.00.661.853 I llama_init_from_model: freq_scale    = 1
0.00.661.854 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.661.856 I ggml_metal_init: allocating
0.00.661.912 I ggml_metal_init: found device: Apple M4
0.00.661.924 I ggml_metal_init: picking default device: Apple M4
0.00.663.477 I ggml_metal_init: using embedded metal library
0.00.669.140 I ggml_metal_init: GPU name:   Apple M4
0.00.669.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.151 I ggml_metal_init: simdgroup reduction   = true
0.00.669.152 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.152 I ggml_metal_init: has residency sets    = true
0.00.669.152 I ggml_metal_init: has bfloat            = true
0.00.669.152 I ggml_metal_init: use bfloat            = true
0.00.669.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.158 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.068 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.618 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.692.622 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.692.659 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.695.940 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.695.942 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.695.942 I llama_init_from_model: graph nodes  = 967
0.00.695.943 I llama_init_from_model: graph splits = 2
0.00.695.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.695.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.511 I 
0.00.724.593 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.611 I perplexity: tokenizing the input ..
0.00.731.578 I perplexity: tokenization took 6.963 ms
0.00.731.597 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.864.797 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.866.177 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.866.190 I llama_perf_context_print:        load time =     714.66 ms
0.00.866.191 I llama_perf_context_print: prompt eval time =     132.29 ms /   128 tokens (    1.03 ms per token,   967.60 tokens per second)
0.00.866.192 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.192 I llama_perf_context_print:       total time =     141.68 ms /   129 tokens
0.00.866.584 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.080s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.030 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.969 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.981 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.982 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.983 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.983 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.984 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.984 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.985 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.987 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.988 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.990 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.829 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.746 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.748 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.748 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.749 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.749 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.749 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.750 I llama_model_loader: - type  f32:  194 tensors
0.00.024.750 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.751 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.751 I print_info: file format = GGUF V3 (latest)
0.00.024.752 I print_info: file type   = Q5_0
0.00.024.757 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.062 I load: special tokens cache size = 25
0.00.039.049 I load: token to piece cache size = 0.2984 MB
0.00.039.053 I print_info: arch             = gptneox
0.00.039.053 I print_info: vocab_only       = 0
0.00.039.054 I print_info: n_ctx_train      = 2048
0.00.039.054 I print_info: n_embd           = 2048
0.00.039.054 I print_info: n_layer          = 24
0.00.039.059 I print_info: n_head           = 16
0.00.039.059 I print_info: n_head_kv        = 16
0.00.039.060 I print_info: n_rot            = 32
0.00.039.061 I print_info: n_swa            = 0
0.00.039.061 I print_info: n_embd_head_k    = 128
0.00.039.062 I print_info: n_embd_head_v    = 128
0.00.039.063 I print_info: n_gqa            = 1
0.00.039.064 I print_info: n_embd_k_gqa     = 2048
0.00.039.064 I print_info: n_embd_v_gqa     = 2048
0.00.039.065 I print_info: f_norm_eps       = 1.0e-05
0.00.039.065 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.066 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.067 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.067 I print_info: f_logit_scale    = 0.0e+00
0.00.039.067 I print_info: n_ff             = 8192
0.00.039.068 I print_info: n_expert         = 0
0.00.039.068 I print_info: n_expert_used    = 0
0.00.039.068 I print_info: causal attn      = 1
0.00.039.068 I print_info: pooling type     = 0
0.00.039.068 I print_info: rope type        = 2
0.00.039.069 I print_info: rope scaling     = linear
0.00.039.092 I print_info: freq_base_train  = 10000.0
0.00.039.094 I print_info: freq_scale_train = 1
0.00.039.094 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.094 I print_info: rope_finetuned   = unknown
0.00.039.094 I print_info: ssm_d_conv       = 0
0.00.039.094 I print_info: ssm_d_inner      = 0
0.00.039.095 I print_info: ssm_d_state      = 0
0.00.039.095 I print_info: ssm_dt_rank      = 0
0.00.039.097 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.097 I print_info: model type       = 1.4B
0.00.039.098 I print_info: model params     = 1.41 B
0.00.039.098 I print_info: general.name     = 1.4B
0.00.039.098 I print_info: vocab type       = BPE
0.00.039.099 I print_info: n_vocab          = 50304
0.00.039.099 I print_info: n_merges         = 50009
0.00.039.099 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.099 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.099 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.100 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.100 I print_info: LF token         = 187 'Ċ'
0.00.039.100 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.101 I print_info: max token length = 1024
0.00.039.101 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.625.012 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.029 I load_tensors: offloading output layer to GPU
0.00.625.030 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.062 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.625.064 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.626.485 I llama_init_from_model: n_seq_max     = 1
0.00.626.487 I llama_init_from_model: n_ctx         = 128
0.00.626.487 I llama_init_from_model: n_ctx_per_seq = 128
0.00.626.488 I llama_init_from_model: n_batch       = 128
0.00.626.488 I llama_init_from_model: n_ubatch      = 128
0.00.626.488 I llama_init_from_model: flash_attn    = 0
0.00.626.491 I llama_init_from_model: freq_base     = 10000.0
0.00.626.492 I llama_init_from_model: freq_scale    = 1
0.00.626.493 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.626.495 I ggml_metal_init: allocating
0.00.626.576 I ggml_metal_init: found device: Apple M4
0.00.626.587 I ggml_metal_init: picking default device: Apple M4
0.00.628.143 I ggml_metal_init: using embedded metal library
0.00.634.489 I ggml_metal_init: GPU name:   Apple M4
0.00.634.494 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.634.495 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.634.495 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.634.496 I ggml_metal_init: simdgroup reduction   = true
0.00.634.496 I ggml_metal_init: simdgroup matrix mul. = true
0.00.634.496 I ggml_metal_init: has residency sets    = true
0.00.634.496 I ggml_metal_init: has bfloat            = true
0.00.634.497 I ggml_metal_init: use bfloat            = true
0.00.634.498 I ggml_metal_init: hasUnifiedMemory      = true
0.00.634.500 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.450 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.941 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.945 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.990 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.298 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.658.299 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.658.300 I llama_init_from_model: graph nodes  = 967
0.00.658.301 I llama_init_from_model: graph splits = 2
0.00.658.303 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.658.303 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.960 I 
0.00.686.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.061 I perplexity: tokenizing the input ..
0.00.693.432 I perplexity: tokenization took 7.368 ms
0.00.693.461 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.540 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.830.878 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.830.893 I llama_perf_context_print:        load time =     676.92 ms
0.00.830.894 I llama_perf_context_print: prompt eval time =     135.22 ms /   128 tokens (    1.06 ms per token,   946.60 tokens per second)
0.00.830.895 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.895 I llama_perf_context_print:       total time =     144.94 ms /   129 tokens
0.00.831.245 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.757 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.760 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.760 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.762 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.764 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.765 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.767 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.767 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.534 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.400 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.402 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.402 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.403 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.404 I llama_model_loader: - type  f32:  194 tensors
0.00.025.404 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.404 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.405 I print_info: file format = GGUF V3 (latest)
0.00.025.405 I print_info: file type   = Q5_1
0.00.025.409 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.361 I load: special tokens cache size = 25
0.00.039.354 I load: token to piece cache size = 0.2984 MB
0.00.039.359 I print_info: arch             = gptneox
0.00.039.359 I print_info: vocab_only       = 0
0.00.039.359 I print_info: n_ctx_train      = 2048
0.00.039.360 I print_info: n_embd           = 2048
0.00.039.360 I print_info: n_layer          = 24
0.00.039.364 I print_info: n_head           = 16
0.00.039.365 I print_info: n_head_kv        = 16
0.00.039.365 I print_info: n_rot            = 32
0.00.039.366 I print_info: n_swa            = 0
0.00.039.366 I print_info: n_embd_head_k    = 128
0.00.039.366 I print_info: n_embd_head_v    = 128
0.00.039.367 I print_info: n_gqa            = 1
0.00.039.367 I print_info: n_embd_k_gqa     = 2048
0.00.039.368 I print_info: n_embd_v_gqa     = 2048
0.00.039.369 I print_info: f_norm_eps       = 1.0e-05
0.00.039.370 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.370 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.370 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.371 I print_info: f_logit_scale    = 0.0e+00
0.00.039.371 I print_info: n_ff             = 8192
0.00.039.371 I print_info: n_expert         = 0
0.00.039.373 I print_info: n_expert_used    = 0
0.00.039.373 I print_info: causal attn      = 1
0.00.039.373 I print_info: pooling type     = 0
0.00.039.373 I print_info: rope type        = 2
0.00.039.374 I print_info: rope scaling     = linear
0.00.039.374 I print_info: freq_base_train  = 10000.0
0.00.039.374 I print_info: freq_scale_train = 1
0.00.039.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.375 I print_info: rope_finetuned   = unknown
0.00.039.375 I print_info: ssm_d_conv       = 0
0.00.039.375 I print_info: ssm_d_inner      = 0
0.00.039.375 I print_info: ssm_d_state      = 0
0.00.039.375 I print_info: ssm_dt_rank      = 0
0.00.039.375 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.376 I print_info: model type       = 1.4B
0.00.039.376 I print_info: model params     = 1.41 B
0.00.039.376 I print_info: general.name     = 1.4B
0.00.039.376 I print_info: vocab type       = BPE
0.00.039.378 I print_info: n_vocab          = 50304
0.00.039.378 I print_info: n_merges         = 50009
0.00.039.378 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.379 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.379 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.379 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.379 I print_info: LF token         = 187 'Ċ'
0.00.039.379 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.380 I print_info: max token length = 1024
0.00.039.380 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.079 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.095 I load_tensors: offloading output layer to GPU
0.00.591.096 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.131 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.591.133 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.592.269 I llama_init_from_model: n_seq_max     = 1
0.00.592.272 I llama_init_from_model: n_ctx         = 128
0.00.592.273 I llama_init_from_model: n_ctx_per_seq = 128
0.00.592.273 I llama_init_from_model: n_batch       = 128
0.00.592.273 I llama_init_from_model: n_ubatch      = 128
0.00.592.274 I llama_init_from_model: flash_attn    = 0
0.00.592.276 I llama_init_from_model: freq_base     = 10000.0
0.00.592.276 I llama_init_from_model: freq_scale    = 1
0.00.592.277 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.592.279 I ggml_metal_init: allocating
0.00.592.342 I ggml_metal_init: found device: Apple M4
0.00.592.353 I ggml_metal_init: picking default device: Apple M4
0.00.593.930 I ggml_metal_init: using embedded metal library
0.00.600.213 I ggml_metal_init: GPU name:   Apple M4
0.00.600.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.217 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.218 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.219 I ggml_metal_init: simdgroup reduction   = true
0.00.600.219 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.219 I ggml_metal_init: has residency sets    = true
0.00.600.220 I ggml_metal_init: has bfloat            = true
0.00.600.220 I ggml_metal_init: use bfloat            = true
0.00.600.221 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.860 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.329 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.333 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.375 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.667 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.669 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.669 I llama_init_from_model: graph nodes  = 967
0.00.624.670 I llama_init_from_model: graph splits = 2
0.00.624.672 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.672 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.772 I 
0.00.652.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.879 I perplexity: tokenizing the input ..
0.00.660.412 I perplexity: tokenization took 7.53 ms
0.00.660.437 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.075 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.797.424 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.797.439 I llama_perf_context_print:        load time =     642.84 ms
0.00.797.440 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.75 tokens per second)
0.00.797.441 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.441 I llama_perf_context_print:       total time =     144.67 ms /   129 tokens
0.00.797.829 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.079s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.535 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.416 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.257 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.160 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.162 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.164 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.165 I llama_model_loader: - type  f32:  194 tensors
0.00.025.165 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.165 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.166 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.166 I print_info: file format = GGUF V3 (latest)
0.00.025.171 I print_info: file type   = Q2_K - Medium
0.00.025.172 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.467 I load: special tokens cache size = 25
0.00.039.463 I load: token to piece cache size = 0.2984 MB
0.00.039.468 I print_info: arch             = gptneox
0.00.039.468 I print_info: vocab_only       = 0
0.00.039.468 I print_info: n_ctx_train      = 2048
0.00.039.468 I print_info: n_embd           = 2048
0.00.039.469 I print_info: n_layer          = 24
0.00.039.473 I print_info: n_head           = 16
0.00.039.474 I print_info: n_head_kv        = 16
0.00.039.474 I print_info: n_rot            = 32
0.00.039.474 I print_info: n_swa            = 0
0.00.039.474 I print_info: n_embd_head_k    = 128
0.00.039.475 I print_info: n_embd_head_v    = 128
0.00.039.475 I print_info: n_gqa            = 1
0.00.039.476 I print_info: n_embd_k_gqa     = 2048
0.00.039.477 I print_info: n_embd_v_gqa     = 2048
0.00.039.477 I print_info: f_norm_eps       = 1.0e-05
0.00.039.478 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.478 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.478 I print_info: f_logit_scale    = 0.0e+00
0.00.039.479 I print_info: n_ff             = 8192
0.00.039.479 I print_info: n_expert         = 0
0.00.039.479 I print_info: n_expert_used    = 0
0.00.039.479 I print_info: causal attn      = 1
0.00.039.479 I print_info: pooling type     = 0
0.00.039.479 I print_info: rope type        = 2
0.00.039.480 I print_info: rope scaling     = linear
0.00.039.480 I print_info: freq_base_train  = 10000.0
0.00.039.480 I print_info: freq_scale_train = 1
0.00.039.480 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.481 I print_info: rope_finetuned   = unknown
0.00.039.481 I print_info: ssm_d_conv       = 0
0.00.039.481 I print_info: ssm_d_inner      = 0
0.00.039.481 I print_info: ssm_d_state      = 0
0.00.039.481 I print_info: ssm_dt_rank      = 0
0.00.039.481 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.482 I print_info: model type       = 1.4B
0.00.039.482 I print_info: model params     = 1.41 B
0.00.039.482 I print_info: general.name     = 1.4B
0.00.039.483 I print_info: vocab type       = BPE
0.00.039.483 I print_info: n_vocab          = 50304
0.00.039.483 I print_info: n_merges         = 50009
0.00.039.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.484 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: LF token         = 187 'Ċ'
0.00.039.487 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: max token length = 1024
0.00.039.488 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.335.336 I load_tensors: offloading 24 repeating layers to GPU
0.00.335.354 I load_tensors: offloading output layer to GPU
0.00.335.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.335.395 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.335.396 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.163 I llama_init_from_model: n_seq_max     = 1
0.00.337.173 I llama_init_from_model: n_ctx         = 128
0.00.337.174 I llama_init_from_model: n_ctx_per_seq = 128
0.00.337.174 I llama_init_from_model: n_batch       = 128
0.00.337.175 I llama_init_from_model: n_ubatch      = 128
0.00.337.175 I llama_init_from_model: flash_attn    = 0
0.00.337.177 I llama_init_from_model: freq_base     = 10000.0
0.00.337.177 I llama_init_from_model: freq_scale    = 1
0.00.337.178 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.337.180 I ggml_metal_init: allocating
0.00.337.248 I ggml_metal_init: found device: Apple M4
0.00.337.259 I ggml_metal_init: picking default device: Apple M4
0.00.339.473 I ggml_metal_init: using embedded metal library
0.00.345.633 I ggml_metal_init: GPU name:   Apple M4
0.00.345.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.644 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.644 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.645 I ggml_metal_init: simdgroup reduction   = true
0.00.345.645 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.646 I ggml_metal_init: has residency sets    = true
0.00.345.646 I ggml_metal_init: has bfloat            = true
0.00.345.646 I ggml_metal_init: use bfloat            = true
0.00.345.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.652 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.047 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.372.043 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.372.053 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.372.136 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.375.753 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.375.755 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.375.755 I llama_init_from_model: graph nodes  = 967
0.00.375.756 I llama_init_from_model: graph splits = 2
0.00.375.758 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.375.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.406.064 I 
0.00.406.141 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.406.161 I perplexity: tokenizing the input ..
0.00.412.497 I perplexity: tokenization took 6.335 ms
0.00.412.513 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.544.591 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.546.071 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.546.083 I llama_perf_context_print:        load time =     396.52 ms
0.00.546.085 I llama_perf_context_print: prompt eval time =     131.66 ms /   128 tokens (    1.03 ms per token,   972.16 tokens per second)
0.00.546.088 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.546.090 I llama_perf_context_print:       total time =     140.02 ms /   129 tokens
0.00.546.483 I ggml_metal_free: deallocating

real	0m0.562s
user	0m0.080s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.102 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.111 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.111 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.112 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.112 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.114 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.114 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.117 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.117 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.118 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.118 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.118 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.120 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.120 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.120 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.989 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.814 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.815 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.816 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.817 I llama_model_loader: - type  f32:  194 tensors
0.00.024.818 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.818 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.818 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.819 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.819 I print_info: file format = GGUF V3 (latest)
0.00.024.820 I print_info: file type   = Q3_K - Medium
0.00.024.821 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.831 I load: special tokens cache size = 25
0.00.038.918 I load: token to piece cache size = 0.2984 MB
0.00.038.922 I print_info: arch             = gptneox
0.00.038.923 I print_info: vocab_only       = 0
0.00.038.923 I print_info: n_ctx_train      = 2048
0.00.038.923 I print_info: n_embd           = 2048
0.00.038.923 I print_info: n_layer          = 24
0.00.038.928 I print_info: n_head           = 16
0.00.038.929 I print_info: n_head_kv        = 16
0.00.038.929 I print_info: n_rot            = 32
0.00.038.929 I print_info: n_swa            = 0
0.00.038.929 I print_info: n_embd_head_k    = 128
0.00.038.929 I print_info: n_embd_head_v    = 128
0.00.038.930 I print_info: n_gqa            = 1
0.00.038.931 I print_info: n_embd_k_gqa     = 2048
0.00.038.934 I print_info: n_embd_v_gqa     = 2048
0.00.038.935 I print_info: f_norm_eps       = 1.0e-05
0.00.038.935 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.935 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.936 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.936 I print_info: f_logit_scale    = 0.0e+00
0.00.038.937 I print_info: n_ff             = 8192
0.00.038.937 I print_info: n_expert         = 0
0.00.038.937 I print_info: n_expert_used    = 0
0.00.038.937 I print_info: causal attn      = 1
0.00.038.937 I print_info: pooling type     = 0
0.00.038.937 I print_info: rope type        = 2
0.00.038.938 I print_info: rope scaling     = linear
0.00.038.940 I print_info: freq_base_train  = 10000.0
0.00.038.940 I print_info: freq_scale_train = 1
0.00.038.940 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.941 I print_info: rope_finetuned   = unknown
0.00.038.941 I print_info: ssm_d_conv       = 0
0.00.038.941 I print_info: ssm_d_inner      = 0
0.00.038.941 I print_info: ssm_d_state      = 0
0.00.038.941 I print_info: ssm_dt_rank      = 0
0.00.038.941 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.942 I print_info: model type       = 1.4B
0.00.038.942 I print_info: model params     = 1.41 B
0.00.038.942 I print_info: general.name     = 1.4B
0.00.038.943 I print_info: vocab type       = BPE
0.00.038.943 I print_info: n_vocab          = 50304
0.00.038.943 I print_info: n_merges         = 50009
0.00.038.943 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.950 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.951 I print_info: LF token         = 187 'Ċ'
0.00.038.951 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.951 I print_info: max token length = 1024
0.00.038.952 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.436.556 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.573 I load_tensors: offloading output layer to GPU
0.00.436.574 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.607 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.608 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.438.229 I llama_init_from_model: n_seq_max     = 1
0.00.438.231 I llama_init_from_model: n_ctx         = 128
0.00.438.232 I llama_init_from_model: n_ctx_per_seq = 128
0.00.438.232 I llama_init_from_model: n_batch       = 128
0.00.438.233 I llama_init_from_model: n_ubatch      = 128
0.00.438.234 I llama_init_from_model: flash_attn    = 0
0.00.438.236 I llama_init_from_model: freq_base     = 10000.0
0.00.438.237 I llama_init_from_model: freq_scale    = 1
0.00.438.237 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.438.239 I ggml_metal_init: allocating
0.00.438.339 I ggml_metal_init: found device: Apple M4
0.00.438.353 I ggml_metal_init: picking default device: Apple M4
0.00.440.196 I ggml_metal_init: using embedded metal library
0.00.446.621 I ggml_metal_init: GPU name:   Apple M4
0.00.446.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.632 I ggml_metal_init: simdgroup reduction   = true
0.00.446.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.633 I ggml_metal_init: has residency sets    = true
0.00.446.633 I ggml_metal_init: has bfloat            = true
0.00.446.634 I ggml_metal_init: use bfloat            = true
0.00.446.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.528 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.469.042 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.469.045 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.469.107 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.472.183 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.472.185 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.472.185 I llama_init_from_model: graph nodes  = 967
0.00.472.186 I llama_init_from_model: graph splits = 2
0.00.472.189 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.472.189 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.986 I 
0.00.503.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.090 I perplexity: tokenizing the input ..
0.00.510.057 I perplexity: tokenization took 6.965 ms
0.00.510.076 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.651.348 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.652.690 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.652.706 I llama_perf_context_print:        load time =     494.11 ms
0.00.652.707 I llama_perf_context_print: prompt eval time =     140.87 ms /   128 tokens (    1.10 ms per token,   908.64 tokens per second)
0.00.652.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.652.709 I llama_perf_context_print:       total time =     149.73 ms /   129 tokens
0.00.653.070 I ggml_metal_free: deallocating

real	0m0.668s
user	0m0.079s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.946 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.896 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.904 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.905 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.906 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.908 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.909 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.909 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.909 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.910 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.912 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.912 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.688 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.734 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.491 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.492 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.494 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.494 I llama_model_loader: - type  f32:  194 tensors
0.00.025.495 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.495 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.495 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.496 I print_info: file format = GGUF V3 (latest)
0.00.025.496 I print_info: file type   = Q4_K - Medium
0.00.025.498 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.638 I load: special tokens cache size = 25
0.00.039.462 I load: token to piece cache size = 0.2984 MB
0.00.039.466 I print_info: arch             = gptneox
0.00.039.466 I print_info: vocab_only       = 0
0.00.039.466 I print_info: n_ctx_train      = 2048
0.00.039.466 I print_info: n_embd           = 2048
0.00.039.466 I print_info: n_layer          = 24
0.00.039.471 I print_info: n_head           = 16
0.00.039.471 I print_info: n_head_kv        = 16
0.00.039.471 I print_info: n_rot            = 32
0.00.039.472 I print_info: n_swa            = 0
0.00.039.472 I print_info: n_embd_head_k    = 128
0.00.039.472 I print_info: n_embd_head_v    = 128
0.00.039.473 I print_info: n_gqa            = 1
0.00.039.473 I print_info: n_embd_k_gqa     = 2048
0.00.039.474 I print_info: n_embd_v_gqa     = 2048
0.00.039.475 I print_info: f_norm_eps       = 1.0e-05
0.00.039.475 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.475 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.475 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.476 I print_info: f_logit_scale    = 0.0e+00
0.00.039.476 I print_info: n_ff             = 8192
0.00.039.476 I print_info: n_expert         = 0
0.00.039.476 I print_info: n_expert_used    = 0
0.00.039.477 I print_info: causal attn      = 1
0.00.039.477 I print_info: pooling type     = 0
0.00.039.477 I print_info: rope type        = 2
0.00.039.477 I print_info: rope scaling     = linear
0.00.039.477 I print_info: freq_base_train  = 10000.0
0.00.039.478 I print_info: freq_scale_train = 1
0.00.039.478 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.478 I print_info: rope_finetuned   = unknown
0.00.039.478 I print_info: ssm_d_conv       = 0
0.00.039.478 I print_info: ssm_d_inner      = 0
0.00.039.478 I print_info: ssm_d_state      = 0
0.00.039.478 I print_info: ssm_dt_rank      = 0
0.00.039.478 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.479 I print_info: model type       = 1.4B
0.00.039.482 I print_info: model params     = 1.41 B
0.00.039.482 I print_info: general.name     = 1.4B
0.00.039.482 I print_info: vocab type       = BPE
0.00.039.482 I print_info: n_vocab          = 50304
0.00.039.483 I print_info: n_merges         = 50009
0.00.039.483 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.483 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.483 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.484 I print_info: LF token         = 187 'Ċ'
0.00.039.484 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.484 I print_info: max token length = 1024
0.00.039.484 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.509.789 I load_tensors: offloading 24 repeating layers to GPU
0.00.509.803 I load_tensors: offloading output layer to GPU
0.00.509.804 I load_tensors: offloaded 25/25 layers to GPU
0.00.509.838 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.509.839 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.511.624 I llama_init_from_model: n_seq_max     = 1
0.00.511.627 I llama_init_from_model: n_ctx         = 128
0.00.511.627 I llama_init_from_model: n_ctx_per_seq = 128
0.00.511.628 I llama_init_from_model: n_batch       = 128
0.00.511.628 I llama_init_from_model: n_ubatch      = 128
0.00.511.629 I llama_init_from_model: flash_attn    = 0
0.00.511.631 I llama_init_from_model: freq_base     = 10000.0
0.00.511.632 I llama_init_from_model: freq_scale    = 1
0.00.511.633 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.511.635 I ggml_metal_init: allocating
0.00.511.720 I ggml_metal_init: found device: Apple M4
0.00.511.733 I ggml_metal_init: picking default device: Apple M4
0.00.513.610 I ggml_metal_init: using embedded metal library
0.00.520.383 I ggml_metal_init: GPU name:   Apple M4
0.00.520.388 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.520.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.520.389 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.520.390 I ggml_metal_init: simdgroup reduction   = true
0.00.520.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.520.390 I ggml_metal_init: has residency sets    = true
0.00.520.391 I ggml_metal_init: has bfloat            = true
0.00.520.391 I ggml_metal_init: use bfloat            = true
0.00.520.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.520.395 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.537.925 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.541.478 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.541.485 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.541.542 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.545.049 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.545.051 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.545.051 I llama_init_from_model: graph nodes  = 967
0.00.545.051 I llama_init_from_model: graph splits = 2
0.00.545.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.545.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.763 I 
0.00.572.833 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.572.855 I perplexity: tokenizing the input ..
0.00.580.540 I perplexity: tokenization took 7.682 ms
0.00.580.566 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.723.553 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.724.889 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.724.908 I llama_perf_context_print:        load time =     562.81 ms
0.00.724.909 I llama_perf_context_print: prompt eval time =     142.04 ms /   128 tokens (    1.11 ms per token,   901.18 tokens per second)
0.00.724.909 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.724.910 I llama_perf_context_print:       total time =     152.15 ms /   129 tokens
0.00.725.264 I ggml_metal_free: deallocating

real	0m0.741s
user	0m0.080s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.905 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.128 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.128 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.128 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.967 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.774 I llama_model_loader: - type  f32:  194 tensors
0.00.024.774 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.774 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.775 I print_info: file format = GGUF V3 (latest)
0.00.024.775 I print_info: file type   = Q5_K - Medium
0.00.024.777 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.799 I load: special tokens cache size = 25
0.00.038.850 I load: token to piece cache size = 0.2984 MB
0.00.038.855 I print_info: arch             = gptneox
0.00.038.855 I print_info: vocab_only       = 0
0.00.038.855 I print_info: n_ctx_train      = 2048
0.00.038.855 I print_info: n_embd           = 2048
0.00.038.856 I print_info: n_layer          = 24
0.00.038.860 I print_info: n_head           = 16
0.00.038.860 I print_info: n_head_kv        = 16
0.00.038.861 I print_info: n_rot            = 32
0.00.038.861 I print_info: n_swa            = 0
0.00.038.861 I print_info: n_embd_head_k    = 128
0.00.038.861 I print_info: n_embd_head_v    = 128
0.00.038.862 I print_info: n_gqa            = 1
0.00.038.863 I print_info: n_embd_k_gqa     = 2048
0.00.038.865 I print_info: n_embd_v_gqa     = 2048
0.00.038.865 I print_info: f_norm_eps       = 1.0e-05
0.00.038.866 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.866 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.866 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.866 I print_info: f_logit_scale    = 0.0e+00
0.00.038.867 I print_info: n_ff             = 8192
0.00.038.867 I print_info: n_expert         = 0
0.00.038.867 I print_info: n_expert_used    = 0
0.00.038.867 I print_info: causal attn      = 1
0.00.038.867 I print_info: pooling type     = 0
0.00.038.867 I print_info: rope type        = 2
0.00.038.868 I print_info: rope scaling     = linear
0.00.038.868 I print_info: freq_base_train  = 10000.0
0.00.038.868 I print_info: freq_scale_train = 1
0.00.038.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.869 I print_info: rope_finetuned   = unknown
0.00.038.869 I print_info: ssm_d_conv       = 0
0.00.038.871 I print_info: ssm_d_inner      = 0
0.00.038.871 I print_info: ssm_d_state      = 0
0.00.038.871 I print_info: ssm_dt_rank      = 0
0.00.038.871 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.871 I print_info: model type       = 1.4B
0.00.038.872 I print_info: model params     = 1.41 B
0.00.038.872 I print_info: general.name     = 1.4B
0.00.038.873 I print_info: vocab type       = BPE
0.00.038.874 I print_info: n_vocab          = 50304
0.00.038.874 I print_info: n_merges         = 50009
0.00.038.874 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: LF token         = 187 'Ċ'
0.00.038.875 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: max token length = 1024
0.00.038.876 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.375 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.385 I load_tensors: offloading output layer to GPU
0.00.599.385 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.424 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.599.426 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.601.192 I llama_init_from_model: n_seq_max     = 1
0.00.601.198 I llama_init_from_model: n_ctx         = 128
0.00.601.198 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.199 I llama_init_from_model: n_batch       = 128
0.00.601.199 I llama_init_from_model: n_ubatch      = 128
0.00.601.199 I llama_init_from_model: flash_attn    = 0
0.00.601.200 I llama_init_from_model: freq_base     = 10000.0
0.00.601.201 I llama_init_from_model: freq_scale    = 1
0.00.601.201 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.204 I ggml_metal_init: allocating
0.00.601.296 I ggml_metal_init: found device: Apple M4
0.00.601.314 I ggml_metal_init: picking default device: Apple M4
0.00.603.454 I ggml_metal_init: using embedded metal library
0.00.610.494 I ggml_metal_init: GPU name:   Apple M4
0.00.610.501 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.503 I ggml_metal_init: simdgroup reduction   = true
0.00.610.504 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.504 I ggml_metal_init: has residency sets    = true
0.00.610.504 I ggml_metal_init: has bfloat            = true
0.00.610.504 I ggml_metal_init: use bfloat            = true
0.00.610.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.509 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.270 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.798 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.802 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.848 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.032 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.034 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.034 I llama_init_from_model: graph nodes  = 967
0.00.635.034 I llama_init_from_model: graph splits = 2
0.00.635.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.574 I 
0.00.669.650 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.669 I perplexity: tokenizing the input ..
0.00.677.016 I perplexity: tokenization took 7.346 ms
0.00.677.037 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.600 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.817.143 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.817.169 I llama_perf_context_print:        load time =     660.66 ms
0.00.817.169 I llama_perf_context_print: prompt eval time =     137.67 ms /   128 tokens (    1.08 ms per token,   929.78 tokens per second)
0.00.817.170 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.171 I llama_perf_context_print:       total time =     147.60 ms /   129 tokens
0.00.817.683 I ggml_metal_free: deallocating

real	0m0.832s
user	0m0.079s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.063 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.921 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.938 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.939 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.939 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.939 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.940 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.940 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.942 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.942 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.416 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.418 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.419 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.419 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.420 I llama_model_loader: - type  f32:  194 tensors
0.00.025.420 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.421 I print_info: file format = GGUF V3 (latest)
0.00.025.421 I print_info: file type   = Q6_K
0.00.025.422 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.390 I load: special tokens cache size = 25
0.00.039.272 I load: token to piece cache size = 0.2984 MB
0.00.039.277 I print_info: arch             = gptneox
0.00.039.277 I print_info: vocab_only       = 0
0.00.039.277 I print_info: n_ctx_train      = 2048
0.00.039.278 I print_info: n_embd           = 2048
0.00.039.278 I print_info: n_layer          = 24
0.00.039.282 I print_info: n_head           = 16
0.00.039.283 I print_info: n_head_kv        = 16
0.00.039.283 I print_info: n_rot            = 32
0.00.039.284 I print_info: n_swa            = 0
0.00.039.284 I print_info: n_embd_head_k    = 128
0.00.039.284 I print_info: n_embd_head_v    = 128
0.00.039.285 I print_info: n_gqa            = 1
0.00.039.286 I print_info: n_embd_k_gqa     = 2048
0.00.039.286 I print_info: n_embd_v_gqa     = 2048
0.00.039.287 I print_info: f_norm_eps       = 1.0e-05
0.00.039.287 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.287 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.287 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.288 I print_info: f_logit_scale    = 0.0e+00
0.00.039.288 I print_info: n_ff             = 8192
0.00.039.288 I print_info: n_expert         = 0
0.00.039.289 I print_info: n_expert_used    = 0
0.00.039.289 I print_info: causal attn      = 1
0.00.039.289 I print_info: pooling type     = 0
0.00.039.289 I print_info: rope type        = 2
0.00.039.289 I print_info: rope scaling     = linear
0.00.039.289 I print_info: freq_base_train  = 10000.0
0.00.039.290 I print_info: freq_scale_train = 1
0.00.039.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.290 I print_info: rope_finetuned   = unknown
0.00.039.290 I print_info: ssm_d_conv       = 0
0.00.039.290 I print_info: ssm_d_inner      = 0
0.00.039.291 I print_info: ssm_d_state      = 0
0.00.039.291 I print_info: ssm_dt_rank      = 0
0.00.039.291 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.291 I print_info: model type       = 1.4B
0.00.039.291 I print_info: model params     = 1.41 B
0.00.039.291 I print_info: general.name     = 1.4B
0.00.039.292 I print_info: vocab type       = BPE
0.00.039.292 I print_info: n_vocab          = 50304
0.00.039.292 I print_info: n_merges         = 50009
0.00.039.295 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.295 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.295 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.296 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.296 I print_info: LF token         = 187 'Ċ'
0.00.039.296 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.296 I print_info: max token length = 1024
0.00.039.297 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.552 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.558 I load_tensors: offloading output layer to GPU
0.00.610.559 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.585 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.610.588 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.612.146 I llama_init_from_model: n_seq_max     = 1
0.00.612.148 I llama_init_from_model: n_ctx         = 128
0.00.612.148 I llama_init_from_model: n_ctx_per_seq = 128
0.00.612.149 I llama_init_from_model: n_batch       = 128
0.00.612.149 I llama_init_from_model: n_ubatch      = 128
0.00.612.149 I llama_init_from_model: flash_attn    = 0
0.00.612.150 I llama_init_from_model: freq_base     = 10000.0
0.00.612.151 I llama_init_from_model: freq_scale    = 1
0.00.612.152 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.612.153 I ggml_metal_init: allocating
0.00.612.200 I ggml_metal_init: found device: Apple M4
0.00.612.213 I ggml_metal_init: picking default device: Apple M4
0.00.613.563 I ggml_metal_init: using embedded metal library
0.00.619.633 I ggml_metal_init: GPU name:   Apple M4
0.00.619.636 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.638 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.638 I ggml_metal_init: simdgroup reduction   = true
0.00.619.638 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.639 I ggml_metal_init: has residency sets    = true
0.00.619.639 I ggml_metal_init: has bfloat            = true
0.00.619.639 I ggml_metal_init: use bfloat            = true
0.00.619.640 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.613 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.043 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.640.047 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.205 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.312 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.643.314 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.643.314 I llama_init_from_model: graph nodes  = 967
0.00.643.315 I llama_init_from_model: graph splits = 2
0.00.643.317 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.025 I 
0.00.681.107 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.127 I perplexity: tokenizing the input ..
0.00.687.646 I perplexity: tokenization took 6.517 ms
0.00.687.662 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.620 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.830.962 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.830.975 I llama_perf_context_print:        load time =     670.95 ms
0.00.830.978 I llama_perf_context_print: prompt eval time =     141.56 ms /   128 tokens (    1.11 ms per token,   904.23 tokens per second)
0.00.830.979 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.979 I llama_perf_context_print:       total time =     149.96 ms /   129 tokens
0.00.831.355 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.076s
sys	0m0.135s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.294 I build: 4820 (1a24c462) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.585 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.690 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.703 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.708 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.225 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.637 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.639 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.640 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.640 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.640 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.641 I llama_model_loader: - type  f32:  194 tensors
0.00.055.642 I llama_model_loader: - type  f16:   98 tensors
0.00.055.642 I print_info: file format = GGUF V3 (latest)
0.00.055.643 I print_info: file type   = all F32 (guessed)
0.00.055.645 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.707 I load: special tokens cache size = 25
0.00.076.632 I load: token to piece cache size = 0.2984 MB
0.00.076.635 I print_info: arch             = gptneox
0.00.076.636 I print_info: vocab_only       = 0
0.00.076.636 I print_info: n_ctx_train      = 2048
0.00.076.636 I print_info: n_embd           = 2048
0.00.076.636 I print_info: n_layer          = 24
0.00.076.640 I print_info: n_head           = 16
0.00.076.643 I print_info: n_head_kv        = 16
0.00.076.643 I print_info: n_rot            = 32
0.00.076.643 I print_info: n_swa            = 0
0.00.076.643 I print_info: n_embd_head_k    = 128
0.00.076.643 I print_info: n_embd_head_v    = 128
0.00.076.644 I print_info: n_gqa            = 1
0.00.076.645 I print_info: n_embd_k_gqa     = 2048
0.00.076.645 I print_info: n_embd_v_gqa     = 2048
0.00.076.646 I print_info: f_norm_eps       = 1.0e-05
0.00.076.646 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.647 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.647 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.647 I print_info: f_logit_scale    = 0.0e+00
0.00.076.648 I print_info: n_ff             = 8192
0.00.076.648 I print_info: n_expert         = 0
0.00.076.648 I print_info: n_expert_used    = 0
0.00.076.648 I print_info: causal attn      = 1
0.00.076.649 I print_info: pooling type     = 0
0.00.076.649 I print_info: rope type        = 2
0.00.076.649 I print_info: rope scaling     = linear
0.00.076.649 I print_info: freq_base_train  = 10000.0
0.00.076.651 I print_info: freq_scale_train = 1
0.00.076.651 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.651 I print_info: rope_finetuned   = unknown
0.00.076.651 I print_info: ssm_d_conv       = 0
0.00.076.651 I print_info: ssm_d_inner      = 0
0.00.076.651 I print_info: ssm_d_state      = 0
0.00.076.652 I print_info: ssm_dt_rank      = 0
0.00.076.652 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.652 I print_info: model type       = 1.4B
0.00.076.652 I print_info: model params     = 1.41 B
0.00.076.653 I print_info: general.name     = 1.4B
0.00.076.653 I print_info: vocab type       = BPE
0.00.076.653 I print_info: n_vocab          = 50304
0.00.076.654 I print_info: n_merges         = 50009
0.00.076.656 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.656 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.656 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.656 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.657 I print_info: LF token         = 187 'Ċ'
0.00.076.657 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.657 I print_info: max token length = 1024
0.00.076.658 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.327.749 I load_tensors: offloading 24 repeating layers to GPU
0.01.327.755 I load_tensors: offloading output layer to GPU
0.01.327.756 I load_tensors: offloaded 25/25 layers to GPU
0.01.327.778 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.327.780 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.328.583 I llama_init_from_model: n_seq_max     = 1
0.01.328.584 I llama_init_from_model: n_ctx         = 128
0.01.328.585 I llama_init_from_model: n_ctx_per_seq = 128
0.01.328.585 I llama_init_from_model: n_batch       = 128
0.01.328.585 I llama_init_from_model: n_ubatch      = 128
0.01.328.586 I llama_init_from_model: flash_attn    = 0
0.01.328.586 I llama_init_from_model: freq_base     = 10000.0
0.01.328.587 I llama_init_from_model: freq_scale    = 1
0.01.328.587 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.328.588 I ggml_metal_init: allocating
0.01.328.621 I ggml_metal_init: found device: Apple M4
0.01.328.631 I ggml_metal_init: picking default device: Apple M4
0.01.329.713 I ggml_metal_init: using embedded metal library
0.01.333.950 I ggml_metal_init: GPU name:   Apple M4
0.01.333.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.333.953 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.333.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.333.954 I ggml_metal_init: simdgroup reduction   = true
0.01.333.954 I ggml_metal_init: simdgroup matrix mul. = true
0.01.333.954 I ggml_metal_init: has residency sets    = true
0.01.333.954 I ggml_metal_init: has bfloat            = true
0.01.333.954 I ggml_metal_init: use bfloat            = true
0.01.333.955 I ggml_metal_init: hasUnifiedMemory      = true
0.01.333.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.346.263 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.348.025 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.348.027 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.348.055 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.349.721 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.349.722 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.349.723 I llama_init_from_model: graph nodes  = 967
0.01.349.723 I llama_init_from_model: graph splits = 2
0.01.349.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.349.725 I 
0.01.349.759 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.349.760 I compute_imatrix: tokenizing the input ..
0.01.353.943 I compute_imatrix: tokenization took 4.182 ms
0.01.353.945 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.619.270 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.622.878 I llama_perf_context_print:        load time =    1595.68 ms
0.01.622.880 I llama_perf_context_print: prompt eval time =     263.55 ms /   128 tokens (    2.06 ms per token,   485.69 tokens per second)
0.01.622.880 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.622.881 I llama_perf_context_print:       total time =    1599.28 ms /   129 tokens
0.01.623.459 I ggml_metal_free: deallocating

real	0m1.806s
user	0m0.125s
sys	0m0.252s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4820 (1a24c462)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120f04d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120f05470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120f05a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120f05fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120f06b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120f070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120f07690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120f07c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120f08140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120f08640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120f08b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120f09660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120f09e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120f0a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120f0ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120f0b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120f0bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120f0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120f0ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120f0d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120f0d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120f0dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120f0e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120f0ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120f0f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120f0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120f104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120f10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120f10cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120f11170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120f11430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120f11cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120f12200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120f124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120f12960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120f12e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120f132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120f13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120f13be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120f14080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120f14520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120f149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120f14e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120f15120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120f15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120f15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120f16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120f16c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120f17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120f17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120f17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120f184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120f18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120f192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120f19750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120f19eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120f1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120f1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120f1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120f1b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120f1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120f1bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120f1c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120f1c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120f1cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120f1cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120f1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120f1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120f1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120f1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120f1e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120f1ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120f1f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120f1f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120f1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120f20180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120f206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120f20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120f21170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120f216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120f21c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120f22160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120f226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120f22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120f23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120f236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120f23bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120f24140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120f24690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120f24be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120f25130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120f25680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120f25bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120f26120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120f26670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120f16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120f26ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120f27290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120f277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120f27d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120f28280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120f287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120f28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120f29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120f297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120f29d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120f2a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120f2a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120f2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120f2b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120f2b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120f2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120f2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120f2c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120f2ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120f2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120f2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120f2d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120f2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120f2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120f2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120f2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120f2ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120f2f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120f2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120f2fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120f301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120f30640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120f30ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120f30f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120f31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120f318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120f31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120f32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120f326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120f32b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120f32fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120f33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120f33920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120f33dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120f34260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120f34700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120f34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120f35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120f354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120f35980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120f35e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120f362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120f36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120f36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120f370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120f37540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120f379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120f37e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120f38320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120f387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120f38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120f39100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120f395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120f39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120f39ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120f3a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120f3a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120f3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120f3b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120f3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120f3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120f3bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120f3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120f3c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120f3cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120f3d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120f3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120f3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120f3dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120f3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120f3e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120f3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120f3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120f3f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120f3fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120f40000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120f404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120f40940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120f40de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120f41280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120f41720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120f41bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120f42060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120f42500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120f429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120f42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120f43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120f43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120f43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120f441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120f447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120f44dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120f453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120f45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120f46060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120f46320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120f46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120f46f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120f47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120f47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120f48070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120f48510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120f48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120f49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120f49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120f49cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120f4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120f4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120f4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120f4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120f4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120f4bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120f4c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120f4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120f4cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120f4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120f4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120f4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120f4e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120f4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120f4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120f4f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120f4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120f4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120f501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120f506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120f50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120f51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120f516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120f51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120f52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120f526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120f52c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120f53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120f536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120f53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120f54160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120f546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120f54c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120f55150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120f556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120f55bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120f56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120f56690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120f56be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120f57130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120f57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120f57bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120f58120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120f58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120f58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120f59110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120f59660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120f59bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120f5a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120f5a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120f5aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120f5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120f5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120f5bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120f5bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120f5c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120f5c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120f5cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120f5d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120f5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120f5db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120f5dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120f5e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120f5e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120f5edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120f5f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120f5f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120f5fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x120f60040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x120f604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x120f60980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x120f60e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x120f612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x120f61760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x120f61c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x120f620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x120f62540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x120f629e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120f62f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120f63650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120f63d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120f64490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120f64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120f64e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120f65660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120f65920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120f65f30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.491.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.491.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x114f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x114f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x114f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x114f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x114f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x114f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x114f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x114f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x114f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x114f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x114f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x114f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x114f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x114f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x114f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x114f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x114f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x114f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x114f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x114f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x114f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x114f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x114f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x114f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x114f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x114f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x114f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x114f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x114f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x114f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x114f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x114f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x114f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x114f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x114f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x114f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x114f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x114f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x114f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x114f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x114f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x114f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x114f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x114f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x114f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x114f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x114f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x114f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x114f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x114f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x114f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x114f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x114f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x114f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x114f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x114f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x114f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x114f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x114f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x114f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x114f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x114f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x114f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x114f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x114f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x114f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x114f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x114f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x114f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x114f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x114f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x114f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x114f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x114f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x114f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x114f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x114f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x114f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x114f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x114f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x114f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x114f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x114f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x114f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x114f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x114f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x114f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x114f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x114f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x114f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x114f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x114f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x114f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x114f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x114f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x114f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x114f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x114f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x114f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x114f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x114f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x114f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x114f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x114f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x114f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x114f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x114f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x114f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x114f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x114f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x114f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x114f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x114f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x114f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x114f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x114f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x114f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x114f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x114f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x114f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x114f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x114f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x114f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x114f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x114f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x114f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x114f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x114f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x114f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x114f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x114f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x114f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x114f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x114f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x114f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x114f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x114f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x114f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x114f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x114f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x114f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x114f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x114f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x114f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x114f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x114f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x114f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x114f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x114f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x114f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x114f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x114f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x114f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x114f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x114f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x114f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x114f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x114f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x114f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x114f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x114f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x114f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x114f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x114f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x114f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x114f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x114f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x114f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x114f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x114f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x114f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x114f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x114f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x114f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x114f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x114f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x114f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x114f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x114f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x114f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x114f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x114f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x114f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x114f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x114f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x114f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x114f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x114f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x114f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x114f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x114f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x114f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x114f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x114f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x114f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x114f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x114f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x114f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x114f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x114f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x114f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x114f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x114f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x114f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x114f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x114f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x114f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x114f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x114f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x114f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x114f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x114f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x114f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x114f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x114f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x114f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x114f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x114f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x114f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x114f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x114f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x114f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x114f5ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x114f5bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x114f5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x114f5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x114f5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x114f5d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x114f5d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x114f5dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x114f5e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114f5e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114f5f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114f5f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114f5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114f606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114f60970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x114f61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114f61420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114f61a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130e07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130e07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130e08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130e084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130e08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130e08da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130e09210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x130e09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130e09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130e09f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130e0a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130e0aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130e0b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130e0bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x130e0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x130e0ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x130e0d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x130e0db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x130e0e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x130e0e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x130e0f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x130e0f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x130e0fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x130e10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x130e10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x130e10fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x130e112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130e11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x130e11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130e11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130e12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130e12990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130e12e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130e130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130e13530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130e139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130e13e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130e14280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130e146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130e14b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130e14fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130e15440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130e158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130e15d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130e16190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130e16600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130e16a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130e16ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130e17350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130e177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130e17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130e180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130e18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130e18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130e18df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130e19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130e197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130e19cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130e1a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130e1a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130e1aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130e1ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130e1b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130e1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130e1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130e1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130e1c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130e1c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x130e1cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130e1d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130e1d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x130e1daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x130e1df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x130e1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x130e1e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x130e1ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x130e1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x130e1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x130e1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x130e1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x130e202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130e20750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130e20bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x130e21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130e214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130e21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x130e21d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130e221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130e22660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x130e22ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130e22f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130e233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130e23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130e23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130e24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130e24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130e249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130e24e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130e256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130e25bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130e26180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130e26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130e26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130e27290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130e27840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130e27df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130e283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130e28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130e28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130e294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130e29a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130e2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130e2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130e2ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130e2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130e2b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130e2ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130e2bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130e2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130e2c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130e2ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130e2d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130e2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130e2dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130e2e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130e2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130e2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x130e2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x130e2f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x130e2fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130e30070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x130e30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130e30a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130e30f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x130e31470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130e31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130e31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130e32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130e32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x130e32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130e33270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130e33770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x130e33c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130e34170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x130e34670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130e34b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x130e35070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130e35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130e35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130e35f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130e36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130e36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130e36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130e37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130e37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130e37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130e38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130e38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130e38c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130e39170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130e39670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130e39b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130e3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130e3a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130e3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130e3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130e3b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130e3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130e3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130e3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130e3c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130e3cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130e3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130e3d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130e3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130e3e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130e3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130e3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130e3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130e3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130e3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130e3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130e40470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130e40970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x130e40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130e41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130e41870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x130e41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130e42270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130e42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x130e42c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130e43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x130e43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130e43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x130e44120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130e446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130e44c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130e45230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130e45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130e45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130e46460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130e46c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130e470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130e473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130e479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130e47fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130e487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130e48c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130e49100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130e495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130e49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130e4a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130e4a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130e4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130e4b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130e4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130e4bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130e4c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130e4c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130e4cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130e4d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130e4d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130e4dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130e4e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130e4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130e4ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130e4f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130e4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130e4fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130e50240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130e50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130e50ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130e51230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130e51780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130e51cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130e52220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130e52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130e52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130e53210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130e53760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130e53cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x130e54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130e54750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130e54ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130e551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130e55740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130e55c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130e561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130e56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130e56c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130e571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130e57720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130e57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130e581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130e58710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130e58c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130e591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130e59700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130e59c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130e5a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130e5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130e5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130e5b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130e5b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130e5bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130e5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130e5c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130e5cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130e5d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130e5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130e5d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130e5ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130e5e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130e5e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130e5ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130e5f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130e5f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130e5f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130e5fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130e602f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130e60790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130e60c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x130e610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x130e61570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x130e61a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x130e61eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x130e62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x130e627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x130e62c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x130e63130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x130e635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x130e63a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130e63fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130e646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130e64e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130e65520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130e65c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130e65f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130e666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130e669b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130e66fc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.542s
user	0m0.280s
sys	0m0.296s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4820 (1a24c462)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15600a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15600b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15600b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15600bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15600c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15600c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15600cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15600d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15600d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15600dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15600e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15600e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15600f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15600fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156010270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156010990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1560110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1560117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156011ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1560126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156012de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156013500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156013c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1560144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156014be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156014ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1560154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156016120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156016660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156016920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156016dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156017080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156017910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156017e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156018110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1560185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156018a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156018ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156019390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156019830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156019cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15601a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15601a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15601aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15601ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15601b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15601b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15601c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15601c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15601ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15601d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15601daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15601e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15601e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15601ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15601f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15601f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15601fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156020110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156020900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156020bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156021500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1560219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156021e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1560222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156022780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156022c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1560230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156023560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156023a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156023ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156024340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156024890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156024de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156025330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156025880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156026320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156026870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156026dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156027310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156027860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156027db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156028300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156028850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156028da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1560292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156029840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156029d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15602a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15602a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15602ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15602b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15602b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15602bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15602c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15601bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15602c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15602cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15602d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15602d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15602ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15602e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15602e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15602eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15602f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15602f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15602feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156030400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156030950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156030ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1560313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156031890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156031d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1560321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156032670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156032b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156032fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156033450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1560338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156033d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156034230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1560346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156035010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1560354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156035950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156035df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156036290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156036730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156036bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156037070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156037510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1560379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156037e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1560382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156038790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156038c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1560390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156039570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156039a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156039eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15603a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15603a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15603ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15603b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15603b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15603ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15603bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15603c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15603c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15603ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15603d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15603d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15603dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15603df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15603e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15603e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15603ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15603f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15603f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15603fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15603ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156040470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156040910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156040db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156041250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1560416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156041b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156042030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1560424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156042970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156042e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1560432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156043750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156043bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156044090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156044530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1560449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156044e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156045310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1560457b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1560460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156046590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156046a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156046ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156047370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156047810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156047cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156048150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1560485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156048b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156049090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1560495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156049b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156049df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15604a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15604aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15604b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15604b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15604bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15604bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15604c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15604cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15604d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15604d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15604dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15604e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15604e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15604ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15604f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15604f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15604fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1560503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1560508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156050e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156051390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1560518e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156051e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156052380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1560528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156052e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156053370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1560538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156053e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156054360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1560548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156054e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156055350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1560558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156055df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156056340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156056890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156056de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156057330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156057880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156057dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156058320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156058870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156058dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156059310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156059860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156059db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15605a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15605a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15605ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15605b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15605b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15605bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15605c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15605c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15605cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15605d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15605d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15605dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15605e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15605e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15605ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15605f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15605f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15605fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1560602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1560607f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156060d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156061290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156061730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156061bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156062070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156062510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1560629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156062e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1560632f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156063790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156063c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1560640d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156064570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156064a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156064eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156065350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1560657f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x156065c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x156066130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1560665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x156066a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x156066f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1560673b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x156067850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x156067cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x156068190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x156068630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156068b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1560692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1560699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15606a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15606a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15606aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15606b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15606b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15606bb80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.384 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.388 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155708b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155708fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155709420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155709890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155709d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15570a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15570a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15570aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15570aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15570b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15570b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15570be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15570c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15570d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15570d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15570e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15570e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15570eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15570f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15570fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1557104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155710bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1557112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155711a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155712130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1557123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1557126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155712b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155712f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155713400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155713900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155713e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155714280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155714540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1557149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155714e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155715380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155715880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155715d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155716280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155716780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155716c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155717180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155717680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155717b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155717ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155718460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1557188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155718d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1557191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155719620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155719a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155719f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15571a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15571a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15571afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15571b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15571b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15571bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15571c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15571c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15571ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15571d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15571d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15571dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15571e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15571e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15571ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15571eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15571f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15571f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15571fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155720130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155720bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155721120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155721670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155722660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155722bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155723100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155723650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1557240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1557250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1557260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155726620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155726b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1557270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155727610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1557280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155728b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1557290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1557295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155729b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15572a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15572a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15572ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15572b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15572b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15572bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15572c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15572c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15572cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15572d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15572d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15572da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15572def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15572e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15572e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15572ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15572f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15572f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15572fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15572ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1557303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155730890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155730d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1557311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155731670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155731b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155731fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155732450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1557328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155732d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1557336d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155733b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155734010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1557344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155734950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155734df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155735290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155735730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155735bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155736070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155736510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1557369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155736e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1557372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155737790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155737c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1557380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155738570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155738a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155738eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155739350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1557397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155739c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15573a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15573a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15573aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15573af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15573b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15573b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15573bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15573c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15573c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15573cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15573cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15573d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15573d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15573dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15573e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15573e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15573eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15573efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15573f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15573f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15573fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155740250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1557406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155740b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155741030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1557414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155741970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155741e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1557422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155742750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155742bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155743090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155743530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1557439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155743e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155744310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1557447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155744d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155745250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1557457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155745cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155745fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1557465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155746bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1557471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1557479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155747e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155748740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155748d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155749540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1557499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155749e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15574a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15574aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15574b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15574b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15574bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15574c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15574c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15574cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf04630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf04aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf04f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf05380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf057f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf05c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf060d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf06540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf069b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf06e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf07290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf07700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf07b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf07fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf08450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf09610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf09a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf09ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf0a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf0a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf0ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf0b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf0b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf0b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf0c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf0c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf0cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf0cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf0d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf0d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf0dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf0e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf0e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf0eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf0fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf10090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf11250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf11fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf12880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf12cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf13160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bf13a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bf13eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bf14320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bf14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf14c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf15070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bf154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf15950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf15dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14bf16230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14bf166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14bf16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14bf16f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14bf173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14bf17860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14bf17cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14bf18140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14bf185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14bf18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bf18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bf19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bf1a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bf1a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bf1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bf1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bf1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bf1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf1c1e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155607ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155608350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1556087c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155608c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1556090a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155609510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155609980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155609df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15560a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15560a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15560ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15560b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15560bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15560c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15560cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15560d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15560db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15560e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15560e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15560f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15560f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15560ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1556106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155610dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1556114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1556117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155611a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155611ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155612350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1556127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155612cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1556131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155613640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155613900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155613d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1556141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155614740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155614c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155615140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155615640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155615b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155616040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155616a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155616f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1556173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155617820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155617c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155618100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1556189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155618e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1556192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15561a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15561a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15561aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15561b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15561b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15561bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15561c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15561c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15561cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15561cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15561d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15561d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15561ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15561e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15561e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15561ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15561f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15561f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15561fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15561ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1556204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155620a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155620f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1556214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155621a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155621f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1556224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155622a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155622f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1556234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155623a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155623f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1556244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1556249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155624f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155625490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1556259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155625f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155626480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1556269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155626f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155627470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1556279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155627f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155628460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1556289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155629450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1556299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155629ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15562a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15562a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15562aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15562b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15562b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15562bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15562c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15562c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15562ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15562d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15562d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15562dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15562e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15562e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15562e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15562ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15562f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15562f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15562fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1556300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155630590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155630a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155630ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155631370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155631810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155631cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155632150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1556325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155632a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155632f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1556333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155633870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155633d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1556341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155634650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155634af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155634f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155635430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1556358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155635d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155636210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1556366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155636b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155636ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155637490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155637930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155637dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155638270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155638710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155638bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155639050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1556394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155639990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155639e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15563a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15563a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15563ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15563b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15563b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15563b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15563be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15563c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15563c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15563cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15563d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15563d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15563da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15563def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15563e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15563e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15563ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15563f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15563f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15563fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15563ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1556403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155640890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155640d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1556411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155641670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155641b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155641fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155642450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1556428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155642d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155643230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1556436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155643b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1556440c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155644610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155644b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1556450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155645370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155645980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1556465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155646d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155647230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1556474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155647b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155648110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155648900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155648da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155649240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1556496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155649e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15564a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15564a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15564ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15564b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15564b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15564be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15564c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15564c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15564ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15564d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15564d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15564de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15564e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15564e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15564ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15564f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15564f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15564fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155650380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1556508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155650e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155651370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1556518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155651e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155652360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1556528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155652e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155653350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1556538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155653df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155654340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155654890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155654de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155655330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155655880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155655dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155656320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155656870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155656dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155657310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155657860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155657db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155658300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155658850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155658da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1556592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155659840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155659d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15565a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15565a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15565ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15565b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15565b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15565bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15565c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15565c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15565ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15565d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15565d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15565da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15565df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15565e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15565e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15565ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15565f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15565f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15565faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15565ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155660430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1556608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155660d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x155661210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1556616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x155661b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x155661ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x155662490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x155662930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x155662dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x155663270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x155663710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x155663bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155664100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155664820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155664f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155665660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155665d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155666040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155666830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155666af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155667100 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.974s
user	0m0.234s
sys	0m0.203s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
