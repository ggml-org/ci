Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.699s
user	0m0.914s
sys	0m1.258s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Built target llava
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-simple
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-arg-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-log
[ 49%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-gguf
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-backend-ops
[ 64%] Built target test-gguf
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-barrier
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-rope
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Built target llama-batched-bench
[ 70%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 76%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-perplexity
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup-create
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup
[ 83%] Generating loading.html.hpp
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-parallel
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-passkey
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-quantize
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Built target llama-retrieval
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.102s
user	0m6.306s
sys	0m9.872s

main: quantize time =  4848.19 ms
main:    total time =  4848.19 ms

main: quantize time =  1857.39 ms
main:    total time =  1857.39 ms

main: quantize time =  2994.46 ms
main:    total time =  2994.46 ms

main: quantize time =  3361.35 ms
main:    total time =  3361.35 ms

main: quantize time =  3098.41 ms
main:    total time =  3098.41 ms

main: quantize time =  5298.06 ms
main:    total time =  5298.06 ms

main: quantize time =  5895.13 ms
main:    total time =  5895.13 ms

main: quantize time =  6949.83 ms
main:    total time =  6949.83 ms

main: quantize time =  6071.34 ms
main:    total time =  6071.34 ms

main: quantize time =  4436.66 ms
main:    total time =  4436.66 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.150 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.333 I main: llama backend init
0.00.000.341 I main: load the model and apply lora adapter, if any
0.00.044.230 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.057.008 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.024 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.030 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.031 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.034 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.034 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.035 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.036 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.036 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.036 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.037 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.053 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.064.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.066.534 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.075.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.075.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.075.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.075.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.075.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.075.957 I llama_model_loader: - type  f32:  194 tensors
0.00.075.957 I llama_model_loader: - type  f16:   98 tensors
0.00.075.958 I print_info: file format = GGUF V3 (latest)
0.00.075.961 I print_info: file type   = all F32 (guessed)
0.00.075.963 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.307 I load: special tokens cache size = 25
0.00.100.146 I load: token to piece cache size = 0.2984 MB
0.00.100.149 I print_info: arch             = gptneox
0.00.100.149 I print_info: vocab_only       = 0
0.00.100.150 I print_info: n_ctx_train      = 2048
0.00.100.150 I print_info: n_embd           = 2048
0.00.100.150 I print_info: n_layer          = 24
0.00.100.154 I print_info: n_head           = 16
0.00.100.155 I print_info: n_head_kv        = 16
0.00.100.155 I print_info: n_rot            = 32
0.00.100.155 I print_info: n_swa            = 0
0.00.100.155 I print_info: n_embd_head_k    = 128
0.00.100.156 I print_info: n_embd_head_v    = 128
0.00.100.156 I print_info: n_gqa            = 1
0.00.100.157 I print_info: n_embd_k_gqa     = 2048
0.00.100.160 I print_info: n_embd_v_gqa     = 2048
0.00.100.161 I print_info: f_norm_eps       = 1.0e-05
0.00.100.161 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.100.162 I print_info: f_clamp_kqv      = 0.0e+00
0.00.100.162 I print_info: f_max_alibi_bias = 0.0e+00
0.00.100.162 I print_info: f_logit_scale    = 0.0e+00
0.00.100.171 I print_info: n_ff             = 8192
0.00.100.173 I print_info: n_expert         = 0
0.00.100.174 I print_info: n_expert_used    = 0
0.00.100.174 I print_info: causal attn      = 1
0.00.100.174 I print_info: pooling type     = 0
0.00.100.176 I print_info: rope type        = 2
0.00.100.176 I print_info: rope scaling     = linear
0.00.100.176 I print_info: freq_base_train  = 10000.0
0.00.100.177 I print_info: freq_scale_train = 1
0.00.100.177 I print_info: n_ctx_orig_yarn  = 2048
0.00.100.178 I print_info: rope_finetuned   = unknown
0.00.100.178 I print_info: ssm_d_conv       = 0
0.00.100.178 I print_info: ssm_d_inner      = 0
0.00.100.178 I print_info: ssm_d_state      = 0
0.00.100.178 I print_info: ssm_dt_rank      = 0
0.00.100.178 I print_info: ssm_dt_b_c_rms   = 0
0.00.100.179 I print_info: model type       = 1.4B
0.00.100.179 I print_info: model params     = 1.41 B
0.00.100.180 I print_info: general.name     = 1.4B
0.00.100.182 I print_info: vocab type       = BPE
0.00.100.182 I print_info: n_vocab          = 50304
0.00.100.182 I print_info: n_merges         = 50009
0.00.100.183 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.100.183 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.100.183 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.100.183 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.100.183 I print_info: LF token         = 128 'Ä'
0.00.100.184 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.100.184 I print_info: max token length = 1024
0.00.137.002 I load_tensors: offloading 24 repeating layers to GPU
0.00.137.005 I load_tensors: offloading output layer to GPU
0.00.137.005 I load_tensors: offloaded 25/25 layers to GPU
0.00.137.028 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.137.030 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.137.311 I llama_init_from_model: n_seq_max     = 1
0.00.137.312 I llama_init_from_model: n_ctx         = 2048
0.00.137.313 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.137.313 I llama_init_from_model: n_batch       = 2048
0.00.137.313 I llama_init_from_model: n_ubatch      = 512
0.00.137.313 I llama_init_from_model: flash_attn    = 0
0.00.137.314 I llama_init_from_model: freq_base     = 10000.0
0.00.137.314 I llama_init_from_model: freq_scale    = 1
0.00.137.315 I ggml_metal_init: allocating
0.00.137.333 I ggml_metal_init: found device: Apple M4
0.00.137.339 I ggml_metal_init: picking default device: Apple M4
0.00.137.910 I ggml_metal_init: using embedded metal library
0.00.150.758 I ggml_metal_init: GPU name:   Apple M4
0.00.150.760 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.150.760 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.150.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.150.761 I ggml_metal_init: simdgroup reduction   = true
0.00.150.761 I ggml_metal_init: simdgroup matrix mul. = true
0.00.150.761 I ggml_metal_init: has residency sets    = true
0.00.150.761 I ggml_metal_init: has bfloat            = true
0.00.150.761 I ggml_metal_init: use bfloat            = true
0.00.150.762 I ggml_metal_init: hasUnifiedMemory      = true
0.00.150.763 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.193.217 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.221.506 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.221.512 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.221.535 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.225.654 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.225.656 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.225.657 I llama_init_from_model: graph nodes  = 967
0.00.225.657 I llama_init_from_model: graph splits = 2
0.00.225.660 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.225.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.225.791 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.291.015 I main: llama threadpool init, n_threads = 4
0.00.291.053 I 
0.00.291.090 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.291.092 I 
0.00.291.133 I sampler seed: 1234
0.00.291.137 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.291.162 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.291.163 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.291.163 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.131.519 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.02.131.520 I llama_perf_context_print:        load time =     245.77 ms
0.02.131.521 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.38 tokens per second)
0.02.131.521 I llama_perf_context_print:        eval time =    1793.92 ms /    63 runs   (   28.47 ms per token,    35.12 tokens per second)
0.02.131.522 I llama_perf_context_print:       total time =    1841.51 ms /    70 tokens
0.02.131.744 I ggml_metal_free: deallocating

real	0m2.507s
user	0m0.133s
sys	0m0.129s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.233 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.233 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.235 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.236 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.238 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.241 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.271 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.395 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.567 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.569 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.569 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.569 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.570 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.570 I llama_model_loader: - type  f32:  194 tensors
0.00.034.571 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.572 I print_info: file format = GGUF V3 (latest)
0.00.034.572 I print_info: file type   = Q8_0
0.00.034.574 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.918 I load: special tokens cache size = 25
0.00.050.857 I load: token to piece cache size = 0.2984 MB
0.00.050.862 I print_info: arch             = gptneox
0.00.050.862 I print_info: vocab_only       = 0
0.00.050.862 I print_info: n_ctx_train      = 2048
0.00.050.862 I print_info: n_embd           = 2048
0.00.050.862 I print_info: n_layer          = 24
0.00.050.869 I print_info: n_head           = 16
0.00.050.870 I print_info: n_head_kv        = 16
0.00.050.870 I print_info: n_rot            = 32
0.00.050.870 I print_info: n_swa            = 0
0.00.050.874 I print_info: n_embd_head_k    = 128
0.00.050.874 I print_info: n_embd_head_v    = 128
0.00.050.875 I print_info: n_gqa            = 1
0.00.050.875 I print_info: n_embd_k_gqa     = 2048
0.00.050.876 I print_info: n_embd_v_gqa     = 2048
0.00.050.877 I print_info: f_norm_eps       = 1.0e-05
0.00.050.885 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.887 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.888 I print_info: f_logit_scale    = 0.0e+00
0.00.050.900 I print_info: n_ff             = 8192
0.00.050.900 I print_info: n_expert         = 0
0.00.050.900 I print_info: n_expert_used    = 0
0.00.050.900 I print_info: causal attn      = 1
0.00.050.902 I print_info: pooling type     = 0
0.00.050.902 I print_info: rope type        = 2
0.00.050.902 I print_info: rope scaling     = linear
0.00.050.903 I print_info: freq_base_train  = 10000.0
0.00.050.903 I print_info: freq_scale_train = 1
0.00.050.903 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.904 I print_info: rope_finetuned   = unknown
0.00.050.904 I print_info: ssm_d_conv       = 0
0.00.050.904 I print_info: ssm_d_inner      = 0
0.00.050.904 I print_info: ssm_d_state      = 0
0.00.050.904 I print_info: ssm_dt_rank      = 0
0.00.050.904 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.905 I print_info: model type       = 1.4B
0.00.050.905 I print_info: model params     = 1.41 B
0.00.050.905 I print_info: general.name     = 1.4B
0.00.050.910 I print_info: vocab type       = BPE
0.00.050.910 I print_info: n_vocab          = 50304
0.00.050.910 I print_info: n_merges         = 50009
0.00.050.910 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.911 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.911 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.911 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.911 I print_info: LF token         = 128 'Ä'
0.00.050.912 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.912 I print_info: max token length = 1024
0.01.029.278 I load_tensors: offloading 24 repeating layers to GPU
0.01.029.283 I load_tensors: offloading output layer to GPU
0.01.029.284 I load_tensors: offloaded 25/25 layers to GPU
0.01.029.307 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.029.310 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.030.455 I llama_init_from_model: n_seq_max     = 1
0.01.030.457 I llama_init_from_model: n_ctx         = 2048
0.01.030.457 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.030.457 I llama_init_from_model: n_batch       = 2048
0.01.030.458 I llama_init_from_model: n_ubatch      = 512
0.01.030.458 I llama_init_from_model: flash_attn    = 0
0.01.030.459 I llama_init_from_model: freq_base     = 10000.0
0.01.030.459 I llama_init_from_model: freq_scale    = 1
0.01.030.460 I ggml_metal_init: allocating
0.01.030.475 I ggml_metal_init: found device: Apple M4
0.01.030.483 I ggml_metal_init: picking default device: Apple M4
0.01.031.681 I ggml_metal_init: using embedded metal library
0.01.037.475 I ggml_metal_init: GPU name:   Apple M4
0.01.037.478 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.037.479 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.037.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.037.481 I ggml_metal_init: simdgroup reduction   = true
0.01.037.481 I ggml_metal_init: simdgroup matrix mul. = true
0.01.037.481 I ggml_metal_init: has residency sets    = true
0.01.037.481 I ggml_metal_init: has bfloat            = true
0.01.037.481 I ggml_metal_init: use bfloat            = true
0.01.037.482 I ggml_metal_init: hasUnifiedMemory      = true
0.01.037.483 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.053.629 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.105.518 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.105.524 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.105.546 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.109.810 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.109.811 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.109.812 I llama_init_from_model: graph nodes  = 967
0.01.109.812 I llama_init_from_model: graph splits = 2
0.01.109.818 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.109.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.109.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.164.034 I main: llama threadpool init, n_threads = 4
0.01.164.074 I 
0.01.164.096 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.164.096 I 
0.01.164.243 I sampler seed: 1234
0.01.164.247 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.164.282 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.164.286 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.164.286 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.254.831 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.02.254.832 I llama_perf_context_print:        load time =    1153.24 ms
0.02.254.833 I llama_perf_context_print: prompt eval time =      45.23 ms /     7 tokens (    6.46 ms per token,   154.76 tokens per second)
0.02.254.834 I llama_perf_context_print:        eval time =    1042.35 ms /    63 runs   (   16.55 ms per token,    60.44 tokens per second)
0.02.254.834 I llama_perf_context_print:       total time =    1091.70 ms /    70 tokens
0.02.255.102 I ggml_metal_free: deallocating

real	0m2.281s
user	0m0.110s
sys	0m0.276s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.014.740 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.483 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.022.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.498 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.499 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.316 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.031.022 I llama_model_loader: - type  f32:  194 tensors
0.00.031.022 I llama_model_loader: - type q4_0:   97 tensors
0.00.031.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.023 I print_info: file format = GGUF V3 (latest)
0.00.031.024 I print_info: file type   = Q4_0
0.00.031.025 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.039.108 I load: special tokens cache size = 25
0.00.045.033 I load: token to piece cache size = 0.2984 MB
0.00.045.036 I print_info: arch             = gptneox
0.00.045.037 I print_info: vocab_only       = 0
0.00.045.037 I print_info: n_ctx_train      = 2048
0.00.045.037 I print_info: n_embd           = 2048
0.00.045.037 I print_info: n_layer          = 24
0.00.045.041 I print_info: n_head           = 16
0.00.045.042 I print_info: n_head_kv        = 16
0.00.045.042 I print_info: n_rot            = 32
0.00.045.042 I print_info: n_swa            = 0
0.00.045.043 I print_info: n_embd_head_k    = 128
0.00.045.043 I print_info: n_embd_head_v    = 128
0.00.045.044 I print_info: n_gqa            = 1
0.00.045.044 I print_info: n_embd_k_gqa     = 2048
0.00.045.045 I print_info: n_embd_v_gqa     = 2048
0.00.045.046 I print_info: f_norm_eps       = 1.0e-05
0.00.045.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.046 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.046 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.046 I print_info: f_logit_scale    = 0.0e+00
0.00.045.047 I print_info: n_ff             = 8192
0.00.045.048 I print_info: n_expert         = 0
0.00.045.048 I print_info: n_expert_used    = 0
0.00.045.048 I print_info: causal attn      = 1
0.00.045.048 I print_info: pooling type     = 0
0.00.045.051 I print_info: rope type        = 2
0.00.045.051 I print_info: rope scaling     = linear
0.00.045.051 I print_info: freq_base_train  = 10000.0
0.00.045.051 I print_info: freq_scale_train = 1
0.00.045.052 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.052 I print_info: rope_finetuned   = unknown
0.00.045.052 I print_info: ssm_d_conv       = 0
0.00.045.052 I print_info: ssm_d_inner      = 0
0.00.045.052 I print_info: ssm_d_state      = 0
0.00.045.052 I print_info: ssm_dt_rank      = 0
0.00.045.052 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.053 I print_info: model type       = 1.4B
0.00.045.053 I print_info: model params     = 1.41 B
0.00.045.053 I print_info: general.name     = 1.4B
0.00.045.054 I print_info: vocab type       = BPE
0.00.045.054 I print_info: n_vocab          = 50304
0.00.045.054 I print_info: n_merges         = 50009
0.00.045.055 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.055 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.056 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.057 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.058 I print_info: LF token         = 128 'Ä'
0.00.045.058 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.058 I print_info: max token length = 1024
0.00.572.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.572.515 I load_tensors: offloading output layer to GPU
0.00.572.516 I load_tensors: offloaded 25/25 layers to GPU
0.00.572.551 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.572.552 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.574.057 I llama_init_from_model: n_seq_max     = 1
0.00.574.062 I llama_init_from_model: n_ctx         = 2048
0.00.574.062 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.574.063 I llama_init_from_model: n_batch       = 2048
0.00.574.063 I llama_init_from_model: n_ubatch      = 512
0.00.574.064 I llama_init_from_model: flash_attn    = 0
0.00.574.066 I llama_init_from_model: freq_base     = 10000.0
0.00.574.067 I llama_init_from_model: freq_scale    = 1
0.00.574.083 I ggml_metal_init: allocating
0.00.574.141 I ggml_metal_init: found device: Apple M4
0.00.574.154 I ggml_metal_init: picking default device: Apple M4
0.00.575.847 I ggml_metal_init: using embedded metal library
0.00.582.387 I ggml_metal_init: GPU name:   Apple M4
0.00.582.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.582.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.582.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.582.395 I ggml_metal_init: simdgroup reduction   = true
0.00.582.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.582.395 I ggml_metal_init: has residency sets    = true
0.00.582.396 I ggml_metal_init: has bfloat            = true
0.00.582.396 I ggml_metal_init: use bfloat            = true
0.00.582.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.582.399 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.601.548 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.655.776 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.655.782 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.655.805 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.660.412 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.660.414 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.660.414 I llama_init_from_model: graph nodes  = 967
0.00.660.415 I llama_init_from_model: graph splits = 2
0.00.660.421 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.660.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.660.540 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.135 I main: llama threadpool init, n_threads = 4
0.00.715.178 I 
0.00.715.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.203 I 
0.00.715.375 I sampler seed: 1234
0.00.715.379 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.715.390 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.715.390 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.715.390 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.403.685 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50390.35 tokens per second)
0.01.403.686 I llama_perf_context_print:        load time =     699.45 ms
0.01.403.686 I llama_perf_context_print: prompt eval time =      48.75 ms /     7 tokens (    6.96 ms per token,   143.59 tokens per second)
0.01.403.687 I llama_perf_context_print:        eval time =     636.60 ms /    63 runs   (   10.10 ms per token,    98.96 tokens per second)
0.01.403.687 I llama_perf_context_print:       total time =     689.49 ms /    70 tokens
0.01.403.883 I ggml_metal_free: deallocating

real	0m1.424s
user	0m0.109s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.859 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.863 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.866 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.871 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.871 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.872 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.873 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.873 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.878 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.600 I llama_model_loader: - type  f32:  194 tensors
0.00.025.600 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.600 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.601 I print_info: file format = GGUF V3 (latest)
0.00.025.601 I print_info: file type   = Q4_1
0.00.025.603 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.746 I load: special tokens cache size = 25
0.00.039.733 I load: token to piece cache size = 0.2984 MB
0.00.039.736 I print_info: arch             = gptneox
0.00.039.736 I print_info: vocab_only       = 0
0.00.039.737 I print_info: n_ctx_train      = 2048
0.00.039.737 I print_info: n_embd           = 2048
0.00.039.737 I print_info: n_layer          = 24
0.00.039.739 I print_info: n_head           = 16
0.00.039.740 I print_info: n_head_kv        = 16
0.00.039.740 I print_info: n_rot            = 32
0.00.039.741 I print_info: n_swa            = 0
0.00.039.742 I print_info: n_embd_head_k    = 128
0.00.039.742 I print_info: n_embd_head_v    = 128
0.00.039.743 I print_info: n_gqa            = 1
0.00.039.744 I print_info: n_embd_k_gqa     = 2048
0.00.039.745 I print_info: n_embd_v_gqa     = 2048
0.00.039.745 I print_info: f_norm_eps       = 1.0e-05
0.00.039.746 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.746 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.746 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.746 I print_info: f_logit_scale    = 0.0e+00
0.00.039.747 I print_info: n_ff             = 8192
0.00.039.747 I print_info: n_expert         = 0
0.00.039.747 I print_info: n_expert_used    = 0
0.00.039.747 I print_info: causal attn      = 1
0.00.039.747 I print_info: pooling type     = 0
0.00.039.751 I print_info: rope type        = 2
0.00.039.752 I print_info: rope scaling     = linear
0.00.039.752 I print_info: freq_base_train  = 10000.0
0.00.039.753 I print_info: freq_scale_train = 1
0.00.039.753 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.753 I print_info: rope_finetuned   = unknown
0.00.039.753 I print_info: ssm_d_conv       = 0
0.00.039.753 I print_info: ssm_d_inner      = 0
0.00.039.754 I print_info: ssm_d_state      = 0
0.00.039.754 I print_info: ssm_dt_rank      = 0
0.00.039.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.754 I print_info: model type       = 1.4B
0.00.039.754 I print_info: model params     = 1.41 B
0.00.039.754 I print_info: general.name     = 1.4B
0.00.039.755 I print_info: vocab type       = BPE
0.00.039.755 I print_info: n_vocab          = 50304
0.00.039.755 I print_info: n_merges         = 50009
0.00.039.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: LF token         = 128 'Ä'
0.00.039.757 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.757 I print_info: max token length = 1024
0.00.625.064 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.078 I load_tensors: offloading output layer to GPU
0.00.625.078 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.112 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.625.113 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.626.540 I llama_init_from_model: n_seq_max     = 1
0.00.626.544 I llama_init_from_model: n_ctx         = 2048
0.00.626.545 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.626.545 I llama_init_from_model: n_batch       = 2048
0.00.626.546 I llama_init_from_model: n_ubatch      = 512
0.00.626.546 I llama_init_from_model: flash_attn    = 0
0.00.626.548 I llama_init_from_model: freq_base     = 10000.0
0.00.626.549 I llama_init_from_model: freq_scale    = 1
0.00.626.555 I ggml_metal_init: allocating
0.00.626.641 I ggml_metal_init: found device: Apple M4
0.00.626.655 I ggml_metal_init: picking default device: Apple M4
0.00.628.462 I ggml_metal_init: using embedded metal library
0.00.635.553 I ggml_metal_init: GPU name:   Apple M4
0.00.635.559 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.560 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.562 I ggml_metal_init: simdgroup reduction   = true
0.00.635.562 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.562 I ggml_metal_init: has residency sets    = true
0.00.635.563 I ggml_metal_init: has bfloat            = true
0.00.635.563 I ggml_metal_init: use bfloat            = true
0.00.635.564 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.566 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.969 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.016 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.024 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.062 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.726 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.728 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.728 I llama_init_from_model: graph nodes  = 967
0.00.720.728 I llama_init_from_model: graph splits = 2
0.00.720.734 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.862 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.863 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.869 I main: llama threadpool init, n_threads = 4
0.00.775.911 I 
0.00.775.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.936 I 
0.00.776.081 I sampler seed: 1234
0.00.776.085 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.096 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.096 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.096 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.511.618 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.511.619 I llama_perf_context_print:        load time =     766.12 ms
0.01.511.620 I llama_perf_context_print: prompt eval time =      49.15 ms /     7 tokens (    7.02 ms per token,   142.42 tokens per second)
0.01.511.621 I llama_perf_context_print:        eval time =     683.50 ms /    63 runs   (   10.85 ms per token,    92.17 tokens per second)
0.01.511.622 I llama_perf_context_print:       total time =     736.63 ms /    70 tokens
0.01.511.855 I ggml_metal_free: deallocating

real	0m1.528s
user	0m0.111s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.701 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.176 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.176 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.177 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.181 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.049 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.838 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.839 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.840 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.840 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.841 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.841 I llama_model_loader: - type  f32:  194 tensors
0.00.024.842 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.842 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.842 I print_info: file format = GGUF V3 (latest)
0.00.024.843 I print_info: file type   = Q5_0
0.00.024.844 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.681 I load: special tokens cache size = 25
0.00.038.571 I load: token to piece cache size = 0.2984 MB
0.00.038.574 I print_info: arch             = gptneox
0.00.038.574 I print_info: vocab_only       = 0
0.00.038.575 I print_info: n_ctx_train      = 2048
0.00.038.575 I print_info: n_embd           = 2048
0.00.038.575 I print_info: n_layer          = 24
0.00.038.578 I print_info: n_head           = 16
0.00.038.579 I print_info: n_head_kv        = 16
0.00.038.579 I print_info: n_rot            = 32
0.00.038.579 I print_info: n_swa            = 0
0.00.038.579 I print_info: n_embd_head_k    = 128
0.00.038.581 I print_info: n_embd_head_v    = 128
0.00.038.582 I print_info: n_gqa            = 1
0.00.038.583 I print_info: n_embd_k_gqa     = 2048
0.00.038.588 I print_info: n_embd_v_gqa     = 2048
0.00.038.589 I print_info: f_norm_eps       = 1.0e-05
0.00.038.589 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.589 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.589 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.590 I print_info: f_logit_scale    = 0.0e+00
0.00.038.590 I print_info: n_ff             = 8192
0.00.038.591 I print_info: n_expert         = 0
0.00.038.591 I print_info: n_expert_used    = 0
0.00.038.591 I print_info: causal attn      = 1
0.00.038.591 I print_info: pooling type     = 0
0.00.038.593 I print_info: rope type        = 2
0.00.038.595 I print_info: rope scaling     = linear
0.00.038.595 I print_info: freq_base_train  = 10000.0
0.00.038.595 I print_info: freq_scale_train = 1
0.00.038.596 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.596 I print_info: rope_finetuned   = unknown
0.00.038.596 I print_info: ssm_d_conv       = 0
0.00.038.596 I print_info: ssm_d_inner      = 0
0.00.038.597 I print_info: ssm_d_state      = 0
0.00.038.597 I print_info: ssm_dt_rank      = 0
0.00.038.598 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.598 I print_info: model type       = 1.4B
0.00.038.598 I print_info: model params     = 1.41 B
0.00.038.599 I print_info: general.name     = 1.4B
0.00.038.599 I print_info: vocab type       = BPE
0.00.038.599 I print_info: n_vocab          = 50304
0.00.038.599 I print_info: n_merges         = 50009
0.00.038.600 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.600 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.600 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.601 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.601 I print_info: LF token         = 128 'Ä'
0.00.038.601 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.602 I print_info: max token length = 1024
0.00.650.254 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.269 I load_tensors: offloading output layer to GPU
0.00.650.270 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.303 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.650.305 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.651.743 I llama_init_from_model: n_seq_max     = 1
0.00.651.750 I llama_init_from_model: n_ctx         = 2048
0.00.651.750 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.751 I llama_init_from_model: n_batch       = 2048
0.00.651.751 I llama_init_from_model: n_ubatch      = 512
0.00.651.751 I llama_init_from_model: flash_attn    = 0
0.00.651.754 I llama_init_from_model: freq_base     = 10000.0
0.00.651.754 I llama_init_from_model: freq_scale    = 1
0.00.651.775 I ggml_metal_init: allocating
0.00.651.859 I ggml_metal_init: found device: Apple M4
0.00.651.873 I ggml_metal_init: picking default device: Apple M4
0.00.653.665 I ggml_metal_init: using embedded metal library
0.00.660.354 I ggml_metal_init: GPU name:   Apple M4
0.00.660.359 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.359 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.361 I ggml_metal_init: simdgroup reduction   = true
0.00.660.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.362 I ggml_metal_init: has residency sets    = true
0.00.660.362 I ggml_metal_init: has bfloat            = true
0.00.660.362 I ggml_metal_init: use bfloat            = true
0.00.660.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.660.371 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.941 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.732.932 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.732.938 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.732.963 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.737.115 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.737.117 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.737.117 I llama_init_from_model: graph nodes  = 967
0.00.737.117 I llama_init_from_model: graph splits = 2
0.00.737.123 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.737.251 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.737.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.717 I main: llama threadpool init, n_threads = 4
0.00.796.761 I 
0.00.796.784 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.786 I 
0.00.796.934 I sampler seed: 1234
0.00.796.938 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.982 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.985 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.985 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.596.876 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.01.596.877 I llama_perf_context_print:        load time =     787.14 ms
0.01.596.877 I llama_perf_context_print: prompt eval time =      53.40 ms /     7 tokens (    7.63 ms per token,   131.08 tokens per second)
0.01.596.879 I llama_perf_context_print:        eval time =     743.59 ms /    63 runs   (   11.80 ms per token,    84.72 tokens per second)
0.01.596.879 I llama_perf_context_print:       total time =     801.04 ms /    70 tokens
0.01.597.122 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.108s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.975 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.495 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.505 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.506 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.507 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.512 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.294 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.995 I llama_model_loader: - type  f32:  194 tensors
0.00.025.995 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.996 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.996 I print_info: file format = GGUF V3 (latest)
0.00.025.997 I print_info: file type   = Q5_1
0.00.025.998 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.878 I load: special tokens cache size = 25
0.00.039.791 I load: token to piece cache size = 0.2984 MB
0.00.039.794 I print_info: arch             = gptneox
0.00.039.794 I print_info: vocab_only       = 0
0.00.039.794 I print_info: n_ctx_train      = 2048
0.00.039.795 I print_info: n_embd           = 2048
0.00.039.795 I print_info: n_layer          = 24
0.00.039.798 I print_info: n_head           = 16
0.00.039.798 I print_info: n_head_kv        = 16
0.00.039.800 I print_info: n_rot            = 32
0.00.039.800 I print_info: n_swa            = 0
0.00.039.800 I print_info: n_embd_head_k    = 128
0.00.039.801 I print_info: n_embd_head_v    = 128
0.00.039.801 I print_info: n_gqa            = 1
0.00.039.802 I print_info: n_embd_k_gqa     = 2048
0.00.039.803 I print_info: n_embd_v_gqa     = 2048
0.00.039.804 I print_info: f_norm_eps       = 1.0e-05
0.00.039.804 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.804 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.804 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.804 I print_info: f_logit_scale    = 0.0e+00
0.00.039.805 I print_info: n_ff             = 8192
0.00.039.805 I print_info: n_expert         = 0
0.00.039.806 I print_info: n_expert_used    = 0
0.00.039.806 I print_info: causal attn      = 1
0.00.039.806 I print_info: pooling type     = 0
0.00.039.806 I print_info: rope type        = 2
0.00.039.806 I print_info: rope scaling     = linear
0.00.039.807 I print_info: freq_base_train  = 10000.0
0.00.039.807 I print_info: freq_scale_train = 1
0.00.039.807 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.808 I print_info: rope_finetuned   = unknown
0.00.039.808 I print_info: ssm_d_conv       = 0
0.00.039.808 I print_info: ssm_d_inner      = 0
0.00.039.808 I print_info: ssm_d_state      = 0
0.00.039.808 I print_info: ssm_dt_rank      = 0
0.00.039.808 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.809 I print_info: model type       = 1.4B
0.00.039.809 I print_info: model params     = 1.41 B
0.00.039.809 I print_info: general.name     = 1.4B
0.00.039.809 I print_info: vocab type       = BPE
0.00.039.810 I print_info: n_vocab          = 50304
0.00.039.810 I print_info: n_merges         = 50009
0.00.039.810 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.811 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.811 I print_info: LF token         = 128 'Ä'
0.00.039.811 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.811 I print_info: max token length = 1024
0.00.698.073 I load_tensors: offloading 24 repeating layers to GPU
0.00.698.076 I load_tensors: offloading output layer to GPU
0.00.698.077 I load_tensors: offloaded 25/25 layers to GPU
0.00.698.098 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.698.101 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.699.277 I llama_init_from_model: n_seq_max     = 1
0.00.699.279 I llama_init_from_model: n_ctx         = 2048
0.00.699.280 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.699.280 I llama_init_from_model: n_batch       = 2048
0.00.699.280 I llama_init_from_model: n_ubatch      = 512
0.00.699.281 I llama_init_from_model: flash_attn    = 0
0.00.699.282 I llama_init_from_model: freq_base     = 10000.0
0.00.699.282 I llama_init_from_model: freq_scale    = 1
0.00.699.284 I ggml_metal_init: allocating
0.00.699.299 I ggml_metal_init: found device: Apple M4
0.00.699.308 I ggml_metal_init: picking default device: Apple M4
0.00.700.739 I ggml_metal_init: using embedded metal library
0.00.706.997 I ggml_metal_init: GPU name:   Apple M4
0.00.707.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.004 I ggml_metal_init: simdgroup reduction   = true
0.00.707.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.004 I ggml_metal_init: has residency sets    = true
0.00.707.005 I ggml_metal_init: has bfloat            = true
0.00.707.005 I ggml_metal_init: use bfloat            = true
0.00.707.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.724.831 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.779.476 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.779.483 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.779.549 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.784.170 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.784.172 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.784.172 I llama_init_from_model: graph nodes  = 967
0.00.784.173 I llama_init_from_model: graph splits = 2
0.00.784.178 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.784.310 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.784.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.844.802 I main: llama threadpool init, n_threads = 4
0.00.844.842 I 
0.00.844.865 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.844.866 I 
0.00.845.018 I sampler seed: 1234
0.00.845.022 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.845.032 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.845.033 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.845.033 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.694.156 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.694.156 I llama_perf_context_print:        load time =     833.91 ms
0.01.694.157 I llama_perf_context_print: prompt eval time =      51.87 ms /     7 tokens (    7.41 ms per token,   134.96 tokens per second)
0.01.694.158 I llama_perf_context_print:        eval time =     794.23 ms /    63 runs   (   12.61 ms per token,    79.32 tokens per second)
0.01.694.158 I llama_perf_context_print:       total time =     850.27 ms /    70 tokens
0.01.694.412 I ggml_metal_free: deallocating

real	0m1.713s
user	0m0.108s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.408 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.324 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.397 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.164 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.165 I llama_model_loader: - type  f32:  194 tensors
0.00.024.165 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.165 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.165 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.166 I print_info: file format = GGUF V3 (latest)
0.00.024.167 I print_info: file type   = Q2_K - Medium
0.00.024.168 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.902 I load: special tokens cache size = 25
0.00.037.813 I load: token to piece cache size = 0.2984 MB
0.00.037.815 I print_info: arch             = gptneox
0.00.037.816 I print_info: vocab_only       = 0
0.00.037.816 I print_info: n_ctx_train      = 2048
0.00.037.816 I print_info: n_embd           = 2048
0.00.037.816 I print_info: n_layer          = 24
0.00.037.819 I print_info: n_head           = 16
0.00.037.820 I print_info: n_head_kv        = 16
0.00.037.820 I print_info: n_rot            = 32
0.00.037.822 I print_info: n_swa            = 0
0.00.037.823 I print_info: n_embd_head_k    = 128
0.00.037.823 I print_info: n_embd_head_v    = 128
0.00.037.824 I print_info: n_gqa            = 1
0.00.037.824 I print_info: n_embd_k_gqa     = 2048
0.00.037.830 I print_info: n_embd_v_gqa     = 2048
0.00.037.830 I print_info: f_norm_eps       = 1.0e-05
0.00.037.831 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.831 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.831 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.831 I print_info: f_logit_scale    = 0.0e+00
0.00.037.832 I print_info: n_ff             = 8192
0.00.037.832 I print_info: n_expert         = 0
0.00.037.833 I print_info: n_expert_used    = 0
0.00.037.833 I print_info: causal attn      = 1
0.00.037.833 I print_info: pooling type     = 0
0.00.037.833 I print_info: rope type        = 2
0.00.037.833 I print_info: rope scaling     = linear
0.00.037.834 I print_info: freq_base_train  = 10000.0
0.00.037.834 I print_info: freq_scale_train = 1
0.00.037.834 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.835 I print_info: rope_finetuned   = unknown
0.00.037.835 I print_info: ssm_d_conv       = 0
0.00.037.836 I print_info: ssm_d_inner      = 0
0.00.037.836 I print_info: ssm_d_state      = 0
0.00.037.836 I print_info: ssm_dt_rank      = 0
0.00.037.837 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.838 I print_info: model type       = 1.4B
0.00.037.838 I print_info: model params     = 1.41 B
0.00.037.838 I print_info: general.name     = 1.4B
0.00.037.839 I print_info: vocab type       = BPE
0.00.037.839 I print_info: n_vocab          = 50304
0.00.037.839 I print_info: n_merges         = 50009
0.00.037.839 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.840 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.840 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.840 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.840 I print_info: LF token         = 128 'Ä'
0.00.037.840 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.841 I print_info: max token length = 1024
0.00.395.421 I load_tensors: offloading 24 repeating layers to GPU
0.00.395.436 I load_tensors: offloading output layer to GPU
0.00.395.437 I load_tensors: offloaded 25/25 layers to GPU
0.00.395.473 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.395.474 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.397.049 I llama_init_from_model: n_seq_max     = 1
0.00.397.054 I llama_init_from_model: n_ctx         = 2048
0.00.397.055 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.397.055 I llama_init_from_model: n_batch       = 2048
0.00.397.056 I llama_init_from_model: n_ubatch      = 512
0.00.397.056 I llama_init_from_model: flash_attn    = 0
0.00.397.058 I llama_init_from_model: freq_base     = 10000.0
0.00.397.062 I llama_init_from_model: freq_scale    = 1
0.00.397.066 I ggml_metal_init: allocating
0.00.397.173 I ggml_metal_init: found device: Apple M4
0.00.397.187 I ggml_metal_init: picking default device: Apple M4
0.00.399.018 I ggml_metal_init: using embedded metal library
0.00.404.532 I ggml_metal_init: GPU name:   Apple M4
0.00.404.550 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.404.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.404.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.404.552 I ggml_metal_init: simdgroup reduction   = true
0.00.404.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.404.553 I ggml_metal_init: has residency sets    = true
0.00.404.553 I ggml_metal_init: has bfloat            = true
0.00.404.553 I ggml_metal_init: use bfloat            = true
0.00.404.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.404.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.425.931 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.483.271 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.483.280 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.483.311 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.487.747 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.487.748 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.487.749 I llama_init_from_model: graph nodes  = 967
0.00.487.749 I llama_init_from_model: graph splits = 2
0.00.487.754 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.487.887 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.487.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.917 I main: llama threadpool init, n_threads = 4
0.00.549.958 I 
0.00.549.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.983 I 
0.00.550.154 I sampler seed: 1234
0.00.550.158 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.550.203 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.550.206 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.550.207 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.237.279 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.237.280 I llama_perf_context_print:        load time =     540.09 ms
0.01.237.281 I llama_perf_context_print: prompt eval time =      44.40 ms /     7 tokens (    6.34 ms per token,   157.65 tokens per second)
0.01.237.282 I llama_perf_context_print:        eval time =     639.81 ms /    63 runs   (   10.16 ms per token,    98.47 tokens per second)
0.01.237.282 I llama_perf_context_print:       total time =     688.31 ms /    70 tokens
0.01.237.544 I ggml_metal_free: deallocating

real	0m1.254s
user	0m0.112s
sys	0m0.169s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.051 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.515 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.523 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.523 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.524 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.402 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.236 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.238 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.238 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.239 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.240 I llama_model_loader: - type  f32:  194 tensors
0.00.025.240 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.240 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.241 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.241 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.242 I print_info: file format = GGUF V3 (latest)
0.00.025.242 I print_info: file type   = Q3_K - Medium
0.00.025.243 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.789 I load: special tokens cache size = 25
0.00.039.988 I load: token to piece cache size = 0.2984 MB
0.00.039.991 I print_info: arch             = gptneox
0.00.039.991 I print_info: vocab_only       = 0
0.00.039.992 I print_info: n_ctx_train      = 2048
0.00.039.992 I print_info: n_embd           = 2048
0.00.039.992 I print_info: n_layer          = 24
0.00.039.995 I print_info: n_head           = 16
0.00.039.996 I print_info: n_head_kv        = 16
0.00.039.996 I print_info: n_rot            = 32
0.00.039.996 I print_info: n_swa            = 0
0.00.039.996 I print_info: n_embd_head_k    = 128
0.00.039.997 I print_info: n_embd_head_v    = 128
0.00.039.997 I print_info: n_gqa            = 1
0.00.039.998 I print_info: n_embd_k_gqa     = 2048
0.00.039.999 I print_info: n_embd_v_gqa     = 2048
0.00.040.000 I print_info: f_norm_eps       = 1.0e-05
0.00.040.000 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.000 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.000 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.001 I print_info: f_logit_scale    = 0.0e+00
0.00.040.001 I print_info: n_ff             = 8192
0.00.040.002 I print_info: n_expert         = 0
0.00.040.002 I print_info: n_expert_used    = 0
0.00.040.002 I print_info: causal attn      = 1
0.00.040.002 I print_info: pooling type     = 0
0.00.040.002 I print_info: rope type        = 2
0.00.040.003 I print_info: rope scaling     = linear
0.00.040.003 I print_info: freq_base_train  = 10000.0
0.00.040.004 I print_info: freq_scale_train = 1
0.00.040.004 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.004 I print_info: rope_finetuned   = unknown
0.00.040.004 I print_info: ssm_d_conv       = 0
0.00.040.004 I print_info: ssm_d_inner      = 0
0.00.040.004 I print_info: ssm_d_state      = 0
0.00.040.005 I print_info: ssm_dt_rank      = 0
0.00.040.005 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.005 I print_info: model type       = 1.4B
0.00.040.006 I print_info: model params     = 1.41 B
0.00.040.006 I print_info: general.name     = 1.4B
0.00.040.006 I print_info: vocab type       = BPE
0.00.040.006 I print_info: n_vocab          = 50304
0.00.040.008 I print_info: n_merges         = 50009
0.00.040.009 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: LF token         = 128 'Ä'
0.00.040.010 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.010 I print_info: max token length = 1024
0.00.522.512 I load_tensors: offloading 24 repeating layers to GPU
0.00.522.520 I load_tensors: offloading output layer to GPU
0.00.522.520 I load_tensors: offloaded 25/25 layers to GPU
0.00.522.538 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.522.540 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.523.187 I llama_init_from_model: n_seq_max     = 1
0.00.523.191 I llama_init_from_model: n_ctx         = 2048
0.00.523.192 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.523.192 I llama_init_from_model: n_batch       = 2048
0.00.523.192 I llama_init_from_model: n_ubatch      = 512
0.00.523.193 I llama_init_from_model: flash_attn    = 0
0.00.523.194 I llama_init_from_model: freq_base     = 10000.0
0.00.523.194 I llama_init_from_model: freq_scale    = 1
0.00.523.196 I ggml_metal_init: allocating
0.00.523.232 I ggml_metal_init: found device: Apple M4
0.00.523.243 I ggml_metal_init: picking default device: Apple M4
0.00.524.242 I ggml_metal_init: using embedded metal library
0.00.528.346 I ggml_metal_init: GPU name:   Apple M4
0.00.528.354 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.528.355 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.528.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.528.356 I ggml_metal_init: simdgroup reduction   = true
0.00.528.356 I ggml_metal_init: simdgroup matrix mul. = true
0.00.528.356 I ggml_metal_init: has residency sets    = true
0.00.528.357 I ggml_metal_init: has bfloat            = true
0.00.528.357 I ggml_metal_init: use bfloat            = true
0.00.528.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.528.361 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.544.248 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.572.734 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.572.741 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.572.766 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.577.034 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.577.036 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.577.036 I llama_init_from_model: graph nodes  = 967
0.00.577.036 I llama_init_from_model: graph splits = 2
0.00.577.042 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.577.176 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.577.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.421 I main: llama threadpool init, n_threads = 4
0.00.633.460 I 
0.00.633.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.480 I 
0.00.633.641 I sampler seed: 1234
0.00.633.645 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.633.656 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.633.657 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.633.657 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.382.045 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50035.24 tokens per second)
0.01.382.046 I llama_perf_context_print:        load time =     623.24 ms
0.01.382.047 I llama_perf_context_print: prompt eval time =      49.50 ms /     7 tokens (    7.07 ms per token,   141.43 tokens per second)
0.01.382.047 I llama_perf_context_print:        eval time =     696.43 ms /    63 runs   (   11.05 ms per token,    90.46 tokens per second)
0.01.382.047 I llama_perf_context_print:       total time =     749.76 ms /    70 tokens
0.01.382.277 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.106s
sys	0m0.145s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.011.703 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.466 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.475 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.475 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.476 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.476 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.476 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.477 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.477 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.479 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.480 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.349 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.396 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.209 I llama_model_loader: - type  f32:  194 tensors
0.00.027.209 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.209 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.209 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.210 I print_info: file format = GGUF V3 (latest)
0.00.027.211 I print_info: file type   = Q4_K - Medium
0.00.027.212 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.225 I load: special tokens cache size = 25
0.00.041.355 I load: token to piece cache size = 0.2984 MB
0.00.041.359 I print_info: arch             = gptneox
0.00.041.359 I print_info: vocab_only       = 0
0.00.041.359 I print_info: n_ctx_train      = 2048
0.00.041.359 I print_info: n_embd           = 2048
0.00.041.359 I print_info: n_layer          = 24
0.00.041.364 I print_info: n_head           = 16
0.00.041.365 I print_info: n_head_kv        = 16
0.00.041.365 I print_info: n_rot            = 32
0.00.041.365 I print_info: n_swa            = 0
0.00.041.366 I print_info: n_embd_head_k    = 128
0.00.041.366 I print_info: n_embd_head_v    = 128
0.00.041.367 I print_info: n_gqa            = 1
0.00.041.368 I print_info: n_embd_k_gqa     = 2048
0.00.041.368 I print_info: n_embd_v_gqa     = 2048
0.00.041.369 I print_info: f_norm_eps       = 1.0e-05
0.00.041.369 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.369 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.372 I print_info: f_logit_scale    = 0.0e+00
0.00.041.373 I print_info: n_ff             = 8192
0.00.041.374 I print_info: n_expert         = 0
0.00.041.374 I print_info: n_expert_used    = 0
0.00.041.374 I print_info: causal attn      = 1
0.00.041.374 I print_info: pooling type     = 0
0.00.041.374 I print_info: rope type        = 2
0.00.041.375 I print_info: rope scaling     = linear
0.00.041.375 I print_info: freq_base_train  = 10000.0
0.00.041.375 I print_info: freq_scale_train = 1
0.00.041.376 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.376 I print_info: rope_finetuned   = unknown
0.00.041.376 I print_info: ssm_d_conv       = 0
0.00.041.376 I print_info: ssm_d_inner      = 0
0.00.041.378 I print_info: ssm_d_state      = 0
0.00.041.378 I print_info: ssm_dt_rank      = 0
0.00.041.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.378 I print_info: model type       = 1.4B
0.00.041.379 I print_info: model params     = 1.41 B
0.00.041.379 I print_info: general.name     = 1.4B
0.00.041.379 I print_info: vocab type       = BPE
0.00.041.379 I print_info: n_vocab          = 50304
0.00.041.379 I print_info: n_merges         = 50009
0.00.041.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.380 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.380 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.380 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.380 I print_info: LF token         = 128 'Ä'
0.00.041.381 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.381 I print_info: max token length = 1024
0.00.515.298 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.314 I load_tensors: offloading output layer to GPU
0.00.515.315 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.351 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.515.352 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.516.855 I llama_init_from_model: n_seq_max     = 1
0.00.516.860 I llama_init_from_model: n_ctx         = 2048
0.00.516.861 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.516.861 I llama_init_from_model: n_batch       = 2048
0.00.516.862 I llama_init_from_model: n_ubatch      = 512
0.00.516.862 I llama_init_from_model: flash_attn    = 0
0.00.516.864 I llama_init_from_model: freq_base     = 10000.0
0.00.516.865 I llama_init_from_model: freq_scale    = 1
0.00.516.867 I ggml_metal_init: allocating
0.00.516.975 I ggml_metal_init: found device: Apple M4
0.00.516.990 I ggml_metal_init: picking default device: Apple M4
0.00.518.837 I ggml_metal_init: using embedded metal library
0.00.525.478 I ggml_metal_init: GPU name:   Apple M4
0.00.525.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.484 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.485 I ggml_metal_init: simdgroup reduction   = true
0.00.525.485 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.486 I ggml_metal_init: has residency sets    = true
0.00.525.486 I ggml_metal_init: has bfloat            = true
0.00.525.486 I ggml_metal_init: use bfloat            = true
0.00.525.487 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.490 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.795 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.598.758 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.598.765 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.598.793 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.603.396 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.603.398 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.603.398 I llama_init_from_model: graph nodes  = 967
0.00.603.399 I llama_init_from_model: graph splits = 2
0.00.603.404 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.603.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.603.533 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.245 I main: llama threadpool init, n_threads = 4
0.00.661.291 I 
0.00.661.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.316 I 
0.00.661.497 I sampler seed: 1234
0.00.661.501 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.661.512 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.661.514 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.661.514 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.431.335 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.01.431.336 I llama_perf_context_print:        load time =     648.65 ms
0.01.431.336 I llama_perf_context_print: prompt eval time =      57.56 ms /     7 tokens (    8.22 ms per token,   121.62 tokens per second)
0.01.431.337 I llama_perf_context_print:        eval time =     709.21 ms /    63 runs   (   11.26 ms per token,    88.83 tokens per second)
0.01.431.337 I llama_perf_context_print:       total time =     770.98 ms /    70 tokens
0.01.431.603 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.111s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.134 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.765 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.770 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.772 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.774 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.775 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.775 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.776 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.776 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.776 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.777 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.779 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.779 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.779 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.636 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.482 I llama_model_loader: - type  f32:  194 tensors
0.00.024.483 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.483 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.483 I print_info: file format = GGUF V3 (latest)
0.00.024.484 I print_info: file type   = Q5_K - Medium
0.00.024.485 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.272 I load: special tokens cache size = 25
0.00.037.974 I load: token to piece cache size = 0.2984 MB
0.00.037.977 I print_info: arch             = gptneox
0.00.037.977 I print_info: vocab_only       = 0
0.00.037.977 I print_info: n_ctx_train      = 2048
0.00.037.977 I print_info: n_embd           = 2048
0.00.037.977 I print_info: n_layer          = 24
0.00.037.980 I print_info: n_head           = 16
0.00.037.981 I print_info: n_head_kv        = 16
0.00.037.981 I print_info: n_rot            = 32
0.00.037.981 I print_info: n_swa            = 0
0.00.037.981 I print_info: n_embd_head_k    = 128
0.00.037.981 I print_info: n_embd_head_v    = 128
0.00.037.982 I print_info: n_gqa            = 1
0.00.037.983 I print_info: n_embd_k_gqa     = 2048
0.00.037.984 I print_info: n_embd_v_gqa     = 2048
0.00.037.984 I print_info: f_norm_eps       = 1.0e-05
0.00.037.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.985 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.985 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.985 I print_info: f_logit_scale    = 0.0e+00
0.00.037.986 I print_info: n_ff             = 8192
0.00.037.986 I print_info: n_expert         = 0
0.00.037.986 I print_info: n_expert_used    = 0
0.00.037.986 I print_info: causal attn      = 1
0.00.037.987 I print_info: pooling type     = 0
0.00.037.987 I print_info: rope type        = 2
0.00.037.987 I print_info: rope scaling     = linear
0.00.037.987 I print_info: freq_base_train  = 10000.0
0.00.037.988 I print_info: freq_scale_train = 1
0.00.037.988 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.988 I print_info: rope_finetuned   = unknown
0.00.037.988 I print_info: ssm_d_conv       = 0
0.00.037.989 I print_info: ssm_d_inner      = 0
0.00.037.989 I print_info: ssm_d_state      = 0
0.00.037.989 I print_info: ssm_dt_rank      = 0
0.00.037.989 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.989 I print_info: model type       = 1.4B
0.00.037.990 I print_info: model params     = 1.41 B
0.00.037.990 I print_info: general.name     = 1.4B
0.00.037.990 I print_info: vocab type       = BPE
0.00.037.991 I print_info: n_vocab          = 50304
0.00.037.991 I print_info: n_merges         = 50009
0.00.037.991 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.991 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.992 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.993 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.993 I print_info: LF token         = 128 'Ä'
0.00.037.993 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.994 I print_info: max token length = 1024
0.00.593.920 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.929 I load_tensors: offloading output layer to GPU
0.00.593.930 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.962 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.593.963 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.595.348 I llama_init_from_model: n_seq_max     = 1
0.00.595.352 I llama_init_from_model: n_ctx         = 2048
0.00.595.352 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.595.353 I llama_init_from_model: n_batch       = 2048
0.00.595.353 I llama_init_from_model: n_ubatch      = 512
0.00.595.353 I llama_init_from_model: flash_attn    = 0
0.00.595.356 I llama_init_from_model: freq_base     = 10000.0
0.00.595.356 I llama_init_from_model: freq_scale    = 1
0.00.595.362 I ggml_metal_init: allocating
0.00.595.469 I ggml_metal_init: found device: Apple M4
0.00.595.509 I ggml_metal_init: picking default device: Apple M4
0.00.596.985 I ggml_metal_init: using embedded metal library
0.00.603.215 I ggml_metal_init: GPU name:   Apple M4
0.00.603.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.221 I ggml_metal_init: simdgroup reduction   = true
0.00.603.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.222 I ggml_metal_init: has residency sets    = true
0.00.603.222 I ggml_metal_init: has bfloat            = true
0.00.603.222 I ggml_metal_init: use bfloat            = true
0.00.603.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.225 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.262 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.674.790 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.674.797 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.674.867 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.441 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.679.444 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.679.444 I llama_init_from_model: graph nodes  = 967
0.00.679.444 I llama_init_from_model: graph splits = 2
0.00.679.449 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.679.582 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.679.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.565 I main: llama threadpool init, n_threads = 4
0.00.739.610 I 
0.00.739.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.637 I 
0.00.739.809 I sampler seed: 1234
0.00.739.813 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.824 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.824 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.824 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.587.130 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.587.131 I llama_perf_context_print:        load time =     729.53 ms
0.01.587.132 I llama_perf_context_print: prompt eval time =      51.20 ms /     7 tokens (    7.31 ms per token,   136.73 tokens per second)
0.01.587.132 I llama_perf_context_print:        eval time =     793.15 ms /    63 runs   (   12.59 ms per token,    79.43 tokens per second)
0.01.587.133 I llama_perf_context_print:       total time =     848.47 ms /    70 tokens
0.01.587.419 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.107s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.838 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.430 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.431 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.433 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.436 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.023 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.024 I llama_model_loader: - type  f32:  194 tensors
0.00.024.024 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.025 I print_info: file format = GGUF V3 (latest)
0.00.024.025 I print_info: file type   = Q6_K
0.00.024.026 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.797 I load: special tokens cache size = 25
0.00.037.707 I load: token to piece cache size = 0.2984 MB
0.00.037.710 I print_info: arch             = gptneox
0.00.037.710 I print_info: vocab_only       = 0
0.00.037.710 I print_info: n_ctx_train      = 2048
0.00.037.710 I print_info: n_embd           = 2048
0.00.037.710 I print_info: n_layer          = 24
0.00.037.713 I print_info: n_head           = 16
0.00.037.714 I print_info: n_head_kv        = 16
0.00.037.714 I print_info: n_rot            = 32
0.00.037.715 I print_info: n_swa            = 0
0.00.037.715 I print_info: n_embd_head_k    = 128
0.00.037.715 I print_info: n_embd_head_v    = 128
0.00.037.716 I print_info: n_gqa            = 1
0.00.037.716 I print_info: n_embd_k_gqa     = 2048
0.00.037.717 I print_info: n_embd_v_gqa     = 2048
0.00.037.717 I print_info: f_norm_eps       = 1.0e-05
0.00.037.719 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.720 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.720 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.720 I print_info: f_logit_scale    = 0.0e+00
0.00.037.721 I print_info: n_ff             = 8192
0.00.037.721 I print_info: n_expert         = 0
0.00.037.721 I print_info: n_expert_used    = 0
0.00.037.721 I print_info: causal attn      = 1
0.00.037.721 I print_info: pooling type     = 0
0.00.037.721 I print_info: rope type        = 2
0.00.037.722 I print_info: rope scaling     = linear
0.00.037.722 I print_info: freq_base_train  = 10000.0
0.00.037.723 I print_info: freq_scale_train = 1
0.00.037.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.723 I print_info: rope_finetuned   = unknown
0.00.037.723 I print_info: ssm_d_conv       = 0
0.00.037.723 I print_info: ssm_d_inner      = 0
0.00.037.723 I print_info: ssm_d_state      = 0
0.00.037.724 I print_info: ssm_dt_rank      = 0
0.00.037.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.724 I print_info: model type       = 1.4B
0.00.037.724 I print_info: model params     = 1.41 B
0.00.037.726 I print_info: general.name     = 1.4B
0.00.037.727 I print_info: vocab type       = BPE
0.00.037.727 I print_info: n_vocab          = 50304
0.00.037.727 I print_info: n_merges         = 50009
0.00.037.728 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.728 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.728 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.728 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.728 I print_info: LF token         = 128 'Ä'
0.00.037.729 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.729 I print_info: max token length = 1024
0.00.630.174 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.191 I load_tensors: offloading output layer to GPU
0.00.630.192 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.228 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.630.230 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.631.739 I llama_init_from_model: n_seq_max     = 1
0.00.631.742 I llama_init_from_model: n_ctx         = 2048
0.00.631.742 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.631.743 I llama_init_from_model: n_batch       = 2048
0.00.631.743 I llama_init_from_model: n_ubatch      = 512
0.00.631.744 I llama_init_from_model: flash_attn    = 0
0.00.631.745 I llama_init_from_model: freq_base     = 10000.0
0.00.631.746 I llama_init_from_model: freq_scale    = 1
0.00.631.747 I ggml_metal_init: allocating
0.00.631.788 I ggml_metal_init: found device: Apple M4
0.00.631.799 I ggml_metal_init: picking default device: Apple M4
0.00.633.287 I ggml_metal_init: using embedded metal library
0.00.639.528 I ggml_metal_init: GPU name:   Apple M4
0.00.639.531 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.532 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.533 I ggml_metal_init: simdgroup reduction   = true
0.00.639.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.534 I ggml_metal_init: has residency sets    = true
0.00.639.534 I ggml_metal_init: has bfloat            = true
0.00.639.534 I ggml_metal_init: use bfloat            = true
0.00.639.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.278 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.845 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.707.853 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.880 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.239 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.712.241 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.712.241 I llama_init_from_model: graph nodes  = 967
0.00.712.241 I llama_init_from_model: graph splits = 2
0.00.712.247 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.712.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.057 I main: llama threadpool init, n_threads = 4
0.00.781.101 I 
0.00.781.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.129 I 
0.00.781.307 I sampler seed: 1234
0.00.781.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.333 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.333 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.333 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.662.491 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.662.492 I llama_perf_context_print:        load time =     771.30 ms
0.01.662.493 I llama_perf_context_print: prompt eval time =      54.11 ms /     7 tokens (    7.73 ms per token,   129.36 tokens per second)
0.01.662.494 I llama_perf_context_print:        eval time =     824.07 ms /    63 runs   (   13.08 ms per token,    76.45 tokens per second)
0.01.662.494 I llama_perf_context_print:       total time =     882.35 ms /    70 tokens
0.01.662.760 I ggml_metal_free: deallocating

real	0m1.680s
user	0m0.107s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.526 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.550 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.756 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.768 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.769 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.770 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.771 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.771 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.773 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.776 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.781 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.784 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.785 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.785 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.408 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.660 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.661 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.661 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.662 I llama_model_loader: - type  f32:  194 tensors
0.00.055.662 I llama_model_loader: - type  f16:   98 tensors
0.00.055.663 I print_info: file format = GGUF V3 (latest)
0.00.055.664 I print_info: file type   = all F32 (guessed)
0.00.055.665 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.480 I load: special tokens cache size = 25
0.00.076.252 I load: token to piece cache size = 0.2984 MB
0.00.076.255 I print_info: arch             = gptneox
0.00.076.256 I print_info: vocab_only       = 0
0.00.076.256 I print_info: n_ctx_train      = 2048
0.00.076.256 I print_info: n_embd           = 2048
0.00.076.256 I print_info: n_layer          = 24
0.00.076.259 I print_info: n_head           = 16
0.00.076.260 I print_info: n_head_kv        = 16
0.00.076.261 I print_info: n_rot            = 32
0.00.076.261 I print_info: n_swa            = 0
0.00.076.261 I print_info: n_embd_head_k    = 128
0.00.076.261 I print_info: n_embd_head_v    = 128
0.00.076.262 I print_info: n_gqa            = 1
0.00.076.263 I print_info: n_embd_k_gqa     = 2048
0.00.076.264 I print_info: n_embd_v_gqa     = 2048
0.00.076.264 I print_info: f_norm_eps       = 1.0e-05
0.00.076.264 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.265 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.265 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.265 I print_info: f_logit_scale    = 0.0e+00
0.00.076.266 I print_info: n_ff             = 8192
0.00.076.266 I print_info: n_expert         = 0
0.00.076.266 I print_info: n_expert_used    = 0
0.00.076.266 I print_info: causal attn      = 1
0.00.076.266 I print_info: pooling type     = 0
0.00.076.266 I print_info: rope type        = 2
0.00.076.267 I print_info: rope scaling     = linear
0.00.076.269 I print_info: freq_base_train  = 10000.0
0.00.076.270 I print_info: freq_scale_train = 1
0.00.076.270 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.270 I print_info: rope_finetuned   = unknown
0.00.076.270 I print_info: ssm_d_conv       = 0
0.00.076.270 I print_info: ssm_d_inner      = 0
0.00.076.270 I print_info: ssm_d_state      = 0
0.00.076.271 I print_info: ssm_dt_rank      = 0
0.00.076.271 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.271 I print_info: model type       = 1.4B
0.00.076.271 I print_info: model params     = 1.41 B
0.00.076.271 I print_info: general.name     = 1.4B
0.00.076.272 I print_info: vocab type       = BPE
0.00.076.272 I print_info: n_vocab          = 50304
0.00.076.272 I print_info: n_merges         = 50009
0.00.076.277 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.277 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.278 I print_info: LF token         = 128 'Ä'
0.00.076.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.278 I print_info: max token length = 1024
0.01.254.864 I load_tensors: offloading 24 repeating layers to GPU
0.01.254.870 I load_tensors: offloading output layer to GPU
0.01.254.872 I load_tensors: offloaded 25/25 layers to GPU
0.01.254.898 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.254.900 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.255.706 I llama_init_from_model: n_seq_max     = 1
0.01.255.707 I llama_init_from_model: n_ctx         = 128
0.01.255.707 I llama_init_from_model: n_ctx_per_seq = 128
0.01.255.708 I llama_init_from_model: n_batch       = 128
0.01.255.708 I llama_init_from_model: n_ubatch      = 128
0.01.255.708 I llama_init_from_model: flash_attn    = 0
0.01.255.709 I llama_init_from_model: freq_base     = 10000.0
0.01.255.709 I llama_init_from_model: freq_scale    = 1
0.01.255.709 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.255.710 I ggml_metal_init: allocating
0.01.255.748 I ggml_metal_init: found device: Apple M4
0.01.255.754 I ggml_metal_init: picking default device: Apple M4
0.01.256.745 I ggml_metal_init: using embedded metal library
0.01.260.667 I ggml_metal_init: GPU name:   Apple M4
0.01.260.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.260.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.260.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.260.671 I ggml_metal_init: simdgroup reduction   = true
0.01.260.671 I ggml_metal_init: simdgroup matrix mul. = true
0.01.260.672 I ggml_metal_init: has residency sets    = true
0.01.260.672 I ggml_metal_init: has bfloat            = true
0.01.260.672 I ggml_metal_init: use bfloat            = true
0.01.260.672 I ggml_metal_init: hasUnifiedMemory      = true
0.01.260.673 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.271.233 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.272.986 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.272.988 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.273.001 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.274.736 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.274.737 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.274.737 I llama_init_from_model: graph nodes  = 967
0.01.274.738 I llama_init_from_model: graph splits = 2
0.01.274.739 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.274.739 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.308.729 I 
0.01.308.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.308.793 I perplexity: tokenizing the input ..
0.01.313.694 I perplexity: tokenization took 4.899 ms
0.01.313.714 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.432.093 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.433.424 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.433.438 I llama_perf_context_print:        load time =    1284.16 ms
0.01.433.439 I llama_perf_context_print: prompt eval time =     118.11 ms /   128 tokens (    0.92 ms per token,  1083.70 tokens per second)
0.01.433.440 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.433.440 I llama_perf_context_print:       total time =     124.71 ms /   129 tokens
0.01.433.813 I ggml_metal_free: deallocating

real	0m1.646s
user	0m0.097s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.183 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.276 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.278 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.278 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.287 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.292 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.903 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.903 I llama_model_loader: - type  f32:  194 tensors
0.00.024.904 I llama_model_loader: - type q8_0:   98 tensors
0.00.024.905 I print_info: file format = GGUF V3 (latest)
0.00.024.905 I print_info: file type   = Q8_0
0.00.024.906 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.296 I load: special tokens cache size = 25
0.00.039.443 I load: token to piece cache size = 0.2984 MB
0.00.039.447 I print_info: arch             = gptneox
0.00.039.447 I print_info: vocab_only       = 0
0.00.039.448 I print_info: n_ctx_train      = 2048
0.00.039.448 I print_info: n_embd           = 2048
0.00.039.448 I print_info: n_layer          = 24
0.00.039.452 I print_info: n_head           = 16
0.00.039.455 I print_info: n_head_kv        = 16
0.00.039.455 I print_info: n_rot            = 32
0.00.039.455 I print_info: n_swa            = 0
0.00.039.455 I print_info: n_embd_head_k    = 128
0.00.039.455 I print_info: n_embd_head_v    = 128
0.00.039.456 I print_info: n_gqa            = 1
0.00.039.457 I print_info: n_embd_k_gqa     = 2048
0.00.039.457 I print_info: n_embd_v_gqa     = 2048
0.00.039.458 I print_info: f_norm_eps       = 1.0e-05
0.00.039.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.459 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.459 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.459 I print_info: f_logit_scale    = 0.0e+00
0.00.039.460 I print_info: n_ff             = 8192
0.00.039.460 I print_info: n_expert         = 0
0.00.039.460 I print_info: n_expert_used    = 0
0.00.039.460 I print_info: causal attn      = 1
0.00.039.462 I print_info: pooling type     = 0
0.00.039.462 I print_info: rope type        = 2
0.00.039.463 I print_info: rope scaling     = linear
0.00.039.463 I print_info: freq_base_train  = 10000.0
0.00.039.463 I print_info: freq_scale_train = 1
0.00.039.464 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.465 I print_info: rope_finetuned   = unknown
0.00.039.465 I print_info: ssm_d_conv       = 0
0.00.039.465 I print_info: ssm_d_inner      = 0
0.00.039.465 I print_info: ssm_d_state      = 0
0.00.039.465 I print_info: ssm_dt_rank      = 0
0.00.039.465 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.466 I print_info: model type       = 1.4B
0.00.039.466 I print_info: model params     = 1.41 B
0.00.039.466 I print_info: general.name     = 1.4B
0.00.039.467 I print_info: vocab type       = BPE
0.00.039.467 I print_info: n_vocab          = 50304
0.00.039.467 I print_info: n_merges         = 50009
0.00.039.467 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.467 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.468 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.468 I print_info: LF token         = 128 'Ä'
0.00.039.468 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.468 I print_info: max token length = 1024
0.00.862.473 I load_tensors: offloading 24 repeating layers to GPU
0.00.862.478 I load_tensors: offloading output layer to GPU
0.00.862.478 I load_tensors: offloaded 25/25 layers to GPU
0.00.862.509 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.862.514 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.863.807 I llama_init_from_model: n_seq_max     = 1
0.00.863.809 I llama_init_from_model: n_ctx         = 128
0.00.863.809 I llama_init_from_model: n_ctx_per_seq = 128
0.00.863.810 I llama_init_from_model: n_batch       = 128
0.00.863.813 I llama_init_from_model: n_ubatch      = 128
0.00.863.814 I llama_init_from_model: flash_attn    = 0
0.00.863.814 I llama_init_from_model: freq_base     = 10000.0
0.00.863.815 I llama_init_from_model: freq_scale    = 1
0.00.863.816 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.863.817 I ggml_metal_init: allocating
0.00.863.893 I ggml_metal_init: found device: Apple M4
0.00.863.905 I ggml_metal_init: picking default device: Apple M4
0.00.865.217 I ggml_metal_init: using embedded metal library
0.00.870.360 I ggml_metal_init: GPU name:   Apple M4
0.00.870.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.870.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.870.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.870.366 I ggml_metal_init: simdgroup reduction   = true
0.00.870.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.870.366 I ggml_metal_init: has residency sets    = true
0.00.870.367 I ggml_metal_init: has bfloat            = true
0.00.870.367 I ggml_metal_init: use bfloat            = true
0.00.870.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.870.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.885.060 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.888.363 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.888.366 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.888.411 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.891.368 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.891.369 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.891.370 I llama_init_from_model: graph nodes  = 967
0.00.891.370 I llama_init_from_model: graph splits = 2
0.00.891.373 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.891.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.918.270 I 
0.00.918.350 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.918.369 I perplexity: tokenizing the input ..
0.00.925.382 I perplexity: tokenization took 7.01 ms
0.00.925.402 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.064.151 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.065.491 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.065.507 I llama_perf_context_print:        load time =     909.08 ms
0.01.065.508 I llama_perf_context_print: prompt eval time =     137.81 ms /   128 tokens (    1.08 ms per token,   928.80 tokens per second)
0.01.065.508 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.065.509 I llama_perf_context_print:       total time =     147.24 ms /   129 tokens
0.01.065.913 I ggml_metal_free: deallocating

real	0m1.080s
user	0m0.076s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.586 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.863 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.869 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.869 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.870 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.872 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.872 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.499 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.500 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.501 I llama_model_loader: - type  f32:  194 tensors
0.00.025.501 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.502 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.502 I print_info: file format = GGUF V3 (latest)
0.00.025.503 I print_info: file type   = Q4_0
0.00.025.504 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.661 I load: special tokens cache size = 25
0.00.039.606 I load: token to piece cache size = 0.2984 MB
0.00.039.609 I print_info: arch             = gptneox
0.00.039.609 I print_info: vocab_only       = 0
0.00.039.609 I print_info: n_ctx_train      = 2048
0.00.039.609 I print_info: n_embd           = 2048
0.00.039.610 I print_info: n_layer          = 24
0.00.039.613 I print_info: n_head           = 16
0.00.039.614 I print_info: n_head_kv        = 16
0.00.039.614 I print_info: n_rot            = 32
0.00.039.614 I print_info: n_swa            = 0
0.00.039.614 I print_info: n_embd_head_k    = 128
0.00.039.614 I print_info: n_embd_head_v    = 128
0.00.039.615 I print_info: n_gqa            = 1
0.00.039.616 I print_info: n_embd_k_gqa     = 2048
0.00.039.617 I print_info: n_embd_v_gqa     = 2048
0.00.039.617 I print_info: f_norm_eps       = 1.0e-05
0.00.039.618 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.618 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.618 I print_info: f_logit_scale    = 0.0e+00
0.00.039.619 I print_info: n_ff             = 8192
0.00.039.619 I print_info: n_expert         = 0
0.00.039.619 I print_info: n_expert_used    = 0
0.00.039.619 I print_info: causal attn      = 1
0.00.039.619 I print_info: pooling type     = 0
0.00.039.619 I print_info: rope type        = 2
0.00.039.621 I print_info: rope scaling     = linear
0.00.039.621 I print_info: freq_base_train  = 10000.0
0.00.039.622 I print_info: freq_scale_train = 1
0.00.039.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.622 I print_info: rope_finetuned   = unknown
0.00.039.622 I print_info: ssm_d_conv       = 0
0.00.039.622 I print_info: ssm_d_inner      = 0
0.00.039.623 I print_info: ssm_d_state      = 0
0.00.039.623 I print_info: ssm_dt_rank      = 0
0.00.039.623 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.623 I print_info: model type       = 1.4B
0.00.039.624 I print_info: model params     = 1.41 B
0.00.039.624 I print_info: general.name     = 1.4B
0.00.039.624 I print_info: vocab type       = BPE
0.00.039.624 I print_info: n_vocab          = 50304
0.00.039.624 I print_info: n_merges         = 50009
0.00.039.625 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.625 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.625 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.625 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.626 I print_info: LF token         = 128 'Ä'
0.00.039.626 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.626 I print_info: max token length = 1024
0.00.566.621 I load_tensors: offloading 24 repeating layers to GPU
0.00.566.635 I load_tensors: offloading output layer to GPU
0.00.566.636 I load_tensors: offloaded 25/25 layers to GPU
0.00.566.671 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.566.672 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.568.043 I llama_init_from_model: n_seq_max     = 1
0.00.568.048 I llama_init_from_model: n_ctx         = 128
0.00.568.049 I llama_init_from_model: n_ctx_per_seq = 128
0.00.568.052 I llama_init_from_model: n_batch       = 128
0.00.568.052 I llama_init_from_model: n_ubatch      = 128
0.00.568.053 I llama_init_from_model: flash_attn    = 0
0.00.568.055 I llama_init_from_model: freq_base     = 10000.0
0.00.568.056 I llama_init_from_model: freq_scale    = 1
0.00.568.061 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.568.063 I ggml_metal_init: allocating
0.00.568.142 I ggml_metal_init: found device: Apple M4
0.00.568.155 I ggml_metal_init: picking default device: Apple M4
0.00.569.981 I ggml_metal_init: using embedded metal library
0.00.575.493 I ggml_metal_init: GPU name:   Apple M4
0.00.575.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.575.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.575.501 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.575.502 I ggml_metal_init: simdgroup reduction   = true
0.00.575.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.575.502 I ggml_metal_init: has residency sets    = true
0.00.575.503 I ggml_metal_init: has bfloat            = true
0.00.575.503 I ggml_metal_init: use bfloat            = true
0.00.575.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.575.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.594.330 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.597.920 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.597.927 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.597.954 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.601.266 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.601.267 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.601.268 I llama_init_from_model: graph nodes  = 967
0.00.601.268 I llama_init_from_model: graph splits = 2
0.00.601.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.601.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.288 I 
0.00.629.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.402 I perplexity: tokenizing the input ..
0.00.636.911 I perplexity: tokenization took 7.506 ms
0.00.636.931 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.773.884 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.775.227 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.775.242 I llama_perf_context_print:        load time =     619.69 ms
0.00.775.243 I llama_perf_context_print: prompt eval time =     136.06 ms /   128 tokens (    1.06 ms per token,   940.75 tokens per second)
0.00.775.243 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.775.244 I llama_perf_context_print:       total time =     145.96 ms /   129 tokens
0.00.775.635 I ggml_metal_free: deallocating

real	0m0.791s
user	0m0.080s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.867 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.871 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.877 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.883 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.631 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.639 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.465 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.466 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.466 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.466 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.467 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.467 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.467 I llama_model_loader: - type  f32:  194 tensors
0.00.024.468 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.468 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.469 I print_info: file format = GGUF V3 (latest)
0.00.024.469 I print_info: file type   = Q4_1
0.00.024.470 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.632 I load: special tokens cache size = 25
0.00.038.603 I load: token to piece cache size = 0.2984 MB
0.00.038.605 I print_info: arch             = gptneox
0.00.038.606 I print_info: vocab_only       = 0
0.00.038.606 I print_info: n_ctx_train      = 2048
0.00.038.606 I print_info: n_embd           = 2048
0.00.038.606 I print_info: n_layer          = 24
0.00.038.610 I print_info: n_head           = 16
0.00.038.611 I print_info: n_head_kv        = 16
0.00.038.612 I print_info: n_rot            = 32
0.00.038.612 I print_info: n_swa            = 0
0.00.038.612 I print_info: n_embd_head_k    = 128
0.00.038.612 I print_info: n_embd_head_v    = 128
0.00.038.613 I print_info: n_gqa            = 1
0.00.038.614 I print_info: n_embd_k_gqa     = 2048
0.00.038.614 I print_info: n_embd_v_gqa     = 2048
0.00.038.615 I print_info: f_norm_eps       = 1.0e-05
0.00.038.615 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.616 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.616 I print_info: f_logit_scale    = 0.0e+00
0.00.038.616 I print_info: n_ff             = 8192
0.00.038.617 I print_info: n_expert         = 0
0.00.038.617 I print_info: n_expert_used    = 0
0.00.038.617 I print_info: causal attn      = 1
0.00.038.617 I print_info: pooling type     = 0
0.00.038.617 I print_info: rope type        = 2
0.00.038.618 I print_info: rope scaling     = linear
0.00.038.618 I print_info: freq_base_train  = 10000.0
0.00.038.618 I print_info: freq_scale_train = 1
0.00.038.618 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.619 I print_info: rope_finetuned   = unknown
0.00.038.619 I print_info: ssm_d_conv       = 0
0.00.038.619 I print_info: ssm_d_inner      = 0
0.00.038.619 I print_info: ssm_d_state      = 0
0.00.038.619 I print_info: ssm_dt_rank      = 0
0.00.038.619 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.620 I print_info: model type       = 1.4B
0.00.038.620 I print_info: model params     = 1.41 B
0.00.038.620 I print_info: general.name     = 1.4B
0.00.038.621 I print_info: vocab type       = BPE
0.00.038.621 I print_info: n_vocab          = 50304
0.00.038.621 I print_info: n_merges         = 50009
0.00.038.621 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.621 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: LF token         = 128 'Ä'
0.00.038.622 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.623 I print_info: max token length = 1024
0.00.633.940 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.952 I load_tensors: offloading output layer to GPU
0.00.633.953 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.981 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.633.982 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.635.336 I llama_init_from_model: n_seq_max     = 1
0.00.635.343 I llama_init_from_model: n_ctx         = 128
0.00.635.344 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.345 I llama_init_from_model: n_batch       = 128
0.00.635.345 I llama_init_from_model: n_ubatch      = 128
0.00.635.346 I llama_init_from_model: flash_attn    = 0
0.00.635.347 I llama_init_from_model: freq_base     = 10000.0
0.00.635.347 I llama_init_from_model: freq_scale    = 1
0.00.635.348 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.350 I ggml_metal_init: allocating
0.00.635.401 I ggml_metal_init: found device: Apple M4
0.00.635.415 I ggml_metal_init: picking default device: Apple M4
0.00.636.999 I ggml_metal_init: using embedded metal library
0.00.642.458 I ggml_metal_init: GPU name:   Apple M4
0.00.642.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.465 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.466 I ggml_metal_init: simdgroup reduction   = true
0.00.642.466 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.467 I ggml_metal_init: has residency sets    = true
0.00.642.467 I ggml_metal_init: has bfloat            = true
0.00.642.467 I ggml_metal_init: use bfloat            = true
0.00.642.468 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.124 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.717 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.665.724 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.665.893 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.403 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.669.405 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.669.406 I llama_init_from_model: graph nodes  = 967
0.00.669.406 I llama_init_from_model: graph splits = 2
0.00.669.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.669.410 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.348 I 
0.00.694.439 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.467 I perplexity: tokenizing the input ..
0.00.701.870 I perplexity: tokenization took 7.401 ms
0.00.701.889 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.105 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.839.440 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.839.454 I llama_perf_context_print:        load time =     685.44 ms
0.00.839.454 I llama_perf_context_print: prompt eval time =     135.29 ms /   128 tokens (    1.06 ms per token,   946.13 tokens per second)
0.00.839.455 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.839.455 I llama_perf_context_print:       total time =     145.11 ms /   129 tokens
0.00.839.851 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.080s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.910 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.114 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.136 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.879 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.627 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.631 I llama_model_loader: - type  f32:  194 tensors
0.00.024.632 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.632 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.633 I print_info: file format = GGUF V3 (latest)
0.00.024.633 I print_info: file type   = Q5_0
0.00.024.634 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.375 I load: special tokens cache size = 25
0.00.038.235 I load: token to piece cache size = 0.2984 MB
0.00.038.238 I print_info: arch             = gptneox
0.00.038.238 I print_info: vocab_only       = 0
0.00.038.238 I print_info: n_ctx_train      = 2048
0.00.038.239 I print_info: n_embd           = 2048
0.00.038.239 I print_info: n_layer          = 24
0.00.038.242 I print_info: n_head           = 16
0.00.038.243 I print_info: n_head_kv        = 16
0.00.038.243 I print_info: n_rot            = 32
0.00.038.243 I print_info: n_swa            = 0
0.00.038.243 I print_info: n_embd_head_k    = 128
0.00.038.243 I print_info: n_embd_head_v    = 128
0.00.038.244 I print_info: n_gqa            = 1
0.00.038.245 I print_info: n_embd_k_gqa     = 2048
0.00.038.248 I print_info: n_embd_v_gqa     = 2048
0.00.038.249 I print_info: f_norm_eps       = 1.0e-05
0.00.038.249 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.249 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.249 I print_info: f_logit_scale    = 0.0e+00
0.00.038.250 I print_info: n_ff             = 8192
0.00.038.250 I print_info: n_expert         = 0
0.00.038.250 I print_info: n_expert_used    = 0
0.00.038.251 I print_info: causal attn      = 1
0.00.038.251 I print_info: pooling type     = 0
0.00.038.251 I print_info: rope type        = 2
0.00.038.251 I print_info: rope scaling     = linear
0.00.038.252 I print_info: freq_base_train  = 10000.0
0.00.038.252 I print_info: freq_scale_train = 1
0.00.038.252 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.252 I print_info: rope_finetuned   = unknown
0.00.038.252 I print_info: ssm_d_conv       = 0
0.00.038.253 I print_info: ssm_d_inner      = 0
0.00.038.254 I print_info: ssm_d_state      = 0
0.00.038.254 I print_info: ssm_dt_rank      = 0
0.00.038.254 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.254 I print_info: model type       = 1.4B
0.00.038.255 I print_info: model params     = 1.41 B
0.00.038.255 I print_info: general.name     = 1.4B
0.00.038.256 I print_info: vocab type       = BPE
0.00.038.256 I print_info: n_vocab          = 50304
0.00.038.256 I print_info: n_merges         = 50009
0.00.038.256 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.256 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.257 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.257 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.257 I print_info: LF token         = 128 'Ä'
0.00.038.257 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.258 I print_info: max token length = 1024
0.00.651.134 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.149 I load_tensors: offloading output layer to GPU
0.00.651.150 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.183 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.651.184 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.652.678 I llama_init_from_model: n_seq_max     = 1
0.00.652.681 I llama_init_from_model: n_ctx         = 128
0.00.652.682 I llama_init_from_model: n_ctx_per_seq = 128
0.00.652.682 I llama_init_from_model: n_batch       = 128
0.00.652.683 I llama_init_from_model: n_ubatch      = 128
0.00.652.683 I llama_init_from_model: flash_attn    = 0
0.00.652.684 I llama_init_from_model: freq_base     = 10000.0
0.00.652.685 I llama_init_from_model: freq_scale    = 1
0.00.652.686 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.652.687 I ggml_metal_init: allocating
0.00.652.705 I ggml_metal_init: found device: Apple M4
0.00.652.715 I ggml_metal_init: picking default device: Apple M4
0.00.654.056 I ggml_metal_init: using embedded metal library
0.00.660.453 I ggml_metal_init: GPU name:   Apple M4
0.00.660.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.458 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.458 I ggml_metal_init: simdgroup reduction   = true
0.00.660.459 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.459 I ggml_metal_init: has residency sets    = true
0.00.660.459 I ggml_metal_init: has bfloat            = true
0.00.660.459 I ggml_metal_init: use bfloat            = true
0.00.660.460 I ggml_metal_init: hasUnifiedMemory      = true
0.00.660.461 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.738 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.447 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.681.455 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.681.483 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.157 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.685.159 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.685.159 I llama_init_from_model: graph nodes  = 967
0.00.685.160 I llama_init_from_model: graph splits = 2
0.00.685.162 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.685.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.655 I 
0.00.717.736 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.755 I perplexity: tokenizing the input ..
0.00.724.053 I perplexity: tokenization took 6.297 ms
0.00.724.068 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.872.045 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.873.386 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.873.402 I llama_perf_context_print:        load time =     708.74 ms
0.00.873.403 I llama_perf_context_print: prompt eval time =     147.43 ms /   128 tokens (    1.15 ms per token,   868.21 tokens per second)
0.00.873.404 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.873.404 I llama_perf_context_print:       total time =     155.75 ms /   129 tokens
0.00.873.786 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.077s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.238 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.309 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.310 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.310 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.311 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.313 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.313 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.151 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.907 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.908 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.909 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.910 I llama_model_loader: - type  f32:  194 tensors
0.00.025.910 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.911 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.911 I print_info: file format = GGUF V3 (latest)
0.00.025.912 I print_info: file type   = Q5_1
0.00.025.912 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.050 I load: special tokens cache size = 25
0.00.039.952 I load: token to piece cache size = 0.2984 MB
0.00.039.954 I print_info: arch             = gptneox
0.00.039.954 I print_info: vocab_only       = 0
0.00.039.955 I print_info: n_ctx_train      = 2048
0.00.039.955 I print_info: n_embd           = 2048
0.00.039.955 I print_info: n_layer          = 24
0.00.039.958 I print_info: n_head           = 16
0.00.039.958 I print_info: n_head_kv        = 16
0.00.039.959 I print_info: n_rot            = 32
0.00.039.959 I print_info: n_swa            = 0
0.00.039.960 I print_info: n_embd_head_k    = 128
0.00.039.961 I print_info: n_embd_head_v    = 128
0.00.039.962 I print_info: n_gqa            = 1
0.00.039.962 I print_info: n_embd_k_gqa     = 2048
0.00.039.963 I print_info: n_embd_v_gqa     = 2048
0.00.039.964 I print_info: f_norm_eps       = 1.0e-05
0.00.039.964 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.964 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.964 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.965 I print_info: f_logit_scale    = 0.0e+00
0.00.039.965 I print_info: n_ff             = 8192
0.00.039.966 I print_info: n_expert         = 0
0.00.039.966 I print_info: n_expert_used    = 0
0.00.039.966 I print_info: causal attn      = 1
0.00.039.966 I print_info: pooling type     = 0
0.00.039.966 I print_info: rope type        = 2
0.00.039.966 I print_info: rope scaling     = linear
0.00.039.967 I print_info: freq_base_train  = 10000.0
0.00.039.967 I print_info: freq_scale_train = 1
0.00.039.967 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.967 I print_info: rope_finetuned   = unknown
0.00.039.968 I print_info: ssm_d_conv       = 0
0.00.039.968 I print_info: ssm_d_inner      = 0
0.00.039.968 I print_info: ssm_d_state      = 0
0.00.039.968 I print_info: ssm_dt_rank      = 0
0.00.039.968 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.969 I print_info: model type       = 1.4B
0.00.039.969 I print_info: model params     = 1.41 B
0.00.039.969 I print_info: general.name     = 1.4B
0.00.039.970 I print_info: vocab type       = BPE
0.00.039.970 I print_info: n_vocab          = 50304
0.00.039.970 I print_info: n_merges         = 50009
0.00.039.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.971 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.973 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.973 I print_info: LF token         = 128 'Ä'
0.00.039.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.974 I print_info: max token length = 1024
0.00.700.159 I load_tensors: offloading 24 repeating layers to GPU
0.00.700.163 I load_tensors: offloading output layer to GPU
0.00.700.165 I load_tensors: offloaded 25/25 layers to GPU
0.00.700.189 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.700.192 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.701.618 I llama_init_from_model: n_seq_max     = 1
0.00.701.621 I llama_init_from_model: n_ctx         = 128
0.00.701.621 I llama_init_from_model: n_ctx_per_seq = 128
0.00.701.622 I llama_init_from_model: n_batch       = 128
0.00.701.622 I llama_init_from_model: n_ubatch      = 128
0.00.701.622 I llama_init_from_model: flash_attn    = 0
0.00.701.623 I llama_init_from_model: freq_base     = 10000.0
0.00.701.624 I llama_init_from_model: freq_scale    = 1
0.00.701.625 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.701.626 I ggml_metal_init: allocating
0.00.701.639 I ggml_metal_init: found device: Apple M4
0.00.701.648 I ggml_metal_init: picking default device: Apple M4
0.00.703.000 I ggml_metal_init: using embedded metal library
0.00.708.946 I ggml_metal_init: GPU name:   Apple M4
0.00.708.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.708.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.708.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.708.952 I ggml_metal_init: simdgroup reduction   = true
0.00.708.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.708.953 I ggml_metal_init: has residency sets    = true
0.00.708.953 I ggml_metal_init: has bfloat            = true
0.00.708.953 I ggml_metal_init: use bfloat            = true
0.00.708.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.708.955 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.775 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.283 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.729.292 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.729.326 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.732.520 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.732.521 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.732.522 I llama_init_from_model: graph nodes  = 967
0.00.732.522 I llama_init_from_model: graph splits = 2
0.00.732.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.732.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.239 I 
0.00.762.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.337 I perplexity: tokenizing the input ..
0.00.769.301 I perplexity: tokenization took 6.96 ms
0.00.769.320 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.917.660 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.919.225 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.919.243 I llama_perf_context_print:        load time =     751.99 ms
0.00.919.244 I llama_perf_context_print: prompt eval time =     147.39 ms /   128 tokens (    1.15 ms per token,   868.42 tokens per second)
0.00.919.244 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.919.245 I llama_perf_context_print:       total time =     157.01 ms /   129 tokens
0.00.919.626 I ggml_metal_free: deallocating

real	0m0.936s
user	0m0.077s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.429 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.435 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.435 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.437 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.438 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.439 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.439 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.440 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.442 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.443 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.297 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.117 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.119 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.120 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.121 I llama_model_loader: - type  f32:  194 tensors
0.00.025.121 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.121 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.122 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.122 I print_info: file format = GGUF V3 (latest)
0.00.025.123 I print_info: file type   = Q2_K - Medium
0.00.025.123 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.591 I load: special tokens cache size = 25
0.00.039.663 I load: token to piece cache size = 0.2984 MB
0.00.039.667 I print_info: arch             = gptneox
0.00.039.667 I print_info: vocab_only       = 0
0.00.039.667 I print_info: n_ctx_train      = 2048
0.00.039.668 I print_info: n_embd           = 2048
0.00.039.668 I print_info: n_layer          = 24
0.00.039.671 I print_info: n_head           = 16
0.00.039.672 I print_info: n_head_kv        = 16
0.00.039.672 I print_info: n_rot            = 32
0.00.039.672 I print_info: n_swa            = 0
0.00.039.673 I print_info: n_embd_head_k    = 128
0.00.039.673 I print_info: n_embd_head_v    = 128
0.00.039.674 I print_info: n_gqa            = 1
0.00.039.674 I print_info: n_embd_k_gqa     = 2048
0.00.039.675 I print_info: n_embd_v_gqa     = 2048
0.00.039.676 I print_info: f_norm_eps       = 1.0e-05
0.00.039.676 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.676 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.676 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.678 I print_info: f_logit_scale    = 0.0e+00
0.00.039.680 I print_info: n_ff             = 8192
0.00.039.680 I print_info: n_expert         = 0
0.00.039.680 I print_info: n_expert_used    = 0
0.00.039.680 I print_info: causal attn      = 1
0.00.039.680 I print_info: pooling type     = 0
0.00.039.681 I print_info: rope type        = 2
0.00.039.681 I print_info: rope scaling     = linear
0.00.039.681 I print_info: freq_base_train  = 10000.0
0.00.039.681 I print_info: freq_scale_train = 1
0.00.039.681 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.682 I print_info: rope_finetuned   = unknown
0.00.039.682 I print_info: ssm_d_conv       = 0
0.00.039.682 I print_info: ssm_d_inner      = 0
0.00.039.682 I print_info: ssm_d_state      = 0
0.00.039.682 I print_info: ssm_dt_rank      = 0
0.00.039.682 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.683 I print_info: model type       = 1.4B
0.00.039.683 I print_info: model params     = 1.41 B
0.00.039.683 I print_info: general.name     = 1.4B
0.00.039.684 I print_info: vocab type       = BPE
0.00.039.684 I print_info: n_vocab          = 50304
0.00.039.684 I print_info: n_merges         = 50009
0.00.039.684 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.687 I print_info: LF token         = 128 'Ä'
0.00.039.687 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.687 I print_info: max token length = 1024
0.00.390.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.390.861 I load_tensors: offloading output layer to GPU
0.00.390.862 I load_tensors: offloaded 25/25 layers to GPU
0.00.390.896 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.390.897 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.392.153 I llama_init_from_model: n_seq_max     = 1
0.00.392.160 I llama_init_from_model: n_ctx         = 128
0.00.392.165 I llama_init_from_model: n_ctx_per_seq = 128
0.00.392.166 I llama_init_from_model: n_batch       = 128
0.00.392.166 I llama_init_from_model: n_ubatch      = 128
0.00.392.167 I llama_init_from_model: flash_attn    = 0
0.00.392.169 I llama_init_from_model: freq_base     = 10000.0
0.00.392.170 I llama_init_from_model: freq_scale    = 1
0.00.392.170 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.392.176 I ggml_metal_init: allocating
0.00.392.239 I ggml_metal_init: found device: Apple M4
0.00.392.254 I ggml_metal_init: picking default device: Apple M4
0.00.394.125 I ggml_metal_init: using embedded metal library
0.00.399.600 I ggml_metal_init: GPU name:   Apple M4
0.00.399.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.399.613 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.399.614 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.399.615 I ggml_metal_init: simdgroup reduction   = true
0.00.399.615 I ggml_metal_init: simdgroup matrix mul. = true
0.00.399.615 I ggml_metal_init: has residency sets    = true
0.00.399.615 I ggml_metal_init: has bfloat            = true
0.00.399.616 I ggml_metal_init: use bfloat            = true
0.00.399.617 I ggml_metal_init: hasUnifiedMemory      = true
0.00.399.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.421.818 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.425.450 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.425.461 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.425.498 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.429.158 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.429.160 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.429.161 I llama_init_from_model: graph nodes  = 967
0.00.429.161 I llama_init_from_model: graph splits = 2
0.00.429.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.429.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.462.529 I 
0.00.462.617 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.462.640 I perplexity: tokenizing the input ..
0.00.468.777 I perplexity: tokenization took 6.135 ms
0.00.468.794 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.395 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.614.768 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.614.784 I llama_perf_context_print:        load time =     452.81 ms
0.00.614.785 I llama_perf_context_print: prompt eval time =     143.72 ms /   128 tokens (    1.12 ms per token,   890.60 tokens per second)
0.00.614.786 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.786 I llama_perf_context_print:       total time =     152.26 ms /   129 tokens
0.00.615.110 I ggml_metal_free: deallocating

real	0m0.629s
user	0m0.080s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.727 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.797 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.811 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.814 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.665 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.745 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.747 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.748 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.748 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.748 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.748 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.749 I llama_model_loader: - type  f32:  194 tensors
0.00.024.750 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.750 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.750 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.750 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.751 I print_info: file format = GGUF V3 (latest)
0.00.024.751 I print_info: file type   = Q3_K - Medium
0.00.024.754 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.911 I load: special tokens cache size = 25
0.00.038.950 I load: token to piece cache size = 0.2984 MB
0.00.038.958 I print_info: arch             = gptneox
0.00.038.959 I print_info: vocab_only       = 0
0.00.038.959 I print_info: n_ctx_train      = 2048
0.00.038.959 I print_info: n_embd           = 2048
0.00.038.959 I print_info: n_layer          = 24
0.00.038.964 I print_info: n_head           = 16
0.00.038.965 I print_info: n_head_kv        = 16
0.00.038.965 I print_info: n_rot            = 32
0.00.038.965 I print_info: n_swa            = 0
0.00.038.965 I print_info: n_embd_head_k    = 128
0.00.038.967 I print_info: n_embd_head_v    = 128
0.00.038.968 I print_info: n_gqa            = 1
0.00.038.968 I print_info: n_embd_k_gqa     = 2048
0.00.038.969 I print_info: n_embd_v_gqa     = 2048
0.00.038.969 I print_info: f_norm_eps       = 1.0e-05
0.00.038.970 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.970 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.971 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.972 I print_info: f_logit_scale    = 0.0e+00
0.00.038.973 I print_info: n_ff             = 8192
0.00.038.973 I print_info: n_expert         = 0
0.00.038.973 I print_info: n_expert_used    = 0
0.00.038.973 I print_info: causal attn      = 1
0.00.038.973 I print_info: pooling type     = 0
0.00.038.973 I print_info: rope type        = 2
0.00.038.974 I print_info: rope scaling     = linear
0.00.038.974 I print_info: freq_base_train  = 10000.0
0.00.038.974 I print_info: freq_scale_train = 1
0.00.038.974 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.975 I print_info: rope_finetuned   = unknown
0.00.038.975 I print_info: ssm_d_conv       = 0
0.00.038.975 I print_info: ssm_d_inner      = 0
0.00.038.975 I print_info: ssm_d_state      = 0
0.00.038.975 I print_info: ssm_dt_rank      = 0
0.00.038.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.979 I print_info: model type       = 1.4B
0.00.038.980 I print_info: model params     = 1.41 B
0.00.038.980 I print_info: general.name     = 1.4B
0.00.038.981 I print_info: vocab type       = BPE
0.00.038.981 I print_info: n_vocab          = 50304
0.00.038.981 I print_info: n_merges         = 50009
0.00.038.981 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.981 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.981 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.982 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.982 I print_info: LF token         = 128 'Ä'
0.00.038.982 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.982 I print_info: max token length = 1024
0.00.429.146 I load_tensors: offloading 24 repeating layers to GPU
0.00.429.159 I load_tensors: offloading output layer to GPU
0.00.429.160 I load_tensors: offloaded 25/25 layers to GPU
0.00.429.195 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.429.196 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.430.536 I llama_init_from_model: n_seq_max     = 1
0.00.430.554 I llama_init_from_model: n_ctx         = 128
0.00.430.555 I llama_init_from_model: n_ctx_per_seq = 128
0.00.430.555 I llama_init_from_model: n_batch       = 128
0.00.430.555 I llama_init_from_model: n_ubatch      = 128
0.00.430.556 I llama_init_from_model: flash_attn    = 0
0.00.430.558 I llama_init_from_model: freq_base     = 10000.0
0.00.430.558 I llama_init_from_model: freq_scale    = 1
0.00.430.559 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.430.561 I ggml_metal_init: allocating
0.00.430.645 I ggml_metal_init: found device: Apple M4
0.00.430.659 I ggml_metal_init: picking default device: Apple M4
0.00.432.529 I ggml_metal_init: using embedded metal library
0.00.438.572 I ggml_metal_init: GPU name:   Apple M4
0.00.438.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.438.583 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.438.584 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.438.584 I ggml_metal_init: simdgroup reduction   = true
0.00.438.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.438.585 I ggml_metal_init: has residency sets    = true
0.00.438.585 I ggml_metal_init: has bfloat            = true
0.00.438.590 I ggml_metal_init: use bfloat            = true
0.00.438.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.438.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.459.278 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.462.917 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.462.922 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.462.952 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.466.450 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.466.452 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.466.453 I llama_init_from_model: graph nodes  = 967
0.00.466.453 I llama_init_from_model: graph splits = 2
0.00.466.456 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.466.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.713 I 
0.00.496.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.820 I perplexity: tokenizing the input ..
0.00.503.627 I perplexity: tokenization took 6.805 ms
0.00.503.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.647.896 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.649.316 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.649.329 I llama_perf_context_print:        load time =     487.98 ms
0.00.649.330 I llama_perf_context_print: prompt eval time =     144.03 ms /   128 tokens (    1.13 ms per token,   888.72 tokens per second)
0.00.649.331 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.331 I llama_perf_context_print:       total time =     152.62 ms /   129 tokens
0.00.649.720 I ggml_metal_free: deallocating

real	0m0.664s
user	0m0.079s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.929 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.545 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.551 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.552 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.552 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.552 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.553 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.554 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.554 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.555 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.555 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.556 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.557 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.557 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.528 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.338 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.338 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.339 I llama_model_loader: - type  f32:  194 tensors
0.00.025.339 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.339 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.340 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.340 I print_info: file format = GGUF V3 (latest)
0.00.025.341 I print_info: file type   = Q4_K - Medium
0.00.025.342 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.324 I load: special tokens cache size = 25
0.00.039.346 I load: token to piece cache size = 0.2984 MB
0.00.039.349 I print_info: arch             = gptneox
0.00.039.349 I print_info: vocab_only       = 0
0.00.039.349 I print_info: n_ctx_train      = 2048
0.00.039.349 I print_info: n_embd           = 2048
0.00.039.350 I print_info: n_layer          = 24
0.00.039.354 I print_info: n_head           = 16
0.00.039.355 I print_info: n_head_kv        = 16
0.00.039.355 I print_info: n_rot            = 32
0.00.039.355 I print_info: n_swa            = 0
0.00.039.355 I print_info: n_embd_head_k    = 128
0.00.039.355 I print_info: n_embd_head_v    = 128
0.00.039.356 I print_info: n_gqa            = 1
0.00.039.357 I print_info: n_embd_k_gqa     = 2048
0.00.039.359 I print_info: n_embd_v_gqa     = 2048
0.00.039.359 I print_info: f_norm_eps       = 1.0e-05
0.00.039.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.360 I print_info: f_logit_scale    = 0.0e+00
0.00.039.361 I print_info: n_ff             = 8192
0.00.039.363 I print_info: n_expert         = 0
0.00.039.363 I print_info: n_expert_used    = 0
0.00.039.363 I print_info: causal attn      = 1
0.00.039.363 I print_info: pooling type     = 0
0.00.039.363 I print_info: rope type        = 2
0.00.039.364 I print_info: rope scaling     = linear
0.00.039.364 I print_info: freq_base_train  = 10000.0
0.00.039.364 I print_info: freq_scale_train = 1
0.00.039.364 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.365 I print_info: rope_finetuned   = unknown
0.00.039.365 I print_info: ssm_d_conv       = 0
0.00.039.365 I print_info: ssm_d_inner      = 0
0.00.039.365 I print_info: ssm_d_state      = 0
0.00.039.365 I print_info: ssm_dt_rank      = 0
0.00.039.365 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.366 I print_info: model type       = 1.4B
0.00.039.366 I print_info: model params     = 1.41 B
0.00.039.366 I print_info: general.name     = 1.4B
0.00.039.367 I print_info: vocab type       = BPE
0.00.039.367 I print_info: n_vocab          = 50304
0.00.039.367 I print_info: n_merges         = 50009
0.00.039.367 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.368 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.368 I print_info: LF token         = 128 'Ä'
0.00.039.368 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.368 I print_info: max token length = 1024
0.00.525.888 I load_tensors: offloading 24 repeating layers to GPU
0.00.525.899 I load_tensors: offloading output layer to GPU
0.00.525.899 I load_tensors: offloaded 25/25 layers to GPU
0.00.525.936 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.525.938 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.527.489 I llama_init_from_model: n_seq_max     = 1
0.00.527.495 I llama_init_from_model: n_ctx         = 128
0.00.527.495 I llama_init_from_model: n_ctx_per_seq = 128
0.00.527.496 I llama_init_from_model: n_batch       = 128
0.00.527.496 I llama_init_from_model: n_ubatch      = 128
0.00.527.496 I llama_init_from_model: flash_attn    = 0
0.00.527.498 I llama_init_from_model: freq_base     = 10000.0
0.00.527.499 I llama_init_from_model: freq_scale    = 1
0.00.527.499 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.527.502 I ggml_metal_init: allocating
0.00.527.583 I ggml_metal_init: found device: Apple M4
0.00.527.597 I ggml_metal_init: picking default device: Apple M4
0.00.529.300 I ggml_metal_init: using embedded metal library
0.00.535.885 I ggml_metal_init: GPU name:   Apple M4
0.00.535.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.535.891 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.535.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.535.893 I ggml_metal_init: simdgroup reduction   = true
0.00.535.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.535.894 I ggml_metal_init: has residency sets    = true
0.00.535.894 I ggml_metal_init: has bfloat            = true
0.00.535.894 I ggml_metal_init: use bfloat            = true
0.00.535.896 I ggml_metal_init: hasUnifiedMemory      = true
0.00.535.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.762 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.557.299 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.557.306 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.557.338 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.560.743 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.560.744 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.560.745 I llama_init_from_model: graph nodes  = 967
0.00.560.745 I llama_init_from_model: graph splits = 2
0.00.560.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.560.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.109 I 
0.00.587.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.211 I perplexity: tokenizing the input ..
0.00.594.572 I perplexity: tokenization took 7.358 ms
0.00.594.598 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.790 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.731.124 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.731.138 I llama_perf_context_print:        load time =     577.17 ms
0.00.731.139 I llama_perf_context_print: prompt eval time =     134.26 ms /   128 tokens (    1.05 ms per token,   953.37 tokens per second)
0.00.731.140 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.140 I llama_perf_context_print:       total time =     144.04 ms /   129 tokens
0.00.731.532 I ggml_metal_free: deallocating

real	0m0.747s
user	0m0.079s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.812 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.813 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.813 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.814 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.815 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.815 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.815 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.816 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.819 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.553 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.266 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.269 I llama_model_loader: - type  f32:  194 tensors
0.00.024.270 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.270 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.270 I print_info: file format = GGUF V3 (latest)
0.00.024.271 I print_info: file type   = Q5_K - Medium
0.00.024.272 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.368 I load: special tokens cache size = 25
0.00.038.273 I load: token to piece cache size = 0.2984 MB
0.00.038.275 I print_info: arch             = gptneox
0.00.038.276 I print_info: vocab_only       = 0
0.00.038.276 I print_info: n_ctx_train      = 2048
0.00.038.276 I print_info: n_embd           = 2048
0.00.038.276 I print_info: n_layer          = 24
0.00.038.279 I print_info: n_head           = 16
0.00.038.280 I print_info: n_head_kv        = 16
0.00.038.280 I print_info: n_rot            = 32
0.00.038.280 I print_info: n_swa            = 0
0.00.038.281 I print_info: n_embd_head_k    = 128
0.00.038.281 I print_info: n_embd_head_v    = 128
0.00.038.282 I print_info: n_gqa            = 1
0.00.038.282 I print_info: n_embd_k_gqa     = 2048
0.00.038.283 I print_info: n_embd_v_gqa     = 2048
0.00.038.284 I print_info: f_norm_eps       = 1.0e-05
0.00.038.284 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.284 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.284 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.285 I print_info: f_logit_scale    = 0.0e+00
0.00.038.285 I print_info: n_ff             = 8192
0.00.038.285 I print_info: n_expert         = 0
0.00.038.286 I print_info: n_expert_used    = 0
0.00.038.286 I print_info: causal attn      = 1
0.00.038.286 I print_info: pooling type     = 0
0.00.038.286 I print_info: rope type        = 2
0.00.038.286 I print_info: rope scaling     = linear
0.00.038.287 I print_info: freq_base_train  = 10000.0
0.00.038.287 I print_info: freq_scale_train = 1
0.00.038.287 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.288 I print_info: rope_finetuned   = unknown
0.00.038.288 I print_info: ssm_d_conv       = 0
0.00.038.288 I print_info: ssm_d_inner      = 0
0.00.038.288 I print_info: ssm_d_state      = 0
0.00.038.288 I print_info: ssm_dt_rank      = 0
0.00.038.288 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.289 I print_info: model type       = 1.4B
0.00.038.291 I print_info: model params     = 1.41 B
0.00.038.291 I print_info: general.name     = 1.4B
0.00.038.292 I print_info: vocab type       = BPE
0.00.038.292 I print_info: n_vocab          = 50304
0.00.038.292 I print_info: n_merges         = 50009
0.00.038.292 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.293 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.293 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.293 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.293 I print_info: LF token         = 128 'Ä'
0.00.038.294 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.294 I print_info: max token length = 1024
0.00.593.619 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.634 I load_tensors: offloading output layer to GPU
0.00.593.635 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.671 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.593.672 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.595.219 I llama_init_from_model: n_seq_max     = 1
0.00.595.222 I llama_init_from_model: n_ctx         = 128
0.00.595.223 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.227 I llama_init_from_model: n_batch       = 128
0.00.595.228 I llama_init_from_model: n_ubatch      = 128
0.00.595.228 I llama_init_from_model: flash_attn    = 0
0.00.595.229 I llama_init_from_model: freq_base     = 10000.0
0.00.595.230 I llama_init_from_model: freq_scale    = 1
0.00.595.231 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.236 I ggml_metal_init: allocating
0.00.595.307 I ggml_metal_init: found device: Apple M4
0.00.595.319 I ggml_metal_init: picking default device: Apple M4
0.00.596.733 I ggml_metal_init: using embedded metal library
0.00.602.868 I ggml_metal_init: GPU name:   Apple M4
0.00.602.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.874 I ggml_metal_init: simdgroup reduction   = true
0.00.602.874 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.874 I ggml_metal_init: has residency sets    = true
0.00.602.874 I ggml_metal_init: has bfloat            = true
0.00.602.875 I ggml_metal_init: use bfloat            = true
0.00.602.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.495 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.622.986 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.622.990 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.016 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.249 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.251 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.251 I llama_init_from_model: graph nodes  = 967
0.00.626.252 I llama_init_from_model: graph splits = 2
0.00.626.254 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.254 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.866 I 
0.00.658.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.977 I perplexity: tokenizing the input ..
0.00.666.138 I perplexity: tokenization took 7.157 ms
0.00.666.161 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.816 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.158 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.174 I llama_perf_context_print:        load time =     649.91 ms
0.00.809.177 I llama_perf_context_print: prompt eval time =     140.77 ms /   128 tokens (    1.10 ms per token,   909.32 tokens per second)
0.00.809.178 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.179 I llama_perf_context_print:       total time =     150.31 ms /   129 tokens
0.00.809.562 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.078s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.276 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.958 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.963 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.965 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.966 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.966 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.967 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.973 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.708 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.720 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.473 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.474 I llama_model_loader: - type  f32:  194 tensors
0.00.024.474 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.475 I print_info: file format = GGUF V3 (latest)
0.00.024.475 I print_info: file type   = Q6_K
0.00.024.476 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.548 I load: special tokens cache size = 25
0.00.038.572 I load: token to piece cache size = 0.2984 MB
0.00.038.574 I print_info: arch             = gptneox
0.00.038.575 I print_info: vocab_only       = 0
0.00.038.575 I print_info: n_ctx_train      = 2048
0.00.038.575 I print_info: n_embd           = 2048
0.00.038.575 I print_info: n_layer          = 24
0.00.038.578 I print_info: n_head           = 16
0.00.038.579 I print_info: n_head_kv        = 16
0.00.038.579 I print_info: n_rot            = 32
0.00.038.579 I print_info: n_swa            = 0
0.00.038.581 I print_info: n_embd_head_k    = 128
0.00.038.581 I print_info: n_embd_head_v    = 128
0.00.038.582 I print_info: n_gqa            = 1
0.00.038.583 I print_info: n_embd_k_gqa     = 2048
0.00.038.584 I print_info: n_embd_v_gqa     = 2048
0.00.038.584 I print_info: f_norm_eps       = 1.0e-05
0.00.038.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.585 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.585 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.585 I print_info: f_logit_scale    = 0.0e+00
0.00.038.586 I print_info: n_ff             = 8192
0.00.038.586 I print_info: n_expert         = 0
0.00.038.587 I print_info: n_expert_used    = 0
0.00.038.587 I print_info: causal attn      = 1
0.00.038.587 I print_info: pooling type     = 0
0.00.038.588 I print_info: rope type        = 2
0.00.038.589 I print_info: rope scaling     = linear
0.00.038.589 I print_info: freq_base_train  = 10000.0
0.00.038.589 I print_info: freq_scale_train = 1
0.00.038.590 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.590 I print_info: rope_finetuned   = unknown
0.00.038.590 I print_info: ssm_d_conv       = 0
0.00.038.591 I print_info: ssm_d_inner      = 0
0.00.038.592 I print_info: ssm_d_state      = 0
0.00.038.592 I print_info: ssm_dt_rank      = 0
0.00.038.592 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.592 I print_info: model type       = 1.4B
0.00.038.592 I print_info: model params     = 1.41 B
0.00.038.593 I print_info: general.name     = 1.4B
0.00.038.593 I print_info: vocab type       = BPE
0.00.038.593 I print_info: n_vocab          = 50304
0.00.038.594 I print_info: n_merges         = 50009
0.00.038.594 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.594 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.594 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.594 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.595 I print_info: LF token         = 128 'Ä'
0.00.038.596 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.596 I print_info: max token length = 1024
0.00.526.381 I load_tensors: offloading 24 repeating layers to GPU
0.00.526.391 I load_tensors: offloading output layer to GPU
0.00.526.392 I load_tensors: offloaded 25/25 layers to GPU
0.00.526.427 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.526.428 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.528.017 I llama_init_from_model: n_seq_max     = 1
0.00.528.020 I llama_init_from_model: n_ctx         = 128
0.00.528.021 I llama_init_from_model: n_ctx_per_seq = 128
0.00.528.021 I llama_init_from_model: n_batch       = 128
0.00.528.025 I llama_init_from_model: n_ubatch      = 128
0.00.528.026 I llama_init_from_model: flash_attn    = 0
0.00.528.027 I llama_init_from_model: freq_base     = 10000.0
0.00.528.028 I llama_init_from_model: freq_scale    = 1
0.00.528.035 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.528.040 I ggml_metal_init: allocating
0.00.528.097 I ggml_metal_init: found device: Apple M4
0.00.528.111 I ggml_metal_init: picking default device: Apple M4
0.00.529.488 I ggml_metal_init: using embedded metal library
0.00.535.845 I ggml_metal_init: GPU name:   Apple M4
0.00.535.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.535.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.535.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.535.852 I ggml_metal_init: simdgroup reduction   = true
0.00.535.852 I ggml_metal_init: simdgroup matrix mul. = true
0.00.535.852 I ggml_metal_init: has residency sets    = true
0.00.535.852 I ggml_metal_init: has bfloat            = true
0.00.535.853 I ggml_metal_init: use bfloat            = true
0.00.535.853 I ggml_metal_init: hasUnifiedMemory      = true
0.00.535.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.051 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.556.609 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.556.618 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.556.650 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.560.074 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.560.075 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.560.076 I llama_init_from_model: graph nodes  = 967
0.00.560.076 I llama_init_from_model: graph splits = 2
0.00.560.079 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.560.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.930 I 
0.00.599.013 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.034 I perplexity: tokenizing the input ..
0.00.605.756 I perplexity: tokenization took 6.72 ms
0.00.605.773 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.735 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.748.156 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.748.170 I llama_perf_context_print:        load time =     589.65 ms
0.00.748.171 I llama_perf_context_print: prompt eval time =     140.02 ms /   128 tokens (    1.09 ms per token,   914.17 tokens per second)
0.00.748.172 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.172 I llama_perf_context_print:       total time =     149.25 ms /   129 tokens
0.00.748.584 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.078s
sys	0m0.126s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.184 I build: 4583 (1a0e87d2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.267 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.787 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.029.799 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.803 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.813 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.815 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.817 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.818 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.818 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.819 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.729 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.920 I llama_model_loader: - type  f32:  194 tensors
0.00.045.920 I llama_model_loader: - type  f16:   98 tensors
0.00.045.921 I print_info: file format = GGUF V3 (latest)
0.00.045.921 I print_info: file type   = all F32 (guessed)
0.00.045.923 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.057.039 I load: special tokens cache size = 25
0.00.065.139 I load: token to piece cache size = 0.2984 MB
0.00.065.142 I print_info: arch             = gptneox
0.00.065.143 I print_info: vocab_only       = 0
0.00.065.143 I print_info: n_ctx_train      = 2048
0.00.065.143 I print_info: n_embd           = 2048
0.00.065.143 I print_info: n_layer          = 24
0.00.065.146 I print_info: n_head           = 16
0.00.065.147 I print_info: n_head_kv        = 16
0.00.065.148 I print_info: n_rot            = 32
0.00.065.151 I print_info: n_swa            = 0
0.00.065.151 I print_info: n_embd_head_k    = 128
0.00.065.151 I print_info: n_embd_head_v    = 128
0.00.065.152 I print_info: n_gqa            = 1
0.00.065.153 I print_info: n_embd_k_gqa     = 2048
0.00.065.153 I print_info: n_embd_v_gqa     = 2048
0.00.065.159 I print_info: f_norm_eps       = 1.0e-05
0.00.065.161 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.161 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.161 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.162 I print_info: f_logit_scale    = 0.0e+00
0.00.065.163 I print_info: n_ff             = 8192
0.00.065.163 I print_info: n_expert         = 0
0.00.065.163 I print_info: n_expert_used    = 0
0.00.065.163 I print_info: causal attn      = 1
0.00.065.163 I print_info: pooling type     = 0
0.00.065.163 I print_info: rope type        = 2
0.00.065.164 I print_info: rope scaling     = linear
0.00.065.164 I print_info: freq_base_train  = 10000.0
0.00.065.164 I print_info: freq_scale_train = 1
0.00.065.168 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.168 I print_info: rope_finetuned   = unknown
0.00.065.168 I print_info: ssm_d_conv       = 0
0.00.065.169 I print_info: ssm_d_inner      = 0
0.00.065.169 I print_info: ssm_d_state      = 0
0.00.065.169 I print_info: ssm_dt_rank      = 0
0.00.065.169 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.169 I print_info: model type       = 1.4B
0.00.065.170 I print_info: model params     = 1.41 B
0.00.065.170 I print_info: general.name     = 1.4B
0.00.065.171 I print_info: vocab type       = BPE
0.00.065.172 I print_info: n_vocab          = 50304
0.00.065.172 I print_info: n_merges         = 50009
0.00.065.172 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.172 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.172 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.173 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.173 I print_info: LF token         = 128 'Ä'
0.00.065.173 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.174 I print_info: max token length = 1024
0.01.234.737 I load_tensors: offloading 24 repeating layers to GPU
0.01.234.741 I load_tensors: offloading output layer to GPU
0.01.234.741 I load_tensors: offloaded 25/25 layers to GPU
0.01.234.768 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.234.770 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.235.603 I llama_init_from_model: n_seq_max     = 1
0.01.235.604 I llama_init_from_model: n_ctx         = 128
0.01.235.604 I llama_init_from_model: n_ctx_per_seq = 128
0.01.235.605 I llama_init_from_model: n_batch       = 128
0.01.235.605 I llama_init_from_model: n_ubatch      = 128
0.01.235.609 I llama_init_from_model: flash_attn    = 0
0.01.235.611 I llama_init_from_model: freq_base     = 10000.0
0.01.235.612 I llama_init_from_model: freq_scale    = 1
0.01.235.613 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.235.617 I ggml_metal_init: allocating
0.01.235.707 I ggml_metal_init: found device: Apple M4
0.01.235.713 I ggml_metal_init: picking default device: Apple M4
0.01.236.869 I ggml_metal_init: using embedded metal library
0.01.240.777 I ggml_metal_init: GPU name:   Apple M4
0.01.240.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.240.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.240.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.240.781 I ggml_metal_init: simdgroup reduction   = true
0.01.240.781 I ggml_metal_init: simdgroup matrix mul. = true
0.01.240.781 I ggml_metal_init: has residency sets    = true
0.01.240.781 I ggml_metal_init: has bfloat            = true
0.01.240.782 I ggml_metal_init: use bfloat            = true
0.01.240.782 I ggml_metal_init: hasUnifiedMemory      = true
0.01.240.783 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.251.444 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.253.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.253.326 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.253.340 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.255.012 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.255.014 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.255.014 I llama_init_from_model: graph nodes  = 967
0.01.255.014 I llama_init_from_model: graph splits = 2
0.01.255.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.255.016 I 
0.01.255.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.255.056 I compute_imatrix: tokenizing the input ..
0.01.259.056 I compute_imatrix: tokenization took 3.999 ms
0.01.259.059 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.528.321 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.531.214 I llama_perf_context_print:        load time =    1511.05 ms
0.01.531.215 I llama_perf_context_print: prompt eval time =     267.49 ms /   128 tokens (    2.09 ms per token,   478.52 tokens per second)
0.01.531.215 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.531.216 I llama_perf_context_print:       total time =    1513.94 ms /   129 tokens
0.01.531.812 I ggml_metal_free: deallocating

real	0m1.740s
user	0m0.118s
sys	0m0.253s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4583 (1a0e87d2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13be04a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13be050c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13be05680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13be05c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13be061e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13be06790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13be06d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13be072f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13be078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13be07da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13be082a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13be087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13be092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13be09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13be0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13be0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13be0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13be0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13be0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13be0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13be0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13be0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13be0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13be0e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13be0ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13be0eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13be0f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13be10130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13be10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13be10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13be10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13be11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13be11920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13be11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13be12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13be125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13be12a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13be12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13be133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13be13840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13be13ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13be14180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13be14620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13be14ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13be14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13be15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13be159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13be162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13be168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13be16ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13be174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13be17b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13be18110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13be18720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13be18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13be193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13be19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13be19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13be1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13be1a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13be1abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13be1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13be1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13be1b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13be1be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13be1c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13be1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13be1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13be1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13be1d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13be1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13be1deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13be1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13be1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13be1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13be1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13be1f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13be1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13be20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13be20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13be20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13be21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13be21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13be21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13be22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13be22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13be22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13be23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13be23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13be23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13be242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13be24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13be24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13be252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13be25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13be25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13be262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13be15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13be26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13be26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13be27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13be27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13be27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13be28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13be28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13be28ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13be29420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13be29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13be29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13be2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13be2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13be2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13be2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13be2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13be2bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13be2c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13be2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13be2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13be2cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13be2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13be2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13be2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13be2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13be2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13be2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13be2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13be2f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13be2f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13be2fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13be302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13be30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13be30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13be31080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13be31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13be319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13be31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13be32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13be327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13be32c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13be330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13be33580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13be33a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13be33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13be34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13be34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13be34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13be35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13be355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13be35a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13be35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13be363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13be36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13be36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13be371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13be37640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13be37ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13be37f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13be38420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13be388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13be38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13be39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13be396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13be39b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13be39fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13be3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13be3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13be3adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13be3b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13be3b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13be3bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13be3c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13be3c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13be3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13be3ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13be3d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13be3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13be3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13be3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13be3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13be3e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13be3ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13be3f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13be3f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13be3fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13be40100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13be405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13be40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13be40ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13be41380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13be41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13be41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13be42160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13be42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13be42b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13be430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13be435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13be43b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13be43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13be44410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13be44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13be45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13be45820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13be45cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13be45f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13be46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13be46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13be47390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13be47830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13be47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13be48170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13be48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13be48e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13be493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13be49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13be49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13be4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13be4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13be4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13be4b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13be4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13be4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13be4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13be4c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13be4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13be4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13be4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13be4de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13be4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13be4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13be4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13be4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13be4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13be4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13be50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13be508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13be50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13be51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13be51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13be51de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13be52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13be52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13be52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13be53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13be53870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13be53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13be54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13be54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13be54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13be55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13be55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13be55da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13be562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13be56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13be56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13be572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13be57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13be57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13be582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13be58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13be58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13be592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13be59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13be59d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13be5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13be5a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13be5ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13be5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13be5b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13be5bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13be5c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13be5c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13be5c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13be5ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13be5d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13be5d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13be5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13be5e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13be5e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13be5ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13be5eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13be5f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13be5f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13be5fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13be60470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13be60b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13be612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13be619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13be61c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13be62480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13be62740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13be62d50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.700.507 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.700.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11bc04bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11bc05030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11bc054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11bc05910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11bc05d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11bc061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11bc06660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11bc06ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11bc06f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11bc073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11bc07820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11bc07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11bc08a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11bc091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11bc099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11bc0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11bc0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11bc0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11bc0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11bc0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11bc0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11bc0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11bc0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11bc0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11bc0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11bc0e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11bc0e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11bc0eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11bc0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11bc0f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11bc0f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11bc0fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11bc10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11bc10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11bc109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11bc10e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11bc112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11bc11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11bc11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11bc11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11bc12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11bc128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11bc12d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11bc131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11bc13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11bc13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11bc13f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11bc14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11bc147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11bc14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11bc150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11bc15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11bc159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11bc15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11bc16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11bc166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11bc16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11bc17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11bc175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11bc17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11bc17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11bc18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11bc18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11bc18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11bc19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11bc194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11bc19950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11bc19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11bc1a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11bc1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11bc1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11bc1af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11bc1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11bc1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11bc1bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11bc1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11bc1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11bc1ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11bc1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11bc1d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11bc1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11bc1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11bc1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11bc1e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11bc1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11bc1eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11bc1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11bc1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11bc1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11bc1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11bc203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11bc20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11bc20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11bc21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11bc21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11bc21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11bc21e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11bc222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11bc22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11bc22bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11bc23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11bc234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11bc23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11bc23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11bc241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11bc24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11bc24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11bc24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11bc253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11bc25820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11bc25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11bc26100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11bc26570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11bc269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11bc26e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11bc272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11bc27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11bc27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11bc28010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11bc28480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11bc288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11bc28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11bc291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11bc29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11bc29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11bc29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11bc2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11bc2a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11bc2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11bc2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11bc2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11bc2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11bc2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11bc2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11bc2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11bc2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11bc2cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11bc2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11bc2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11bc2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11bc2e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11bc2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11bc2ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11bc2ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11bc2f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11bc2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11bc2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11bc300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11bc30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11bc309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11bc30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11bc31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11bc316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11bc31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11bc31fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11bc32440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11bc328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11bc32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11bc33190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11bc33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11bc33a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11bc33ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11bc34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11bc347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11bc34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11bc350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11bc35cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11bc35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11bc36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11bc366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11bc36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11bc36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11bc37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11bc37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11bc37cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11bc38160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11bc385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11bc38a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11bc38eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11bc39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11bc39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11bc39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11bc3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11bc3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11bc3a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11bc3adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11bc3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11bc3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11bc3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11bc3bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11bc3c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11bc3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11bc3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11bc3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11bc3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11bc3da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11bc3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11bc3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11bc3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11bc3ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11bc3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11bc3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11bc3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11bc3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11bc403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11bc40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11bc40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11bc410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11bc41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11bc41b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11bc42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11bc42950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11bc42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11bc434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11bc43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11bc44050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11bc44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11bc44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11bc45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11bc45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11bc45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11bc462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11bc46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11bc46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11bc47410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11bc479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11bc47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11bc48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11bc48b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11bc490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11bc49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11bc49c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11bc4a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11bc4a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11bc4ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11bc4b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11bc4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11bc4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11bc4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11bc4ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11bc4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11bc4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11bc4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11bc4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11bc4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11bc4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11bc4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11bc4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11bc4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11bc503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11bc50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11bc50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11bc51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11bc51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11bc52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11bc52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11bc52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11bc531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11bc53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11bc53d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11bc54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11bc548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11bc54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11bc55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11bc55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11bc55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11bc56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11bc56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11bc57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11bc57550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11bc57a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11bc57f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11bc58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11bc58950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11bc58e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11bc59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11bc59850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11bc59d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11bc5a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11bc5a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11bc5ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11bc5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11bc5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11bc5c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11bc5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11bc5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11bc5d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11bc5d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11bc5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11bc5e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11bc5e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13be62a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13be446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13be440c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13be44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13be17dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13be177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13be19dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13be46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13be0f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13be15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13be16580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13be16b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13be15040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13be171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13be0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13be04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13be189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13be1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13be26a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13be61f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13be11350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13be11610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13be46e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13be452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13be0f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13be0fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13be0fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13be631b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13be63470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13be63730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13be639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13be63cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13be63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13be64230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13be644f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13be647b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13be64a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13be64d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13be64ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13be652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13be65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13be65830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13be65af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13be65db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13be66070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13be66330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13be665f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13be668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13be66b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13be66e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13be670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13be673b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13be67670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13be67930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13be67bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13be67eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13be68170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13be68430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13be686f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13be689b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13be68c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13be68f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13be691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13be694b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13be69770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13be69a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13be69cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13be69fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13be6a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13be6a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13be6a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13be6aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13be6ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13be6b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13be6b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13be6b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13be6b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13be6bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13be6bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13be6c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13be6c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13be6c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13be6c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13be6cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13be6ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13be6d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13be6d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13be6d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13be6d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13be6dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13be6def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13be6e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13be6e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13be6e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13be6e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13be6ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13be6ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13be6f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13be6f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13be6f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13be6fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13be6fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13be6fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13be702b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13be70570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13be70830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13be70af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13be70db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13be71070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13be71330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13be715f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13be718b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13be71b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13be71e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13be720f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13be723b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13be72670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13be72930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13be72bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13be72eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13be73170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13be73430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13be736f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13be739b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13be73c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13be73f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13be741f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13be744b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13be74770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13be74a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13be74cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13be74fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13be75270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13be75530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13be757f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13be75ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13be75d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13be76030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13be762f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13be765b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13be76870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13be76b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13be76df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13be770b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13be77370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13be77630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13be778f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13be77bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13be77e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13be78130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13be783f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13be786b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13be78970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13be78c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13be78ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13be791b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13be79470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13be79730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13be799f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13be79cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13be79f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13be7a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13be7a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13be7a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13be7aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13be7ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13be7aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13be7b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13be7b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13be7b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13be7baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13be7bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13be7c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13be7c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13be7c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13be7c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13be7cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13be7ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13be7d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13be7d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13be7d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13be7d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13be7dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13be7deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13be7e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13be7e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13be7e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13be7e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13be7ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13be7ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13be7f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13be7f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13be7f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13be7fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13be7fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13be7ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13be80270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13be80530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13be807f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13be80ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13be80d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13be81030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13be812f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13be815b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13be81870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13be81b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13be81df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13be820b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13be82370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13be82630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13be82c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13be82ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13be83180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13be83440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13be83700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13be839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13be83c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13be83f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13be84200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13be844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13be84780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13be84a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13be84d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13be84fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13be85280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13be85540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13be85800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13be85ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13be85d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13be86040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13be86590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13be86ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13be87030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13be87580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13be87ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13be88020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13be88570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13be88ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13be89010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13be89560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13be89ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13be8a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13be8a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13be8aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13be8aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13be8b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13be8ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13be8bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13be8c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13be8ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13be8cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13be8d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13be8da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13be8dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13be8e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13be8ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13be8efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13be8f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13be8fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13be8ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13be904f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13be90a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13be90f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13be914e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13be91a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13be91f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13be924d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13be92790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13be92a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13be92f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13be93450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13be93950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13be93e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13be94350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13be94850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13be94d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13be95250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13be95750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13be95c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13be96150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13be96650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13be96b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13be97050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13be97a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13be98180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13be988a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13be98fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13be99280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13be99a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13be99d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13be9a340 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.782s
user	0m0.281s
sys	0m0.306s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4583 (1a0e87d2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159707b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159708280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159708830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159708de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159709390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159709940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159709ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15970a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15970aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15970af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15970b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15970b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15970c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15970cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15970d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15970db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15970e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15970e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15970f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15970f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15970ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1597106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159710de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159711680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159712060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159712670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1597132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159713820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159713ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159713f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159714240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159714ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159715010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1597152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159715770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159715c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1597160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159716550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1597169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159716e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159717330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1597177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159717c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159717f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159718540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159718b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159719470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159719a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15971a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15971a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15971acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15971b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15971b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15971c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15971c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15971ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15971ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15971d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15971dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15971dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15971e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15971e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15971eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15971f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15971f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15971f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15971fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159720280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159720720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159720bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159721060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159721500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159721a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159721fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1597224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159722a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159722f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1597234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159723a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159723f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1597244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159724a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159724f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1597254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159725a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159725f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1597264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159726a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159726f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1597274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1597279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159727f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159728490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1597289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159728f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159729480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159719160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1597298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15972a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15972a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15972ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15972b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15972b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15972bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15972c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15972c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15972cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15972d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15972d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15972db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15972e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15972e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15972ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15972eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15972f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15972f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15972fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159730170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159730610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159730ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159730f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1597313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159731890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159731d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1597321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159732670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159732b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159732fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159733450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1597338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159733d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159734230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1597346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159734b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159735010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1597354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159735950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159735df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159736290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159736730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159736bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159737070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159737510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1597379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159737e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1597382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159738790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159738c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1597390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159739570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159739a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159739eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15973a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15973a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15973ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15973b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15973b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15973ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15973bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15973c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15973c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15973ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15973d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15973d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15973dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15973df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15973e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15973e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15973ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15973f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15973f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15973fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15973ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159740470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159740910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159740db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159741250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1597416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159741b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159742030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1597424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159742970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159742e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1597432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159743750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159743bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159744090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159744530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1597449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159744e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159745310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1597457b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159745d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159746250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1597467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159746cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159746fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1597475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159747bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1597481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1597489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159748e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159749130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159749740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159749d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15974a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15974a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15974ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15974b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15974bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15974c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15974c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15974cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15974d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15974d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15974dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15974e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15974e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15974eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15974eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15974f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15974fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15974ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159750a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159750fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159751a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159751fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159752510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159752a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159752fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159753500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159753a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159753fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1597544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159754a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159754f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1597554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159755a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159755f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1597564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159756a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159756f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1597574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159757a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159757f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1597584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159758a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159758f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1597594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1597599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159759f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15975a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15975a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15975af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15975b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15975b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15975bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15975c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15975c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15975cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15975d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15975d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15975df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15975e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15975e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15975ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15975f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15975f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15975fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159760010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1597604b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159760950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159760df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159761290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159761730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159761bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159762070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159762510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1597629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159762f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159763620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159763d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159764460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159764b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159764e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159765630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1597658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159765f00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.093.091 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159606930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159606da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159607210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1596097a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159609c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15960a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15960a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15960a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15960add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15960b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15960b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15960bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15960c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15960d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15960d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15960dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15960e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15960ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15960f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15960fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159610390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159610ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1596111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1596118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159612010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1596122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159612590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159612a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159612e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1596132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159613750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159613c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1596140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1596143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159614c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159615100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159615570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1596159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159615e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1596162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159616730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159616ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159617010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159617480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1596178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159617d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1596181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159618640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159618ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159618f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159619390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159619800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159619c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15961a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15961a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15961aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15961afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15961b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15961b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15961bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15961c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15961c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15961ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15961ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15961d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15961d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15961dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15961e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15961e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15961e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15961ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15961f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15961f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15961fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15961ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159620410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159620880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159620cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159621160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1596215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159621a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159621eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159622320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x159622790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159622c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159623070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1596234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159623950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159623dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159624230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1596246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159624b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159624f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1596253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159625860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159625cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159626140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1596265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159626a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159627300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159627770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159627be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159628050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1596284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159628930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159628da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159629210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159629680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x159629af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159629f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15962a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15962a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15962acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15962b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15962b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15962ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15962be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15962c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15962c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15962cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15962d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15962d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15962d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15962dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15962e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15962e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15962ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15962ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15962f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15962f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15962fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x159630100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159630570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1596309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159630e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1596312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159631730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159631ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159632010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159632480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1596328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159632d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1596331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159633640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159633f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159634390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159634800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159634c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1596350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159635550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1596359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159635e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1596362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159636b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159636ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159637460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1596378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159637d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1596381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159638620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159638a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159638f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159639b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159639df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15963a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15963a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15963a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15963ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15963b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15963b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15963bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15963bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15963c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15963c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15963cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15963d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15963d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15963da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15963ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15963e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15963e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15963ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15963f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15963f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15963f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15963fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159640250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1596406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159640b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159640fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159641880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159641cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159642160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1596425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159642a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159642eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159643320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159643880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159644200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159644670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159644ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159644f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159645470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159645980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1596464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1596467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159646d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159647330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1596478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159647eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159648470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159648a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159648ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1596495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159649b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15964a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15964a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15964acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15964b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15964b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15964bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15964c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15964c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15964cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15964d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15964dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15964e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15964e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15964ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15964f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15964f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15964fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1596502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1596508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159650e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159651430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1596519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159651fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159652570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159652b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1596530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1596536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159653c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159654230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1596547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159655370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159655930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159655ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1596564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159656a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159657030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1596575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159657bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159658170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159658730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159658cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1596592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159659870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159659e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15965a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15965a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15965aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15965b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15965b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15965bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15965c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15965c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15965ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15965d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15965d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15965dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15965e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15965e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15965eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15965efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15965f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15965fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1596605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159660d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159661420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1596616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159661ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159662190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1596627a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159765bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159747880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159747270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159747e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15971af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15971a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15971cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159749a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159712320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159718e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159719730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159719d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1597181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15971a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159711320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1597071a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15971bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15971d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x159729bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159765100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159714500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1597147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15974a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1597484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159712930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159712bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159712eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159766360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159766620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1597668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159766ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159766e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159767120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1597673e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1597676a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159767960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159767c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159767ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1597681a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159768460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159768720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1597689e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159768ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159768f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159769220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1597694e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1597697a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159769a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159769d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159769fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15976a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15976a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15976a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15976aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15976ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15976b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15976b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15976b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15976b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15976bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15976be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15976c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15976c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15976c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15976c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15976cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15976cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15976d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15976d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15976d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15976d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15976dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15976df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15976e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15976e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15976e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15976ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15976ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15976efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15976f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15976f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15976f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15976faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15976fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x159770020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1597702e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1597705a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159770860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159770b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159770de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1597710a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159771360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159771620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1597718e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159771ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159771e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159772120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1597723e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1597726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159772960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159772c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159772ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1597731a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159773460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159773720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1597739e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159773ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159773f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159774220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1597744e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1597747a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159774a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159774d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159774fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1597752a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159775560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159775820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159775ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159775da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159776060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159776320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1597765e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1597768a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159776b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159776e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1597770e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1597773a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159777660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159777920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159777be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159777ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159778160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159778420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1597786e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1597789a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159778c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159778f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1597791e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1597794a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159779760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159779a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159779ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159779fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15977a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15977a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15977a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15977aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15977ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15977b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15977b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15977b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15977b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15977bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15977bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15977c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15977c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15977c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15977c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15977cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15977ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15977d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15977d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15977d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15977d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15977dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15977dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15977e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15977e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15977e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15977e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15977eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15977ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15977f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15977f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15977f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15977fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15977fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15977ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1597802a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159780560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159780820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x159780ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159780da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159781060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159781320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1597815e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1597818a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159781b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159781e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1597820e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1597823a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159782660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159782920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159782be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159782ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159783160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159783420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1597836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1597839a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159783c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159783f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1597841e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1597844a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159784760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159784a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159784ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159784fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159785260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159785520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1597857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159785db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159786070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159786330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1597865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1597868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159786b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159786e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1597870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1597873b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159787670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159787930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159787bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159787eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159788170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159788430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1597886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1597889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159788c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159788f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1597891f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1597894b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159789770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159789a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159789cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159789fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15978a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15978a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15978a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15978aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15978ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15978b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15978b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15978b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15978b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15978bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15978bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15978c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15978c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15978cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15978d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15978d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15978ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15978e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15978e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15978edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15978f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15978f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15978fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159790300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159790850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159790da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1597912f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159791840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159791d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1597922e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159792830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159792d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159793040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159793300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159793800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159793d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159794200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159794700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159794c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159795100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159795600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159795b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159796000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159796500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159796a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159796f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159797400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159797900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159798310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159798a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159799150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159799870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159799b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15979a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15979a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15979abf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.912s
user	0m0.235s
sys	0m0.135s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
