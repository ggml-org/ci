+ cmake -DCMAKE_BUILD_TYPE=Release ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.785s
user	0m0.407s
sys	0m0.383s
+ make -j
[  5%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  5%] Built target BUILD_INFO
[  5%] Building C object CMakeFiles/ggml.dir/k_quants.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml.c.o
/home/ggml/work/llama.cpp/k_quants.c: In function ‘ggml_vec_dot_q2_K_q8_K’:
/home/ggml/work/llama.cpp/k_quants.c:1356:36: warning: missing braces around initializer [-Wmissing-braces]
 1356 |         const int16x8x2_t mins16 = {vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(mins))), vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(mins)))};
      |                                    ^
      |                                     {                                                                                                      }
/home/ggml/work/llama.cpp/k_quants.c: In function ‘ggml_vec_dot_q6_K_q8_K’:
/home/ggml/work/llama.cpp/k_quants.c:3717:38: warning: missing braces around initializer [-Wmissing-braces]
 3717 |         const int16x8x2_t q6scales = {vmovl_s8(vget_low_s8(scales)), vmovl_s8(vget_high_s8(scales))};
      |                                      ^
      |                                       {                                                            }
[  5%] Built target ggml
[  8%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  8%] Linking C static library libggml_static.a
[  8%] Built target ggml_static
[ 10%] Linking CXX static library libllama.a
[ 10%] Built target llama
[ 11%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 14%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 19%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 20%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 22%] Linking CXX executable ../bin/test-c
[ 22%] Built target test-c
/home/ggml/work/llama.cpp/examples/benchmark/benchmark-matmult.cpp:24:6: warning: no previous declaration for ‘void ggml_graph_compute_helper(std::vector<unsigned char>&, ggml_cgraph*, int)’ [-Wmissing-declarations]
   24 | void ggml_graph_compute_helper(std::vector<uint8_t> & buf, ggml_cgraph * graph, int n_threads) {
      |      ^~~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/benchmark/benchmark-matmult.cpp:35:7: warning: no previous declaration for ‘float tensor_sum_elements(const ggml_tensor*)’ [-Wmissing-declarations]
   35 | float tensor_sum_elements(const ggml_tensor * tensor) {
      |       ^~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/benchmark/benchmark-matmult.cpp:47:6: warning: no previous declaration for ‘void tensor_dump(const ggml_tensor*, const char*)’ [-Wmissing-declarations]
   47 | void tensor_dump(const ggml_tensor * tensor, const char * name) {
      |      ^~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/benchmark/benchmark-matmult.cpp:62:6: warning: no previous declaration for ‘void print_usage(int, char**, benchmark_params_struct)’ [-Wmissing-declarations]
   62 | void print_usage(int /*argc*/, char ** argv, struct benchmark_params_struct params) {
      |      ^~~~~~~~~~~
[ 23%] Linking CXX executable ../../bin/benchmark
[ 25%] Linking CXX executable ../../bin/quantize
[ 25%] Built target benchmark
[ 25%] Built target quantize
[ 26%] Linking CXX executable ../../bin/quantize-stats
[ 26%] Built target quantize-stats
[ 26%] Built target common
[ 27%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 42%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 44%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 45%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 47%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 48%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 50%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 51%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 52%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 54%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 55%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 57%] Building CXX object examples/embd-input/CMakeFiles/embdinput.dir/embd-input-lib.cpp.o
[ 58%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 60%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 61%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 63%] Linking CXX executable ../bin/test-quantize-fns
/home/ggml/work/llama.cpp/pocs/vdot/q8dot.cpp:57:6: warning: no previous declaration for ‘void fillQ80blocks(std::vector<block_q8_0>&, std::mt19937&)’ [-Wmissing-declarations]
   57 | void fillQ80blocks(std::vector<block_q8_0>& blocks, std::mt19937& rndm) {
      |      ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/pocs/vdot/q8dot.cpp:69:7: warning: no previous declaration for ‘float simpleDot(const block_q4_0&, const block_q8_0&)’ [-Wmissing-declarations]
   69 | float simpleDot(const block_q4_0& x, const block_q8_0& y) {
      |       ^~~~~~~~~
/home/ggml/work/llama.cpp/pocs/vdot/q8dot.cpp:84:7: warning: no previous declaration for ‘float simpleDot(const block_q4_1&, const block_q8_0&)’ [-Wmissing-declarations]
   84 | float simpleDot(const block_q4_1& x, const block_q8_0& y) {
      |       ^~~~~~~~~
[ 63%] Built target test-quantize-fns
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:33:6: warning: no previous declaration for ‘void init_random_normal_distribution(random_normal_distribution*, int, float, float, float, float)’ [-Wmissing-declarations]
   33 | void init_random_normal_distribution(struct random_normal_distribution * rnd, int seed, float mean, float std, float min, float max) {
      |      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:40:6: warning: no previous declaration for ‘void init_random_uniform_distribution(random_uniform_distribution*, int, float, float)’ [-Wmissing-declarations]
   40 | void init_random_uniform_distribution(struct random_uniform_distribution * rnd, int seed, float min, float max) {
      |      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:45:5: warning: no previous declaration for ‘int clamp(int, int, int)’ [-Wmissing-declarations]
   45 | int clamp(const int v, const int min, const int max) {
      |     ^~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:49:7: warning: no previous declaration for ‘float fclamp(float, float, float)’ [-Wmissing-declarations]
   49 | float fclamp(const float v, const float min, const float max) {
      |       ^~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:53:7: warning: no previous declaration for ‘float frand()’ [-Wmissing-declarations]
   53 | float frand() {
      |       ^~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:57:7: warning: no previous declaration for ‘float frand_normal(random_normal_distribution*)’ [-Wmissing-declarations]
   57 | float frand_normal(struct random_normal_distribution * rnd) {
      |       ^~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:61:7: warning: no previous declaration for ‘float frand_uniform(random_uniform_distribution*)’ [-Wmissing-declarations]
   61 | float frand_uniform(struct random_uniform_distribution * rnd) {
      |       ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:65:22: warning: no previous declaration for ‘ggml_tensor* randomize_tensor_normal(ggml_tensor*, random_normal_distribution*)’ [-Wmissing-declarations]
   65 | struct ggml_tensor * randomize_tensor_normal(struct ggml_tensor * tensor, struct random_normal_distribution * rnd) {
      |                      ^~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:114:22: warning: no previous declaration for ‘ggml_tensor* randomize_tensor_uniform(ggml_tensor*, random_uniform_distribution*)’ [-Wmissing-declarations]
  114 | struct ggml_tensor * randomize_tensor_uniform(struct ggml_tensor * tensor, struct random_uniform_distribution * rnd) {
      |                      ^~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:289:6: warning: no previous declaration for ‘void print_params(my_llama_hparams*)’ [-Wmissing-declarations]
  289 | void print_params(struct my_llama_hparams * params) {
      |      ^~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:299:6: warning: no previous declaration for ‘void init_model(my_llama_model*)’ [-Wmissing-declarations]
  299 | void init_model(struct my_llama_model * model) {
      |      ^~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:366:6: warning: no previous declaration for ‘void set_param_model(my_llama_model*)’ [-Wmissing-declarations]
  366 | void set_param_model(struct my_llama_model * model) {
      |      ^~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:392:6: warning: no previous declaration for ‘void randomize_model(my_llama_model*, int, float, float, float, float)’ [-Wmissing-declarations]
  392 | void randomize_model(struct my_llama_model * model, int seed, float mean, float std, float min, float max) {
      |      ^~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:421:6: warning: no previous declaration for ‘void assert_shape_1d(ggml_tensor*, int64_t)’ [-Wmissing-declarations]
  421 | void assert_shape_1d(struct ggml_tensor * tensor, int64_t ne0) {
      |      ^~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:426:6: warning: no previous declaration for ‘void assert_shape_2d(ggml_tensor*, int64_t, int64_t)’ [-Wmissing-declarations]
  426 | void assert_shape_2d(struct ggml_tensor * tensor, int64_t ne0, int64_t ne1) {
      |      ^~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:432:6: warning: no previous declaration for ‘void assert_shape_3d(ggml_tensor*, int64_t, int64_t, int64_t)’ [-Wmissing-declarations]
  432 | void assert_shape_3d(struct ggml_tensor * tensor, int64_t ne0, int64_t ne1, int64_t ne2) {
      |      ^~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:439:6: warning: no previous declaration for ‘void assert_shape_4d(ggml_tensor*, int64_t, int64_t, int64_t, int64_t)’ [-Wmissing-declarations]
  439 | void assert_shape_4d(struct ggml_tensor * tensor, int64_t ne0, int64_t ne1, int64_t ne2, int64_t ne3) {
      |      ^~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:493:19: warning: no previous declaration for ‘hash_map* new_hash_map()’ [-Wmissing-declarations]
  493 | struct hash_map * new_hash_map() {
      |                   ^~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:502:6: warning: no previous declaration for ‘void free_hash_map(hash_map*)’ [-Wmissing-declarations]
  502 | void free_hash_map(struct hash_map * map) {
      |      ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:533:22: warning: no previous declaration for ‘ggml_tensor* ggml_recompute_graph_node(ggml_context*, ggml_cgraph*, hash_map*, ggml_tensor*)’ [-Wmissing-declarations]
  533 | struct ggml_tensor * ggml_recompute_graph_node(
      |                      ^~~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:599:6: warning: no previous declaration for ‘void ggml_build_backward_gradient_checkpointing(ggml_context*, ggml_cgraph*, ggml_cgraph*, ggml_cgraph*, ggml_tensor**, int)’ [-Wmissing-declarations]
  599 | void ggml_build_backward_gradient_checkpointing(
      |      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:645:22: warning: no previous declaration for ‘ggml_tensor* llama_build_train_graphs(my_llama_model*, ggml_allocr*, ggml_context*, ggml_cgraph*, ggml_cgraph*, ggml_cgraph*, ggml_tensor**, ggml_tensor*, ggml_tensor*, int, int, bool, bool)’ [-Wmissing-declarations]
  645 | struct ggml_tensor * llama_build_train_graphs(
      |                      ^~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:829:6: warning: no previous declaration for ‘void set_f32_3d(ggml_tensor*, int64_t, int64_t, int64_t, float)’ [-Wmissing-declarations]
  829 | void set_f32_3d(struct ggml_tensor * tensor, int64_t i0, int64_t i1, int64_t i2, float value) {
      |      ^~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:834:6: warning: no previous declaration for ‘void set_f32_2d(ggml_tensor*, int64_t, int64_t, float)’ [-Wmissing-declarations]
  834 | void set_f32_2d(struct ggml_tensor * tensor, int64_t i0, int64_t i1, float value) {
      |      ^~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:839:6: warning: no previous declaration for ‘void set_i32_2d(ggml_tensor*, int64_t, int64_t, int32_t)’ [-Wmissing-declarations]
  839 | void set_i32_2d(struct ggml_tensor * tensor, int64_t i0, int64_t i1, int32_t value) {
      |      ^~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:844:7: warning: no previous declaration for ‘float get_f32_2d(ggml_tensor*, int64_t, int64_t)’ [-Wmissing-declarations]
  844 | float get_f32_2d(struct ggml_tensor * tensor, int64_t i0, int64_t i1) {
      |       ^~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:849:9: warning: no previous declaration for ‘int32_t get_i32_2d(ggml_tensor*, int64_t, int64_t)’ [-Wmissing-declarations]
  849 | int32_t get_i32_2d(struct ggml_tensor * tensor, int64_t i0, int64_t i1) {
      |         ^~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:854:6: warning: no previous declaration for ‘void print_row(ggml_tensor*, int)’ [-Wmissing-declarations]
  854 | void print_row(struct ggml_tensor * probs, int i) {
      |      ^~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:862:6: warning: no previous declaration for ‘void print_matrix(ggml_tensor*)’ [-Wmissing-declarations]
  862 | void print_matrix(struct ggml_tensor * probs) {
      |      ^~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:873:6: warning: no previous declaration for ‘void get_example_targets(llama_context*, const int*, size_t, const llama_token*, size_t, int, ggml_tensor*, ggml_tensor*, ggml_tensor*)’ [-Wmissing-declarations]
  873 | void get_example_targets(struct llama_context * lctx, const int * train_samples, size_t n_train_samples, const llama_token * train_data, size_t n_train_data, int example_id, struct ggml_tensor * tokens_input, struct ggml_tensor * target_logits, struct ggml_tensor * target_probs) {
      |      ^~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:893:6: warning: no previous declaration for ‘void get_example_targets_batch(llama_context*, const int*, size_t, const llama_token*, size_t, int, ggml_tensor*, ggml_tensor*, ggml_tensor*)’ [-Wmissing-declarations]
  893 | void get_example_targets_batch(struct llama_context * lctx, const int * train_samples, size_t n_train_samples, const llama_token * train_data, size_t n_train_data, int example_id, struct ggml_tensor * tokens_input, struct ggml_tensor * target_logits, struct ggml_tensor * target_probs) {
      |      ^~~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:928:5: warning: no previous declaration for ‘int tokenize_file(llama_context*, const char*, std::vector<int>&)’ [-Wmissing-declarations]
  928 | int tokenize_file(struct llama_context * lctx, const char * filename, std::vector<llama_token>& out) {
      |     ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:999:6: warning: no previous declaration for ‘void shuffle_ints(int*, int*)’ [-Wmissing-declarations]
  999 | void shuffle_ints(int * begin, int * end) {
      |      ^~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1033:6: warning: no previous declaration for ‘bool are_same_layout(ggml_tensor*, ggml_tensor*)’ [-Wmissing-declarations]
 1033 | bool are_same_layout(struct ggml_tensor * a, struct ggml_tensor * b) {
      |      ^~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1043:6: warning: no previous declaration for ‘void read_tensor_by_name(ggml_tensor*, ggml_context*, const char*)’ [-Wmissing-declarations]
 1043 | void read_tensor_by_name(struct ggml_tensor * dst, struct ggml_context * ctx, const char * name) {
      |      ^~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1056:6: warning: no previous declaration for ‘void load_opt_context_gguf(gguf_context*, ggml_context*, ggml_opt_context*)’ [-Wmissing-declarations]
 1056 | void load_opt_context_gguf(struct gguf_context * fctx, struct ggml_context * f_ggml_ctx, struct ggml_opt_context * opt) {
      |      ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1117:6: warning: no previous declaration for ‘void save_opt_context_gguf(gguf_context*, ggml_opt_context*)’ [-Wmissing-declarations]
 1117 | void save_opt_context_gguf(struct gguf_context * fctx, struct ggml_opt_context * opt) {
      |      ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1184:6: warning: no previous declaration for ‘void load_llama_model_gguf(gguf_context*, ggml_context*, my_llama_model*)’ [-Wmissing-declarations]
 1184 | void load_llama_model_gguf(struct gguf_context * fctx, struct ggml_context * f_ggml_ctx, struct my_llama_model * model) {
      |      ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1255:6: warning: no previous declaration for ‘void save_llama_model_gguf(gguf_context*, const char*, my_llama_model*)’ [-Wmissing-declarations]
 1255 | void save_llama_model_gguf(struct gguf_context * fctx, const char * fn_vocab_model, struct my_llama_model * model) {
      |      ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1398:6: warning: no previous declaration for ‘void save_llama_model_file(const char*, const char*, my_llama_model*)’ [-Wmissing-declarations]
 1398 | void save_llama_model_file(const char * filename, const char * fn_vocab_model, struct my_llama_model * model) {
      |      ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1409:6: warning: no previous declaration for ‘void load_checkpoint_gguf(gguf_context*, ggml_context*, my_llama_model*, ggml_opt_context*)’ [-Wmissing-declarations]
 1409 | void load_checkpoint_gguf(struct gguf_context * fctx, struct ggml_context * f_ggml_ctx, struct my_llama_model * model, struct ggml_opt_context * opt) {
      |      ^~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1423:6: warning: no previous declaration for ‘void save_checkpoint_gguf(gguf_context*, const char*, my_llama_model*, ggml_opt_context*)’ [-Wmissing-declarations]
 1423 | void save_checkpoint_gguf(struct gguf_context * fctx, const char * fn_vocab_model, struct my_llama_model * model, struct ggml_opt_context * opt) {
      |      ^~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1434:6: warning: no previous declaration for ‘bool load_checkpoint_file(const char*, my_llama_model*, ggml_opt_context*)’ [-Wmissing-declarations]
 1434 | bool load_checkpoint_file(const char * filename, struct my_llama_model * model, struct ggml_opt_context * opt) {
      |      ^~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1449:6: warning: no previous declaration for ‘void save_checkpoint_file(const char*, const char*, my_llama_model*, ggml_opt_context*)’ [-Wmissing-declarations]
 1449 | void save_checkpoint_file(const char * filename, const char * fn_vocab_model, struct my_llama_model * model, struct ggml_opt_context * opt) {
      |      ^~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1460:7: warning: no previous declaration for ‘float cosine_decay(int, float, int)’ [-Wmissing-declarations]
 1460 | float cosine_decay(const int decay_steps, const float minimum, int step) {
      |       ^~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1469:7: warning: no previous declaration for ‘float cosine_decay_restart(int, float, int, float, bool)’ [-Wmissing-declarations]
 1469 | float cosine_decay_restart(int decay_steps, const float minimum, int step, float restart_step_mult, bool enable_restart) {
      |       ^~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1537:21: warning: no previous declaration for ‘train_params get_default_train_params()’ [-Wmissing-declarations]
 1537 | struct train_params get_default_train_params() {
      |                     ^~~~~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1597:6: warning: no previous declaration for ‘void train_print_usage(int, char**, const train_params*)’ [-Wmissing-declarations]
 1597 | void train_print_usage(int /*argc*/, char ** argv, const struct train_params * params) {
      |      ^~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1654:6: warning: no previous declaration for ‘bool train_params_parse(int, char**, train_params*)’ [-Wmissing-declarations]
 1654 | bool train_params_parse(int argc, char ** argv, struct train_params * params) {
      |      ^~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:1948:6: warning: no previous declaration for ‘void opt_callback(void*, float*)’ [-Wmissing-declarations]
 1948 | void opt_callback(void * vdata, float * sched) {
      |      ^~~~~~~~~~~~
[ 64%] Linking CXX executable ../../bin/simple
[ 66%] Linking CXX executable ../../bin/q8dot
/home/ggml/work/llama.cpp/examples/baby-llama/baby-llama.cpp: In function ‘int main(int, char**)’:
/home/ggml/work/llama.cpp/examples/baby-llama/baby-llama.cpp:612:98: warning: ‘kv_self.llama_kv_cache::v’ may be used uninitialized in this function [-Wmaybe-uninitialized]
  612 |                         (il*n_ctx)*ggml_element_size(kv_self.v)*n_embd + n_past*ggml_element_size(kv_self.v));
      |                                                                                 ~~~~~~~~~~~~~~~~~^~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/baby-llama/baby-llama.cpp:1582:27: note: ‘kv_self.llama_kv_cache::v’ was declared here
 1582 |     struct llama_kv_cache kv_self;
      |                           ^~~~~~~
/home/ggml/work/llama.cpp/examples/baby-llama/baby-llama.cpp:610:101: warning: ‘kv_self.llama_kv_cache::k’ may be used uninitialized in this function [-Wmaybe-uninitialized]
  610 |                 kc = ggml_set_1d(ctx0, kc, ggml_reshape_1d(ctx0, Kcur, n_embd*N), (ggml_element_size(kv_self.k)*n_embd)*(il*n_ctx + n_past));
      |                                                                                    ~~~~~~~~~~~~~~~~~^~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/baby-llama/baby-llama.cpp:1582:27: note: ‘kv_self.llama_kv_cache::k’ was declared here
 1582 |     struct llama_kv_cache kv_self;
      |                           ^~~~~~~
[ 67%] Linking CXX executable ../bin/test-grammar-parser
[ 69%] Linking CXX executable ../bin/test-sampling
[ 70%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 72%] Linking CXX executable ../../bin/embedding
[ 72%] Built target simple
[ 73%] Linking CXX executable ../../bin/vdot
[ 73%] Built target q8dot
[ 73%] Built target test-grammar-parser
[ 75%] Linking CXX executable ../../bin/save-load-state
[ 75%] Built target test-sampling
[ 75%] Built target test-tokenizer-1-llama
[ 75%] Built target embedding
[ 75%] Built target vdot
[ 76%] Linking CXX executable ../../bin/beam-search
[ 77%] Linking CXX executable ../bin/test-quantize-perf
[ 79%] Linking CXX executable ../../bin/baby-llama
[ 79%] Built target save-load-state
[ 79%] Built target beam-search
[ 79%] Built target baby-llama
[ 79%] Built target test-quantize-perf
[ 80%] Linking CXX static library libembdinput.a
[ 82%] Linking CXX executable ../bin/test-grad0
[ 82%] Built target embdinput
[ 83%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 85%] Building CXX object examples/embd-input/CMakeFiles/embd-input-test.dir/embd-input-test.cpp.o
[ 85%] Built target test-grad0
[ 86%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 86%] Built target test-tokenizer-0-falcon
[ 86%] Built target test-tokenizer-0-llama
[ 88%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 89%] Linking CXX executable ../../bin/perplexity
[ 91%] Linking CXX executable ../../bin/speculative
[ 92%] Linking CXX executable ../../bin/embd-input-test
[ 92%] Built target convert-llama2c-to-ggml
[ 92%] Built target perplexity
[ 92%] Built target speculative
[ 92%] Built target embd-input-test
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp: In function ‘ggml_tensor* llama_build_train_graphs(my_llama_model*, ggml_allocr*, ggml_context*, ggml_cgraph*, ggml_cgraph*, ggml_cgraph*, ggml_tensor**, ggml_tensor*, ggml_tensor*, int, int, bool, bool)’:
/home/ggml/work/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:735:68: warning: ‘kv_scale’ may be used uninitialized in this function [-Wmaybe-uninitialized]
  735 |             struct ggml_tensor * t16_1 = ggml_scale_inplace        (ctx, t16_0, kv_scale);          set_name(t16_1, "t16_1"); assert_shape_4d(t16_1, N, N, n_head, n_batch);
      |                                          ~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~
[ 94%] Linking CXX executable ../../bin/train-text-from-scratch
[ 94%] Built target train-text-from-scratch
[ 95%] Linking CXX executable ../../bin/main
[ 95%] Built target main
[ 97%] Linking CXX executable ../../bin/llama-bench
[ 97%] Built target llama-bench
[ 98%] Linking CXX executable ../bin/test-llama-grammar
[ 98%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	0m55.746s
user	1m42.974s
sys	0m6.538s
+ ctest --output-on-failure -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-release
    Start 1: test-quantize-fns
1/8 Test #1: test-quantize-fns ................   Passed    0.01 sec
    Start 2: test-quantize-perf
2/8 Test #2: test-quantize-perf ...............   Passed    0.05 sec
    Start 3: test-sampling
3/8 Test #3: test-sampling ....................   Passed    0.00 sec
    Start 4: test-tokenizer-0-llama
4/8 Test #4: test-tokenizer-0-llama ...........   Passed    0.02 sec
    Start 5: test-tokenizer-1-llama
5/8 Test #5: test-tokenizer-1-llama ...........   Passed    3.11 sec
    Start 6: test-grammar-parser
6/8 Test #6: test-grammar-parser ..............   Passed    0.00 sec
    Start 7: test-llama-grammar
7/8 Test #7: test-llama-grammar ...............   Passed    0.00 sec
    Start 8: test-grad0
8/8 Test #8: test-grad0 .......................   Passed    2.98 sec

100% tests passed, 0 tests failed out of 8

Total Test time (real) =   6.19 sec

real	0m6.204s
user	0m6.118s
sys	0m4.149s
