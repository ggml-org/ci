Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.031s
user	0m1.010s
sys	0m1.420s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking C executable ../bin/test-c
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-simple
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-chat
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-gguf
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-gguf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Built target test-backend-ops
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Built target test-quantize-perf
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-embedding
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gritlm
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-eval-callback
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Built target llama-infill
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-cli
[ 82%] Built target llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Built target llama-passkey
[ 83%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Built target llama-perplexity
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 92%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-gen-docs
[ 95%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.100s
user	0m6.503s
sys	0m10.107s

main: quantize time =  5249.26 ms
main:    total time =  5249.26 ms

main: quantize time =  1866.10 ms
main:    total time =  1866.10 ms

main: quantize time =  1772.01 ms
main:    total time =  1772.01 ms

main: quantize time =  2124.43 ms
main:    total time =  2124.43 ms

main: quantize time =  6106.86 ms
main:    total time =  6106.86 ms

main: quantize time =  4948.57 ms
main:    total time =  4948.57 ms

main: quantize time =  5843.20 ms
main:    total time =  5843.20 ms

main: quantize time =  6715.52 ms
main:    total time =  6715.52 ms

main: quantize time =  5816.81 ms
main:    total time =  5816.81 ms

main: quantize time =  4521.65 ms
main:    total time =  4521.65 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.179 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.372 I main: llama backend init
0.00.000.378 I main: load the model and apply lora adapter, if any
0.00.046.755 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.060.630 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.643 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.664 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.666 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.667 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.667 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.668 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.673 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.673 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.674 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.069.170 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.071.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.242 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.078.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.246 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.247 I llama_model_loader: - type  f32:  194 tensors
0.00.078.248 I llama_model_loader: - type  f16:   98 tensors
0.00.078.249 I print_info: file format = GGUF V3 (latest)
0.00.078.250 I print_info: file type   = all F32 (guessed)
0.00.078.252 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.830 I load: special tokens cache size = 25
0.00.099.916 I load: token to piece cache size = 0.2984 MB
0.00.099.919 I print_info: arch             = gptneox
0.00.099.919 I print_info: vocab_only       = 0
0.00.099.919 I print_info: n_ctx_train      = 2048
0.00.099.920 I print_info: n_embd           = 2048
0.00.099.920 I print_info: n_layer          = 24
0.00.099.923 I print_info: n_head           = 16
0.00.099.924 I print_info: n_head_kv        = 16
0.00.099.924 I print_info: n_rot            = 32
0.00.099.925 I print_info: n_swa            = 0
0.00.099.925 I print_info: n_embd_head_k    = 128
0.00.099.925 I print_info: n_embd_head_v    = 128
0.00.099.928 I print_info: n_gqa            = 1
0.00.099.928 I print_info: n_embd_k_gqa     = 2048
0.00.099.929 I print_info: n_embd_v_gqa     = 2048
0.00.099.930 I print_info: f_norm_eps       = 1.0e-05
0.00.099.930 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.099.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.099.931 I print_info: f_max_alibi_bias = 0.0e+00
0.00.099.931 I print_info: f_logit_scale    = 0.0e+00
0.00.099.932 I print_info: n_ff             = 8192
0.00.099.932 I print_info: n_expert         = 0
0.00.099.932 I print_info: n_expert_used    = 0
0.00.099.932 I print_info: causal attn      = 1
0.00.099.932 I print_info: pooling type     = 0
0.00.099.932 I print_info: rope type        = 2
0.00.099.933 I print_info: rope scaling     = linear
0.00.099.933 I print_info: freq_base_train  = 10000.0
0.00.099.934 I print_info: freq_scale_train = 1
0.00.099.934 I print_info: n_ctx_orig_yarn  = 2048
0.00.099.934 I print_info: rope_finetuned   = unknown
0.00.099.934 I print_info: ssm_d_conv       = 0
0.00.099.934 I print_info: ssm_d_inner      = 0
0.00.099.934 I print_info: ssm_d_state      = 0
0.00.099.935 I print_info: ssm_dt_rank      = 0
0.00.099.935 I print_info: ssm_dt_b_c_rms   = 0
0.00.099.935 I print_info: model type       = 1.4B
0.00.099.935 I print_info: model params     = 1.41 B
0.00.099.935 I print_info: general.name     = 1.4B
0.00.099.937 I print_info: vocab type       = BPE
0.00.099.938 I print_info: n_vocab          = 50304
0.00.099.938 I print_info: n_merges         = 50009
0.00.099.938 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.099.938 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.099.938 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.099.939 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.099.940 I print_info: LF token         = 187 'Ċ'
0.00.099.941 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.099.941 I print_info: max token length = 1024
0.00.099.941 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.151.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.151.796 I load_tensors: offloading output layer to GPU
0.00.151.797 I load_tensors: offloaded 25/25 layers to GPU
0.00.151.823 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.151.824 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.152.424 I llama_init_from_model: n_seq_max     = 1
0.00.152.425 I llama_init_from_model: n_ctx         = 2048
0.00.152.425 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.152.425 I llama_init_from_model: n_batch       = 2048
0.00.152.425 I llama_init_from_model: n_ubatch      = 512
0.00.152.426 I llama_init_from_model: flash_attn    = 0
0.00.152.426 I llama_init_from_model: freq_base     = 10000.0
0.00.152.426 I llama_init_from_model: freq_scale    = 1
0.00.152.428 I ggml_metal_init: allocating
0.00.152.488 I ggml_metal_init: found device: Apple M4
0.00.152.493 I ggml_metal_init: picking default device: Apple M4
0.00.153.189 I ggml_metal_init: using embedded metal library
0.00.409.776 I ggml_metal_init: GPU name:   Apple M4
0.00.409.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.409.792 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.409.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.409.794 I ggml_metal_init: simdgroup reduction   = true
0.00.409.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.409.794 I ggml_metal_init: has residency sets    = true
0.00.409.795 I ggml_metal_init: has bfloat            = true
0.00.409.795 I ggml_metal_init: use bfloat            = true
0.00.409.797 I ggml_metal_init: hasUnifiedMemory      = true
0.00.409.803 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.446.940 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.483.970 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.483.980 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.484.039 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.487.638 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.487.642 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.487.642 I llama_init_from_model: graph nodes  = 967
0.00.487.642 I llama_init_from_model: graph splits = 2
0.00.487.649 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.487.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.487.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.537 I main: llama threadpool init, n_threads = 4
0.00.554.583 I 
0.00.554.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.621 I 
0.00.554.807 I sampler seed: 1234
0.00.554.813 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.554.842 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.554.844 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.554.844 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.382.925 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.02.382.926 I llama_perf_context_print:        load time =     506.87 ms
0.02.382.926 I llama_perf_context_print: prompt eval time =      43.90 ms /     7 tokens (    6.27 ms per token,   159.45 tokens per second)
0.02.382.927 I llama_perf_context_print:        eval time =    1781.20 ms /    63 runs   (   28.27 ms per token,    35.37 tokens per second)
0.02.382.927 I llama_perf_context_print:       total time =    1829.29 ms /    70 tokens
0.02.383.129 I ggml_metal_free: deallocating

real	0m2.861s
user	0m0.146s
sys	0m0.159s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.160 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.161 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.162 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.163 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.163 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.975 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.996 I llama_model_loader: - type  f32:  194 tensors
0.00.034.997 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.997 I print_info: file format = GGUF V3 (latest)
0.00.034.998 I print_info: file type   = Q8_0
0.00.035.001 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.478 I load: special tokens cache size = 25
0.00.049.601 I load: token to piece cache size = 0.2984 MB
0.00.049.606 I print_info: arch             = gptneox
0.00.049.606 I print_info: vocab_only       = 0
0.00.049.606 I print_info: n_ctx_train      = 2048
0.00.049.607 I print_info: n_embd           = 2048
0.00.049.609 I print_info: n_layer          = 24
0.00.049.612 I print_info: n_head           = 16
0.00.049.613 I print_info: n_head_kv        = 16
0.00.049.613 I print_info: n_rot            = 32
0.00.049.614 I print_info: n_swa            = 0
0.00.049.614 I print_info: n_embd_head_k    = 128
0.00.049.614 I print_info: n_embd_head_v    = 128
0.00.049.615 I print_info: n_gqa            = 1
0.00.049.615 I print_info: n_embd_k_gqa     = 2048
0.00.049.616 I print_info: n_embd_v_gqa     = 2048
0.00.049.617 I print_info: f_norm_eps       = 1.0e-05
0.00.049.619 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.620 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.620 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.620 I print_info: f_logit_scale    = 0.0e+00
0.00.049.621 I print_info: n_ff             = 8192
0.00.049.621 I print_info: n_expert         = 0
0.00.049.621 I print_info: n_expert_used    = 0
0.00.049.621 I print_info: causal attn      = 1
0.00.049.621 I print_info: pooling type     = 0
0.00.049.622 I print_info: rope type        = 2
0.00.049.622 I print_info: rope scaling     = linear
0.00.049.622 I print_info: freq_base_train  = 10000.0
0.00.049.623 I print_info: freq_scale_train = 1
0.00.049.623 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.625 I print_info: rope_finetuned   = unknown
0.00.049.625 I print_info: ssm_d_conv       = 0
0.00.049.625 I print_info: ssm_d_inner      = 0
0.00.049.625 I print_info: ssm_d_state      = 0
0.00.049.625 I print_info: ssm_dt_rank      = 0
0.00.049.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.626 I print_info: model type       = 1.4B
0.00.049.626 I print_info: model params     = 1.41 B
0.00.049.626 I print_info: general.name     = 1.4B
0.00.049.627 I print_info: vocab type       = BPE
0.00.049.627 I print_info: n_vocab          = 50304
0.00.049.627 I print_info: n_merges         = 50009
0.00.049.627 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.628 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.628 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.628 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.628 I print_info: LF token         = 187 'Ċ'
0.00.049.629 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.629 I print_info: max token length = 1024
0.00.049.629 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.331.165 I load_tensors: offloading 24 repeating layers to GPU
0.01.331.171 I load_tensors: offloading output layer to GPU
0.01.331.172 I load_tensors: offloaded 25/25 layers to GPU
0.01.331.192 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.331.194 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.331.981 I llama_init_from_model: n_seq_max     = 1
0.01.331.982 I llama_init_from_model: n_ctx         = 2048
0.01.331.983 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.331.983 I llama_init_from_model: n_batch       = 2048
0.01.331.983 I llama_init_from_model: n_ubatch      = 512
0.01.331.984 I llama_init_from_model: flash_attn    = 0
0.01.331.985 I llama_init_from_model: freq_base     = 10000.0
0.01.331.985 I llama_init_from_model: freq_scale    = 1
0.01.331.986 I ggml_metal_init: allocating
0.01.331.998 I ggml_metal_init: found device: Apple M4
0.01.332.005 I ggml_metal_init: picking default device: Apple M4
0.01.333.280 I ggml_metal_init: using embedded metal library
0.01.338.528 I ggml_metal_init: GPU name:   Apple M4
0.01.338.531 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.338.532 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.338.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.338.533 I ggml_metal_init: simdgroup reduction   = true
0.01.338.534 I ggml_metal_init: simdgroup matrix mul. = true
0.01.338.534 I ggml_metal_init: has residency sets    = true
0.01.338.534 I ggml_metal_init: has bfloat            = true
0.01.338.534 I ggml_metal_init: use bfloat            = true
0.01.338.535 I ggml_metal_init: hasUnifiedMemory      = true
0.01.338.538 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.354.604 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.411.778 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.411.785 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.411.819 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.416.620 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.416.623 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.416.623 I llama_init_from_model: graph nodes  = 967
0.01.416.623 I llama_init_from_model: graph splits = 2
0.01.416.629 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.416.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.416.763 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.474.005 I main: llama threadpool init, n_threads = 4
0.01.474.055 I 
0.01.474.077 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.474.077 I 
0.01.474.256 I sampler seed: 1234
0.01.474.261 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.474.272 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.474.273 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.474.273 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.564.143 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.02.564.144 I llama_perf_context_print:        load time =    1463.39 ms
0.02.564.145 I llama_perf_context_print: prompt eval time =      48.95 ms /     7 tokens (    6.99 ms per token,   143.02 tokens per second)
0.02.564.146 I llama_perf_context_print:        eval time =    1037.99 ms /    63 runs   (   16.48 ms per token,    60.69 tokens per second)
0.02.564.146 I llama_perf_context_print:       total time =    1090.88 ms /    70 tokens
0.02.564.401 I ggml_metal_free: deallocating

real	0m2.583s
user	0m0.108s
sys	0m0.290s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.013.298 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.205 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.212 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.216 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.217 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.217 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.218 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.218 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.220 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.222 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.223 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.970 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.798 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.799 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.800 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.800 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.800 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.801 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.801 I llama_model_loader: - type  f32:  194 tensors
0.00.042.802 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.802 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.803 I print_info: file format = GGUF V3 (latest)
0.00.042.803 I print_info: file type   = Q4_0
0.00.042.804 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.053.335 I load: special tokens cache size = 25
0.00.061.949 I load: token to piece cache size = 0.2984 MB
0.00.061.953 I print_info: arch             = gptneox
0.00.061.953 I print_info: vocab_only       = 0
0.00.061.954 I print_info: n_ctx_train      = 2048
0.00.061.954 I print_info: n_embd           = 2048
0.00.061.954 I print_info: n_layer          = 24
0.00.061.958 I print_info: n_head           = 16
0.00.061.959 I print_info: n_head_kv        = 16
0.00.061.959 I print_info: n_rot            = 32
0.00.061.959 I print_info: n_swa            = 0
0.00.061.959 I print_info: n_embd_head_k    = 128
0.00.061.960 I print_info: n_embd_head_v    = 128
0.00.061.960 I print_info: n_gqa            = 1
0.00.061.962 I print_info: n_embd_k_gqa     = 2048
0.00.061.963 I print_info: n_embd_v_gqa     = 2048
0.00.061.964 I print_info: f_norm_eps       = 1.0e-05
0.00.061.964 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.964 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.965 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.967 I print_info: f_logit_scale    = 0.0e+00
0.00.061.968 I print_info: n_ff             = 8192
0.00.061.968 I print_info: n_expert         = 0
0.00.061.968 I print_info: n_expert_used    = 0
0.00.061.968 I print_info: causal attn      = 1
0.00.061.968 I print_info: pooling type     = 0
0.00.061.969 I print_info: rope type        = 2
0.00.061.969 I print_info: rope scaling     = linear
0.00.061.970 I print_info: freq_base_train  = 10000.0
0.00.061.970 I print_info: freq_scale_train = 1
0.00.061.970 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.971 I print_info: rope_finetuned   = unknown
0.00.061.971 I print_info: ssm_d_conv       = 0
0.00.061.971 I print_info: ssm_d_inner      = 0
0.00.061.971 I print_info: ssm_d_state      = 0
0.00.061.971 I print_info: ssm_dt_rank      = 0
0.00.061.972 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.972 I print_info: model type       = 1.4B
0.00.061.972 I print_info: model params     = 1.41 B
0.00.061.972 I print_info: general.name     = 1.4B
0.00.061.974 I print_info: vocab type       = BPE
0.00.061.974 I print_info: n_vocab          = 50304
0.00.061.975 I print_info: n_merges         = 50009
0.00.061.975 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.975 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.975 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.976 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.976 I print_info: LF token         = 187 'Ċ'
0.00.061.976 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.976 I print_info: max token length = 1024
0.00.061.977 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.017 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.028 I load_tensors: offloading output layer to GPU
0.00.604.029 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.062 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.604.063 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.605.691 I llama_init_from_model: n_seq_max     = 1
0.00.605.696 I llama_init_from_model: n_ctx         = 2048
0.00.605.696 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.605.697 I llama_init_from_model: n_batch       = 2048
0.00.605.697 I llama_init_from_model: n_ubatch      = 512
0.00.605.698 I llama_init_from_model: flash_attn    = 0
0.00.605.699 I llama_init_from_model: freq_base     = 10000.0
0.00.605.699 I llama_init_from_model: freq_scale    = 1
0.00.605.702 I ggml_metal_init: allocating
0.00.605.752 I ggml_metal_init: found device: Apple M4
0.00.605.765 I ggml_metal_init: picking default device: Apple M4
0.00.607.434 I ggml_metal_init: using embedded metal library
0.00.613.956 I ggml_metal_init: GPU name:   Apple M4
0.00.613.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.963 I ggml_metal_init: simdgroup reduction   = true
0.00.613.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.964 I ggml_metal_init: has residency sets    = true
0.00.613.964 I ggml_metal_init: has bfloat            = true
0.00.613.964 I ggml_metal_init: use bfloat            = true
0.00.613.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.482 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.386 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.689.394 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.689.430 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.694.057 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.694.059 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.694.059 I llama_init_from_model: graph nodes  = 967
0.00.694.059 I llama_init_from_model: graph splits = 2
0.00.694.070 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.694.194 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.361 I main: llama threadpool init, n_threads = 4
0.00.750.406 I 
0.00.750.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.432 I 
0.00.750.578 I sampler seed: 1234
0.00.750.582 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.625 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.629 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.629 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.431.233 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49203.05 tokens per second)
0.01.431.234 I llama_perf_context_print:        load time =     736.35 ms
0.01.431.235 I llama_perf_context_print: prompt eval time =      48.04 ms /     7 tokens (    6.86 ms per token,   145.70 tokens per second)
0.01.431.235 I llama_perf_context_print:        eval time =     629.60 ms /    63 runs   (    9.99 ms per token,   100.06 tokens per second)
0.01.431.236 I llama_perf_context_print:       total time =     681.59 ms /    70 tokens
0.01.431.454 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.118s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.048 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.998 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.004 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.004 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.005 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.005 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.005 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.006 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.011 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.011 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.012 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.822 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.588 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.588 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.589 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.589 I llama_model_loader: - type  f32:  194 tensors
0.00.031.590 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.590 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.591 I print_info: file format = GGUF V3 (latest)
0.00.031.591 I print_info: file type   = Q4_1
0.00.031.592 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.039.537 I load: special tokens cache size = 25
0.00.045.612 I load: token to piece cache size = 0.2984 MB
0.00.045.615 I print_info: arch             = gptneox
0.00.045.615 I print_info: vocab_only       = 0
0.00.045.615 I print_info: n_ctx_train      = 2048
0.00.045.615 I print_info: n_embd           = 2048
0.00.045.615 I print_info: n_layer          = 24
0.00.045.618 I print_info: n_head           = 16
0.00.045.619 I print_info: n_head_kv        = 16
0.00.045.619 I print_info: n_rot            = 32
0.00.045.619 I print_info: n_swa            = 0
0.00.045.621 I print_info: n_embd_head_k    = 128
0.00.045.622 I print_info: n_embd_head_v    = 128
0.00.045.622 I print_info: n_gqa            = 1
0.00.045.623 I print_info: n_embd_k_gqa     = 2048
0.00.045.629 I print_info: n_embd_v_gqa     = 2048
0.00.045.630 I print_info: f_norm_eps       = 1.0e-05
0.00.045.630 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.630 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.631 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.634 I print_info: f_logit_scale    = 0.0e+00
0.00.045.635 I print_info: n_ff             = 8192
0.00.045.635 I print_info: n_expert         = 0
0.00.045.636 I print_info: n_expert_used    = 0
0.00.045.636 I print_info: causal attn      = 1
0.00.045.636 I print_info: pooling type     = 0
0.00.045.637 I print_info: rope type        = 2
0.00.045.638 I print_info: rope scaling     = linear
0.00.045.639 I print_info: freq_base_train  = 10000.0
0.00.045.639 I print_info: freq_scale_train = 1
0.00.045.639 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.639 I print_info: rope_finetuned   = unknown
0.00.045.639 I print_info: ssm_d_conv       = 0
0.00.045.639 I print_info: ssm_d_inner      = 0
0.00.045.640 I print_info: ssm_d_state      = 0
0.00.045.640 I print_info: ssm_dt_rank      = 0
0.00.045.640 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.640 I print_info: model type       = 1.4B
0.00.045.640 I print_info: model params     = 1.41 B
0.00.045.640 I print_info: general.name     = 1.4B
0.00.045.641 I print_info: vocab type       = BPE
0.00.045.641 I print_info: n_vocab          = 50304
0.00.045.641 I print_info: n_merges         = 50009
0.00.045.641 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.641 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.643 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.643 I print_info: LF token         = 187 'Ċ'
0.00.045.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.643 I print_info: max token length = 1024
0.00.045.644 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.569.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.569.903 I load_tensors: offloading output layer to GPU
0.00.569.904 I load_tensors: offloaded 25/25 layers to GPU
0.00.569.935 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.569.936 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.571.568 I llama_init_from_model: n_seq_max     = 1
0.00.571.575 I llama_init_from_model: n_ctx         = 2048
0.00.571.575 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.571.576 I llama_init_from_model: n_batch       = 2048
0.00.571.577 I llama_init_from_model: n_ubatch      = 512
0.00.571.577 I llama_init_from_model: flash_attn    = 0
0.00.571.578 I llama_init_from_model: freq_base     = 10000.0
0.00.571.578 I llama_init_from_model: freq_scale    = 1
0.00.571.584 I ggml_metal_init: allocating
0.00.571.636 I ggml_metal_init: found device: Apple M4
0.00.571.647 I ggml_metal_init: picking default device: Apple M4
0.00.573.829 I ggml_metal_init: using embedded metal library
0.00.580.501 I ggml_metal_init: GPU name:   Apple M4
0.00.580.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.580.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.580.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.580.510 I ggml_metal_init: simdgroup reduction   = true
0.00.580.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.580.510 I ggml_metal_init: has residency sets    = true
0.00.580.511 I ggml_metal_init: has bfloat            = true
0.00.580.511 I ggml_metal_init: use bfloat            = true
0.00.580.512 I ggml_metal_init: hasUnifiedMemory      = true
0.00.580.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.600.366 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.653.044 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.653.050 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.653.099 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.657.477 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.657.479 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.657.479 I llama_init_from_model: graph nodes  = 967
0.00.657.480 I llama_init_from_model: graph splits = 2
0.00.657.485 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.657.611 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.657.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.250 I main: llama threadpool init, n_threads = 4
0.00.716.294 I 
0.00.716.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.316 I 
0.00.716.485 I sampler seed: 1234
0.00.716.490 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.716.510 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.716.510 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.716.510 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.447.478 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.01.447.478 I llama_perf_context_print:        load time =     706.49 ms
0.01.447.479 I llama_perf_context_print: prompt eval time =      48.94 ms /     7 tokens (    6.99 ms per token,   143.04 tokens per second)
0.01.447.481 I llama_perf_context_print:        eval time =     679.27 ms /    63 runs   (   10.78 ms per token,    92.75 tokens per second)
0.01.447.481 I llama_perf_context_print:       total time =     731.93 ms /    70 tokens
0.01.447.765 I ggml_metal_free: deallocating

real	0m1.464s
user	0m0.110s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.906 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.797 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.734 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.587 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.587 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.588 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.588 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.588 I llama_model_loader: - type  f32:  194 tensors
0.00.029.589 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.589 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.590 I print_info: file format = GGUF V3 (latest)
0.00.029.590 I print_info: file type   = Q5_0
0.00.029.591 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.038.012 I load: special tokens cache size = 25
0.00.043.944 I load: token to piece cache size = 0.2984 MB
0.00.043.946 I print_info: arch             = gptneox
0.00.043.947 I print_info: vocab_only       = 0
0.00.043.947 I print_info: n_ctx_train      = 2048
0.00.043.947 I print_info: n_embd           = 2048
0.00.043.947 I print_info: n_layer          = 24
0.00.043.949 I print_info: n_head           = 16
0.00.043.950 I print_info: n_head_kv        = 16
0.00.043.950 I print_info: n_rot            = 32
0.00.043.950 I print_info: n_swa            = 0
0.00.043.951 I print_info: n_embd_head_k    = 128
0.00.043.953 I print_info: n_embd_head_v    = 128
0.00.043.953 I print_info: n_gqa            = 1
0.00.043.954 I print_info: n_embd_k_gqa     = 2048
0.00.043.955 I print_info: n_embd_v_gqa     = 2048
0.00.043.955 I print_info: f_norm_eps       = 1.0e-05
0.00.043.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.956 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.956 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.956 I print_info: f_logit_scale    = 0.0e+00
0.00.043.956 I print_info: n_ff             = 8192
0.00.043.957 I print_info: n_expert         = 0
0.00.043.957 I print_info: n_expert_used    = 0
0.00.043.957 I print_info: causal attn      = 1
0.00.043.957 I print_info: pooling type     = 0
0.00.043.957 I print_info: rope type        = 2
0.00.043.957 I print_info: rope scaling     = linear
0.00.043.958 I print_info: freq_base_train  = 10000.0
0.00.043.959 I print_info: freq_scale_train = 1
0.00.043.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.960 I print_info: rope_finetuned   = unknown
0.00.043.960 I print_info: ssm_d_conv       = 0
0.00.043.960 I print_info: ssm_d_inner      = 0
0.00.043.960 I print_info: ssm_d_state      = 0
0.00.043.960 I print_info: ssm_dt_rank      = 0
0.00.043.960 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.961 I print_info: model type       = 1.4B
0.00.043.961 I print_info: model params     = 1.41 B
0.00.043.961 I print_info: general.name     = 1.4B
0.00.043.961 I print_info: vocab type       = BPE
0.00.043.962 I print_info: n_vocab          = 50304
0.00.043.962 I print_info: n_merges         = 50009
0.00.043.962 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.962 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.962 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.962 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.963 I print_info: LF token         = 187 'Ċ'
0.00.043.963 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.963 I print_info: max token length = 1024
0.00.043.964 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.740.105 I load_tensors: offloading 24 repeating layers to GPU
0.00.740.113 I load_tensors: offloading output layer to GPU
0.00.740.114 I load_tensors: offloaded 25/25 layers to GPU
0.00.740.132 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.740.136 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.741.055 I llama_init_from_model: n_seq_max     = 1
0.00.741.059 I llama_init_from_model: n_ctx         = 2048
0.00.741.060 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.741.060 I llama_init_from_model: n_batch       = 2048
0.00.741.060 I llama_init_from_model: n_ubatch      = 512
0.00.741.061 I llama_init_from_model: flash_attn    = 0
0.00.741.062 I llama_init_from_model: freq_base     = 10000.0
0.00.741.063 I llama_init_from_model: freq_scale    = 1
0.00.741.064 I ggml_metal_init: allocating
0.00.741.101 I ggml_metal_init: found device: Apple M4
0.00.741.113 I ggml_metal_init: picking default device: Apple M4
0.00.742.179 I ggml_metal_init: using embedded metal library
0.00.746.336 I ggml_metal_init: GPU name:   Apple M4
0.00.746.346 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.347 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.348 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.348 I ggml_metal_init: simdgroup reduction   = true
0.00.746.348 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.349 I ggml_metal_init: has residency sets    = true
0.00.746.349 I ggml_metal_init: has bfloat            = true
0.00.746.349 I ggml_metal_init: use bfloat            = true
0.00.746.351 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.762.552 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.792.811 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.792.817 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.792.851 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.798.072 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.798.074 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.798.075 I llama_init_from_model: graph nodes  = 967
0.00.798.075 I llama_init_from_model: graph splits = 2
0.00.798.085 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.798.196 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.798.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.853.014 I main: llama threadpool init, n_threads = 4
0.00.853.058 I 
0.00.853.080 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.853.082 I 
0.00.853.236 I sampler seed: 1234
0.00.853.241 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.853.252 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.853.252 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.853.252 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.649.044 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50461.98 tokens per second)
0.01.649.045 I llama_perf_context_print:        load time =     841.39 ms
0.01.649.045 I llama_perf_context_print: prompt eval time =      42.88 ms /     7 tokens (    6.13 ms per token,   163.25 tokens per second)
0.01.649.046 I llama_perf_context_print:        eval time =     750.20 ms /    63 runs   (   11.91 ms per token,    83.98 tokens per second)
0.01.649.046 I llama_perf_context_print:       total time =     796.75 ms /    70 tokens
0.01.649.292 I ggml_metal_free: deallocating

real	0m1.673s
user	0m0.106s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.104 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.734 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.741 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.743 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.743 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.743 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.752 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.752 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.753 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.757 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.613 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.657 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.658 I llama_model_loader: - type  f32:  194 tensors
0.00.033.659 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.659 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.660 I print_info: file format = GGUF V3 (latest)
0.00.033.660 I print_info: file type   = Q5_1
0.00.033.661 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.146 I load: special tokens cache size = 25
0.00.048.277 I load: token to piece cache size = 0.2984 MB
0.00.048.280 I print_info: arch             = gptneox
0.00.048.280 I print_info: vocab_only       = 0
0.00.048.280 I print_info: n_ctx_train      = 2048
0.00.048.281 I print_info: n_embd           = 2048
0.00.048.281 I print_info: n_layer          = 24
0.00.048.285 I print_info: n_head           = 16
0.00.048.285 I print_info: n_head_kv        = 16
0.00.048.286 I print_info: n_rot            = 32
0.00.048.286 I print_info: n_swa            = 0
0.00.048.289 I print_info: n_embd_head_k    = 128
0.00.048.289 I print_info: n_embd_head_v    = 128
0.00.048.289 I print_info: n_gqa            = 1
0.00.048.290 I print_info: n_embd_k_gqa     = 2048
0.00.048.291 I print_info: n_embd_v_gqa     = 2048
0.00.048.292 I print_info: f_norm_eps       = 1.0e-05
0.00.048.292 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.292 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.292 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.292 I print_info: f_logit_scale    = 0.0e+00
0.00.048.293 I print_info: n_ff             = 8192
0.00.048.293 I print_info: n_expert         = 0
0.00.048.294 I print_info: n_expert_used    = 0
0.00.048.294 I print_info: causal attn      = 1
0.00.048.294 I print_info: pooling type     = 0
0.00.048.295 I print_info: rope type        = 2
0.00.048.297 I print_info: rope scaling     = linear
0.00.048.297 I print_info: freq_base_train  = 10000.0
0.00.048.298 I print_info: freq_scale_train = 1
0.00.048.298 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.298 I print_info: rope_finetuned   = unknown
0.00.048.298 I print_info: ssm_d_conv       = 0
0.00.048.298 I print_info: ssm_d_inner      = 0
0.00.048.298 I print_info: ssm_d_state      = 0
0.00.048.298 I print_info: ssm_dt_rank      = 0
0.00.048.299 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.299 I print_info: model type       = 1.4B
0.00.048.303 I print_info: model params     = 1.41 B
0.00.048.303 I print_info: general.name     = 1.4B
0.00.048.304 I print_info: vocab type       = BPE
0.00.048.304 I print_info: n_vocab          = 50304
0.00.048.304 I print_info: n_merges         = 50009
0.00.048.305 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.305 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.305 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.305 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.305 I print_info: LF token         = 187 'Ċ'
0.00.048.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.306 I print_info: max token length = 1024
0.00.048.307 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.140.739 I load_tensors: offloading 24 repeating layers to GPU
0.01.140.757 I load_tensors: offloading output layer to GPU
0.01.140.758 I load_tensors: offloaded 25/25 layers to GPU
0.01.140.790 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.01.140.791 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.01.141.863 I llama_init_from_model: n_seq_max     = 1
0.01.141.866 I llama_init_from_model: n_ctx         = 2048
0.01.141.867 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.141.867 I llama_init_from_model: n_batch       = 2048
0.01.141.868 I llama_init_from_model: n_ubatch      = 512
0.01.141.868 I llama_init_from_model: flash_attn    = 0
0.01.141.870 I llama_init_from_model: freq_base     = 10000.0
0.01.141.870 I llama_init_from_model: freq_scale    = 1
0.01.141.872 I ggml_metal_init: allocating
0.01.141.919 I ggml_metal_init: found device: Apple M4
0.01.141.933 I ggml_metal_init: picking default device: Apple M4
0.01.143.499 I ggml_metal_init: using embedded metal library
0.01.149.892 I ggml_metal_init: GPU name:   Apple M4
0.01.149.897 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.149.898 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.149.899 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.149.899 I ggml_metal_init: simdgroup reduction   = true
0.01.149.900 I ggml_metal_init: simdgroup matrix mul. = true
0.01.149.900 I ggml_metal_init: has residency sets    = true
0.01.149.900 I ggml_metal_init: has bfloat            = true
0.01.149.900 I ggml_metal_init: use bfloat            = true
0.01.149.901 I ggml_metal_init: hasUnifiedMemory      = true
0.01.149.904 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.168.242 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.226.738 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.226.743 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.226.782 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.230.770 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.230.771 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.230.771 I llama_init_from_model: graph nodes  = 967
0.01.230.771 I llama_init_from_model: graph splits = 2
0.01.230.774 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.230.904 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.230.905 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.286.148 I main: llama threadpool init, n_threads = 4
0.01.286.198 I 
0.01.286.220 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.286.221 I 
0.01.286.369 I sampler seed: 1234
0.01.286.374 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.286.409 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.286.412 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.286.413 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.118.940 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.02.118.941 I llama_perf_context_print:        load time =    1276.32 ms
0.02.118.942 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.80 tokens per second)
0.02.118.943 I llama_perf_context_print:        eval time =     787.59 ms /    63 runs   (   12.50 ms per token,    79.99 tokens per second)
0.02.118.944 I llama_perf_context_print:       total time =     833.51 ms /    70 tokens
0.02.119.226 I ggml_metal_free: deallocating

real	0m2.137s
user	0m0.111s
sys	0m0.223s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.015.607 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.187 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.192 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.194 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.195 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.195 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.196 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.197 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.197 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.199 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.201 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.203 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.156 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.517 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.518 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.518 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.035.519 I llama_model_loader: - type  f32:  194 tensors
0.00.035.519 I llama_model_loader: - type q2_K:   49 tensors
0.00.035.519 I llama_model_loader: - type q3_K:   48 tensors
0.00.035.520 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.520 I print_info: file format = GGUF V3 (latest)
0.00.035.521 I print_info: file type   = Q2_K - Medium
0.00.035.522 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.047.436 I load: special tokens cache size = 25
0.00.057.331 I load: token to piece cache size = 0.2984 MB
0.00.057.335 I print_info: arch             = gptneox
0.00.057.335 I print_info: vocab_only       = 0
0.00.057.336 I print_info: n_ctx_train      = 2048
0.00.057.336 I print_info: n_embd           = 2048
0.00.057.336 I print_info: n_layer          = 24
0.00.057.340 I print_info: n_head           = 16
0.00.057.341 I print_info: n_head_kv        = 16
0.00.057.341 I print_info: n_rot            = 32
0.00.057.341 I print_info: n_swa            = 0
0.00.057.342 I print_info: n_embd_head_k    = 128
0.00.057.342 I print_info: n_embd_head_v    = 128
0.00.057.343 I print_info: n_gqa            = 1
0.00.057.344 I print_info: n_embd_k_gqa     = 2048
0.00.057.345 I print_info: n_embd_v_gqa     = 2048
0.00.057.346 I print_info: f_norm_eps       = 1.0e-05
0.00.057.346 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.346 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.347 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.347 I print_info: f_logit_scale    = 0.0e+00
0.00.057.348 I print_info: n_ff             = 8192
0.00.057.348 I print_info: n_expert         = 0
0.00.057.348 I print_info: n_expert_used    = 0
0.00.057.348 I print_info: causal attn      = 1
0.00.057.349 I print_info: pooling type     = 0
0.00.057.349 I print_info: rope type        = 2
0.00.057.349 I print_info: rope scaling     = linear
0.00.057.350 I print_info: freq_base_train  = 10000.0
0.00.057.351 I print_info: freq_scale_train = 1
0.00.057.351 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.351 I print_info: rope_finetuned   = unknown
0.00.057.351 I print_info: ssm_d_conv       = 0
0.00.057.352 I print_info: ssm_d_inner      = 0
0.00.057.352 I print_info: ssm_d_state      = 0
0.00.057.352 I print_info: ssm_dt_rank      = 0
0.00.057.352 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.353 I print_info: model type       = 1.4B
0.00.057.353 I print_info: model params     = 1.41 B
0.00.057.353 I print_info: general.name     = 1.4B
0.00.057.354 I print_info: vocab type       = BPE
0.00.057.354 I print_info: n_vocab          = 50304
0.00.057.354 I print_info: n_merges         = 50009
0.00.057.357 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.357 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.358 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.358 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.358 I print_info: LF token         = 187 'Ċ'
0.00.057.359 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.359 I print_info: max token length = 1024
0.00.057.359 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.362.950 I load_tensors: offloading 24 repeating layers to GPU
0.00.362.964 I load_tensors: offloading output layer to GPU
0.00.362.964 I load_tensors: offloaded 25/25 layers to GPU
0.00.363.000 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.363.004 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.364.473 I llama_init_from_model: n_seq_max     = 1
0.00.364.475 I llama_init_from_model: n_ctx         = 2048
0.00.364.476 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.364.476 I llama_init_from_model: n_batch       = 2048
0.00.364.477 I llama_init_from_model: n_ubatch      = 512
0.00.364.477 I llama_init_from_model: flash_attn    = 0
0.00.364.480 I llama_init_from_model: freq_base     = 10000.0
0.00.364.480 I llama_init_from_model: freq_scale    = 1
0.00.364.483 I ggml_metal_init: allocating
0.00.364.602 I ggml_metal_init: found device: Apple M4
0.00.364.617 I ggml_metal_init: picking default device: Apple M4
0.00.366.591 I ggml_metal_init: using embedded metal library
0.00.372.372 I ggml_metal_init: GPU name:   Apple M4
0.00.372.384 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.372.385 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.372.386 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.372.387 I ggml_metal_init: simdgroup reduction   = true
0.00.372.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.372.388 I ggml_metal_init: has residency sets    = true
0.00.372.388 I ggml_metal_init: has bfloat            = true
0.00.372.388 I ggml_metal_init: use bfloat            = true
0.00.372.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.372.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.797 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.449.678 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.449.684 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.449.719 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.453.788 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.453.790 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.453.790 I llama_init_from_model: graph nodes  = 967
0.00.453.790 I llama_init_from_model: graph splits = 2
0.00.453.796 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.453.922 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.453.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.511.735 I main: llama threadpool init, n_threads = 4
0.00.511.778 I 
0.00.511.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.511.801 I 
0.00.511.978 I sampler seed: 1234
0.00.511.983 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.512.024 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.512.027 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.512.028 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.186.839 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.186.840 I llama_perf_context_print:        load time =     495.41 ms
0.01.186.840 I llama_perf_context_print: prompt eval time =      35.76 ms /     7 tokens (    5.11 ms per token,   195.75 tokens per second)
0.01.186.841 I llama_perf_context_print:        eval time =     636.21 ms /    63 runs   (   10.10 ms per token,    99.02 tokens per second)
0.01.186.841 I llama_perf_context_print:       total time =     675.82 ms /    70 tokens
0.01.187.107 I ggml_metal_free: deallocating

real	0m1.222s
user	0m0.126s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.838 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.302 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.024.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.056 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.865 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.032.865 I llama_model_loader: - type  f32:  194 tensors
0.00.032.865 I llama_model_loader: - type q3_K:   25 tensors
0.00.032.866 I llama_model_loader: - type q4_K:   71 tensors
0.00.032.866 I llama_model_loader: - type q5_K:    1 tensors
0.00.032.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.866 I print_info: file format = GGUF V3 (latest)
0.00.032.867 I print_info: file type   = Q3_K - Medium
0.00.032.868 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.040.860 I load: special tokens cache size = 25
0.00.046.583 I load: token to piece cache size = 0.2984 MB
0.00.046.586 I print_info: arch             = gptneox
0.00.046.586 I print_info: vocab_only       = 0
0.00.046.586 I print_info: n_ctx_train      = 2048
0.00.046.586 I print_info: n_embd           = 2048
0.00.046.587 I print_info: n_layer          = 24
0.00.046.589 I print_info: n_head           = 16
0.00.046.590 I print_info: n_head_kv        = 16
0.00.046.590 I print_info: n_rot            = 32
0.00.046.590 I print_info: n_swa            = 0
0.00.046.591 I print_info: n_embd_head_k    = 128
0.00.046.591 I print_info: n_embd_head_v    = 128
0.00.046.591 I print_info: n_gqa            = 1
0.00.046.592 I print_info: n_embd_k_gqa     = 2048
0.00.046.593 I print_info: n_embd_v_gqa     = 2048
0.00.046.593 I print_info: f_norm_eps       = 1.0e-05
0.00.046.595 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.595 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.595 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.595 I print_info: f_logit_scale    = 0.0e+00
0.00.046.596 I print_info: n_ff             = 8192
0.00.046.596 I print_info: n_expert         = 0
0.00.046.596 I print_info: n_expert_used    = 0
0.00.046.598 I print_info: causal attn      = 1
0.00.046.600 I print_info: pooling type     = 0
0.00.046.600 I print_info: rope type        = 2
0.00.046.600 I print_info: rope scaling     = linear
0.00.046.601 I print_info: freq_base_train  = 10000.0
0.00.046.601 I print_info: freq_scale_train = 1
0.00.046.601 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.603 I print_info: rope_finetuned   = unknown
0.00.046.603 I print_info: ssm_d_conv       = 0
0.00.046.603 I print_info: ssm_d_inner      = 0
0.00.046.603 I print_info: ssm_d_state      = 0
0.00.046.603 I print_info: ssm_dt_rank      = 0
0.00.046.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.604 I print_info: model type       = 1.4B
0.00.046.604 I print_info: model params     = 1.41 B
0.00.046.604 I print_info: general.name     = 1.4B
0.00.046.605 I print_info: vocab type       = BPE
0.00.046.605 I print_info: n_vocab          = 50304
0.00.046.605 I print_info: n_merges         = 50009
0.00.046.605 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.607 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.607 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.607 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.607 I print_info: LF token         = 187 'Ċ'
0.00.046.608 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.608 I print_info: max token length = 1024
0.00.046.608 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.511.981 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.994 I load_tensors: offloading output layer to GPU
0.00.511.994 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.026 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.512.028 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.513.555 I llama_init_from_model: n_seq_max     = 1
0.00.513.557 I llama_init_from_model: n_ctx         = 2048
0.00.513.558 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.513.558 I llama_init_from_model: n_batch       = 2048
0.00.513.558 I llama_init_from_model: n_ubatch      = 512
0.00.513.559 I llama_init_from_model: flash_attn    = 0
0.00.513.561 I llama_init_from_model: freq_base     = 10000.0
0.00.513.562 I llama_init_from_model: freq_scale    = 1
0.00.513.565 I ggml_metal_init: allocating
0.00.513.641 I ggml_metal_init: found device: Apple M4
0.00.513.655 I ggml_metal_init: picking default device: Apple M4
0.00.515.569 I ggml_metal_init: using embedded metal library
0.00.521.208 I ggml_metal_init: GPU name:   Apple M4
0.00.521.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.228 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.229 I ggml_metal_init: simdgroup reduction   = true
0.00.521.229 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.229 I ggml_metal_init: has residency sets    = true
0.00.521.230 I ggml_metal_init: has bfloat            = true
0.00.521.230 I ggml_metal_init: use bfloat            = true
0.00.521.233 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.273 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.601.593 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.601.600 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.601.643 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.606.918 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.606.920 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.606.921 I llama_init_from_model: graph nodes  = 967
0.00.606.921 I llama_init_from_model: graph splits = 2
0.00.606.927 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.607.051 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.607.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.700 I main: llama threadpool init, n_threads = 4
0.00.665.742 I 
0.00.665.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.768 I 
0.00.665.906 I sampler seed: 1234
0.00.665.911 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.665.921 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.665.921 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.665.922 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.418.720 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.418.720 I llama_perf_context_print:        load time =     656.09 ms
0.01.418.721 I llama_perf_context_print: prompt eval time =      49.86 ms /     7 tokens (    7.12 ms per token,   140.40 tokens per second)
0.01.418.722 I llama_perf_context_print:        eval time =     700.09 ms /    63 runs   (   11.11 ms per token,    89.99 tokens per second)
0.01.418.723 I llama_perf_context_print:       total time =     753.79 ms /    70 tokens
0.01.418.950 I ggml_metal_free: deallocating

real	0m1.436s
user	0m0.112s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.778 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.027.783 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.785 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.789 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.789 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.791 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.791 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.792 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.792 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.793 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.794 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.795 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.795 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.607 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.974 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.975 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.975 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.036.975 I llama_model_loader: - type  f32:  194 tensors
0.00.036.976 I llama_model_loader: - type q4_K:   61 tensors
0.00.036.976 I llama_model_loader: - type q5_K:   24 tensors
0.00.036.976 I llama_model_loader: - type q6_K:   13 tensors
0.00.036.977 I print_info: file format = GGUF V3 (latest)
0.00.036.977 I print_info: file type   = Q4_K - Medium
0.00.036.978 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.871 I load: special tokens cache size = 25
0.00.052.973 I load: token to piece cache size = 0.2984 MB
0.00.052.976 I print_info: arch             = gptneox
0.00.052.976 I print_info: vocab_only       = 0
0.00.052.976 I print_info: n_ctx_train      = 2048
0.00.052.977 I print_info: n_embd           = 2048
0.00.052.977 I print_info: n_layer          = 24
0.00.052.979 I print_info: n_head           = 16
0.00.052.980 I print_info: n_head_kv        = 16
0.00.052.982 I print_info: n_rot            = 32
0.00.052.982 I print_info: n_swa            = 0
0.00.052.983 I print_info: n_embd_head_k    = 128
0.00.052.983 I print_info: n_embd_head_v    = 128
0.00.052.984 I print_info: n_gqa            = 1
0.00.052.984 I print_info: n_embd_k_gqa     = 2048
0.00.052.985 I print_info: n_embd_v_gqa     = 2048
0.00.052.986 I print_info: f_norm_eps       = 1.0e-05
0.00.052.986 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.986 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.986 I print_info: f_logit_scale    = 0.0e+00
0.00.052.987 I print_info: n_ff             = 8192
0.00.052.987 I print_info: n_expert         = 0
0.00.052.987 I print_info: n_expert_used    = 0
0.00.052.987 I print_info: causal attn      = 1
0.00.052.989 I print_info: pooling type     = 0
0.00.052.990 I print_info: rope type        = 2
0.00.052.991 I print_info: rope scaling     = linear
0.00.052.991 I print_info: freq_base_train  = 10000.0
0.00.052.991 I print_info: freq_scale_train = 1
0.00.052.992 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.992 I print_info: rope_finetuned   = unknown
0.00.052.992 I print_info: ssm_d_conv       = 0
0.00.052.992 I print_info: ssm_d_inner      = 0
0.00.052.992 I print_info: ssm_d_state      = 0
0.00.052.992 I print_info: ssm_dt_rank      = 0
0.00.052.993 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.993 I print_info: model type       = 1.4B
0.00.052.993 I print_info: model params     = 1.41 B
0.00.052.993 I print_info: general.name     = 1.4B
0.00.052.994 I print_info: vocab type       = BPE
0.00.052.994 I print_info: n_vocab          = 50304
0.00.052.994 I print_info: n_merges         = 50009
0.00.052.994 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.995 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.995 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.996 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.996 I print_info: LF token         = 187 'Ċ'
0.00.052.997 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.997 I print_info: max token length = 1024
0.00.052.997 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.618.361 I load_tensors: offloading 24 repeating layers to GPU
0.00.618.379 I load_tensors: offloading output layer to GPU
0.00.618.380 I load_tensors: offloaded 25/25 layers to GPU
0.00.618.414 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.618.415 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.619.828 I llama_init_from_model: n_seq_max     = 1
0.00.619.830 I llama_init_from_model: n_ctx         = 2048
0.00.619.831 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.619.831 I llama_init_from_model: n_batch       = 2048
0.00.619.832 I llama_init_from_model: n_ubatch      = 512
0.00.619.832 I llama_init_from_model: flash_attn    = 0
0.00.619.834 I llama_init_from_model: freq_base     = 10000.0
0.00.619.835 I llama_init_from_model: freq_scale    = 1
0.00.619.837 I ggml_metal_init: allocating
0.00.619.917 I ggml_metal_init: found device: Apple M4
0.00.619.932 I ggml_metal_init: picking default device: Apple M4
0.00.621.805 I ggml_metal_init: using embedded metal library
0.00.627.396 I ggml_metal_init: GPU name:   Apple M4
0.00.627.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.404 I ggml_metal_init: simdgroup reduction   = true
0.00.627.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.405 I ggml_metal_init: has residency sets    = true
0.00.627.405 I ggml_metal_init: has bfloat            = true
0.00.627.405 I ggml_metal_init: use bfloat            = true
0.00.627.406 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.502 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.658 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.707.666 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.708 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.568 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.712.570 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.712.570 I llama_init_from_model: graph nodes  = 967
0.00.712.570 I llama_init_from_model: graph splits = 2
0.00.712.577 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.712.701 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.628 I main: llama threadpool init, n_threads = 4
0.00.769.678 I 
0.00.769.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.704 I 
0.00.769.851 I sampler seed: 1234
0.00.769.856 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.867 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.872 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.872 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.534.674 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.534.675 I llama_perf_context_print:        load time =     760.05 ms
0.01.534.676 I llama_perf_context_print: prompt eval time =      59.28 ms /     7 tokens (    8.47 ms per token,   118.08 tokens per second)
0.01.534.677 I llama_perf_context_print:        eval time =     702.64 ms /    63 runs   (   11.15 ms per token,    89.66 tokens per second)
0.01.534.678 I llama_perf_context_print:       total time =     765.77 ms /    70 tokens
0.01.534.899 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.113s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.647 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.979 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.984 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.986 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.986 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.987 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.987 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.987 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.989 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.989 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.990 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.991 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.775 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.549 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.549 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.550 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.550 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.550 I llama_model_loader: - type  f32:  194 tensors
0.00.026.551 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.551 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.552 I print_info: file format = GGUF V3 (latest)
0.00.026.552 I print_info: file type   = Q5_K - Medium
0.00.026.553 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.586 I load: special tokens cache size = 25
0.00.040.604 I load: token to piece cache size = 0.2984 MB
0.00.040.607 I print_info: arch             = gptneox
0.00.040.607 I print_info: vocab_only       = 0
0.00.040.607 I print_info: n_ctx_train      = 2048
0.00.040.608 I print_info: n_embd           = 2048
0.00.040.608 I print_info: n_layer          = 24
0.00.040.611 I print_info: n_head           = 16
0.00.040.611 I print_info: n_head_kv        = 16
0.00.040.612 I print_info: n_rot            = 32
0.00.040.614 I print_info: n_swa            = 0
0.00.040.614 I print_info: n_embd_head_k    = 128
0.00.040.614 I print_info: n_embd_head_v    = 128
0.00.040.615 I print_info: n_gqa            = 1
0.00.040.616 I print_info: n_embd_k_gqa     = 2048
0.00.040.616 I print_info: n_embd_v_gqa     = 2048
0.00.040.617 I print_info: f_norm_eps       = 1.0e-05
0.00.040.617 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.617 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.617 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.618 I print_info: f_logit_scale    = 0.0e+00
0.00.040.618 I print_info: n_ff             = 8192
0.00.040.618 I print_info: n_expert         = 0
0.00.040.619 I print_info: n_expert_used    = 0
0.00.040.619 I print_info: causal attn      = 1
0.00.040.619 I print_info: pooling type     = 0
0.00.040.620 I print_info: rope type        = 2
0.00.040.621 I print_info: rope scaling     = linear
0.00.040.621 I print_info: freq_base_train  = 10000.0
0.00.040.622 I print_info: freq_scale_train = 1
0.00.040.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.622 I print_info: rope_finetuned   = unknown
0.00.040.622 I print_info: ssm_d_conv       = 0
0.00.040.622 I print_info: ssm_d_inner      = 0
0.00.040.627 I print_info: ssm_d_state      = 0
0.00.040.627 I print_info: ssm_dt_rank      = 0
0.00.040.627 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.627 I print_info: model type       = 1.4B
0.00.040.628 I print_info: model params     = 1.41 B
0.00.040.628 I print_info: general.name     = 1.4B
0.00.040.628 I print_info: vocab type       = BPE
0.00.040.630 I print_info: n_vocab          = 50304
0.00.040.630 I print_info: n_merges         = 50009
0.00.040.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.631 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.631 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.631 I print_info: LF token         = 187 'Ċ'
0.00.040.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.631 I print_info: max token length = 1024
0.00.040.632 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.616.874 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.890 I load_tensors: offloading output layer to GPU
0.00.616.891 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.924 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.616.931 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.618.341 I llama_init_from_model: n_seq_max     = 1
0.00.618.345 I llama_init_from_model: n_ctx         = 2048
0.00.618.346 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.618.346 I llama_init_from_model: n_batch       = 2048
0.00.618.347 I llama_init_from_model: n_ubatch      = 512
0.00.618.347 I llama_init_from_model: flash_attn    = 0
0.00.618.349 I llama_init_from_model: freq_base     = 10000.0
0.00.618.350 I llama_init_from_model: freq_scale    = 1
0.00.618.355 I ggml_metal_init: allocating
0.00.618.432 I ggml_metal_init: found device: Apple M4
0.00.618.445 I ggml_metal_init: picking default device: Apple M4
0.00.620.225 I ggml_metal_init: using embedded metal library
0.00.626.704 I ggml_metal_init: GPU name:   Apple M4
0.00.626.708 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.709 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.710 I ggml_metal_init: simdgroup reduction   = true
0.00.626.710 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.710 I ggml_metal_init: has residency sets    = true
0.00.626.711 I ggml_metal_init: has bfloat            = true
0.00.626.711 I ggml_metal_init: use bfloat            = true
0.00.626.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.111 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.385 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.702.396 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.702.435 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.706.627 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.706.629 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.706.629 I llama_init_from_model: graph nodes  = 967
0.00.706.629 I llama_init_from_model: graph splits = 2
0.00.706.634 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.706.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.706.750 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.817 I main: llama threadpool init, n_threads = 4
0.00.768.864 I 
0.00.768.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.890 I 
0.00.769.036 I sampler seed: 1234
0.00.769.040 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.051 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.051 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.051 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.609.711 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.01.609.711 I llama_perf_context_print:        load time =     757.43 ms
0.01.609.712 I llama_perf_context_print: prompt eval time =      52.68 ms /     7 tokens (    7.53 ms per token,   132.88 tokens per second)
0.01.609.713 I llama_perf_context_print:        eval time =     785.12 ms /    63 runs   (   12.46 ms per token,    80.24 tokens per second)
0.01.609.713 I llama_perf_context_print:       total time =     841.64 ms /    70 tokens
0.01.609.976 I ggml_metal_free: deallocating

real	0m1.627s
user	0m0.108s
sys	0m0.232s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.828 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.524 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.526 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.527 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.527 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.530 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.173 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.799 I llama_model_loader: - type  f32:  194 tensors
0.00.024.799 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.800 I print_info: file format = GGUF V3 (latest)
0.00.024.800 I print_info: file type   = Q6_K
0.00.024.801 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.521 I load: special tokens cache size = 25
0.00.038.404 I load: token to piece cache size = 0.2984 MB
0.00.038.407 I print_info: arch             = gptneox
0.00.038.408 I print_info: vocab_only       = 0
0.00.038.408 I print_info: n_ctx_train      = 2048
0.00.038.408 I print_info: n_embd           = 2048
0.00.038.408 I print_info: n_layer          = 24
0.00.038.411 I print_info: n_head           = 16
0.00.038.412 I print_info: n_head_kv        = 16
0.00.038.412 I print_info: n_rot            = 32
0.00.038.412 I print_info: n_swa            = 0
0.00.038.413 I print_info: n_embd_head_k    = 128
0.00.038.413 I print_info: n_embd_head_v    = 128
0.00.038.413 I print_info: n_gqa            = 1
0.00.038.414 I print_info: n_embd_k_gqa     = 2048
0.00.038.415 I print_info: n_embd_v_gqa     = 2048
0.00.038.415 I print_info: f_norm_eps       = 1.0e-05
0.00.038.416 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.416 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.416 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.416 I print_info: f_logit_scale    = 0.0e+00
0.00.038.417 I print_info: n_ff             = 8192
0.00.038.417 I print_info: n_expert         = 0
0.00.038.417 I print_info: n_expert_used    = 0
0.00.038.418 I print_info: causal attn      = 1
0.00.038.418 I print_info: pooling type     = 0
0.00.038.418 I print_info: rope type        = 2
0.00.038.418 I print_info: rope scaling     = linear
0.00.038.420 I print_info: freq_base_train  = 10000.0
0.00.038.421 I print_info: freq_scale_train = 1
0.00.038.421 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.421 I print_info: rope_finetuned   = unknown
0.00.038.421 I print_info: ssm_d_conv       = 0
0.00.038.421 I print_info: ssm_d_inner      = 0
0.00.038.422 I print_info: ssm_d_state      = 0
0.00.038.422 I print_info: ssm_dt_rank      = 0
0.00.038.423 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.423 I print_info: model type       = 1.4B
0.00.038.424 I print_info: model params     = 1.41 B
0.00.038.424 I print_info: general.name     = 1.4B
0.00.038.424 I print_info: vocab type       = BPE
0.00.038.424 I print_info: n_vocab          = 50304
0.00.038.424 I print_info: n_merges         = 50009
0.00.038.425 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.425 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.425 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.425 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.425 I print_info: LF token         = 187 'Ċ'
0.00.038.426 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.427 I print_info: max token length = 1024
0.00.038.427 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.644.682 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.685 I load_tensors: offloading output layer to GPU
0.00.644.685 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.709 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.644.711 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.646.205 I llama_init_from_model: n_seq_max     = 1
0.00.646.207 I llama_init_from_model: n_ctx         = 2048
0.00.646.208 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.646.208 I llama_init_from_model: n_batch       = 2048
0.00.646.208 I llama_init_from_model: n_ubatch      = 512
0.00.646.209 I llama_init_from_model: flash_attn    = 0
0.00.646.210 I llama_init_from_model: freq_base     = 10000.0
0.00.646.210 I llama_init_from_model: freq_scale    = 1
0.00.646.211 I ggml_metal_init: allocating
0.00.646.252 I ggml_metal_init: found device: Apple M4
0.00.646.266 I ggml_metal_init: picking default device: Apple M4
0.00.647.745 I ggml_metal_init: using embedded metal library
0.00.653.926 I ggml_metal_init: GPU name:   Apple M4
0.00.653.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.931 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.932 I ggml_metal_init: simdgroup reduction   = true
0.00.653.932 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.932 I ggml_metal_init: has residency sets    = true
0.00.653.933 I ggml_metal_init: has bfloat            = true
0.00.653.933 I ggml_metal_init: use bfloat            = true
0.00.653.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.935 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.671.077 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.420 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.725.425 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.725.458 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.730.806 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.730.808 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.730.808 I llama_init_from_model: graph nodes  = 967
0.00.730.808 I llama_init_from_model: graph splits = 2
0.00.730.814 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.730.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.730.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.057 I main: llama threadpool init, n_threads = 4
0.00.796.100 I 
0.00.796.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.130 I 
0.00.796.295 I sampler seed: 1234
0.00.796.299 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.310 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.310 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.310 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.670.042 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.01.670.042 I llama_perf_context_print:        load time =     786.51 ms
0.01.670.044 I llama_perf_context_print: prompt eval time =      57.66 ms /     7 tokens (    8.24 ms per token,   121.39 tokens per second)
0.01.670.044 I llama_perf_context_print:        eval time =     813.17 ms /    63 runs   (   12.91 ms per token,    77.47 tokens per second)
0.01.670.045 I llama_perf_context_print:       total time =     874.70 ms /    70 tokens
0.01.670.292 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.107s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.598 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.331 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.702 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.709 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.711 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.711 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.804 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.425 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.426 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.426 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.426 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.427 I llama_model_loader: - type  f32:  194 tensors
0.00.055.427 I llama_model_loader: - type  f16:   98 tensors
0.00.055.428 I print_info: file format = GGUF V3 (latest)
0.00.055.429 I print_info: file type   = all F32 (guessed)
0.00.055.430 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.764 I load: special tokens cache size = 25
0.00.076.763 I load: token to piece cache size = 0.2984 MB
0.00.076.767 I print_info: arch             = gptneox
0.00.076.767 I print_info: vocab_only       = 0
0.00.076.767 I print_info: n_ctx_train      = 2048
0.00.076.767 I print_info: n_embd           = 2048
0.00.076.768 I print_info: n_layer          = 24
0.00.076.770 I print_info: n_head           = 16
0.00.076.771 I print_info: n_head_kv        = 16
0.00.076.771 I print_info: n_rot            = 32
0.00.076.771 I print_info: n_swa            = 0
0.00.076.772 I print_info: n_embd_head_k    = 128
0.00.076.774 I print_info: n_embd_head_v    = 128
0.00.076.775 I print_info: n_gqa            = 1
0.00.076.776 I print_info: n_embd_k_gqa     = 2048
0.00.076.777 I print_info: n_embd_v_gqa     = 2048
0.00.076.777 I print_info: f_norm_eps       = 1.0e-05
0.00.076.777 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.778 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.778 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.778 I print_info: f_logit_scale    = 0.0e+00
0.00.076.779 I print_info: n_ff             = 8192
0.00.076.779 I print_info: n_expert         = 0
0.00.076.779 I print_info: n_expert_used    = 0
0.00.076.779 I print_info: causal attn      = 1
0.00.076.779 I print_info: pooling type     = 0
0.00.076.779 I print_info: rope type        = 2
0.00.076.784 I print_info: rope scaling     = linear
0.00.076.785 I print_info: freq_base_train  = 10000.0
0.00.076.786 I print_info: freq_scale_train = 1
0.00.076.786 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.786 I print_info: rope_finetuned   = unknown
0.00.076.786 I print_info: ssm_d_conv       = 0
0.00.076.787 I print_info: ssm_d_inner      = 0
0.00.076.787 I print_info: ssm_d_state      = 0
0.00.076.787 I print_info: ssm_dt_rank      = 0
0.00.076.787 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.787 I print_info: model type       = 1.4B
0.00.076.788 I print_info: model params     = 1.41 B
0.00.076.788 I print_info: general.name     = 1.4B
0.00.076.788 I print_info: vocab type       = BPE
0.00.076.789 I print_info: n_vocab          = 50304
0.00.076.789 I print_info: n_merges         = 50009
0.00.076.789 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.789 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.789 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.790 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.790 I print_info: LF token         = 187 'Ċ'
0.00.076.790 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.790 I print_info: max token length = 1024
0.00.076.791 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.428.241 I load_tensors: offloading 24 repeating layers to GPU
0.01.428.247 I load_tensors: offloading output layer to GPU
0.01.428.248 I load_tensors: offloaded 25/25 layers to GPU
0.01.428.276 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.428.277 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.428.952 I llama_init_from_model: n_seq_max     = 1
0.01.428.954 I llama_init_from_model: n_ctx         = 128
0.01.428.954 I llama_init_from_model: n_ctx_per_seq = 128
0.01.428.954 I llama_init_from_model: n_batch       = 128
0.01.428.954 I llama_init_from_model: n_ubatch      = 128
0.01.428.955 I llama_init_from_model: flash_attn    = 0
0.01.428.956 I llama_init_from_model: freq_base     = 10000.0
0.01.428.956 I llama_init_from_model: freq_scale    = 1
0.01.428.957 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.428.960 I ggml_metal_init: allocating
0.01.429.064 I ggml_metal_init: found device: Apple M4
0.01.429.071 I ggml_metal_init: picking default device: Apple M4
0.01.430.185 I ggml_metal_init: using embedded metal library
0.01.434.055 I ggml_metal_init: GPU name:   Apple M4
0.01.434.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.434.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.434.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.434.058 I ggml_metal_init: simdgroup reduction   = true
0.01.434.058 I ggml_metal_init: simdgroup matrix mul. = true
0.01.434.058 I ggml_metal_init: has residency sets    = true
0.01.434.059 I ggml_metal_init: has bfloat            = true
0.01.434.059 I ggml_metal_init: use bfloat            = true
0.01.434.060 I ggml_metal_init: hasUnifiedMemory      = true
0.01.434.061 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.443.912 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.445.557 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.445.560 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.445.587 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.447.252 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.447.253 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.447.253 I llama_init_from_model: graph nodes  = 967
0.01.447.253 I llama_init_from_model: graph splits = 2
0.01.447.255 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.447.255 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.482.069 I 
0.01.482.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.482.106 I perplexity: tokenizing the input ..
0.01.486.123 I perplexity: tokenization took 4.016 ms
0.01.486.127 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.605.212 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.607.514 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.607.554 I llama_perf_context_print:        load time =    1457.73 ms
0.01.607.555 I llama_perf_context_print: prompt eval time =     118.85 ms /   128 tokens (    0.93 ms per token,  1076.99 tokens per second)
0.01.607.556 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.607.556 I llama_perf_context_print:       total time =     125.49 ms /   129 tokens
0.01.607.934 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.100s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.269 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.984 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.762 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.762 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.764 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.766 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.767 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.767 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.771 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.611 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.630 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.522 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.525 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.526 I llama_model_loader: - type  f32:  194 tensors
0.00.025.526 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.527 I print_info: file format = GGUF V3 (latest)
0.00.025.527 I print_info: file type   = Q8_0
0.00.025.534 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.619 I load: special tokens cache size = 25
0.00.039.547 I load: token to piece cache size = 0.2984 MB
0.00.039.552 I print_info: arch             = gptneox
0.00.039.552 I print_info: vocab_only       = 0
0.00.039.552 I print_info: n_ctx_train      = 2048
0.00.039.553 I print_info: n_embd           = 2048
0.00.039.553 I print_info: n_layer          = 24
0.00.039.558 I print_info: n_head           = 16
0.00.039.559 I print_info: n_head_kv        = 16
0.00.039.559 I print_info: n_rot            = 32
0.00.039.559 I print_info: n_swa            = 0
0.00.039.559 I print_info: n_embd_head_k    = 128
0.00.039.559 I print_info: n_embd_head_v    = 128
0.00.039.560 I print_info: n_gqa            = 1
0.00.039.561 I print_info: n_embd_k_gqa     = 2048
0.00.039.561 I print_info: n_embd_v_gqa     = 2048
0.00.039.562 I print_info: f_norm_eps       = 1.0e-05
0.00.039.562 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.563 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.563 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.563 I print_info: f_logit_scale    = 0.0e+00
0.00.039.563 I print_info: n_ff             = 8192
0.00.039.564 I print_info: n_expert         = 0
0.00.039.564 I print_info: n_expert_used    = 0
0.00.039.564 I print_info: causal attn      = 1
0.00.039.564 I print_info: pooling type     = 0
0.00.039.564 I print_info: rope type        = 2
0.00.039.564 I print_info: rope scaling     = linear
0.00.039.565 I print_info: freq_base_train  = 10000.0
0.00.039.566 I print_info: freq_scale_train = 1
0.00.039.568 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.568 I print_info: rope_finetuned   = unknown
0.00.039.569 I print_info: ssm_d_conv       = 0
0.00.039.569 I print_info: ssm_d_inner      = 0
0.00.039.569 I print_info: ssm_d_state      = 0
0.00.039.569 I print_info: ssm_dt_rank      = 0
0.00.039.569 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.569 I print_info: model type       = 1.4B
0.00.039.570 I print_info: model params     = 1.41 B
0.00.039.570 I print_info: general.name     = 1.4B
0.00.039.570 I print_info: vocab type       = BPE
0.00.039.571 I print_info: n_vocab          = 50304
0.00.039.571 I print_info: n_merges         = 50009
0.00.039.571 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.571 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.571 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.571 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.572 I print_info: LF token         = 187 'Ċ'
0.00.039.572 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.572 I print_info: max token length = 1024
0.00.039.572 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.905.899 I load_tensors: offloading 24 repeating layers to GPU
0.00.905.907 I load_tensors: offloading output layer to GPU
0.00.905.908 I load_tensors: offloaded 25/25 layers to GPU
0.00.905.929 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.905.930 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.906.856 I llama_init_from_model: n_seq_max     = 1
0.00.906.859 I llama_init_from_model: n_ctx         = 128
0.00.906.860 I llama_init_from_model: n_ctx_per_seq = 128
0.00.906.860 I llama_init_from_model: n_batch       = 128
0.00.906.860 I llama_init_from_model: n_ubatch      = 128
0.00.906.861 I llama_init_from_model: flash_attn    = 0
0.00.906.862 I llama_init_from_model: freq_base     = 10000.0
0.00.906.862 I llama_init_from_model: freq_scale    = 1
0.00.906.863 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.906.864 I ggml_metal_init: allocating
0.00.906.900 I ggml_metal_init: found device: Apple M4
0.00.906.912 I ggml_metal_init: picking default device: Apple M4
0.00.907.942 I ggml_metal_init: using embedded metal library
0.00.911.983 I ggml_metal_init: GPU name:   Apple M4
0.00.911.987 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.911.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.911.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.911.988 I ggml_metal_init: simdgroup reduction   = true
0.00.911.988 I ggml_metal_init: simdgroup matrix mul. = true
0.00.911.988 I ggml_metal_init: has residency sets    = true
0.00.911.988 I ggml_metal_init: has bfloat            = true
0.00.911.989 I ggml_metal_init: use bfloat            = true
0.00.911.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.911.991 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.922.994 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.924.556 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.924.558 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.924.585 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.926.250 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.926.251 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.926.251 I llama_init_from_model: graph nodes  = 967
0.00.926.252 I llama_init_from_model: graph splits = 2
0.00.926.253 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.926.253 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.949.764 I 
0.00.949.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.949.803 I perplexity: tokenizing the input ..
0.00.953.724 I perplexity: tokenization took 3.919 ms
0.00.953.731 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.090.974 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.092.394 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.092.416 I llama_perf_context_print:        load time =     939.77 ms
0.01.092.417 I llama_perf_context_print: prompt eval time =     137.01 ms /   128 tokens (    1.07 ms per token,   934.22 tokens per second)
0.01.092.418 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.092.418 I llama_perf_context_print:       total time =     142.65 ms /   129 tokens
0.01.092.760 I ggml_metal_free: deallocating

real	0m1.110s
user	0m0.066s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.349 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.312 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.524 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.285 I llama_model_loader: - type  f32:  194 tensors
0.00.030.286 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.286 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.287 I print_info: file format = GGUF V3 (latest)
0.00.030.287 I print_info: file type   = Q4_0
0.00.030.288 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.038.469 I load: special tokens cache size = 25
0.00.044.411 I load: token to piece cache size = 0.2984 MB
0.00.044.414 I print_info: arch             = gptneox
0.00.044.415 I print_info: vocab_only       = 0
0.00.044.415 I print_info: n_ctx_train      = 2048
0.00.044.415 I print_info: n_embd           = 2048
0.00.044.415 I print_info: n_layer          = 24
0.00.044.418 I print_info: n_head           = 16
0.00.044.419 I print_info: n_head_kv        = 16
0.00.044.419 I print_info: n_rot            = 32
0.00.044.420 I print_info: n_swa            = 0
0.00.044.420 I print_info: n_embd_head_k    = 128
0.00.044.420 I print_info: n_embd_head_v    = 128
0.00.044.421 I print_info: n_gqa            = 1
0.00.044.422 I print_info: n_embd_k_gqa     = 2048
0.00.044.422 I print_info: n_embd_v_gqa     = 2048
0.00.044.423 I print_info: f_norm_eps       = 1.0e-05
0.00.044.423 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.426 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.426 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.426 I print_info: f_logit_scale    = 0.0e+00
0.00.044.427 I print_info: n_ff             = 8192
0.00.044.427 I print_info: n_expert         = 0
0.00.044.427 I print_info: n_expert_used    = 0
0.00.044.427 I print_info: causal attn      = 1
0.00.044.427 I print_info: pooling type     = 0
0.00.044.427 I print_info: rope type        = 2
0.00.044.428 I print_info: rope scaling     = linear
0.00.044.428 I print_info: freq_base_train  = 10000.0
0.00.044.429 I print_info: freq_scale_train = 1
0.00.044.429 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.429 I print_info: rope_finetuned   = unknown
0.00.044.429 I print_info: ssm_d_conv       = 0
0.00.044.429 I print_info: ssm_d_inner      = 0
0.00.044.429 I print_info: ssm_d_state      = 0
0.00.044.430 I print_info: ssm_dt_rank      = 0
0.00.044.430 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.430 I print_info: model type       = 1.4B
0.00.044.430 I print_info: model params     = 1.41 B
0.00.044.431 I print_info: general.name     = 1.4B
0.00.044.431 I print_info: vocab type       = BPE
0.00.044.431 I print_info: n_vocab          = 50304
0.00.044.431 I print_info: n_merges         = 50009
0.00.044.432 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.432 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.433 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.433 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.434 I print_info: LF token         = 187 'Ċ'
0.00.044.434 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.434 I print_info: max token length = 1024
0.00.044.435 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.553 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.569 I load_tensors: offloading output layer to GPU
0.00.599.570 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.604 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.599.605 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.601.269 I llama_init_from_model: n_seq_max     = 1
0.00.601.271 I llama_init_from_model: n_ctx         = 128
0.00.601.272 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.273 I llama_init_from_model: n_batch       = 128
0.00.601.273 I llama_init_from_model: n_ubatch      = 128
0.00.601.273 I llama_init_from_model: flash_attn    = 0
0.00.601.276 I llama_init_from_model: freq_base     = 10000.0
0.00.601.276 I llama_init_from_model: freq_scale    = 1
0.00.601.277 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.279 I ggml_metal_init: allocating
0.00.601.347 I ggml_metal_init: found device: Apple M4
0.00.601.361 I ggml_metal_init: picking default device: Apple M4
0.00.603.160 I ggml_metal_init: using embedded metal library
0.00.609.750 I ggml_metal_init: GPU name:   Apple M4
0.00.609.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.756 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.758 I ggml_metal_init: simdgroup reduction   = true
0.00.609.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.758 I ggml_metal_init: has residency sets    = true
0.00.609.759 I ggml_metal_init: has bfloat            = true
0.00.609.759 I ggml_metal_init: use bfloat            = true
0.00.609.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.762 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.715 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.231 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.238 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.290 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.620 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.622 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.622 I llama_init_from_model: graph nodes  = 967
0.00.635.623 I llama_init_from_model: graph splits = 2
0.00.635.626 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.491 I 
0.00.661.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.572 I perplexity: tokenizing the input ..
0.00.668.713 I perplexity: tokenization took 7.137 ms
0.00.668.725 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.195 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.793.544 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.793.569 I llama_perf_context_print:        load time =     651.17 ms
0.00.793.570 I llama_perf_context_print: prompt eval time =     122.58 ms /   128 tokens (    0.96 ms per token,  1044.25 tokens per second)
0.00.793.570 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.571 I llama_perf_context_print:       total time =     132.08 ms /   129 tokens
0.00.793.945 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.080s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.934 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.942 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.943 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.943 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.948 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.949 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.695 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.672 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.493 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.493 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.494 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.495 I llama_model_loader: - type  f32:  194 tensors
0.00.024.495 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.496 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.496 I print_info: file format = GGUF V3 (latest)
0.00.024.499 I print_info: file type   = Q4_1
0.00.024.500 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.777 I load: special tokens cache size = 25
0.00.038.913 I load: token to piece cache size = 0.2984 MB
0.00.038.917 I print_info: arch             = gptneox
0.00.038.918 I print_info: vocab_only       = 0
0.00.038.918 I print_info: n_ctx_train      = 2048
0.00.038.918 I print_info: n_embd           = 2048
0.00.038.918 I print_info: n_layer          = 24
0.00.038.923 I print_info: n_head           = 16
0.00.038.924 I print_info: n_head_kv        = 16
0.00.038.924 I print_info: n_rot            = 32
0.00.038.924 I print_info: n_swa            = 0
0.00.038.924 I print_info: n_embd_head_k    = 128
0.00.038.925 I print_info: n_embd_head_v    = 128
0.00.038.925 I print_info: n_gqa            = 1
0.00.038.926 I print_info: n_embd_k_gqa     = 2048
0.00.038.927 I print_info: n_embd_v_gqa     = 2048
0.00.038.927 I print_info: f_norm_eps       = 1.0e-05
0.00.038.928 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.928 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.928 I print_info: f_logit_scale    = 0.0e+00
0.00.038.928 I print_info: n_ff             = 8192
0.00.038.929 I print_info: n_expert         = 0
0.00.038.929 I print_info: n_expert_used    = 0
0.00.038.929 I print_info: causal attn      = 1
0.00.038.929 I print_info: pooling type     = 0
0.00.038.929 I print_info: rope type        = 2
0.00.038.929 I print_info: rope scaling     = linear
0.00.038.930 I print_info: freq_base_train  = 10000.0
0.00.038.930 I print_info: freq_scale_train = 1
0.00.038.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.930 I print_info: rope_finetuned   = unknown
0.00.038.930 I print_info: ssm_d_conv       = 0
0.00.038.930 I print_info: ssm_d_inner      = 0
0.00.038.931 I print_info: ssm_d_state      = 0
0.00.038.931 I print_info: ssm_dt_rank      = 0
0.00.038.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.931 I print_info: model type       = 1.4B
0.00.038.931 I print_info: model params     = 1.41 B
0.00.038.931 I print_info: general.name     = 1.4B
0.00.038.932 I print_info: vocab type       = BPE
0.00.038.932 I print_info: n_vocab          = 50304
0.00.038.932 I print_info: n_merges         = 50009
0.00.038.932 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.933 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.933 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.933 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.933 I print_info: LF token         = 187 'Ċ'
0.00.038.934 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.934 I print_info: max token length = 1024
0.00.038.934 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.352 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.365 I load_tensors: offloading output layer to GPU
0.00.610.366 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.406 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.610.410 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.611.995 I llama_init_from_model: n_seq_max     = 1
0.00.611.997 I llama_init_from_model: n_ctx         = 128
0.00.611.998 I llama_init_from_model: n_ctx_per_seq = 128
0.00.611.998 I llama_init_from_model: n_batch       = 128
0.00.611.999 I llama_init_from_model: n_ubatch      = 128
0.00.611.999 I llama_init_from_model: flash_attn    = 0
0.00.612.002 I llama_init_from_model: freq_base     = 10000.0
0.00.612.002 I llama_init_from_model: freq_scale    = 1
0.00.612.003 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.612.005 I ggml_metal_init: allocating
0.00.612.086 I ggml_metal_init: found device: Apple M4
0.00.612.101 I ggml_metal_init: picking default device: Apple M4
0.00.613.905 I ggml_metal_init: using embedded metal library
0.00.620.720 I ggml_metal_init: GPU name:   Apple M4
0.00.620.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.727 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.727 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.728 I ggml_metal_init: simdgroup reduction   = true
0.00.620.728 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.729 I ggml_metal_init: has residency sets    = true
0.00.620.729 I ggml_metal_init: has bfloat            = true
0.00.620.729 I ggml_metal_init: use bfloat            = true
0.00.620.730 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.451 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.936 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.641.943 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.018 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.332 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.334 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.335 I llama_init_from_model: graph nodes  = 967
0.00.645.335 I llama_init_from_model: graph splits = 2
0.00.645.338 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.338 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.411 I 
0.00.669.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.495 I perplexity: tokenizing the input ..
0.00.677.178 I perplexity: tokenization took 7.679 ms
0.00.677.193 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.372 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.813.702 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.813.723 I llama_perf_context_print:        load time =     660.59 ms
0.00.813.723 I llama_perf_context_print: prompt eval time =     134.29 ms /   128 tokens (    1.05 ms per token,   953.14 tokens per second)
0.00.813.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.725 I llama_perf_context_print:       total time =     144.32 ms /   129 tokens
0.00.814.075 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.080s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.466 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.293 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.305 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.306 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.306 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.307 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.310 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.311 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.314 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.314 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.207 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.024 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.024 I llama_model_loader: - type  f32:  194 tensors
0.00.025.025 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.025 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.026 I print_info: file format = GGUF V3 (latest)
0.00.025.026 I print_info: file type   = Q5_0
0.00.025.027 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.403 I load: special tokens cache size = 25
0.00.039.523 I load: token to piece cache size = 0.2984 MB
0.00.039.528 I print_info: arch             = gptneox
0.00.039.528 I print_info: vocab_only       = 0
0.00.039.528 I print_info: n_ctx_train      = 2048
0.00.039.529 I print_info: n_embd           = 2048
0.00.039.529 I print_info: n_layer          = 24
0.00.039.533 I print_info: n_head           = 16
0.00.039.534 I print_info: n_head_kv        = 16
0.00.039.534 I print_info: n_rot            = 32
0.00.039.535 I print_info: n_swa            = 0
0.00.039.535 I print_info: n_embd_head_k    = 128
0.00.039.535 I print_info: n_embd_head_v    = 128
0.00.039.536 I print_info: n_gqa            = 1
0.00.039.536 I print_info: n_embd_k_gqa     = 2048
0.00.039.537 I print_info: n_embd_v_gqa     = 2048
0.00.039.538 I print_info: f_norm_eps       = 1.0e-05
0.00.039.540 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.540 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.540 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.540 I print_info: f_logit_scale    = 0.0e+00
0.00.039.541 I print_info: n_ff             = 8192
0.00.039.541 I print_info: n_expert         = 0
0.00.039.541 I print_info: n_expert_used    = 0
0.00.039.541 I print_info: causal attn      = 1
0.00.039.541 I print_info: pooling type     = 0
0.00.039.542 I print_info: rope type        = 2
0.00.039.542 I print_info: rope scaling     = linear
0.00.039.542 I print_info: freq_base_train  = 10000.0
0.00.039.542 I print_info: freq_scale_train = 1
0.00.039.543 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.543 I print_info: rope_finetuned   = unknown
0.00.039.543 I print_info: ssm_d_conv       = 0
0.00.039.543 I print_info: ssm_d_inner      = 0
0.00.039.543 I print_info: ssm_d_state      = 0
0.00.039.543 I print_info: ssm_dt_rank      = 0
0.00.039.543 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.544 I print_info: model type       = 1.4B
0.00.039.544 I print_info: model params     = 1.41 B
0.00.039.544 I print_info: general.name     = 1.4B
0.00.039.545 I print_info: vocab type       = BPE
0.00.039.545 I print_info: n_vocab          = 50304
0.00.039.545 I print_info: n_merges         = 50009
0.00.039.545 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: LF token         = 187 'Ċ'
0.00.039.547 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.547 I print_info: max token length = 1024
0.00.039.547 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.474 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.488 I load_tensors: offloading output layer to GPU
0.00.600.489 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.523 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.600.527 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.602.209 I llama_init_from_model: n_seq_max     = 1
0.00.602.211 I llama_init_from_model: n_ctx         = 128
0.00.602.212 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.212 I llama_init_from_model: n_batch       = 128
0.00.602.213 I llama_init_from_model: n_ubatch      = 128
0.00.602.213 I llama_init_from_model: flash_attn    = 0
0.00.602.215 I llama_init_from_model: freq_base     = 10000.0
0.00.602.215 I llama_init_from_model: freq_scale    = 1
0.00.602.216 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.219 I ggml_metal_init: allocating
0.00.602.296 I ggml_metal_init: found device: Apple M4
0.00.602.311 I ggml_metal_init: picking default device: Apple M4
0.00.604.120 I ggml_metal_init: using embedded metal library
0.00.610.897 I ggml_metal_init: GPU name:   Apple M4
0.00.610.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.904 I ggml_metal_init: simdgroup reduction   = true
0.00.610.904 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.904 I ggml_metal_init: has residency sets    = true
0.00.610.905 I ggml_metal_init: has bfloat            = true
0.00.610.905 I ggml_metal_init: use bfloat            = true
0.00.610.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.143 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.733 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.739 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.787 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.962 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.964 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.964 I llama_init_from_model: graph nodes  = 967
0.00.635.964 I llama_init_from_model: graph splits = 2
0.00.635.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.441 I 
0.00.667.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.531 I perplexity: tokenizing the input ..
0.00.674.523 I perplexity: tokenization took 6.99 ms
0.00.674.528 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.146 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.821.587 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.821.614 I llama_perf_context_print:        load time =     657.97 ms
0.00.821.615 I llama_perf_context_print: prompt eval time =     145.39 ms /   128 tokens (    1.14 ms per token,   880.38 tokens per second)
0.00.821.615 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.616 I llama_perf_context_print:       total time =     154.18 ms /   129 tokens
0.00.822.006 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.079s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.067 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.100 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.105 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.108 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.109 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.110 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.111 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.111 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.112 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.112 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.112 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.113 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.115 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.118 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.722 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.723 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.723 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.724 I llama_model_loader: - type  f32:  194 tensors
0.00.024.724 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.726 I print_info: file format = GGUF V3 (latest)
0.00.024.730 I print_info: file type   = Q5_1
0.00.024.732 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.656 I load: special tokens cache size = 25
0.00.038.615 I load: token to piece cache size = 0.2984 MB
0.00.038.620 I print_info: arch             = gptneox
0.00.038.620 I print_info: vocab_only       = 0
0.00.038.620 I print_info: n_ctx_train      = 2048
0.00.038.620 I print_info: n_embd           = 2048
0.00.038.621 I print_info: n_layer          = 24
0.00.038.625 I print_info: n_head           = 16
0.00.038.626 I print_info: n_head_kv        = 16
0.00.038.626 I print_info: n_rot            = 32
0.00.038.626 I print_info: n_swa            = 0
0.00.038.626 I print_info: n_embd_head_k    = 128
0.00.038.628 I print_info: n_embd_head_v    = 128
0.00.038.629 I print_info: n_gqa            = 1
0.00.038.629 I print_info: n_embd_k_gqa     = 2048
0.00.038.630 I print_info: n_embd_v_gqa     = 2048
0.00.038.632 I print_info: f_norm_eps       = 1.0e-05
0.00.038.632 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.632 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.633 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.633 I print_info: f_logit_scale    = 0.0e+00
0.00.038.633 I print_info: n_ff             = 8192
0.00.038.634 I print_info: n_expert         = 0
0.00.038.634 I print_info: n_expert_used    = 0
0.00.038.634 I print_info: causal attn      = 1
0.00.038.634 I print_info: pooling type     = 0
0.00.038.634 I print_info: rope type        = 2
0.00.038.634 I print_info: rope scaling     = linear
0.00.038.635 I print_info: freq_base_train  = 10000.0
0.00.038.635 I print_info: freq_scale_train = 1
0.00.038.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.635 I print_info: rope_finetuned   = unknown
0.00.038.635 I print_info: ssm_d_conv       = 0
0.00.038.635 I print_info: ssm_d_inner      = 0
0.00.038.636 I print_info: ssm_d_state      = 0
0.00.038.636 I print_info: ssm_dt_rank      = 0
0.00.038.636 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.636 I print_info: model type       = 1.4B
0.00.038.637 I print_info: model params     = 1.41 B
0.00.038.637 I print_info: general.name     = 1.4B
0.00.038.637 I print_info: vocab type       = BPE
0.00.038.637 I print_info: n_vocab          = 50304
0.00.038.638 I print_info: n_merges         = 50009
0.00.038.638 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: LF token         = 187 'Ċ'
0.00.038.642 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.642 I print_info: max token length = 1024
0.00.038.643 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.902 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.918 I load_tensors: offloading output layer to GPU
0.00.622.918 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.952 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.622.954 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.624.717 I llama_init_from_model: n_seq_max     = 1
0.00.624.723 I llama_init_from_model: n_ctx         = 128
0.00.624.724 I llama_init_from_model: n_ctx_per_seq = 128
0.00.624.724 I llama_init_from_model: n_batch       = 128
0.00.624.725 I llama_init_from_model: n_ubatch      = 128
0.00.624.726 I llama_init_from_model: flash_attn    = 0
0.00.624.729 I llama_init_from_model: freq_base     = 10000.0
0.00.624.730 I llama_init_from_model: freq_scale    = 1
0.00.624.732 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.624.736 I ggml_metal_init: allocating
0.00.624.770 I ggml_metal_init: found device: Apple M4
0.00.624.782 I ggml_metal_init: picking default device: Apple M4
0.00.626.231 I ggml_metal_init: using embedded metal library
0.00.632.720 I ggml_metal_init: GPU name:   Apple M4
0.00.632.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.726 I ggml_metal_init: simdgroup reduction   = true
0.00.632.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.727 I ggml_metal_init: has residency sets    = true
0.00.632.727 I ggml_metal_init: has bfloat            = true
0.00.632.727 I ggml_metal_init: use bfloat            = true
0.00.632.728 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.730 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.064 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.653.507 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.653.514 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.653.575 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.656.888 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.656.890 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.656.890 I llama_init_from_model: graph nodes  = 967
0.00.656.891 I llama_init_from_model: graph splits = 2
0.00.656.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.656.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.523 I 
0.00.686.603 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.611 I perplexity: tokenizing the input ..
0.00.694.236 I perplexity: tokenization took 7.621 ms
0.00.694.243 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.830.079 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.831.426 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.831.451 I llama_perf_context_print:        load time =     677.45 ms
0.00.831.452 I llama_perf_context_print: prompt eval time =     134.98 ms /   128 tokens (    1.05 ms per token,   948.29 tokens per second)
0.00.831.452 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.453 I llama_perf_context_print:       total time =     144.93 ms /   129 tokens
0.00.831.838 I ggml_metal_free: deallocating

real	0m0.847s
user	0m0.080s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.919 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.873 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.875 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.876 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.876 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.878 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.878 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.879 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.881 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.883 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.744 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.632 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.633 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.634 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.634 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.634 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.635 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.635 I llama_model_loader: - type  f32:  194 tensors
0.00.025.636 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.636 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.636 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.637 I print_info: file format = GGUF V3 (latest)
0.00.025.638 I print_info: file type   = Q2_K - Medium
0.00.025.639 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.915 I load: special tokens cache size = 25
0.00.040.134 I load: token to piece cache size = 0.2984 MB
0.00.040.138 I print_info: arch             = gptneox
0.00.040.139 I print_info: vocab_only       = 0
0.00.040.139 I print_info: n_ctx_train      = 2048
0.00.040.139 I print_info: n_embd           = 2048
0.00.040.139 I print_info: n_layer          = 24
0.00.040.144 I print_info: n_head           = 16
0.00.040.145 I print_info: n_head_kv        = 16
0.00.040.145 I print_info: n_rot            = 32
0.00.040.145 I print_info: n_swa            = 0
0.00.040.145 I print_info: n_embd_head_k    = 128
0.00.040.145 I print_info: n_embd_head_v    = 128
0.00.040.146 I print_info: n_gqa            = 1
0.00.040.147 I print_info: n_embd_k_gqa     = 2048
0.00.040.148 I print_info: n_embd_v_gqa     = 2048
0.00.040.148 I print_info: f_norm_eps       = 1.0e-05
0.00.040.148 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.149 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.149 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.149 I print_info: f_logit_scale    = 0.0e+00
0.00.040.150 I print_info: n_ff             = 8192
0.00.040.150 I print_info: n_expert         = 0
0.00.040.150 I print_info: n_expert_used    = 0
0.00.040.150 I print_info: causal attn      = 1
0.00.040.150 I print_info: pooling type     = 0
0.00.040.150 I print_info: rope type        = 2
0.00.040.151 I print_info: rope scaling     = linear
0.00.040.151 I print_info: freq_base_train  = 10000.0
0.00.040.151 I print_info: freq_scale_train = 1
0.00.040.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.151 I print_info: rope_finetuned   = unknown
0.00.040.152 I print_info: ssm_d_conv       = 0
0.00.040.152 I print_info: ssm_d_inner      = 0
0.00.040.154 I print_info: ssm_d_state      = 0
0.00.040.155 I print_info: ssm_dt_rank      = 0
0.00.040.155 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.155 I print_info: model type       = 1.4B
0.00.040.155 I print_info: model params     = 1.41 B
0.00.040.155 I print_info: general.name     = 1.4B
0.00.040.156 I print_info: vocab type       = BPE
0.00.040.156 I print_info: n_vocab          = 50304
0.00.040.157 I print_info: n_merges         = 50009
0.00.040.158 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.158 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.158 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.158 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.158 I print_info: LF token         = 187 'Ċ'
0.00.040.159 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.159 I print_info: max token length = 1024
0.00.040.159 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.333.498 I load_tensors: offloading 24 repeating layers to GPU
0.00.333.516 I load_tensors: offloading output layer to GPU
0.00.333.517 I load_tensors: offloaded 25/25 layers to GPU
0.00.333.554 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.333.555 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.335.234 I llama_init_from_model: n_seq_max     = 1
0.00.335.237 I llama_init_from_model: n_ctx         = 128
0.00.335.238 I llama_init_from_model: n_ctx_per_seq = 128
0.00.335.238 I llama_init_from_model: n_batch       = 128
0.00.335.238 I llama_init_from_model: n_ubatch      = 128
0.00.335.239 I llama_init_from_model: flash_attn    = 0
0.00.335.241 I llama_init_from_model: freq_base     = 10000.0
0.00.335.242 I llama_init_from_model: freq_scale    = 1
0.00.335.242 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.335.245 I ggml_metal_init: allocating
0.00.335.329 I ggml_metal_init: found device: Apple M4
0.00.335.343 I ggml_metal_init: picking default device: Apple M4
0.00.337.149 I ggml_metal_init: using embedded metal library
0.00.342.638 I ggml_metal_init: GPU name:   Apple M4
0.00.342.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.656 I ggml_metal_init: simdgroup reduction   = true
0.00.342.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.657 I ggml_metal_init: has residency sets    = true
0.00.342.657 I ggml_metal_init: has bfloat            = true
0.00.342.657 I ggml_metal_init: use bfloat            = true
0.00.342.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.663 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.363.637 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.367.302 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.367.309 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.367.377 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.370.768 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.370.770 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.370.770 I llama_init_from_model: graph nodes  = 967
0.00.370.771 I llama_init_from_model: graph splits = 2
0.00.370.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.370.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.401.289 I 
0.00.401.370 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.401.377 I perplexity: tokenizing the input ..
0.00.407.941 I perplexity: tokenization took 6.561 ms
0.00.407.947 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.546.369 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.547.706 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.547.736 I llama_perf_context_print:        load time =     391.36 ms
0.00.547.739 I llama_perf_context_print: prompt eval time =     137.88 ms /   128 tokens (    1.08 ms per token,   928.37 tokens per second)
0.00.547.741 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.547.742 I llama_perf_context_print:       total time =     146.45 ms /   129 tokens
0.00.548.125 I ggml_metal_free: deallocating

real	0m0.564s
user	0m0.080s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.018 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.020 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.021 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.021 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.021 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.022 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.022 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.023 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.023 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.024 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.024 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.025 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.026 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.846 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.860 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.627 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.628 I llama_model_loader: - type  f32:  194 tensors
0.00.024.628 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.629 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.629 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.629 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.630 I print_info: file format = GGUF V3 (latest)
0.00.024.635 I print_info: file type   = Q3_K - Medium
0.00.024.636 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.875 I load: special tokens cache size = 25
0.00.039.009 I load: token to piece cache size = 0.2984 MB
0.00.039.013 I print_info: arch             = gptneox
0.00.039.013 I print_info: vocab_only       = 0
0.00.039.014 I print_info: n_ctx_train      = 2048
0.00.039.014 I print_info: n_embd           = 2048
0.00.039.014 I print_info: n_layer          = 24
0.00.039.018 I print_info: n_head           = 16
0.00.039.019 I print_info: n_head_kv        = 16
0.00.039.019 I print_info: n_rot            = 32
0.00.039.019 I print_info: n_swa            = 0
0.00.039.021 I print_info: n_embd_head_k    = 128
0.00.039.021 I print_info: n_embd_head_v    = 128
0.00.039.022 I print_info: n_gqa            = 1
0.00.039.023 I print_info: n_embd_k_gqa     = 2048
0.00.039.025 I print_info: n_embd_v_gqa     = 2048
0.00.039.025 I print_info: f_norm_eps       = 1.0e-05
0.00.039.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.027 I print_info: f_logit_scale    = 0.0e+00
0.00.039.029 I print_info: n_ff             = 8192
0.00.039.029 I print_info: n_expert         = 0
0.00.039.029 I print_info: n_expert_used    = 0
0.00.039.029 I print_info: causal attn      = 1
0.00.039.029 I print_info: pooling type     = 0
0.00.039.030 I print_info: rope type        = 2
0.00.039.030 I print_info: rope scaling     = linear
0.00.039.030 I print_info: freq_base_train  = 10000.0
0.00.039.030 I print_info: freq_scale_train = 1
0.00.039.030 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.062 I print_info: rope_finetuned   = unknown
0.00.039.064 I print_info: ssm_d_conv       = 0
0.00.039.064 I print_info: ssm_d_inner      = 0
0.00.039.065 I print_info: ssm_d_state      = 0
0.00.039.065 I print_info: ssm_dt_rank      = 0
0.00.039.065 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.065 I print_info: model type       = 1.4B
0.00.039.067 I print_info: model params     = 1.41 B
0.00.039.067 I print_info: general.name     = 1.4B
0.00.039.067 I print_info: vocab type       = BPE
0.00.039.067 I print_info: n_vocab          = 50304
0.00.039.068 I print_info: n_merges         = 50009
0.00.039.068 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.068 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.068 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.068 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: LF token         = 187 'Ċ'
0.00.039.069 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: max token length = 1024
0.00.039.069 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.053 I load_tensors: offloading output layer to GPU
0.00.442.053 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.086 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.088 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.443.785 I llama_init_from_model: n_seq_max     = 1
0.00.443.788 I llama_init_from_model: n_ctx         = 128
0.00.443.789 I llama_init_from_model: n_ctx_per_seq = 128
0.00.443.789 I llama_init_from_model: n_batch       = 128
0.00.443.790 I llama_init_from_model: n_ubatch      = 128
0.00.443.790 I llama_init_from_model: flash_attn    = 0
0.00.443.792 I llama_init_from_model: freq_base     = 10000.0
0.00.443.793 I llama_init_from_model: freq_scale    = 1
0.00.443.793 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.443.796 I ggml_metal_init: allocating
0.00.443.874 I ggml_metal_init: found device: Apple M4
0.00.443.889 I ggml_metal_init: picking default device: Apple M4
0.00.445.678 I ggml_metal_init: using embedded metal library
0.00.451.358 I ggml_metal_init: GPU name:   Apple M4
0.00.451.366 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.367 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.368 I ggml_metal_init: simdgroup reduction   = true
0.00.451.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.369 I ggml_metal_init: has residency sets    = true
0.00.451.369 I ggml_metal_init: has bfloat            = true
0.00.451.369 I ggml_metal_init: use bfloat            = true
0.00.451.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.373 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.291 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.474.961 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.474.968 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.475.039 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.478.370 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.478.371 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.478.372 I llama_init_from_model: graph nodes  = 967
0.00.478.372 I llama_init_from_model: graph splits = 2
0.00.478.376 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.688 I 
0.00.505.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.779 I perplexity: tokenizing the input ..
0.00.513.073 I perplexity: tokenization took 7.293 ms
0.00.513.079 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.655.987 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.657.336 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.657.363 I llama_perf_context_print:        load time =     496.80 ms
0.00.657.364 I llama_perf_context_print: prompt eval time =     142.03 ms /   128 tokens (    1.11 ms per token,   901.21 tokens per second)
0.00.657.364 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.657.365 I llama_perf_context_print:       total time =     151.68 ms /   129 tokens
0.00.657.738 I ggml_metal_free: deallocating

real	0m0.671s
user	0m0.081s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.816 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.872 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.873 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.873 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.874 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.875 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.877 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.878 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.878 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.881 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.881 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.718 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.523 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.524 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.525 I llama_model_loader: - type  f32:  194 tensors
0.00.024.525 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.526 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.526 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.527 I print_info: file format = GGUF V3 (latest)
0.00.024.527 I print_info: file type   = Q4_K - Medium
0.00.024.528 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.389 I load: special tokens cache size = 25
0.00.038.432 I load: token to piece cache size = 0.2984 MB
0.00.038.436 I print_info: arch             = gptneox
0.00.038.436 I print_info: vocab_only       = 0
0.00.038.437 I print_info: n_ctx_train      = 2048
0.00.038.437 I print_info: n_embd           = 2048
0.00.038.437 I print_info: n_layer          = 24
0.00.038.441 I print_info: n_head           = 16
0.00.038.442 I print_info: n_head_kv        = 16
0.00.038.442 I print_info: n_rot            = 32
0.00.038.443 I print_info: n_swa            = 0
0.00.038.443 I print_info: n_embd_head_k    = 128
0.00.038.443 I print_info: n_embd_head_v    = 128
0.00.038.444 I print_info: n_gqa            = 1
0.00.038.445 I print_info: n_embd_k_gqa     = 2048
0.00.038.445 I print_info: n_embd_v_gqa     = 2048
0.00.038.446 I print_info: f_norm_eps       = 1.0e-05
0.00.038.446 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.446 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.447 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.447 I print_info: f_logit_scale    = 0.0e+00
0.00.038.447 I print_info: n_ff             = 8192
0.00.038.447 I print_info: n_expert         = 0
0.00.038.447 I print_info: n_expert_used    = 0
0.00.038.448 I print_info: causal attn      = 1
0.00.038.448 I print_info: pooling type     = 0
0.00.038.448 I print_info: rope type        = 2
0.00.038.448 I print_info: rope scaling     = linear
0.00.038.448 I print_info: freq_base_train  = 10000.0
0.00.038.449 I print_info: freq_scale_train = 1
0.00.038.449 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.449 I print_info: rope_finetuned   = unknown
0.00.038.449 I print_info: ssm_d_conv       = 0
0.00.038.449 I print_info: ssm_d_inner      = 0
0.00.038.449 I print_info: ssm_d_state      = 0
0.00.038.450 I print_info: ssm_dt_rank      = 0
0.00.038.450 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.450 I print_info: model type       = 1.4B
0.00.038.450 I print_info: model params     = 1.41 B
0.00.038.451 I print_info: general.name     = 1.4B
0.00.038.451 I print_info: vocab type       = BPE
0.00.038.452 I print_info: n_vocab          = 50304
0.00.038.452 I print_info: n_merges         = 50009
0.00.038.455 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.455 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.455 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.455 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.456 I print_info: LF token         = 187 'Ċ'
0.00.038.456 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.456 I print_info: max token length = 1024
0.00.038.456 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.815 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.827 I load_tensors: offloading output layer to GPU
0.00.538.828 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.862 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.863 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.540.535 I llama_init_from_model: n_seq_max     = 1
0.00.540.537 I llama_init_from_model: n_ctx         = 128
0.00.540.538 I llama_init_from_model: n_ctx_per_seq = 128
0.00.540.539 I llama_init_from_model: n_batch       = 128
0.00.540.539 I llama_init_from_model: n_ubatch      = 128
0.00.540.540 I llama_init_from_model: flash_attn    = 0
0.00.540.542 I llama_init_from_model: freq_base     = 10000.0
0.00.540.543 I llama_init_from_model: freq_scale    = 1
0.00.540.543 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.540.548 I ggml_metal_init: allocating
0.00.540.618 I ggml_metal_init: found device: Apple M4
0.00.540.631 I ggml_metal_init: picking default device: Apple M4
0.00.542.295 I ggml_metal_init: using embedded metal library
0.00.548.942 I ggml_metal_init: GPU name:   Apple M4
0.00.548.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.548.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.548.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.548.952 I ggml_metal_init: simdgroup reduction   = true
0.00.548.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.548.953 I ggml_metal_init: has residency sets    = true
0.00.548.953 I ggml_metal_init: has bfloat            = true
0.00.548.953 I ggml_metal_init: use bfloat            = true
0.00.548.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.548.959 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.568.198 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.571.842 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.571.850 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.571.903 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.575.083 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.575.085 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.575.086 I llama_init_from_model: graph nodes  = 967
0.00.575.086 I llama_init_from_model: graph splits = 2
0.00.575.088 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.575.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.932 I 
0.00.603.018 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.027 I perplexity: tokenizing the input ..
0.00.610.467 I perplexity: tokenization took 7.437 ms
0.00.610.475 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.083 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.745.386 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.745.407 I llama_perf_context_print:        load time =     594.11 ms
0.00.745.407 I llama_perf_context_print: prompt eval time =     132.72 ms /   128 tokens (    1.04 ms per token,   964.42 tokens per second)
0.00.745.408 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.410 I llama_perf_context_print:       total time =     142.48 ms /   129 tokens
0.00.745.789 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.081s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.641 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.650 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.654 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.654 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.655 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.655 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.656 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.533 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.334 I llama_model_loader: - type  f32:  194 tensors
0.00.025.334 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.335 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.336 I print_info: file format = GGUF V3 (latest)
0.00.025.336 I print_info: file type   = Q5_K - Medium
0.00.025.337 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.208 I load: special tokens cache size = 25
0.00.039.152 I load: token to piece cache size = 0.2984 MB
0.00.039.157 I print_info: arch             = gptneox
0.00.039.157 I print_info: vocab_only       = 0
0.00.039.157 I print_info: n_ctx_train      = 2048
0.00.039.157 I print_info: n_embd           = 2048
0.00.039.158 I print_info: n_layer          = 24
0.00.039.162 I print_info: n_head           = 16
0.00.039.162 I print_info: n_head_kv        = 16
0.00.039.163 I print_info: n_rot            = 32
0.00.039.163 I print_info: n_swa            = 0
0.00.039.163 I print_info: n_embd_head_k    = 128
0.00.039.163 I print_info: n_embd_head_v    = 128
0.00.039.164 I print_info: n_gqa            = 1
0.00.039.165 I print_info: n_embd_k_gqa     = 2048
0.00.039.165 I print_info: n_embd_v_gqa     = 2048
0.00.039.166 I print_info: f_norm_eps       = 1.0e-05
0.00.039.166 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.166 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.167 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.167 I print_info: f_logit_scale    = 0.0e+00
0.00.039.167 I print_info: n_ff             = 8192
0.00.039.168 I print_info: n_expert         = 0
0.00.039.168 I print_info: n_expert_used    = 0
0.00.039.168 I print_info: causal attn      = 1
0.00.039.168 I print_info: pooling type     = 0
0.00.039.168 I print_info: rope type        = 2
0.00.039.168 I print_info: rope scaling     = linear
0.00.039.169 I print_info: freq_base_train  = 10000.0
0.00.039.169 I print_info: freq_scale_train = 1
0.00.039.169 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.171 I print_info: rope_finetuned   = unknown
0.00.039.171 I print_info: ssm_d_conv       = 0
0.00.039.171 I print_info: ssm_d_inner      = 0
0.00.039.171 I print_info: ssm_d_state      = 0
0.00.039.172 I print_info: ssm_dt_rank      = 0
0.00.039.172 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.172 I print_info: model type       = 1.4B
0.00.039.172 I print_info: model params     = 1.41 B
0.00.039.172 I print_info: general.name     = 1.4B
0.00.039.173 I print_info: vocab type       = BPE
0.00.039.173 I print_info: n_vocab          = 50304
0.00.039.173 I print_info: n_merges         = 50009
0.00.039.173 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.173 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: LF token         = 187 'Ċ'
0.00.039.174 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.175 I print_info: max token length = 1024
0.00.039.175 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.404 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.419 I load_tensors: offloading output layer to GPU
0.00.584.420 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.456 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.584.459 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.586.216 I llama_init_from_model: n_seq_max     = 1
0.00.586.219 I llama_init_from_model: n_ctx         = 128
0.00.586.220 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.221 I llama_init_from_model: n_batch       = 128
0.00.586.221 I llama_init_from_model: n_ubatch      = 128
0.00.586.222 I llama_init_from_model: flash_attn    = 0
0.00.586.225 I llama_init_from_model: freq_base     = 10000.0
0.00.586.225 I llama_init_from_model: freq_scale    = 1
0.00.586.226 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.230 I ggml_metal_init: allocating
0.00.586.361 I ggml_metal_init: found device: Apple M4
0.00.586.375 I ggml_metal_init: picking default device: Apple M4
0.00.588.312 I ggml_metal_init: using embedded metal library
0.00.594.885 I ggml_metal_init: GPU name:   Apple M4
0.00.594.891 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.893 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.894 I ggml_metal_init: simdgroup reduction   = true
0.00.594.894 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.894 I ggml_metal_init: has residency sets    = true
0.00.594.894 I ggml_metal_init: has bfloat            = true
0.00.594.895 I ggml_metal_init: use bfloat            = true
0.00.594.896 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.278 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.761 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.615.765 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.803 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.053 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.055 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.055 I llama_init_from_model: graph nodes  = 967
0.00.619.056 I llama_init_from_model: graph splits = 2
0.00.619.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.687 I 
0.00.654.772 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.781 I perplexity: tokenizing the input ..
0.00.661.603 I perplexity: tokenization took 6.819 ms
0.00.661.612 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.155 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.804.493 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.804.519 I llama_perf_context_print:        load time =     644.78 ms
0.00.804.520 I llama_perf_context_print: prompt eval time =     140.58 ms /   128 tokens (    1.10 ms per token,   910.49 tokens per second)
0.00.804.520 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.521 I llama_perf_context_print:       total time =     149.83 ms /   129 tokens
0.00.804.880 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.078s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.593 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.598 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.603 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.603 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.603 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.603 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.605 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.607 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.611 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.611 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.612 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.846 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.906 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.989 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.989 I llama_model_loader: - type  f32:  194 tensors
0.00.024.990 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.990 I print_info: file format = GGUF V3 (latest)
0.00.024.991 I print_info: file type   = Q6_K
0.00.024.992 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.958 I load: special tokens cache size = 25
0.00.038.956 I load: token to piece cache size = 0.2984 MB
0.00.038.960 I print_info: arch             = gptneox
0.00.038.960 I print_info: vocab_only       = 0
0.00.038.961 I print_info: n_ctx_train      = 2048
0.00.038.961 I print_info: n_embd           = 2048
0.00.038.961 I print_info: n_layer          = 24
0.00.038.965 I print_info: n_head           = 16
0.00.038.965 I print_info: n_head_kv        = 16
0.00.038.966 I print_info: n_rot            = 32
0.00.038.966 I print_info: n_swa            = 0
0.00.038.966 I print_info: n_embd_head_k    = 128
0.00.038.966 I print_info: n_embd_head_v    = 128
0.00.038.969 I print_info: n_gqa            = 1
0.00.038.969 I print_info: n_embd_k_gqa     = 2048
0.00.038.970 I print_info: n_embd_v_gqa     = 2048
0.00.038.971 I print_info: f_norm_eps       = 1.0e-05
0.00.038.971 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.971 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.971 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.971 I print_info: f_logit_scale    = 0.0e+00
0.00.038.972 I print_info: n_ff             = 8192
0.00.038.972 I print_info: n_expert         = 0
0.00.038.973 I print_info: n_expert_used    = 0
0.00.038.973 I print_info: causal attn      = 1
0.00.038.973 I print_info: pooling type     = 0
0.00.038.973 I print_info: rope type        = 2
0.00.038.973 I print_info: rope scaling     = linear
0.00.038.974 I print_info: freq_base_train  = 10000.0
0.00.038.974 I print_info: freq_scale_train = 1
0.00.038.974 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.974 I print_info: rope_finetuned   = unknown
0.00.038.974 I print_info: ssm_d_conv       = 0
0.00.038.974 I print_info: ssm_d_inner      = 0
0.00.038.975 I print_info: ssm_d_state      = 0
0.00.038.975 I print_info: ssm_dt_rank      = 0
0.00.038.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.975 I print_info: model type       = 1.4B
0.00.038.975 I print_info: model params     = 1.41 B
0.00.038.975 I print_info: general.name     = 1.4B
0.00.038.976 I print_info: vocab type       = BPE
0.00.038.976 I print_info: n_vocab          = 50304
0.00.038.976 I print_info: n_merges         = 50009
0.00.038.976 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.977 I print_info: LF token         = 187 'Ċ'
0.00.038.977 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.978 I print_info: max token length = 1024
0.00.038.978 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.605.459 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.474 I load_tensors: offloading output layer to GPU
0.00.605.474 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.508 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.605.510 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.607.128 I llama_init_from_model: n_seq_max     = 1
0.00.607.132 I llama_init_from_model: n_ctx         = 128
0.00.607.133 I llama_init_from_model: n_ctx_per_seq = 128
0.00.607.133 I llama_init_from_model: n_batch       = 128
0.00.607.133 I llama_init_from_model: n_ubatch      = 128
0.00.607.134 I llama_init_from_model: flash_attn    = 0
0.00.607.136 I llama_init_from_model: freq_base     = 10000.0
0.00.607.136 I llama_init_from_model: freq_scale    = 1
0.00.607.137 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.607.140 I ggml_metal_init: allocating
0.00.607.195 I ggml_metal_init: found device: Apple M4
0.00.607.210 I ggml_metal_init: picking default device: Apple M4
0.00.608.650 I ggml_metal_init: using embedded metal library
0.00.614.925 I ggml_metal_init: GPU name:   Apple M4
0.00.614.929 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.931 I ggml_metal_init: simdgroup reduction   = true
0.00.614.931 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.932 I ggml_metal_init: has residency sets    = true
0.00.614.932 I ggml_metal_init: has bfloat            = true
0.00.614.932 I ggml_metal_init: use bfloat            = true
0.00.614.933 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.050 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.531 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.635.537 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.635.590 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.638.746 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.638.748 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.638.748 I llama_init_from_model: graph nodes  = 967
0.00.638.748 I llama_init_from_model: graph splits = 2
0.00.638.751 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.638.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.940 I 
0.00.673.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.032 I perplexity: tokenizing the input ..
0.00.680.128 I perplexity: tokenization took 7.093 ms
0.00.680.144 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.858 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.812.259 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.812.281 I llama_perf_context_print:        load time =     664.11 ms
0.00.812.282 I llama_perf_context_print: prompt eval time =     130.17 ms /   128 tokens (    1.02 ms per token,   983.31 tokens per second)
0.00.812.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.283 I llama_perf_context_print:       total time =     139.34 ms /   129 tokens
0.00.812.611 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.078s
sys	0m0.140s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.431 I build: 4796 (80c41ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.361 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.852 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.858 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.861 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.861 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.862 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.863 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.867 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.913 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.456 I llama_model_loader: - type  f32:  194 tensors
0.00.055.457 I llama_model_loader: - type  f16:   98 tensors
0.00.055.457 I print_info: file format = GGUF V3 (latest)
0.00.055.458 I print_info: file type   = all F32 (guessed)
0.00.055.459 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.392 I load: special tokens cache size = 25
0.00.075.403 I load: token to piece cache size = 0.2984 MB
0.00.075.406 I print_info: arch             = gptneox
0.00.075.406 I print_info: vocab_only       = 0
0.00.075.407 I print_info: n_ctx_train      = 2048
0.00.075.407 I print_info: n_embd           = 2048
0.00.075.407 I print_info: n_layer          = 24
0.00.075.410 I print_info: n_head           = 16
0.00.075.411 I print_info: n_head_kv        = 16
0.00.075.411 I print_info: n_rot            = 32
0.00.075.414 I print_info: n_swa            = 0
0.00.075.414 I print_info: n_embd_head_k    = 128
0.00.075.414 I print_info: n_embd_head_v    = 128
0.00.075.415 I print_info: n_gqa            = 1
0.00.075.416 I print_info: n_embd_k_gqa     = 2048
0.00.075.416 I print_info: n_embd_v_gqa     = 2048
0.00.075.421 I print_info: f_norm_eps       = 1.0e-05
0.00.075.421 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.422 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.422 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.422 I print_info: f_logit_scale    = 0.0e+00
0.00.075.423 I print_info: n_ff             = 8192
0.00.075.423 I print_info: n_expert         = 0
0.00.075.423 I print_info: n_expert_used    = 0
0.00.075.424 I print_info: causal attn      = 1
0.00.075.424 I print_info: pooling type     = 0
0.00.075.424 I print_info: rope type        = 2
0.00.075.424 I print_info: rope scaling     = linear
0.00.075.425 I print_info: freq_base_train  = 10000.0
0.00.075.425 I print_info: freq_scale_train = 1
0.00.075.425 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.425 I print_info: rope_finetuned   = unknown
0.00.075.425 I print_info: ssm_d_conv       = 0
0.00.075.425 I print_info: ssm_d_inner      = 0
0.00.075.426 I print_info: ssm_d_state      = 0
0.00.075.426 I print_info: ssm_dt_rank      = 0
0.00.075.426 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.426 I print_info: model type       = 1.4B
0.00.075.427 I print_info: model params     = 1.41 B
0.00.075.427 I print_info: general.name     = 1.4B
0.00.075.427 I print_info: vocab type       = BPE
0.00.075.427 I print_info: n_vocab          = 50304
0.00.075.428 I print_info: n_merges         = 50009
0.00.075.428 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.428 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.428 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.429 I print_info: LF token         = 187 'Ċ'
0.00.075.429 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.429 I print_info: max token length = 1024
0.00.075.430 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.307.903 I load_tensors: offloading 24 repeating layers to GPU
0.01.307.906 I load_tensors: offloading output layer to GPU
0.01.307.907 I load_tensors: offloaded 25/25 layers to GPU
0.01.307.933 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.307.935 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.308.933 I llama_init_from_model: n_seq_max     = 1
0.01.308.934 I llama_init_from_model: n_ctx         = 128
0.01.308.934 I llama_init_from_model: n_ctx_per_seq = 128
0.01.308.935 I llama_init_from_model: n_batch       = 128
0.01.308.935 I llama_init_from_model: n_ubatch      = 128
0.01.308.935 I llama_init_from_model: flash_attn    = 0
0.01.308.936 I llama_init_from_model: freq_base     = 10000.0
0.01.308.936 I llama_init_from_model: freq_scale    = 1
0.01.308.936 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.308.939 I ggml_metal_init: allocating
0.01.308.973 I ggml_metal_init: found device: Apple M4
0.01.308.978 I ggml_metal_init: picking default device: Apple M4
0.01.310.031 I ggml_metal_init: using embedded metal library
0.01.314.388 I ggml_metal_init: GPU name:   Apple M4
0.01.314.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.314.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.314.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.314.391 I ggml_metal_init: simdgroup reduction   = true
0.01.314.391 I ggml_metal_init: simdgroup matrix mul. = true
0.01.314.392 I ggml_metal_init: has residency sets    = true
0.01.314.392 I ggml_metal_init: has bfloat            = true
0.01.314.392 I ggml_metal_init: use bfloat            = true
0.01.314.392 I ggml_metal_init: hasUnifiedMemory      = true
0.01.314.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.348.281 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.350.550 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.350.552 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.350.579 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.352.449 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.352.451 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.352.451 I llama_init_from_model: graph nodes  = 967
0.01.352.452 I llama_init_from_model: graph splits = 2
0.01.352.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.352.453 I 
0.01.352.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.352.497 I compute_imatrix: tokenizing the input ..
0.01.357.391 I compute_imatrix: tokenization took 4.893 ms
0.01.357.394 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.570.896 I compute_imatrix: 0.21 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.574.343 I llama_perf_context_print:        load time =    1546.54 ms
0.01.574.344 I llama_perf_context_print: prompt eval time =     211.53 ms /   128 tokens (    1.65 ms per token,   605.12 tokens per second)
0.01.574.344 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.574.345 I llama_perf_context_print:       total time =    1549.98 ms /   129 tokens
0.01.574.919 I ggml_metal_free: deallocating

real	0m1.765s
user	0m0.128s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4796 (80c41ddd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134106b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134109f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13410a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13410aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13410af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13410b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13410ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13410bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13410c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13410c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13410cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13410d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13410d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13410e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13410e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13410f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13410f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13410ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134110630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134110fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134111700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134111e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134112540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134112c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134113380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134113640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134113c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1341148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134114e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1341150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134115560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134115820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1341160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1341165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1341168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134116d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1341171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134117690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134117b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134117fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134118470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134118910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134118db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134119250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134119510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134119b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13411a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13411aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13411b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13411b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13411bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13411c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13411c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13411ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13411d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13411db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13411dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13411e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13411e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13411f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13411f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13411f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13411fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134120140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1341205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134120a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134120f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1341213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134121860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134121d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1341221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134122640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134122ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134123030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134123580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134123ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134124020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134124570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134124ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134125010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134125560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134125ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134126000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134126550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134126aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134126ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134127540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134127a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134127fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134128530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134128a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134128fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134129520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134129a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134129fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13412a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13412aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13411a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13412aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13412b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13412bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13412c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13412c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13412cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13412d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13412d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13412dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13412e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13412e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13412eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13412f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13412f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13412fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134130030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1341304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134130970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134130e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1341312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134131750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134131bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134132090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134132530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1341329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134132e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134133310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1341337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134133c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1341340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134134590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134134a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134134ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134135370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134135810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134135cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134136150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1341365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134136a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134136f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1341373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134137870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134137d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1341381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134138650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134138af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134138f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134139430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1341398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134139d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13413a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13413a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13413ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13413aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13413b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13413b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13413bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13413c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13413c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13413cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13413d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13413d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13413d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13413de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13413e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13413e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13413ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13413f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13413f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13413f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13413fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134140330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1341407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134140c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134141110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1341415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134141a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134141ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134142390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134142830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134142cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134143170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134143610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134143ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134143f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1341443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134144890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134144d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1341451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134145670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134145b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134145fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134146450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1341468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134146d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1341472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134147830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134147d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1341482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134148590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134148ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1341491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1341497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134149fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13414a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13414a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13414ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13414b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13414bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13414bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13414c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13414c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13414d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13414d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13414db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13414e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13414e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13414eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13414f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13414f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13414fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134150080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1341505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134150b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134151070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1341515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134151b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134152060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1341525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134152b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134153050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1341535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134153af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134154040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134154590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134154ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134155030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134155580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134155ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134156020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134156570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134156ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134157010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134157560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134157ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134158000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134158550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134158aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134158ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134159540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134159a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134159fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13415a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13415aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13415afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13415b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13415ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13415bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13415c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13415ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13415cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13415d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13415da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13415dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13415e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13415ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13415ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13415f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13415fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13415fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134160370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134160810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134160cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134161150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1341615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134161a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134161f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1341623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134162870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134162d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1341631b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134163650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134163af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134163f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x134164430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1341648d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x134164d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x134165210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1341656b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x134165b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x134165ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x134166490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x134166930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x134166dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134167320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134167a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134168160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134168880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134168fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134169260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134169a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134169d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13416a320 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.718.152 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.155 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134169fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134148e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134148850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134149470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13411c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13411bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134113900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13411a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13411ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13411b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134119de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1341197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13411b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134105800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13411eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13412b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134169520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134115c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134149a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134113f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1341141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134114490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13416a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13416aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13416ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13416afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13416b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13416b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13416b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13416bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13416bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13416c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13416c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13416c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13416c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13416cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13416ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13416d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13416d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13416d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13416d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13416dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13416de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13416e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13416e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13416e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13416e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13416ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13416ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13416f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13416f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13416f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13416fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13416fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13416ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134170240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134170500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1341707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134170a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134170d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134171000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1341712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134171580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134171840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134171b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134171dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134172080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134172340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134172600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1341728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134172b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134172e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134173100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1341733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134173680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134173940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134173c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134173ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134174180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134174440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134174700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1341749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134174c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134174f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134175200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1341754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134175780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134175a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134175d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134175fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134176280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134176540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134176800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134176ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134176d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134177040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134177300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1341775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134177880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134177b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134177e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1341780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134178380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134178640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134178900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134178bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134178e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134179140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134179400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1341796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134179980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134179c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134179f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13417a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13417a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13417a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13417aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13417acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13417af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13417b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13417b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13417b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13417ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13417bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13417c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13417c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13417c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13417c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13417cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13417cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13417d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13417d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13417d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13417d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13417db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13417de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13417e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13417e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13417e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13417e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13417ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13417eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13417f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13417f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13417f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13417f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13417fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13417ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134180200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1341804c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134180780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134180a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134180d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134180fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134181280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134181540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134181800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134181ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134181d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134182040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134182300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1341825c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134182880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134182b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134182e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1341830c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134183380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134183640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134183900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134183bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134183e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134184140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134184400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1341846c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134184980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134184c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134184f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1341851c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134185480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134185740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134185a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134185cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134185f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134186240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134186500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1341867c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134186a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134186d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134187000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1341872c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134187580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134187840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134187b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134187dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134188080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134188340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134188600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1341888c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134188b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134188e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134189100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1341893c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134189680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134189940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134189c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134189ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13418a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13418a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13418a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13418a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13418af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13418b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13418b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13418b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13418ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13418bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13418c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13418c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13418c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13418c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13418cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13418cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13418d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13418d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13418d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13418d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13418db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13418de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13418e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13418e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13418e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13418e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13418ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13418eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13418f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13418f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13418f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13418f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13418fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13418ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134190210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1341904d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134190790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134190a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134190d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134190fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134191290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134191550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134191810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134191ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134191d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134192050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134192310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134192860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134192db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134193300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134193850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134193da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1341942f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134194840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134194d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1341952e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134195830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134195d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1341962d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134196820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134196d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134197030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1341972f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1341977f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134197cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1341981f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1341986f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134198bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1341990f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1341995f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134199af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134199ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13419a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13419a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13419aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13419b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13419b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13419bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13419c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13419c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13419ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13419d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13419d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13419dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13419e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13419e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13419eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13419f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13419fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1341a0340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1341a0a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1341a0d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1341a1510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1341a17d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1341a1de0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117f06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117f07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117f07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117f08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117f08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117f09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117f09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117f0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117f0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117f0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117f0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117f0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117f0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117f0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117f0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117f0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117f0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117f0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117f0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117f0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117f0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117f0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117f0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117f0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117f0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117f102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117f10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117f10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117f10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117f11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117f118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117f11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117f12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117f12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117f12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117f13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117f137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117f13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117f140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117f14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117f149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117f14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117f15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117f156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117f15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117f15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117f16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117f16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117f17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117f17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117f17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117f18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117f184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117f18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117f18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117f19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117f196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117f19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117f19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117f1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117f1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117f1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117f1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117f1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117f1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117f1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117f1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117f1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117f1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117f1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117f1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117f1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117f1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117f1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117f1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117f1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117f1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117f1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117f1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117f1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117f20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117f20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117f20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117f20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117f212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117f21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117f21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117f22420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117f22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117f22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117f234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117f23a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117f24000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117f245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117f24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117f25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117f256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117f25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117f26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117f267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117f26d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117f27330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117f278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117f27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117f282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117f28ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117f291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117f296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117f29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117f2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117f2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117f2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117f2afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117f2b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117f2b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117f2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117f2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117f2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117f2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117f2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117f2d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117f2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117f2e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117f2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117f2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117f2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117f2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117f2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117f2ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117f304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117f309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117f30ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117f313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117f318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117f31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117f322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117f327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117f32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117f331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117f336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117f33be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117f340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117f345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117f34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117f34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117f354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117f359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117f35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117f363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117f368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117f36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117f372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117f377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117f37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117f381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117f386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117f390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117f395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117f39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117f39fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117f3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117f3a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117f3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117f3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117f3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117f3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117f3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117f3cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117f3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117f3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117f3dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117f3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117f3e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117f3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117f3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117f3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117f3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117f403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117f408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117f40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117f41440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117f419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117f41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117f425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117f42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117f431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117f439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117f43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117f44120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117f44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117f44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117f45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117f459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117f45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117f46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117f46ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117f47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117f47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117f47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117f48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117f48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117f48aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117f48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117f49540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117f49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117f49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117f4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117f4aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117f4afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117f4b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117f4ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117f4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117f4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117f4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117f4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117f4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117f4da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117f4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117f4e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117f4ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117f4ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117f4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117f4fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117f4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117f504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117f50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117f50f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117f514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117f51a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117f51f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117f524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117f52a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117f52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117f534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117f539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117f53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117f54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117f549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117f54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117f55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117f559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117f55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117f56470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117f569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117f56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117f57460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117f579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117f57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117f58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117f589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117f58ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117f59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117f598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117f59d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117f5a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117f5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117f5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117f5b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117f5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117f5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117f5bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117f5c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117f5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117f5cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117f5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117f5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117f5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x117f5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x117f5e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x117f5e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x117f5ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x117f5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x117f5f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x117f5fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x117f5fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x117f60340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x117f607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117f60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117f61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117f61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117f62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117f629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117f62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117f63460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117f63720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117f63d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.766s
user	0m0.278s
sys	0m0.309s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4796 (80c41ddd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123f10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123f11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123f11640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123f11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123f121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123f12750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123f12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123f132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123f13860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123f13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123f14760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123f15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123f15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123f16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123f16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123f17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123f177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123f17ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123f18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123f18db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123f194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123f1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123f1abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123f1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123f1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123f1c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123f1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123f1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123f1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123f1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123f1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123f1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123f1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123f1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123f1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123f1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123f1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123f1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123f20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123f205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123f20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123f20d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123f21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123f21960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123f22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123f22890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123f22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123f234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123f23ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123f240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123f246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123f24ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123f25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123f25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123f25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123f260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123f26b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123f27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123f274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123f27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123f27e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123f282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123f28750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123f28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123f29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123f299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123f29e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123f2a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123f2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123f2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123f2b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123f2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123f2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123f2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123f2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123f2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123f2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123f2d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123f2dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123f2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123f2e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123f2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123f2f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123f2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123f2fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123f302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123f30800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123f30d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123f312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123f317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123f31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123f32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123f21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123f32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123f32eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123f33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123f33950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123f33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123f343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123f34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123f34e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123f353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123f35930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123f363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123f36920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123f36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123f373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123f37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123f37d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123f381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123f38640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123f38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123f38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123f39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123f398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123f39d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123f3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123f3a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123f3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123f3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123f3b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123f3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123f3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123f3c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123f3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123f3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123f3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123f3d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123f3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123f3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123f3e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123f3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123f3ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123f3f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123f3f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123f3fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123f40320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123f407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123f40c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123f41100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123f415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123f41a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123f41ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123f42380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123f42820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123f42cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123f43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123f43600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123f43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123f443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123f44880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123f44d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123f451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123f45660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123f45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123f45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123f46440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123f468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123f46d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123f47220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123f476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123f47b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123f48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123f484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123f48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123f48de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123f49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123f49720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123f49bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123f4a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123f4a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123f4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123f4ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123f4b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123f4b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123f4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123f4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123f4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123f4ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123f4cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123f4d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123f4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123f4dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123f4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123f4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123f4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123f4f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123f4f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123f4fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123f4fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123f503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123f509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123f50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123f517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123f51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123f51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123f52550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123f52b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123f53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123f537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123f53c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123f54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123f548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123f54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123f55380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123f558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123f55e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123f56370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123f568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123f56e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123f57360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123f578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123f57e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123f58350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123f588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123f58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123f59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123f59890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123f59de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123f5a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123f5a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123f5add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123f5b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123f5b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123f5bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123f5c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123f5c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123f5cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123f5d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123f5d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123f5dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123f5e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123f5e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123f5ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123f5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123f5f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123f5fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123f602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123f60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123f60d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123f612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123f61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123f61d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123f622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123f62800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123f62d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123f632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123f637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123f63d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123f64290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123f647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123f64d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123f65280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123f657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123f65d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123f66270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123f667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123f66d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123f67260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123f67700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123f67ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123f68040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123f684e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123f68980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123f68e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123f692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123f69760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123f69c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123f6a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123f6a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123f6a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123f6ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123f6b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123f6b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x123f6bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x123f6c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x123f6c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x123f6ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x123f6cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x123f6d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x123f6d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x123f6dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x123f6e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x123f6e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123f6eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123f6f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123f6f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123f700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123f707d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123f70a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123f71280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123f71540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123f71b50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.552 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.557 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125004be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125005050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1250054c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125005930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125005da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125006210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125006680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125006af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125006f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1250073d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125007840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125007f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125008a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1250091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1250099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12500a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12500a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12500af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12500b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12500be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12500c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12500cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12500d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12500dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12500e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12500e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12500e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12500ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12500f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12500f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12500f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12500fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1250102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125010570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1250109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125010e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1250112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125011730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125011ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125012010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125012480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1250128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125012d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1250131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125013640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125013ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125013f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125014390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125014800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125014c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1250150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125015550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1250159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125015e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1250162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125016710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125016c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125017180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1250175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125017a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125017ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125018340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1250187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125018c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125019090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125019500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125019970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125019de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12501a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12501a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12501ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12501afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12501b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12501b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12501bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12501c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12501c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12501ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12501ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12501d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12501d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12501dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12501e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12501e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12501e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12501edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12501f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12501f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12501fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12501ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1250203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125020860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125020cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125021140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1250215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125021a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125021e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125022300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125022770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125022be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125023050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1250234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125023930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125023da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125024210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125024680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125024af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125024f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1250253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125025840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125025cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125026120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125026590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125026a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125026e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1250272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125027750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125027bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125028030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1250284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125028910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125028d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1250291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125029660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125029ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125029f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12502a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12502a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12502ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12502b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12502b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12502b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12502be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12502c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12502c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12502cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12502d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12502d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12502d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12502dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12502e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12502e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12502eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12502ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12502f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12502f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12502fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1250300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125030550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1250309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125030e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1250312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125031710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125031b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125031ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125032460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1250328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125032d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1250331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125033620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125033a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125033f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125034370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1250347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125034c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1250350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125035cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125035fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125036270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1250366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125036b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125036fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125037430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1250378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125038180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1250385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125038ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1250397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12503a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12503a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12503a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12503ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12503b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12503b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12503bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12503bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12503c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12503c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12503ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12503d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12503d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12503da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12503deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12503e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12503e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12503ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12503f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12503f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12503fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12503ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1250403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125040830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125040ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125041110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125041630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125041b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1250426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125042970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125042f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1250434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125043ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125044070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125044630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125044bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1250451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125045770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125045d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1250462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1250468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125046e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125047430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1250479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125047fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125048570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125048b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1250490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1250496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125049c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12504a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12504a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12504adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12504b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12504b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12504bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12504c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12504ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12504d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12504d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12504dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12504e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12504e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12504ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12504f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12504f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12504fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1250503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1250509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125050f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125051530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125051af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1250520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125052670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125052c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1250531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1250537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125053d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125054330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1250548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125054eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125055470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125055a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125055ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1250565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125056b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125057070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125057570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125057a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125057f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125058470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125058970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125058e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125059370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125059870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125059d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12505a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12505a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12505ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12505b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12505b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12505bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12505c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12505c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12505ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12505cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12505d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12505d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12505de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12505e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12505e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12505f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12505f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1250600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1250607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125060aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125061290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125061550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125061b60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123e053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123e05820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123e05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123e06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123e06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123e069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123e06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123e072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123e07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123e07cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123e08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123e087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123e092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123e09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123e0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123e0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123e0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123e0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123e0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123e0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123e0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123e0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123e0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123e0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123e0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123e0ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123e0eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123e0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123e0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123e0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123e101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123e106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123e10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123e10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123e11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123e116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123e11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123e11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123e12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123e128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123e12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123e13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123e13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123e13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123e13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123e14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123e147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123e14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123e150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123e15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123e15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123e15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123e16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123e166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123e16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123e16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123e17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123e17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123e17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123e18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123e18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123e18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123e19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123e194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123e19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123e19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123e1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123e1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123e1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123e1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123e1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123e1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123e1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123e1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123e1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f004280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f0046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f004b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f004fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f005440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f0058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f005d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f006190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f006600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f006a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f006ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f007350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f0077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f007c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f0080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f008510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f008980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f008df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f009260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f0096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f009b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f009fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f00a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f00ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f00b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f00b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f00bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f00c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f00c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f00ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f00d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f00d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f00df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f00e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f00eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f00f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f00f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f00fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f010170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f010670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f010b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f011070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f011570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f011a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f011f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f012470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f012970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f012e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f013370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f013870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f013d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f014270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f014770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f014c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f015170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f015670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f015b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f016070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f016570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f016a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f016f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f017470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f017970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f017e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f018370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f018870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f018d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f019270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f019770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f019c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f01a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f01a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f01ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f01b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f01b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f01ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f01bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f01c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f01c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f01ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f01d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f01d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f01dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f01e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f01e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f01ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f01f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f01f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f01fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f020070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f020570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f020a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f020f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f021470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f021970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f021e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f022370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f022870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f022d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f023270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f023770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f023c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f024170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f024670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f024b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f025070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f025570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f025a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f025f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f026470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f026970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f026e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f027370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f027870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f027d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f028270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f028770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f028c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f029170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f029720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f029cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f02a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f02a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f02ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f02b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f02ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f02c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f02c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f02c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f02cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f02d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f02ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f02e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f02e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f02eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f02f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f02f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f02fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f030340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f030890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f030de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f031330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f031880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f031dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f032320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f032870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f032dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f033310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f033860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f033db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f034300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f034850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f034da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f0352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f035840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f035d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f0362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f036830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f036d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f0372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f037820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f037d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f0382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f038810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f038d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f0392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f039800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f039d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f03a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f03a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f03ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f03b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f03b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f03bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f03c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f03c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f03cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f03d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f03d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f03dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f03e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f03e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f03ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f03f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f03f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f03fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f040240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f040790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f040ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f041230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f041780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f041cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f042170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f042610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f042ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f042f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f0433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f043890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f043d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f0441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f044670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f044fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f045450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f0458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f045d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f046230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11f0466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11f046b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11f047010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11f0474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11f047950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11f047df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11f048290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11f048730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11f048bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11f049070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f0495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f049ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f04a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f04ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f04b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f04b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f04bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f04bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f04c5c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.958s
user	0m0.231s
sys	0m0.189s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
