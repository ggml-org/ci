### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.38 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.53 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.75 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.11 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.32 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.79 sec*proc (28 tests)

Total Test time (real) = 221.80 sec

real	3m41.833s
user	7m40.887s
sys	0m6.175s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.12 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.32 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.12 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.08 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.28 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.09 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.27 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  50.93 sec*proc (28 tests)

Total Test time (real) =  50.94 sec

real	0m50.950s
user	1m10.993s
sys	0m5.435s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.086 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.274 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.732 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.738 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.738 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.739 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.739 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.740 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.740 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.741 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.741 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.741 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.742 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.745 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.745 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.748 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.748 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.748 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.749 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.749 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.022.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.023.461 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.462 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.023.462 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.023.463 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.023.463 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.023.463 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.023.464 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.023.464 I llama_model_loader: - type  f32:  124 tensors
0.00.023.464 I llama_model_loader: - type  f16:   73 tensors
0.00.025.740 I llm_load_vocab: special tokens cache size = 5
0.00.026.924 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.026.947 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.026.948 I llm_load_print_meta: arch             = bert
0.00.026.948 I llm_load_print_meta: vocab type       = WPM
0.00.026.948 I llm_load_print_meta: n_vocab          = 30522
0.00.026.949 I llm_load_print_meta: n_merges         = 0
0.00.026.949 I llm_load_print_meta: vocab_only       = 0
0.00.026.949 I llm_load_print_meta: n_ctx_train      = 512
0.00.026.949 I llm_load_print_meta: n_embd           = 384
0.00.026.949 I llm_load_print_meta: n_layer          = 12
0.00.026.953 I llm_load_print_meta: n_head           = 12
0.00.026.954 I llm_load_print_meta: n_head_kv        = 12
0.00.026.954 I llm_load_print_meta: n_rot            = 32
0.00.026.954 I llm_load_print_meta: n_swa            = 0
0.00.026.954 I llm_load_print_meta: n_embd_head_k    = 32
0.00.026.955 I llm_load_print_meta: n_embd_head_v    = 32
0.00.026.955 I llm_load_print_meta: n_gqa            = 1
0.00.026.956 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.026.956 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.026.957 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.026.958 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.026.958 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.026.958 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.026.958 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.026.959 I llm_load_print_meta: n_ff             = 1536
0.00.026.959 I llm_load_print_meta: n_expert         = 0
0.00.026.959 I llm_load_print_meta: n_expert_used    = 0
0.00.026.959 I llm_load_print_meta: causal attn      = 0
0.00.026.959 I llm_load_print_meta: pooling type     = 2
0.00.026.959 I llm_load_print_meta: rope type        = 2
0.00.026.960 I llm_load_print_meta: rope scaling     = linear
0.00.026.960 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.026.962 I llm_load_print_meta: freq_scale_train = 1
0.00.026.962 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.026.963 I llm_load_print_meta: rope_finetuned   = unknown
0.00.026.963 I llm_load_print_meta: ssm_d_conv       = 0
0.00.026.963 I llm_load_print_meta: ssm_d_inner      = 0
0.00.026.963 I llm_load_print_meta: ssm_d_state      = 0
0.00.026.963 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.026.963 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.026.963 I llm_load_print_meta: model type       = 33M
0.00.026.964 I llm_load_print_meta: model ftype      = F16
0.00.026.964 I llm_load_print_meta: model params     = 33.21 M
0.00.026.965 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.026.965 I llm_load_print_meta: general.name     = Bge Small
0.00.026.965 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.026.966 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.026.967 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.026.967 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.026.967 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.026.967 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.026.968 I llm_load_print_meta: max token length = 21
0.00.028.180 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.028.180 I llm_load_tensors: offloading output layer to GPU
0.00.028.180 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.028.202 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.028.204 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.028.537 I llama_new_context_with_model: n_seq_max     = 1
0.00.028.538 I llama_new_context_with_model: n_ctx         = 512
0.00.028.538 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.028.538 I llama_new_context_with_model: n_batch       = 2048
0.00.028.539 I llama_new_context_with_model: n_ubatch      = 2048
0.00.028.539 I llama_new_context_with_model: flash_attn    = 0
0.00.028.539 I llama_new_context_with_model: freq_base     = 10000.0
0.00.028.539 I llama_new_context_with_model: freq_scale    = 1
0.00.028.540 I ggml_metal_init: allocating
0.00.028.543 I ggml_metal_init: found device: Apple M4
0.00.028.545 I ggml_metal_init: picking default device: Apple M4
0.00.029.176 I ggml_metal_init: using embedded metal library
0.00.031.629 I ggml_metal_init: GPU name:   Apple M4
0.00.031.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.031.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.031.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.031.632 I ggml_metal_init: simdgroup reduction   = true
0.00.031.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.031.632 I ggml_metal_init: has bfloat            = true
0.00.031.633 I ggml_metal_init: use bfloat            = true
0.00.031.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.031.634 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.041.928 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.042.417 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.042.419 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.042.420 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.043.035 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.043.036 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.043.036 I llama_new_context_with_model: graph nodes  = 429
0.00.043.036 I llama_new_context_with_model: graph splits = 2
0.00.043.058 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.043.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.048.310 I 
0.00.048.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.048.899 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.053.131 I llama_perf_context_print:        load time =      30.03 ms
0.00.053.133 I llama_perf_context_print: prompt eval time =       4.12 ms /     9 tokens (    0.46 ms per token,  2187.12 tokens per second)
0.00.053.134 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.053.134 I llama_perf_context_print:       total time =       4.82 ms /    10 tokens
0.00.053.340 I ggml_metal_free: deallocating

real	0m0.226s
user	0m0.035s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.033 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.731 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.517 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.522 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.523 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.525 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.525 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.526 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.526 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.527 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.528 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.529 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.529 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.531 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.532 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.532 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.533 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.533 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.533 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.534 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.012.659 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.258 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.259 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.259 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.259 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.260 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.260 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.260 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.261 I llama_model_loader: - type  f32:  124 tensors
0.00.013.261 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.562 I llm_load_vocab: special tokens cache size = 5
0.00.016.754 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.016.764 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.016.765 I llm_load_print_meta: arch             = bert
0.00.016.765 I llm_load_print_meta: vocab type       = WPM
0.00.016.766 I llm_load_print_meta: n_vocab          = 30522
0.00.016.766 I llm_load_print_meta: n_merges         = 0
0.00.016.766 I llm_load_print_meta: vocab_only       = 0
0.00.016.766 I llm_load_print_meta: n_ctx_train      = 512
0.00.016.766 I llm_load_print_meta: n_embd           = 384
0.00.016.767 I llm_load_print_meta: n_layer          = 12
0.00.016.770 I llm_load_print_meta: n_head           = 12
0.00.016.770 I llm_load_print_meta: n_head_kv        = 12
0.00.016.771 I llm_load_print_meta: n_rot            = 32
0.00.016.771 I llm_load_print_meta: n_swa            = 0
0.00.016.771 I llm_load_print_meta: n_embd_head_k    = 32
0.00.016.779 I llm_load_print_meta: n_embd_head_v    = 32
0.00.016.780 I llm_load_print_meta: n_gqa            = 1
0.00.016.781 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.016.781 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.016.782 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.016.783 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.016.783 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.016.783 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.016.783 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.016.784 I llm_load_print_meta: n_ff             = 1536
0.00.016.784 I llm_load_print_meta: n_expert         = 0
0.00.016.784 I llm_load_print_meta: n_expert_used    = 0
0.00.016.784 I llm_load_print_meta: causal attn      = 0
0.00.016.784 I llm_load_print_meta: pooling type     = 2
0.00.016.784 I llm_load_print_meta: rope type        = 2
0.00.016.785 I llm_load_print_meta: rope scaling     = linear
0.00.016.785 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.016.785 I llm_load_print_meta: freq_scale_train = 1
0.00.016.785 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.016.786 I llm_load_print_meta: rope_finetuned   = unknown
0.00.016.787 I llm_load_print_meta: ssm_d_conv       = 0
0.00.016.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.016.788 I llm_load_print_meta: ssm_d_state      = 0
0.00.016.788 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.016.788 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.016.788 I llm_load_print_meta: model type       = 33M
0.00.016.788 I llm_load_print_meta: model ftype      = Q8_0
0.00.016.789 I llm_load_print_meta: model params     = 33.21 M
0.00.016.789 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.016.789 I llm_load_print_meta: general.name     = Bge Small
0.00.016.792 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.016.792 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.016.792 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.016.792 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.016.792 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.016.792 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.016.793 I llm_load_print_meta: max token length = 21
0.00.017.933 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.017.933 I llm_load_tensors: offloading output layer to GPU
0.00.017.934 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.017.942 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.017.943 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.018.294 I llama_new_context_with_model: n_seq_max     = 1
0.00.018.294 I llama_new_context_with_model: n_ctx         = 512
0.00.018.295 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.018.295 I llama_new_context_with_model: n_batch       = 2048
0.00.018.295 I llama_new_context_with_model: n_ubatch      = 2048
0.00.018.295 I llama_new_context_with_model: flash_attn    = 0
0.00.018.296 I llama_new_context_with_model: freq_base     = 10000.0
0.00.018.296 I llama_new_context_with_model: freq_scale    = 1
0.00.018.297 I ggml_metal_init: allocating
0.00.018.302 I ggml_metal_init: found device: Apple M4
0.00.018.306 I ggml_metal_init: picking default device: Apple M4
0.00.018.925 I ggml_metal_init: using embedded metal library
0.00.021.282 I ggml_metal_init: GPU name:   Apple M4
0.00.021.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.284 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.284 I ggml_metal_init: simdgroup reduction   = true
0.00.021.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.285 I ggml_metal_init: has bfloat            = true
0.00.021.285 I ggml_metal_init: use bfloat            = true
0.00.021.285 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.391 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.031.871 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.873 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.875 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.456 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.457 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.457 I llama_new_context_with_model: graph nodes  = 429
0.00.032.457 I llama_new_context_with_model: graph splits = 2
0.00.032.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.032.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.011 I 
0.00.037.033 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.037.561 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.911 I llama_perf_context_print:        load time =      28.28 ms
0.00.041.912 I llama_perf_context_print: prompt eval time =       4.22 ms /     9 tokens (    0.47 ms per token,  2131.19 tokens per second)
0.00.041.913 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.914 I llama_perf_context_print:       total time =       4.90 ms /    10 tokens
0.00.042.107 I ggml_metal_free: deallocating

real	0m0.053s
user	0m0.028s
sys	0m0.014s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.157 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.798 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.233 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.239 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.241 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.243 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.244 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.245 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.246 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.247 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.248 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.249 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.249 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.253 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.254 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.254 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.799 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.799 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.800 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.800 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.800 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.801 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.801 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.801 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.802 I llama_model_loader: - type  f32:   40 tensors
0.00.048.804 I llama_model_loader: - type  f16:   30 tensors
0.00.067.224 W llm_load_vocab: empty token at index 5
0.00.071.956 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.073.332 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.367 I llm_load_vocab: special tokens cache size = 5
0.00.330.298 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.330.305 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.330.306 I llm_load_print_meta: arch             = jina-bert-v2
0.00.330.307 I llm_load_print_meta: vocab type       = BPE
0.00.330.307 I llm_load_print_meta: n_vocab          = 61056
0.00.330.307 I llm_load_print_meta: n_merges         = 39382
0.00.330.307 I llm_load_print_meta: vocab_only       = 0
0.00.330.308 I llm_load_print_meta: n_ctx_train      = 8192
0.00.330.308 I llm_load_print_meta: n_embd           = 384
0.00.330.308 I llm_load_print_meta: n_layer          = 4
0.00.330.317 I llm_load_print_meta: n_head           = 12
0.00.330.319 I llm_load_print_meta: n_head_kv        = 12
0.00.330.319 I llm_load_print_meta: n_rot            = 32
0.00.330.319 I llm_load_print_meta: n_swa            = 0
0.00.330.319 I llm_load_print_meta: n_embd_head_k    = 32
0.00.330.319 I llm_load_print_meta: n_embd_head_v    = 32
0.00.330.320 I llm_load_print_meta: n_gqa            = 1
0.00.330.321 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.330.324 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.330.325 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.330.326 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.330.327 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.330.327 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.330.327 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.330.328 I llm_load_print_meta: n_ff             = 1536
0.00.330.328 I llm_load_print_meta: n_expert         = 0
0.00.330.328 I llm_load_print_meta: n_expert_used    = 0
0.00.330.328 I llm_load_print_meta: causal attn      = 0
0.00.330.328 I llm_load_print_meta: pooling type     = -1
0.00.330.328 I llm_load_print_meta: rope type        = -1
0.00.330.329 I llm_load_print_meta: rope scaling     = linear
0.00.330.329 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.330.329 I llm_load_print_meta: freq_scale_train = 1
0.00.330.329 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.330.330 I llm_load_print_meta: rope_finetuned   = unknown
0.00.330.330 I llm_load_print_meta: ssm_d_conv       = 0
0.00.330.330 I llm_load_print_meta: ssm_d_inner      = 0
0.00.330.330 I llm_load_print_meta: ssm_d_state      = 0
0.00.330.330 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.330.330 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.330.335 I llm_load_print_meta: model type       = 33M
0.00.330.336 I llm_load_print_meta: model ftype      = F16
0.00.330.336 I llm_load_print_meta: model params     = 32.90 M
0.00.330.337 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.330.337 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.330.337 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.330.338 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.330.338 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.330.338 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.330.338 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.330.338 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.330.338 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.330.339 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.330.339 I llm_load_print_meta: max token length = 45
0.00.331.537 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.331.537 I llm_load_tensors: offloading output layer to GPU
0.00.331.537 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.331.566 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.568 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.332.449 I llama_new_context_with_model: n_seq_max     = 1
0.00.332.450 I llama_new_context_with_model: n_ctx         = 8192
0.00.332.450 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.332.450 I llama_new_context_with_model: n_batch       = 2048
0.00.332.450 I llama_new_context_with_model: n_ubatch      = 2048
0.00.332.450 I llama_new_context_with_model: flash_attn    = 0
0.00.332.451 I llama_new_context_with_model: freq_base     = 10000.0
0.00.332.451 I llama_new_context_with_model: freq_scale    = 1
0.00.332.451 I ggml_metal_init: allocating
0.00.332.454 I ggml_metal_init: found device: Apple M4
0.00.332.457 I ggml_metal_init: picking default device: Apple M4
0.00.333.328 I ggml_metal_init: using embedded metal library
0.00.336.077 I ggml_metal_init: GPU name:   Apple M4
0.00.336.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.336.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.336.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.336.080 I ggml_metal_init: simdgroup reduction   = true
0.00.336.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.336.080 I ggml_metal_init: has bfloat            = true
0.00.336.080 I ggml_metal_init: use bfloat            = true
0.00.336.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.336.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.541 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.348.102 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.348.104 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.348.105 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.348.706 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.348.707 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.348.708 I llama_new_context_with_model: graph nodes  = 154
0.00.348.708 I llama_new_context_with_model: graph splits = 2
0.00.348.727 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.596 I 
0.00.360.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.774 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.774 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.777 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.777 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.780 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.780 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.303 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.030 I llama_perf_context_print:        load time =     337.79 ms
0.00.365.031 I llama_perf_context_print: prompt eval time =       3.72 ms /    62 tokens (    0.06 ms per token, 16666.67 tokens per second)
0.00.365.031 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.033 I llama_perf_context_print:       total time =       4.43 ms /    63 tokens
0.00.365.214 I ggml_metal_free: deallocating

real	0m1.121s
user	0m0.337s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.101 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.256 I main: llama backend init
0.00.000.262 I main: load the model and apply lora adapter, if any
0.00.067.028 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.078.067 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.078.083 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.078.087 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.078.088 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.078.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.078.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.078.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.078.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.078.108 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.078.109 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.078.110 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.078.111 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.078.112 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.078.112 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.078.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.078.118 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.078.119 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.085.203 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.087.399 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.096.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.096.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.096.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.096.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.096.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.096.025 I llama_model_loader: - type  f32:  194 tensors
0.00.096.025 I llama_model_loader: - type  f16:   98 tensors
0.00.131.081 I llm_load_vocab: special tokens cache size = 25
0.00.138.316 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.138.319 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.138.319 I llm_load_print_meta: arch             = gptneox
0.00.138.319 I llm_load_print_meta: vocab type       = BPE
0.00.138.320 I llm_load_print_meta: n_vocab          = 50304
0.00.138.320 I llm_load_print_meta: n_merges         = 50009
0.00.138.320 I llm_load_print_meta: vocab_only       = 0
0.00.138.320 I llm_load_print_meta: n_ctx_train      = 2048
0.00.138.320 I llm_load_print_meta: n_embd           = 2048
0.00.138.321 I llm_load_print_meta: n_layer          = 24
0.00.138.324 I llm_load_print_meta: n_head           = 16
0.00.138.325 I llm_load_print_meta: n_head_kv        = 16
0.00.138.327 I llm_load_print_meta: n_rot            = 32
0.00.138.327 I llm_load_print_meta: n_swa            = 0
0.00.138.327 I llm_load_print_meta: n_embd_head_k    = 128
0.00.138.327 I llm_load_print_meta: n_embd_head_v    = 128
0.00.138.328 I llm_load_print_meta: n_gqa            = 1
0.00.138.328 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.138.329 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.138.330 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.138.330 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.138.330 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.138.330 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.138.331 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.138.331 I llm_load_print_meta: n_ff             = 8192
0.00.138.332 I llm_load_print_meta: n_expert         = 0
0.00.138.332 I llm_load_print_meta: n_expert_used    = 0
0.00.138.332 I llm_load_print_meta: causal attn      = 1
0.00.138.332 I llm_load_print_meta: pooling type     = 0
0.00.138.332 I llm_load_print_meta: rope type        = 2
0.00.138.332 I llm_load_print_meta: rope scaling     = linear
0.00.138.333 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.138.333 I llm_load_print_meta: freq_scale_train = 1
0.00.138.333 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.138.333 I llm_load_print_meta: rope_finetuned   = unknown
0.00.138.333 I llm_load_print_meta: ssm_d_conv       = 0
0.00.138.334 I llm_load_print_meta: ssm_d_inner      = 0
0.00.138.334 I llm_load_print_meta: ssm_d_state      = 0
0.00.138.334 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.138.334 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.138.334 I llm_load_print_meta: model type       = 1.4B
0.00.138.335 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.138.335 I llm_load_print_meta: model params     = 1.41 B
0.00.138.336 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.138.336 I llm_load_print_meta: general.name     = 1.4B
0.00.138.336 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.138.338 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.138.338 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.138.338 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.138.339 I llm_load_print_meta: LF token         = 128 ''
0.00.138.339 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.138.339 I llm_load_print_meta: max token length = 1024
0.00.141.013 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.141.014 I llm_load_tensors: offloading output layer to GPU
0.00.141.014 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.141.033 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.141.034 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.142.043 I llama_new_context_with_model: n_seq_max     = 1
0.00.142.044 I llama_new_context_with_model: n_ctx         = 2048
0.00.142.044 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.142.044 I llama_new_context_with_model: n_batch       = 2048
0.00.142.045 I llama_new_context_with_model: n_ubatch      = 512
0.00.142.045 I llama_new_context_with_model: flash_attn    = 0
0.00.142.045 I llama_new_context_with_model: freq_base     = 10000.0
0.00.142.046 I llama_new_context_with_model: freq_scale    = 1
0.00.142.046 I ggml_metal_init: allocating
0.00.142.054 I ggml_metal_init: found device: Apple M4
0.00.142.056 I ggml_metal_init: picking default device: Apple M4
0.00.142.769 I ggml_metal_init: using embedded metal library
0.00.157.483 I ggml_metal_init: GPU name:   Apple M4
0.00.157.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.157.485 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.157.485 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.157.486 I ggml_metal_init: simdgroup reduction   = true
0.00.157.486 I ggml_metal_init: simdgroup matrix mul. = true
0.00.157.486 I ggml_metal_init: has bfloat            = true
0.00.157.486 I ggml_metal_init: use bfloat            = true
0.00.157.487 I ggml_metal_init: hasUnifiedMemory      = true
0.00.157.487 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.251.090 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.270.929 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.270.935 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.270.959 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.271.987 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.271.989 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.271.989 I llama_new_context_with_model: graph nodes  = 967
0.00.271.990 I llama_new_context_with_model: graph splits = 2
0.00.272.016 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.272.150 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.272.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.661 I main: llama threadpool init, n_threads = 4
0.00.356.694 I 
0.00.356.715 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.715 I 
0.00.356.794 I sampler seed: 1234
0.00.356.798 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.356.822 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.356.824 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.356.824 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.199.855 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.02.199.855 I llama_perf_context_print:        load time =     289.62 ms
0.02.199.856 I llama_perf_context_print: prompt eval time =      43.82 ms /     7 tokens (    6.26 ms per token,   159.73 tokens per second)
0.02.199.856 I llama_perf_context_print:        eval time =    1796.21 ms /    63 runs   (   28.51 ms per token,    35.07 tokens per second)
0.02.199.857 I llama_perf_context_print:       total time =    1843.20 ms /    70 tokens
0.02.200.028 I ggml_metal_free: deallocating

real	0m2.516s
user	0m0.148s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.933 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.872 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.978 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.983 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.987 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.987 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.988 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.988 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.990 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.990 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.990 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.991 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.991 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.991 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.992 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.994 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.623 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.502 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.503 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.504 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.505 I llama_model_loader: - type  f32:  194 tensors
0.00.054.506 I llama_model_loader: - type  f16:   98 tensors
0.00.081.320 I llm_load_vocab: special tokens cache size = 25
0.00.087.393 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.396 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.396 I llm_load_print_meta: arch             = gptneox
0.00.087.396 I llm_load_print_meta: vocab type       = BPE
0.00.087.396 I llm_load_print_meta: n_vocab          = 50304
0.00.087.396 I llm_load_print_meta: n_merges         = 50009
0.00.087.397 I llm_load_print_meta: vocab_only       = 0
0.00.087.397 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.397 I llm_load_print_meta: n_embd           = 2048
0.00.087.397 I llm_load_print_meta: n_layer          = 24
0.00.087.400 I llm_load_print_meta: n_head           = 16
0.00.087.406 I llm_load_print_meta: n_head_kv        = 16
0.00.087.406 I llm_load_print_meta: n_rot            = 32
0.00.087.406 I llm_load_print_meta: n_swa            = 0
0.00.087.406 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.406 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.407 I llm_load_print_meta: n_gqa            = 1
0.00.087.408 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.408 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.409 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.409 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.410 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.410 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.410 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.411 I llm_load_print_meta: n_ff             = 8192
0.00.087.412 I llm_load_print_meta: n_expert         = 0
0.00.087.412 I llm_load_print_meta: n_expert_used    = 0
0.00.087.412 I llm_load_print_meta: causal attn      = 1
0.00.087.412 I llm_load_print_meta: pooling type     = 0
0.00.087.412 I llm_load_print_meta: rope type        = 2
0.00.087.412 I llm_load_print_meta: rope scaling     = linear
0.00.087.413 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.413 I llm_load_print_meta: freq_scale_train = 1
0.00.087.413 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.413 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.414 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.414 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.417 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.417 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.417 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.418 I llm_load_print_meta: model type       = 1.4B
0.00.087.418 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.419 I llm_load_print_meta: model params     = 1.41 B
0.00.087.419 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.419 I llm_load_print_meta: general.name     = 1.4B
0.00.087.420 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.420 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.420 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.420 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.420 I llm_load_print_meta: LF token         = 128 ''
0.00.087.420 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.422 I llm_load_print_meta: max token length = 1024
0.00.089.300 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.301 I llm_load_tensors: offloading output layer to GPU
0.00.089.301 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.312 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.313 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.190 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.191 I llama_new_context_with_model: n_ctx         = 128
0.00.090.191 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.191 I llama_new_context_with_model: n_batch       = 128
0.00.090.191 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.192 I llama_new_context_with_model: flash_attn    = 0
0.00.090.192 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.192 I llama_new_context_with_model: freq_scale    = 1
0.00.090.193 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.193 I ggml_metal_init: allocating
0.00.090.197 I ggml_metal_init: found device: Apple M4
0.00.090.199 I ggml_metal_init: picking default device: Apple M4
0.00.090.834 I ggml_metal_init: using embedded metal library
0.00.093.376 I ggml_metal_init: GPU name:   Apple M4
0.00.093.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.378 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.378 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.378 I ggml_metal_init: simdgroup reduction   = true
0.00.093.378 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.379 I ggml_metal_init: has bfloat            = true
0.00.093.379 I ggml_metal_init: use bfloat            = true
0.00.093.379 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.380 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.574 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.103.889 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.892 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.908 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.832 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.833 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.834 I llama_new_context_with_model: graph nodes  = 967
0.00.104.834 I llama_new_context_with_model: graph splits = 2
0.00.104.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.392.933 I 
0.01.392.970 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.393.022 I perplexity: tokenizing the input ..
0.01.406.658 I perplexity: tokenization took 13.632 ms
0.01.406.665 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.528.942 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.530.593 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.530.620 I llama_perf_context_print:        load time =    1370.05 ms
0.01.530.622 I llama_perf_context_print: prompt eval time =     121.31 ms /   128 tokens (    0.95 ms per token,  1055.14 tokens per second)
0.01.530.624 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.530.625 I llama_perf_context_print:       total time =     137.69 ms /   129 tokens
0.01.531.328 I ggml_metal_free: deallocating

real	0m1.719s
user	0m0.125s
sys	0m0.242s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.923 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.407 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.409 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.410 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.410 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.411 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.412 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.413 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.414 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.362 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.248 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.249 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.249 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.250 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.250 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.251 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.251 I llama_model_loader: - type  f32:  194 tensors
0.00.032.252 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.699 I llm_load_vocab: special tokens cache size = 25
0.00.059.660 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.664 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.665 I llm_load_print_meta: arch             = gptneox
0.00.059.665 I llm_load_print_meta: vocab type       = BPE
0.00.059.665 I llm_load_print_meta: n_vocab          = 50304
0.00.059.665 I llm_load_print_meta: n_merges         = 50009
0.00.059.666 I llm_load_print_meta: vocab_only       = 0
0.00.059.666 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.666 I llm_load_print_meta: n_embd           = 2048
0.00.059.668 I llm_load_print_meta: n_layer          = 24
0.00.059.675 I llm_load_print_meta: n_head           = 16
0.00.059.675 I llm_load_print_meta: n_head_kv        = 16
0.00.059.675 I llm_load_print_meta: n_rot            = 32
0.00.059.676 I llm_load_print_meta: n_swa            = 0
0.00.059.678 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.678 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.679 I llm_load_print_meta: n_gqa            = 1
0.00.059.680 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.681 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.681 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.682 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.682 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.682 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.683 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.683 I llm_load_print_meta: n_ff             = 8192
0.00.059.683 I llm_load_print_meta: n_expert         = 0
0.00.059.684 I llm_load_print_meta: n_expert_used    = 0
0.00.059.684 I llm_load_print_meta: causal attn      = 1
0.00.059.684 I llm_load_print_meta: pooling type     = 0
0.00.059.684 I llm_load_print_meta: rope type        = 2
0.00.059.684 I llm_load_print_meta: rope scaling     = linear
0.00.059.685 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.686 I llm_load_print_meta: freq_scale_train = 1
0.00.059.686 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.687 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.687 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.687 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.687 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.687 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.687 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.687 I llm_load_print_meta: model type       = 1.4B
0.00.059.688 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.688 I llm_load_print_meta: model params     = 1.41 B
0.00.059.688 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.689 I llm_load_print_meta: general.name     = 1.4B
0.00.059.689 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.689 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.690 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.690 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.691 I llm_load_print_meta: LF token         = 128 ''
0.00.059.691 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.691 I llm_load_print_meta: max token length = 1024
0.00.062.102 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.102 I llm_load_tensors: offloading output layer to GPU
0.00.062.102 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.113 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.115 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.117 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.118 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.118 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.118 I llama_new_context_with_model: n_batch       = 2048
0.00.063.118 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.118 I llama_new_context_with_model: flash_attn    = 0
0.00.063.119 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.119 I llama_new_context_with_model: freq_scale    = 1
0.00.063.119 I ggml_metal_init: allocating
0.00.063.126 I ggml_metal_init: found device: Apple M4
0.00.063.128 I ggml_metal_init: picking default device: Apple M4
0.00.063.879 I ggml_metal_init: using embedded metal library
0.00.066.431 I ggml_metal_init: GPU name:   Apple M4
0.00.066.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.433 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.434 I ggml_metal_init: simdgroup reduction   = true
0.00.066.434 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.434 I ggml_metal_init: has bfloat            = true
0.00.066.434 I ggml_metal_init: use bfloat            = true
0.00.066.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.435 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.057 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.545 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.561 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.594 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.744 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.748 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.748 I llama_new_context_with_model: graph nodes  = 967
0.00.103.748 I llama_new_context_with_model: graph splits = 2
0.00.103.768 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.898 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.899 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.399.218 I main: llama threadpool init, n_threads = 4
0.01.399.254 I 
0.01.399.276 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.399.276 I 
0.01.399.550 I sampler seed: 1234
0.01.399.555 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.399.590 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.399.592 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.399.592 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.489.826 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62500.00 tokens per second)
0.02.489.827 I llama_perf_context_print:        load time =    1389.29 ms
0.02.489.827 I llama_perf_context_print: prompt eval time =      43.81 ms /     7 tokens (    6.26 ms per token,   159.78 tokens per second)
0.02.489.828 I llama_perf_context_print:        eval time =    1043.65 ms /    63 runs   (   16.57 ms per token,    60.37 tokens per second)
0.02.489.828 I llama_perf_context_print:       total time =    1090.61 ms /    70 tokens
0.02.489.998 I ggml_metal_free: deallocating

real	0m2.509s
user	0m0.114s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.304 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.793 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.560 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.562 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.362 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.821 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.145 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.145 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.146 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.147 I llama_model_loader: - type  f32:  194 tensors
0.00.035.148 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.704 I llm_load_vocab: special tokens cache size = 25
0.00.066.859 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.862 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.862 I llm_load_print_meta: arch             = gptneox
0.00.066.863 I llm_load_print_meta: vocab type       = BPE
0.00.066.863 I llm_load_print_meta: n_vocab          = 50304
0.00.066.863 I llm_load_print_meta: n_merges         = 50009
0.00.066.863 I llm_load_print_meta: vocab_only       = 0
0.00.066.863 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.864 I llm_load_print_meta: n_embd           = 2048
0.00.066.865 I llm_load_print_meta: n_layer          = 24
0.00.066.871 I llm_load_print_meta: n_head           = 16
0.00.066.871 I llm_load_print_meta: n_head_kv        = 16
0.00.066.872 I llm_load_print_meta: n_rot            = 32
0.00.066.873 I llm_load_print_meta: n_swa            = 0
0.00.066.873 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.874 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.874 I llm_load_print_meta: n_gqa            = 1
0.00.066.875 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.876 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.877 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.877 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.878 I llm_load_print_meta: n_ff             = 8192
0.00.066.878 I llm_load_print_meta: n_expert         = 0
0.00.066.878 I llm_load_print_meta: n_expert_used    = 0
0.00.066.878 I llm_load_print_meta: causal attn      = 1
0.00.066.879 I llm_load_print_meta: pooling type     = 0
0.00.066.879 I llm_load_print_meta: rope type        = 2
0.00.066.879 I llm_load_print_meta: rope scaling     = linear
0.00.066.879 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.879 I llm_load_print_meta: freq_scale_train = 1
0.00.066.881 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.881 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.881 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.881 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.882 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.882 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.882 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.882 I llm_load_print_meta: model type       = 1.4B
0.00.066.882 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.883 I llm_load_print_meta: model params     = 1.41 B
0.00.066.883 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.883 I llm_load_print_meta: general.name     = 1.4B
0.00.066.884 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.884 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.884 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.884 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.884 I llm_load_print_meta: LF token         = 128 ''
0.00.066.885 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.885 I llm_load_print_meta: max token length = 1024
0.00.068.828 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.828 I llm_load_tensors: offloading output layer to GPU
0.00.068.828 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.839 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.840 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.785 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.786 I llama_new_context_with_model: n_ctx         = 128
0.00.069.786 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.787 I llama_new_context_with_model: n_batch       = 128
0.00.069.787 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.787 I llama_new_context_with_model: flash_attn    = 0
0.00.069.787 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.788 I llama_new_context_with_model: freq_scale    = 1
0.00.069.788 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.788 I ggml_metal_init: allocating
0.00.069.791 I ggml_metal_init: found device: Apple M4
0.00.069.793 I ggml_metal_init: picking default device: Apple M4
0.00.070.507 I ggml_metal_init: using embedded metal library
0.00.073.002 I ggml_metal_init: GPU name:   Apple M4
0.00.073.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.004 I ggml_metal_init: simdgroup reduction   = true
0.00.073.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.004 I ggml_metal_init: has bfloat            = true
0.00.073.005 I ggml_metal_init: use bfloat            = true
0.00.073.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.146 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.642 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.645 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.661 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.701 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.702 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.703 I llama_new_context_with_model: graph nodes  = 967
0.00.085.703 I llama_new_context_with_model: graph splits = 2
0.00.085.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.717 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.075.029 I 
0.01.075.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.075.066 I perplexity: tokenizing the input ..
0.01.083.154 I perplexity: tokenization took 8.087 ms
0.01.083.159 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.206.824 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.208.045 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.208.058 I llama_perf_context_print:        load time =    1061.23 ms
0.01.208.059 I llama_perf_context_print: prompt eval time =     123.44 ms /   128 tokens (    0.96 ms per token,  1036.97 tokens per second)
0.01.208.059 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.208.060 I llama_perf_context_print:       total time =     133.03 ms /   129 tokens
0.01.208.408 I ggml_metal_free: deallocating

real	0m1.228s
user	0m0.094s
sys	0m0.176s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.017.122 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.618 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.626 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.627 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.983 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.984 I llama_model_loader: - type  f32:  194 tensors
0.00.040.985 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.985 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.418 I llm_load_vocab: special tokens cache size = 25
0.00.074.934 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.938 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.938 I llm_load_print_meta: arch             = gptneox
0.00.074.939 I llm_load_print_meta: vocab type       = BPE
0.00.074.939 I llm_load_print_meta: n_vocab          = 50304
0.00.074.939 I llm_load_print_meta: n_merges         = 50009
0.00.074.939 I llm_load_print_meta: vocab_only       = 0
0.00.074.940 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.940 I llm_load_print_meta: n_embd           = 2048
0.00.074.940 I llm_load_print_meta: n_layer          = 24
0.00.074.944 I llm_load_print_meta: n_head           = 16
0.00.074.945 I llm_load_print_meta: n_head_kv        = 16
0.00.074.945 I llm_load_print_meta: n_rot            = 32
0.00.074.945 I llm_load_print_meta: n_swa            = 0
0.00.074.946 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.946 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.947 I llm_load_print_meta: n_gqa            = 1
0.00.074.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.948 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.949 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.950 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.950 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.950 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.950 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.951 I llm_load_print_meta: n_ff             = 8192
0.00.074.952 I llm_load_print_meta: n_expert         = 0
0.00.074.952 I llm_load_print_meta: n_expert_used    = 0
0.00.074.952 I llm_load_print_meta: causal attn      = 1
0.00.074.952 I llm_load_print_meta: pooling type     = 0
0.00.074.952 I llm_load_print_meta: rope type        = 2
0.00.074.954 I llm_load_print_meta: rope scaling     = linear
0.00.074.954 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.955 I llm_load_print_meta: freq_scale_train = 1
0.00.074.955 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.956 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.956 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.956 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.956 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.956 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.957 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.957 I llm_load_print_meta: model type       = 1.4B
0.00.074.957 I llm_load_print_meta: model ftype      = Q4_0
0.00.074.958 I llm_load_print_meta: model params     = 1.41 B
0.00.074.959 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.074.959 I llm_load_print_meta: general.name     = 1.4B
0.00.074.959 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.959 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.962 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.962 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.963 I llm_load_print_meta: LF token         = 128 ''
0.00.074.963 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.963 I llm_load_print_meta: max token length = 1024
0.00.077.778 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.779 I llm_load_tensors: offloading output layer to GPU
0.00.077.779 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.791 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.077.793 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.079.166 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.168 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.168 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.168 I llama_new_context_with_model: n_batch       = 2048
0.00.079.169 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.169 I llama_new_context_with_model: flash_attn    = 0
0.00.079.169 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.170 I llama_new_context_with_model: freq_scale    = 1
0.00.079.170 I ggml_metal_init: allocating
0.00.079.175 I ggml_metal_init: found device: Apple M4
0.00.079.177 I ggml_metal_init: picking default device: Apple M4
0.00.080.113 I ggml_metal_init: using embedded metal library
0.00.083.961 I ggml_metal_init: GPU name:   Apple M4
0.00.083.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.965 I ggml_metal_init: simdgroup reduction   = true
0.00.083.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.965 I ggml_metal_init: has bfloat            = true
0.00.083.965 I ggml_metal_init: use bfloat            = true
0.00.083.966 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.033 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.125.773 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.784 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.809 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.972 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.974 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.975 I llama_new_context_with_model: graph nodes  = 967
0.00.126.975 I llama_new_context_with_model: graph splits = 2
0.00.126.994 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.127.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.127.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.806 I main: llama threadpool init, n_threads = 4
0.00.730.851 I 
0.00.730.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.875 I 
0.00.731.105 I sampler seed: 1234
0.00.731.111 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.122 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.122 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.122 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.423.563 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.423.563 I llama_perf_context_print:        load time =     713.68 ms
0.01.423.564 I llama_perf_context_print: prompt eval time =      44.30 ms /     7 tokens (    6.33 ms per token,   158.00 tokens per second)
0.01.423.565 I llama_perf_context_print:        eval time =     644.99 ms /    63 runs   (   10.24 ms per token,    97.68 tokens per second)
0.01.423.565 I llama_perf_context_print:       total time =     692.76 ms /    70 tokens
0.01.423.754 I ggml_metal_free: deallocating

real	0m1.442s
user	0m0.125s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.265 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.183 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.848 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.854 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.855 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.855 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.855 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.856 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.857 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.858 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.858 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.859 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.859 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.861 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.863 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.863 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.863 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.785 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.855 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.757 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.758 I llama_model_loader: - type  f32:  194 tensors
0.00.024.758 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.758 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.886 I llm_load_vocab: special tokens cache size = 25
0.00.051.786 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.789 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.789 I llm_load_print_meta: arch             = gptneox
0.00.051.790 I llm_load_print_meta: vocab type       = BPE
0.00.051.790 I llm_load_print_meta: n_vocab          = 50304
0.00.051.790 I llm_load_print_meta: n_merges         = 50009
0.00.051.790 I llm_load_print_meta: vocab_only       = 0
0.00.051.791 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.791 I llm_load_print_meta: n_embd           = 2048
0.00.051.791 I llm_load_print_meta: n_layer          = 24
0.00.051.793 I llm_load_print_meta: n_head           = 16
0.00.051.794 I llm_load_print_meta: n_head_kv        = 16
0.00.051.794 I llm_load_print_meta: n_rot            = 32
0.00.051.794 I llm_load_print_meta: n_swa            = 0
0.00.051.794 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.795 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.795 I llm_load_print_meta: n_gqa            = 1
0.00.051.796 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.798 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.798 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.798 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.799 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.799 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.801 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.801 I llm_load_print_meta: n_ff             = 8192
0.00.051.802 I llm_load_print_meta: n_expert         = 0
0.00.051.802 I llm_load_print_meta: n_expert_used    = 0
0.00.051.802 I llm_load_print_meta: causal attn      = 1
0.00.051.802 I llm_load_print_meta: pooling type     = 0
0.00.051.803 I llm_load_print_meta: rope type        = 2
0.00.051.803 I llm_load_print_meta: rope scaling     = linear
0.00.051.804 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.804 I llm_load_print_meta: freq_scale_train = 1
0.00.051.804 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.805 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.805 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.805 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.805 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.805 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.805 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.806 I llm_load_print_meta: model type       = 1.4B
0.00.051.806 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.806 I llm_load_print_meta: model params     = 1.41 B
0.00.051.808 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.808 I llm_load_print_meta: general.name     = 1.4B
0.00.051.808 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.808 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.809 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.809 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.809 I llm_load_print_meta: LF token         = 128 ''
0.00.051.809 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.810 I llm_load_print_meta: max token length = 1024
0.00.053.809 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.810 I llm_load_tensors: offloading output layer to GPU
0.00.053.810 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.820 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.821 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.792 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.793 I llama_new_context_with_model: n_ctx         = 128
0.00.054.793 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.793 I llama_new_context_with_model: n_batch       = 128
0.00.054.793 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.794 I llama_new_context_with_model: flash_attn    = 0
0.00.054.794 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.794 I llama_new_context_with_model: freq_scale    = 1
0.00.054.795 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.795 I ggml_metal_init: allocating
0.00.054.801 I ggml_metal_init: found device: Apple M4
0.00.054.803 I ggml_metal_init: picking default device: Apple M4
0.00.055.362 I ggml_metal_init: using embedded metal library
0.00.057.682 I ggml_metal_init: GPU name:   Apple M4
0.00.057.684 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.684 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.685 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.685 I ggml_metal_init: simdgroup reduction   = true
0.00.057.685 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.685 I ggml_metal_init: has bfloat            = true
0.00.057.685 I ggml_metal_init: use bfloat            = true
0.00.057.686 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.436 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.661 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.667 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.682 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.552 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.554 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.554 I llama_new_context_with_model: graph nodes  = 967
0.00.069.554 I llama_new_context_with_model: graph splits = 2
0.00.069.567 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.354 I 
0.00.649.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.412 I perplexity: tokenizing the input ..
0.00.657.311 I perplexity: tokenization took 7.896 ms
0.00.657.315 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.745 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.780.909 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.780.927 I llama_perf_context_print:        load time =     639.16 ms
0.00.780.929 I llama_perf_context_print: prompt eval time =     122.20 ms /   128 tokens (    0.95 ms per token,  1047.42 tokens per second)
0.00.780.930 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.930 I llama_perf_context_print:       total time =     131.58 ms /   129 tokens
0.00.781.372 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.079s
sys	0m0.096s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.865 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.607 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.552 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.551 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.551 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.552 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.552 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.552 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.553 I llama_model_loader: - type  f32:  194 tensors
0.00.033.553 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.553 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.867 I llm_load_vocab: special tokens cache size = 25
0.00.062.997 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.000 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.000 I llm_load_print_meta: arch             = gptneox
0.00.063.000 I llm_load_print_meta: vocab type       = BPE
0.00.063.000 I llm_load_print_meta: n_vocab          = 50304
0.00.063.000 I llm_load_print_meta: n_merges         = 50009
0.00.063.001 I llm_load_print_meta: vocab_only       = 0
0.00.063.001 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.001 I llm_load_print_meta: n_embd           = 2048
0.00.063.001 I llm_load_print_meta: n_layer          = 24
0.00.063.003 I llm_load_print_meta: n_head           = 16
0.00.063.004 I llm_load_print_meta: n_head_kv        = 16
0.00.063.004 I llm_load_print_meta: n_rot            = 32
0.00.063.006 I llm_load_print_meta: n_swa            = 0
0.00.063.006 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.006 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.007 I llm_load_print_meta: n_gqa            = 1
0.00.063.008 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.008 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.009 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.009 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.009 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.009 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.009 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.010 I llm_load_print_meta: n_ff             = 8192
0.00.063.010 I llm_load_print_meta: n_expert         = 0
0.00.063.010 I llm_load_print_meta: n_expert_used    = 0
0.00.063.010 I llm_load_print_meta: causal attn      = 1
0.00.063.010 I llm_load_print_meta: pooling type     = 0
0.00.063.011 I llm_load_print_meta: rope type        = 2
0.00.063.011 I llm_load_print_meta: rope scaling     = linear
0.00.063.011 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.011 I llm_load_print_meta: freq_scale_train = 1
0.00.063.011 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.012 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.012 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.012 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.012 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.012 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.013 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.013 I llm_load_print_meta: model type       = 1.4B
0.00.063.013 I llm_load_print_meta: model ftype      = Q4_1
0.00.063.014 I llm_load_print_meta: model params     = 1.41 B
0.00.063.014 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.063.014 I llm_load_print_meta: general.name     = 1.4B
0.00.063.014 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.015 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.015 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.016 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.016 I llm_load_print_meta: LF token         = 128 ''
0.00.063.016 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.017 I llm_load_print_meta: max token length = 1024
0.00.065.056 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.056 I llm_load_tensors: offloading output layer to GPU
0.00.065.056 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.067 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.065.068 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.066.029 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.030 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.030 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.030 I llama_new_context_with_model: n_batch       = 2048
0.00.066.030 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.030 I llama_new_context_with_model: flash_attn    = 0
0.00.066.031 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.031 I llama_new_context_with_model: freq_scale    = 1
0.00.066.031 I ggml_metal_init: allocating
0.00.066.034 I ggml_metal_init: found device: Apple M4
0.00.066.036 I ggml_metal_init: picking default device: Apple M4
0.00.066.630 I ggml_metal_init: using embedded metal library
0.00.069.170 I ggml_metal_init: GPU name:   Apple M4
0.00.069.172 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.173 I ggml_metal_init: simdgroup reduction   = true
0.00.069.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.173 I ggml_metal_init: has bfloat            = true
0.00.069.173 I ggml_metal_init: use bfloat            = true
0.00.069.174 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.385 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.099.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.011 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.040 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.114 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.116 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.116 I llama_new_context_with_model: graph nodes  = 967
0.00.100.116 I llama_new_context_with_model: graph splits = 2
0.00.100.132 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.964.011 I main: llama threadpool init, n_threads = 4
0.00.964.048 I 
0.00.964.068 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.964.068 I 
0.00.964.303 I sampler seed: 1234
0.00.964.307 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.964.351 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.964.353 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.964.353 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.686.862 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63791.55 tokens per second)
0.01.686.863 I llama_perf_context_print:        load time =     955.14 ms
0.01.686.864 I llama_perf_context_print: prompt eval time =      43.58 ms /     7 tokens (    6.23 ms per token,   160.64 tokens per second)
0.01.686.864 I llama_perf_context_print:        eval time =     676.01 ms /    63 runs   (   10.73 ms per token,    93.19 tokens per second)
0.01.686.864 I llama_perf_context_print:       total time =     722.85 ms /    70 tokens
0.01.687.050 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.113s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.962 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.609 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.610 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.611 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.611 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.612 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.612 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.613 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.613 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.613 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.614 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.614 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.617 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.561 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.583 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.584 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.585 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.586 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.586 I llama_model_loader: - type  f32:  194 tensors
0.00.023.587 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.587 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.598 I llm_load_vocab: special tokens cache size = 25
0.00.050.478 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.481 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.481 I llm_load_print_meta: arch             = gptneox
0.00.050.482 I llm_load_print_meta: vocab type       = BPE
0.00.050.482 I llm_load_print_meta: n_vocab          = 50304
0.00.050.482 I llm_load_print_meta: n_merges         = 50009
0.00.050.482 I llm_load_print_meta: vocab_only       = 0
0.00.050.482 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.483 I llm_load_print_meta: n_embd           = 2048
0.00.050.483 I llm_load_print_meta: n_layer          = 24
0.00.050.485 I llm_load_print_meta: n_head           = 16
0.00.050.486 I llm_load_print_meta: n_head_kv        = 16
0.00.050.488 I llm_load_print_meta: n_rot            = 32
0.00.050.488 I llm_load_print_meta: n_swa            = 0
0.00.050.488 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.488 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.489 I llm_load_print_meta: n_gqa            = 1
0.00.050.490 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.491 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.491 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.492 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.492 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.492 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.492 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.493 I llm_load_print_meta: n_ff             = 8192
0.00.050.493 I llm_load_print_meta: n_expert         = 0
0.00.050.493 I llm_load_print_meta: n_expert_used    = 0
0.00.050.493 I llm_load_print_meta: causal attn      = 1
0.00.050.494 I llm_load_print_meta: pooling type     = 0
0.00.050.494 I llm_load_print_meta: rope type        = 2
0.00.050.494 I llm_load_print_meta: rope scaling     = linear
0.00.050.494 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.495 I llm_load_print_meta: freq_scale_train = 1
0.00.050.495 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.495 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.495 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.495 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.495 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.496 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.497 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.498 I llm_load_print_meta: model type       = 1.4B
0.00.050.498 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.498 I llm_load_print_meta: model params     = 1.41 B
0.00.050.499 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.499 I llm_load_print_meta: general.name     = 1.4B
0.00.050.499 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.499 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.500 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.500 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.500 I llm_load_print_meta: LF token         = 128 ''
0.00.050.501 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.503 I llm_load_print_meta: max token length = 1024
0.00.052.492 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.492 I llm_load_tensors: offloading output layer to GPU
0.00.052.492 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.503 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.504 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.396 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.396 I llama_new_context_with_model: n_ctx         = 128
0.00.053.397 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.397 I llama_new_context_with_model: n_batch       = 128
0.00.053.397 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.397 I llama_new_context_with_model: flash_attn    = 0
0.00.053.398 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.398 I llama_new_context_with_model: freq_scale    = 1
0.00.053.398 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.399 I ggml_metal_init: allocating
0.00.053.405 I ggml_metal_init: found device: Apple M4
0.00.053.407 I ggml_metal_init: picking default device: Apple M4
0.00.053.952 I ggml_metal_init: using embedded metal library
0.00.056.269 I ggml_metal_init: GPU name:   Apple M4
0.00.056.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.271 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.271 I ggml_metal_init: simdgroup reduction   = true
0.00.056.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.271 I ggml_metal_init: has bfloat            = true
0.00.056.272 I ggml_metal_init: use bfloat            = true
0.00.056.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.530 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.760 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.762 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.775 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.641 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.642 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.642 I llama_new_context_with_model: graph nodes  = 967
0.00.067.643 I llama_new_context_with_model: graph splits = 2
0.00.067.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.695 I 
0.00.743.736 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.751 I perplexity: tokenizing the input ..
0.00.752.138 I perplexity: tokenization took 8.386 ms
0.00.752.142 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.874.741 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.875.891 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.875.908 I llama_perf_context_print:        load time =     734.73 ms
0.00.875.909 I llama_perf_context_print: prompt eval time =     122.36 ms /   128 tokens (    0.96 ms per token,  1046.12 tokens per second)
0.00.875.910 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.875.910 I llama_perf_context_print:       total time =     132.22 ms /   129 tokens
0.00.876.270 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.079s
sys	0m0.123s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.488 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.894 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.899 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.899 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.900 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.900 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.902 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.902 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.907 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.908 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.456 I llama_model_loader: - type  f32:  194 tensors
0.00.025.457 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.854 I llm_load_vocab: special tokens cache size = 25
0.00.051.830 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.833 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.834 I llm_load_print_meta: arch             = gptneox
0.00.051.834 I llm_load_print_meta: vocab type       = BPE
0.00.051.834 I llm_load_print_meta: n_vocab          = 50304
0.00.051.834 I llm_load_print_meta: n_merges         = 50009
0.00.051.835 I llm_load_print_meta: vocab_only       = 0
0.00.051.835 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.835 I llm_load_print_meta: n_embd           = 2048
0.00.051.835 I llm_load_print_meta: n_layer          = 24
0.00.051.837 I llm_load_print_meta: n_head           = 16
0.00.051.838 I llm_load_print_meta: n_head_kv        = 16
0.00.051.838 I llm_load_print_meta: n_rot            = 32
0.00.051.838 I llm_load_print_meta: n_swa            = 0
0.00.051.838 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.839 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.839 I llm_load_print_meta: n_gqa            = 1
0.00.051.840 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.841 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.841 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.842 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.843 I llm_load_print_meta: n_ff             = 8192
0.00.051.843 I llm_load_print_meta: n_expert         = 0
0.00.051.843 I llm_load_print_meta: n_expert_used    = 0
0.00.051.845 I llm_load_print_meta: causal attn      = 1
0.00.051.847 I llm_load_print_meta: pooling type     = 0
0.00.051.847 I llm_load_print_meta: rope type        = 2
0.00.051.847 I llm_load_print_meta: rope scaling     = linear
0.00.051.847 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.848 I llm_load_print_meta: freq_scale_train = 1
0.00.051.849 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.849 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.849 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.850 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.850 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.850 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.850 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.850 I llm_load_print_meta: model type       = 1.4B
0.00.051.850 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.851 I llm_load_print_meta: model params     = 1.41 B
0.00.051.851 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.852 I llm_load_print_meta: general.name     = 1.4B
0.00.051.852 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.852 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.852 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.852 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.853 I llm_load_print_meta: LF token         = 128 ''
0.00.051.857 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.857 I llm_load_print_meta: max token length = 1024
0.00.053.881 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.881 I llm_load_tensors: offloading output layer to GPU
0.00.053.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.892 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.893 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.828 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.829 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.829 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.829 I llama_new_context_with_model: n_batch       = 2048
0.00.054.829 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.830 I llama_new_context_with_model: flash_attn    = 0
0.00.054.830 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.830 I llama_new_context_with_model: freq_scale    = 1
0.00.054.831 I ggml_metal_init: allocating
0.00.054.834 I ggml_metal_init: found device: Apple M4
0.00.054.836 I ggml_metal_init: picking default device: Apple M4
0.00.055.461 I ggml_metal_init: using embedded metal library
0.00.057.826 I ggml_metal_init: GPU name:   Apple M4
0.00.057.828 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.828 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.828 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.829 I ggml_metal_init: simdgroup reduction   = true
0.00.057.829 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.829 I ggml_metal_init: has bfloat            = true
0.00.057.829 I ggml_metal_init: use bfloat            = true
0.00.057.829 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.831 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.532 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.940 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.946 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.965 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.017 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.018 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.018 I llama_new_context_with_model: graph nodes  = 967
0.00.089.019 I llama_new_context_with_model: graph splits = 2
0.00.089.030 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.183 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.184 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.844 I main: llama threadpool init, n_threads = 4
0.00.767.878 I 
0.00.767.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.899 I 
0.00.768.133 I sampler seed: 1234
0.00.768.138 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.175 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.179 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.179 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.556.051 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.556.051 I llama_perf_context_print:        load time =     757.35 ms
0.01.556.052 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.30 tokens per second)
0.01.556.053 I llama_perf_context_print:        eval time =     741.61 ms /    63 runs   (   11.77 ms per token,    84.95 tokens per second)
0.01.556.054 I llama_perf_context_print:       total time =     788.21 ms /    70 tokens
0.01.556.266 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.394 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.832 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.834 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.835 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.835 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.835 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.836 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.837 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.837 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.837 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.838 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.838 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.838 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.841 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.842 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.570 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.566 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.375 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.377 I llama_model_loader: - type  f32:  194 tensors
0.00.025.377 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.378 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.710 I llm_load_vocab: special tokens cache size = 25
0.00.051.389 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.391 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.392 I llm_load_print_meta: arch             = gptneox
0.00.051.392 I llm_load_print_meta: vocab type       = BPE
0.00.051.392 I llm_load_print_meta: n_vocab          = 50304
0.00.051.392 I llm_load_print_meta: n_merges         = 50009
0.00.051.393 I llm_load_print_meta: vocab_only       = 0
0.00.051.393 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.393 I llm_load_print_meta: n_embd           = 2048
0.00.051.393 I llm_load_print_meta: n_layer          = 24
0.00.051.395 I llm_load_print_meta: n_head           = 16
0.00.051.396 I llm_load_print_meta: n_head_kv        = 16
0.00.051.396 I llm_load_print_meta: n_rot            = 32
0.00.051.399 I llm_load_print_meta: n_swa            = 0
0.00.051.399 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.399 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.400 I llm_load_print_meta: n_gqa            = 1
0.00.051.400 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.401 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.402 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.402 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.402 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.402 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.403 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.403 I llm_load_print_meta: n_ff             = 8192
0.00.051.403 I llm_load_print_meta: n_expert         = 0
0.00.051.404 I llm_load_print_meta: n_expert_used    = 0
0.00.051.404 I llm_load_print_meta: causal attn      = 1
0.00.051.404 I llm_load_print_meta: pooling type     = 0
0.00.051.404 I llm_load_print_meta: rope type        = 2
0.00.051.404 I llm_load_print_meta: rope scaling     = linear
0.00.051.405 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.405 I llm_load_print_meta: freq_scale_train = 1
0.00.051.405 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.405 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.405 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.406 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.406 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.406 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.406 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.406 I llm_load_print_meta: model type       = 1.4B
0.00.051.407 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.407 I llm_load_print_meta: model params     = 1.41 B
0.00.051.408 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.408 I llm_load_print_meta: general.name     = 1.4B
0.00.051.408 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.409 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.409 I llm_load_print_meta: LF token         = 128 ''
0.00.051.411 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.411 I llm_load_print_meta: max token length = 1024
0.00.053.368 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.368 I llm_load_tensors: offloading output layer to GPU
0.00.053.368 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.379 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.380 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.294 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.295 I llama_new_context_with_model: n_ctx         = 128
0.00.054.295 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.295 I llama_new_context_with_model: n_batch       = 128
0.00.054.295 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.295 I llama_new_context_with_model: flash_attn    = 0
0.00.054.296 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.296 I llama_new_context_with_model: freq_scale    = 1
0.00.054.296 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.297 I ggml_metal_init: allocating
0.00.054.303 I ggml_metal_init: found device: Apple M4
0.00.054.305 I ggml_metal_init: picking default device: Apple M4
0.00.054.858 I ggml_metal_init: using embedded metal library
0.00.057.184 I ggml_metal_init: GPU name:   Apple M4
0.00.057.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.186 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.187 I ggml_metal_init: simdgroup reduction   = true
0.00.057.187 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.187 I ggml_metal_init: has bfloat            = true
0.00.057.187 I ggml_metal_init: use bfloat            = true
0.00.057.188 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.188 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.658 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.901 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.903 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.917 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.770 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.771 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.771 I llama_new_context_with_model: graph nodes  = 967
0.00.068.771 I llama_new_context_with_model: graph splits = 2
0.00.068.784 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.900 I 
0.00.702.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.952 I perplexity: tokenizing the input ..
0.00.711.189 I perplexity: tokenization took 8.235 ms
0.00.711.194 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.846.417 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.847.699 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.847.725 I llama_perf_context_print:        load time =     691.50 ms
0.00.847.725 I llama_perf_context_print: prompt eval time =     135.00 ms /   128 tokens (    1.05 ms per token,   948.15 tokens per second)
0.00.847.726 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.726 I llama_perf_context_print:       total time =     144.83 ms /   129 tokens
0.00.848.156 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.077s
sys	0m0.107s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.830 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.087 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.091 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.096 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.100 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.100 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.103 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.104 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.104 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.052 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.054 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.056 I llama_model_loader: - type  f32:  194 tensors
0.00.025.056 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.056 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.542 I llm_load_vocab: special tokens cache size = 25
0.00.051.507 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.509 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.509 I llm_load_print_meta: arch             = gptneox
0.00.051.510 I llm_load_print_meta: vocab type       = BPE
0.00.051.510 I llm_load_print_meta: n_vocab          = 50304
0.00.051.510 I llm_load_print_meta: n_merges         = 50009
0.00.051.510 I llm_load_print_meta: vocab_only       = 0
0.00.051.511 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.511 I llm_load_print_meta: n_embd           = 2048
0.00.051.511 I llm_load_print_meta: n_layer          = 24
0.00.051.514 I llm_load_print_meta: n_head           = 16
0.00.051.515 I llm_load_print_meta: n_head_kv        = 16
0.00.051.515 I llm_load_print_meta: n_rot            = 32
0.00.051.515 I llm_load_print_meta: n_swa            = 0
0.00.051.515 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.516 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.516 I llm_load_print_meta: n_gqa            = 1
0.00.051.519 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.520 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.520 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.521 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.521 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.521 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.528 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.536 I llm_load_print_meta: n_ff             = 8192
0.00.051.536 I llm_load_print_meta: n_expert         = 0
0.00.051.536 I llm_load_print_meta: n_expert_used    = 0
0.00.051.538 I llm_load_print_meta: causal attn      = 1
0.00.051.539 I llm_load_print_meta: pooling type     = 0
0.00.051.539 I llm_load_print_meta: rope type        = 2
0.00.051.540 I llm_load_print_meta: rope scaling     = linear
0.00.051.540 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.540 I llm_load_print_meta: freq_scale_train = 1
0.00.051.541 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.541 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.541 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.541 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.541 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.541 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.541 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.542 I llm_load_print_meta: model type       = 1.4B
0.00.051.542 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.542 I llm_load_print_meta: model params     = 1.41 B
0.00.051.543 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.543 I llm_load_print_meta: general.name     = 1.4B
0.00.051.543 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.543 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.544 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.544 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.544 I llm_load_print_meta: LF token         = 128 ''
0.00.051.544 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.544 I llm_load_print_meta: max token length = 1024
0.00.053.579 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.580 I llm_load_tensors: offloading output layer to GPU
0.00.053.580 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.590 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.591 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.490 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.490 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.490 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.491 I llama_new_context_with_model: n_batch       = 2048
0.00.054.491 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.491 I llama_new_context_with_model: flash_attn    = 0
0.00.054.492 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.492 I llama_new_context_with_model: freq_scale    = 1
0.00.054.492 I ggml_metal_init: allocating
0.00.054.497 I ggml_metal_init: found device: Apple M4
0.00.054.500 I ggml_metal_init: picking default device: Apple M4
0.00.055.119 I ggml_metal_init: using embedded metal library
0.00.057.401 I ggml_metal_init: GPU name:   Apple M4
0.00.057.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.404 I ggml_metal_init: simdgroup reduction   = true
0.00.057.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.405 I ggml_metal_init: has bfloat            = true
0.00.057.405 I ggml_metal_init: use bfloat            = true
0.00.057.406 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.092 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.228 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.236 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.255 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.309 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.310 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.310 I llama_new_context_with_model: graph nodes  = 967
0.00.087.311 I llama_new_context_with_model: graph splits = 2
0.00.087.326 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.441 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.442 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.595 I main: llama threadpool init, n_threads = 4
0.00.702.634 I 
0.00.702.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.656 I 
0.00.702.895 I sampler seed: 1234
0.00.702.900 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.935 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.965 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.548.902 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.548.902 I llama_perf_context_print:        load time =     693.76 ms
0.01.548.903 I llama_perf_context_print: prompt eval time =      45.16 ms /     7 tokens (    6.45 ms per token,   155.00 tokens per second)
0.01.548.904 I llama_perf_context_print:        eval time =     797.68 ms /    63 runs   (   12.66 ms per token,    78.98 tokens per second)
0.01.548.906 I llama_perf_context_print:       total time =     846.31 ms /    70 tokens
0.01.549.071 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.110s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.187 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.661 I llama_model_loader: - type  f32:  194 tensors
0.00.023.661 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.662 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.111 I llm_load_vocab: special tokens cache size = 25
0.00.049.924 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.928 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.928 I llm_load_print_meta: arch             = gptneox
0.00.049.928 I llm_load_print_meta: vocab type       = BPE
0.00.049.929 I llm_load_print_meta: n_vocab          = 50304
0.00.049.929 I llm_load_print_meta: n_merges         = 50009
0.00.049.930 I llm_load_print_meta: vocab_only       = 0
0.00.049.931 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.931 I llm_load_print_meta: n_embd           = 2048
0.00.049.931 I llm_load_print_meta: n_layer          = 24
0.00.049.933 I llm_load_print_meta: n_head           = 16
0.00.049.936 I llm_load_print_meta: n_head_kv        = 16
0.00.049.936 I llm_load_print_meta: n_rot            = 32
0.00.049.937 I llm_load_print_meta: n_swa            = 0
0.00.049.937 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.937 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.938 I llm_load_print_meta: n_gqa            = 1
0.00.049.939 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.939 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.940 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.942 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.942 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.942 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.942 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.943 I llm_load_print_meta: n_ff             = 8192
0.00.049.943 I llm_load_print_meta: n_expert         = 0
0.00.049.943 I llm_load_print_meta: n_expert_used    = 0
0.00.049.943 I llm_load_print_meta: causal attn      = 1
0.00.049.943 I llm_load_print_meta: pooling type     = 0
0.00.049.943 I llm_load_print_meta: rope type        = 2
0.00.049.944 I llm_load_print_meta: rope scaling     = linear
0.00.049.946 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.947 I llm_load_print_meta: freq_scale_train = 1
0.00.049.947 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.948 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.948 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.948 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.948 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.951 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.952 I llm_load_print_meta: model type       = 1.4B
0.00.049.952 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.954 I llm_load_print_meta: model params     = 1.41 B
0.00.049.955 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.955 I llm_load_print_meta: general.name     = 1.4B
0.00.049.955 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.955 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.955 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.956 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.956 I llm_load_print_meta: LF token         = 128 ''
0.00.049.956 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.956 I llm_load_print_meta: max token length = 1024
0.00.052.055 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.055 I llm_load_tensors: offloading output layer to GPU
0.00.052.056 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.066 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.067 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.057 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.058 I llama_new_context_with_model: n_ctx         = 128
0.00.053.058 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.058 I llama_new_context_with_model: n_batch       = 128
0.00.053.058 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.058 I llama_new_context_with_model: flash_attn    = 0
0.00.053.059 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.059 I llama_new_context_with_model: freq_scale    = 1
0.00.053.059 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.060 I ggml_metal_init: allocating
0.00.053.065 I ggml_metal_init: found device: Apple M4
0.00.053.068 I ggml_metal_init: picking default device: Apple M4
0.00.053.628 I ggml_metal_init: using embedded metal library
0.00.055.962 I ggml_metal_init: GPU name:   Apple M4
0.00.055.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.964 I ggml_metal_init: simdgroup reduction   = true
0.00.055.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.964 I ggml_metal_init: has bfloat            = true
0.00.055.965 I ggml_metal_init: use bfloat            = true
0.00.055.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.429 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.955 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.960 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.976 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.850 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.851 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.851 I llama_new_context_with_model: graph nodes  = 967
0.00.067.852 I llama_new_context_with_model: graph splits = 2
0.00.067.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.857 I 
0.00.642.898 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.911 I perplexity: tokenizing the input ..
0.00.650.868 I perplexity: tokenization took 7.955 ms
0.00.650.871 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.743 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.786.892 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.786.907 I llama_perf_context_print:        load time =     633.66 ms
0.00.786.908 I llama_perf_context_print: prompt eval time =     134.65 ms /   128 tokens (    1.05 ms per token,   950.65 tokens per second)
0.00.786.909 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.909 I llama_perf_context_print:       total time =     144.05 ms /   129 tokens
0.00.787.219 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.077s
sys	0m0.109s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.483 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.167 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.172 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.176 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.177 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.178 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.178 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.179 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.029 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.047 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.751 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.752 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.752 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.753 I llama_model_loader: - type  f32:  194 tensors
0.00.023.753 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.753 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.753 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.766 I llm_load_vocab: special tokens cache size = 25
0.00.050.563 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.566 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.566 I llm_load_print_meta: arch             = gptneox
0.00.050.566 I llm_load_print_meta: vocab type       = BPE
0.00.050.567 I llm_load_print_meta: n_vocab          = 50304
0.00.050.567 I llm_load_print_meta: n_merges         = 50009
0.00.050.567 I llm_load_print_meta: vocab_only       = 0
0.00.050.567 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.567 I llm_load_print_meta: n_embd           = 2048
0.00.050.568 I llm_load_print_meta: n_layer          = 24
0.00.050.570 I llm_load_print_meta: n_head           = 16
0.00.050.571 I llm_load_print_meta: n_head_kv        = 16
0.00.050.571 I llm_load_print_meta: n_rot            = 32
0.00.050.572 I llm_load_print_meta: n_swa            = 0
0.00.050.573 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.575 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.575 I llm_load_print_meta: n_gqa            = 1
0.00.050.576 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.577 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.578 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.578 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.578 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.578 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.579 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.579 I llm_load_print_meta: n_ff             = 8192
0.00.050.581 I llm_load_print_meta: n_expert         = 0
0.00.050.581 I llm_load_print_meta: n_expert_used    = 0
0.00.050.581 I llm_load_print_meta: causal attn      = 1
0.00.050.581 I llm_load_print_meta: pooling type     = 0
0.00.050.581 I llm_load_print_meta: rope type        = 2
0.00.050.582 I llm_load_print_meta: rope scaling     = linear
0.00.050.582 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.583 I llm_load_print_meta: freq_scale_train = 1
0.00.050.583 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.583 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.583 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.584 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.584 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.585 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.585 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.589 I llm_load_print_meta: model type       = 1.4B
0.00.050.590 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.590 I llm_load_print_meta: model params     = 1.41 B
0.00.050.590 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.590 I llm_load_print_meta: general.name     = 1.4B
0.00.050.591 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.591 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.591 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.591 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.592 I llm_load_print_meta: LF token         = 128 ''
0.00.050.592 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.592 I llm_load_print_meta: max token length = 1024
0.00.052.502 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.503 I llm_load_tensors: offloading output layer to GPU
0.00.052.503 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.514 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.515 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.422 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.423 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.423 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.423 I llama_new_context_with_model: n_batch       = 2048
0.00.053.423 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.423 I llama_new_context_with_model: flash_attn    = 0
0.00.053.424 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.424 I llama_new_context_with_model: freq_scale    = 1
0.00.053.425 I ggml_metal_init: allocating
0.00.053.428 I ggml_metal_init: found device: Apple M4
0.00.053.430 I ggml_metal_init: picking default device: Apple M4
0.00.054.032 I ggml_metal_init: using embedded metal library
0.00.056.360 I ggml_metal_init: GPU name:   Apple M4
0.00.056.361 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.362 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.362 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.362 I ggml_metal_init: simdgroup reduction   = true
0.00.056.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.363 I ggml_metal_init: has bfloat            = true
0.00.056.363 I ggml_metal_init: use bfloat            = true
0.00.056.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.446 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.534 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.543 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.566 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.546 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.548 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.548 I llama_new_context_with_model: graph nodes  = 967
0.00.087.548 I llama_new_context_with_model: graph splits = 2
0.00.087.564 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.443.992 I main: llama threadpool init, n_threads = 4
0.00.444.031 I 
0.00.444.053 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.053 I 
0.00.444.282 I sampler seed: 1234
0.00.444.286 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.328 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.332 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.332 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.121.074 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.121.075 I llama_perf_context_print:        load time =     434.50 ms
0.01.121.076 I llama_perf_context_print: prompt eval time =      35.86 ms /     7 tokens (    5.12 ms per token,   195.23 tokens per second)
0.01.121.077 I llama_perf_context_print:        eval time =     637.92 ms /    63 runs   (   10.13 ms per token,    98.76 tokens per second)
0.01.121.077 I llama_perf_context_print:       total time =     677.09 ms /    70 tokens
0.01.121.252 I ggml_metal_free: deallocating

real	0m1.139s
user	0m0.111s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.172 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.698 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.710 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.711 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.713 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.714 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.655 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.590 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.591 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.591 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.592 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.592 I llama_model_loader: - type  f32:  194 tensors
0.00.024.593 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.593 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.593 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.567 I llm_load_vocab: special tokens cache size = 25
0.00.051.432 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.435 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.435 I llm_load_print_meta: arch             = gptneox
0.00.051.436 I llm_load_print_meta: vocab type       = BPE
0.00.051.436 I llm_load_print_meta: n_vocab          = 50304
0.00.051.436 I llm_load_print_meta: n_merges         = 50009
0.00.051.436 I llm_load_print_meta: vocab_only       = 0
0.00.051.436 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.437 I llm_load_print_meta: n_embd           = 2048
0.00.051.437 I llm_load_print_meta: n_layer          = 24
0.00.051.439 I llm_load_print_meta: n_head           = 16
0.00.051.440 I llm_load_print_meta: n_head_kv        = 16
0.00.051.442 I llm_load_print_meta: n_rot            = 32
0.00.051.442 I llm_load_print_meta: n_swa            = 0
0.00.051.442 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.443 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.443 I llm_load_print_meta: n_gqa            = 1
0.00.051.444 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.445 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.446 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.446 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.446 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.446 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.446 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.447 I llm_load_print_meta: n_ff             = 8192
0.00.051.447 I llm_load_print_meta: n_expert         = 0
0.00.051.447 I llm_load_print_meta: n_expert_used    = 0
0.00.051.448 I llm_load_print_meta: causal attn      = 1
0.00.051.448 I llm_load_print_meta: pooling type     = 0
0.00.051.453 I llm_load_print_meta: rope type        = 2
0.00.051.453 I llm_load_print_meta: rope scaling     = linear
0.00.051.454 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.454 I llm_load_print_meta: freq_scale_train = 1
0.00.051.454 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.454 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.455 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.455 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.455 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.455 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.455 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.457 I llm_load_print_meta: model type       = 1.4B
0.00.051.457 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.457 I llm_load_print_meta: model params     = 1.41 B
0.00.051.458 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.458 I llm_load_print_meta: general.name     = 1.4B
0.00.051.458 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.458 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.458 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.459 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.459 I llm_load_print_meta: LF token         = 128 ''
0.00.051.459 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.459 I llm_load_print_meta: max token length = 1024
0.00.053.343 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.343 I llm_load_tensors: offloading output layer to GPU
0.00.053.343 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.354 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.356 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.300 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.301 I llama_new_context_with_model: n_ctx         = 128
0.00.054.301 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.301 I llama_new_context_with_model: n_batch       = 128
0.00.054.301 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.302 I llama_new_context_with_model: flash_attn    = 0
0.00.054.302 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.302 I llama_new_context_with_model: freq_scale    = 1
0.00.054.303 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.303 I ggml_metal_init: allocating
0.00.054.306 I ggml_metal_init: found device: Apple M4
0.00.054.308 I ggml_metal_init: picking default device: Apple M4
0.00.054.873 I ggml_metal_init: using embedded metal library
0.00.057.229 I ggml_metal_init: GPU name:   Apple M4
0.00.057.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.230 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.231 I ggml_metal_init: simdgroup reduction   = true
0.00.057.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.231 I ggml_metal_init: has bfloat            = true
0.00.057.231 I ggml_metal_init: use bfloat            = true
0.00.057.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.098 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.450 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.454 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.470 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.339 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.340 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.340 I llama_new_context_with_model: graph nodes  = 967
0.00.069.340 I llama_new_context_with_model: graph splits = 2
0.00.069.352 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.382.691 I 
0.00.382.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.382.737 I perplexity: tokenizing the input ..
0.00.390.428 I perplexity: tokenization took 7.689 ms
0.00.390.431 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.523.271 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.524.547 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.524.567 I llama_perf_context_print:        load time =     372.51 ms
0.00.524.568 I llama_perf_context_print: prompt eval time =     132.61 ms /   128 tokens (    1.04 ms per token,   965.23 tokens per second)
0.00.524.569 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.524.570 I llama_perf_context_print:       total time =     141.88 ms /   129 tokens
0.00.525.073 I ggml_metal_free: deallocating

real	0m0.539s
user	0m0.079s
sys	0m0.065s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.577 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.628 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.633 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.635 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.636 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.638 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.518 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.622 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.474 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.475 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.476 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.477 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.477 I llama_model_loader: - type  f32:  194 tensors
0.00.025.478 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.478 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.478 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.478 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.908 I llm_load_vocab: special tokens cache size = 25
0.00.051.764 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.767 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.767 I llm_load_print_meta: arch             = gptneox
0.00.051.768 I llm_load_print_meta: vocab type       = BPE
0.00.051.768 I llm_load_print_meta: n_vocab          = 50304
0.00.051.768 I llm_load_print_meta: n_merges         = 50009
0.00.051.768 I llm_load_print_meta: vocab_only       = 0
0.00.051.768 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.768 I llm_load_print_meta: n_embd           = 2048
0.00.051.769 I llm_load_print_meta: n_layer          = 24
0.00.051.772 I llm_load_print_meta: n_head           = 16
0.00.051.772 I llm_load_print_meta: n_head_kv        = 16
0.00.051.773 I llm_load_print_meta: n_rot            = 32
0.00.051.773 I llm_load_print_meta: n_swa            = 0
0.00.051.773 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.773 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.774 I llm_load_print_meta: n_gqa            = 1
0.00.051.775 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.777 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.778 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.778 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.778 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.779 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.779 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.779 I llm_load_print_meta: n_ff             = 8192
0.00.051.780 I llm_load_print_meta: n_expert         = 0
0.00.051.780 I llm_load_print_meta: n_expert_used    = 0
0.00.051.780 I llm_load_print_meta: causal attn      = 1
0.00.051.780 I llm_load_print_meta: pooling type     = 0
0.00.051.780 I llm_load_print_meta: rope type        = 2
0.00.051.781 I llm_load_print_meta: rope scaling     = linear
0.00.051.787 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.790 I llm_load_print_meta: freq_scale_train = 1
0.00.051.790 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.791 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.791 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.792 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.792 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.792 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.792 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.793 I llm_load_print_meta: model type       = 1.4B
0.00.051.793 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.793 I llm_load_print_meta: model params     = 1.41 B
0.00.051.794 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.794 I llm_load_print_meta: general.name     = 1.4B
0.00.051.794 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.795 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.795 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.795 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.795 I llm_load_print_meta: LF token         = 128 ''
0.00.051.795 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.796 I llm_load_print_meta: max token length = 1024
0.00.053.356 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.356 I llm_load_tensors: offloading output layer to GPU
0.00.053.356 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.366 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.367 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.210 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.211 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.211 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.211 I llama_new_context_with_model: n_batch       = 2048
0.00.054.211 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.211 I llama_new_context_with_model: flash_attn    = 0
0.00.054.212 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.212 I llama_new_context_with_model: freq_scale    = 1
0.00.054.213 I ggml_metal_init: allocating
0.00.054.216 I ggml_metal_init: found device: Apple M4
0.00.054.218 I ggml_metal_init: picking default device: Apple M4
0.00.054.802 I ggml_metal_init: using embedded metal library
0.00.057.159 I ggml_metal_init: GPU name:   Apple M4
0.00.057.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.161 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.161 I ggml_metal_init: simdgroup reduction   = true
0.00.057.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.162 I ggml_metal_init: has bfloat            = true
0.00.057.162 I ggml_metal_init: use bfloat            = true
0.00.057.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.687 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.650 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.663 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.685 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.655 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.656 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.657 I llama_new_context_with_model: graph nodes  = 967
0.00.086.657 I llama_new_context_with_model: graph splits = 2
0.00.086.672 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.815 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.552.904 I main: llama threadpool init, n_threads = 4
0.00.553.009 I 
0.00.553.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.553.039 I 
0.00.553.279 I sampler seed: 1234
0.00.553.286 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.553.338 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.553.359 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.553.361 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.290.565 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47940.58 tokens per second)
0.01.290.565 I llama_perf_context_print:        load time =     543.32 ms
0.01.290.567 I llama_perf_context_print: prompt eval time =      40.66 ms /     7 tokens (    5.81 ms per token,   172.18 tokens per second)
0.01.290.568 I llama_perf_context_print:        eval time =     694.06 ms /    63 runs   (   11.02 ms per token,    90.77 tokens per second)
0.01.290.568 I llama_perf_context_print:       total time =     737.67 ms /    70 tokens
0.01.290.774 I ggml_metal_free: deallocating

real	0m1.309s
user	0m0.110s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.845 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.673 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.679 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.360 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.361 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.362 I llama_model_loader: - type  f32:  194 tensors
0.00.023.362 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.362 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.363 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.363 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.629 I llm_load_vocab: special tokens cache size = 25
0.00.049.713 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.716 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.716 I llm_load_print_meta: arch             = gptneox
0.00.049.717 I llm_load_print_meta: vocab type       = BPE
0.00.049.717 I llm_load_print_meta: n_vocab          = 50304
0.00.049.717 I llm_load_print_meta: n_merges         = 50009
0.00.049.717 I llm_load_print_meta: vocab_only       = 0
0.00.049.718 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.718 I llm_load_print_meta: n_embd           = 2048
0.00.049.718 I llm_load_print_meta: n_layer          = 24
0.00.049.721 I llm_load_print_meta: n_head           = 16
0.00.049.722 I llm_load_print_meta: n_head_kv        = 16
0.00.049.722 I llm_load_print_meta: n_rot            = 32
0.00.049.724 I llm_load_print_meta: n_swa            = 0
0.00.049.724 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.724 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.725 I llm_load_print_meta: n_gqa            = 1
0.00.049.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.726 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.727 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.727 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.727 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.728 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.728 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.728 I llm_load_print_meta: n_ff             = 8192
0.00.049.729 I llm_load_print_meta: n_expert         = 0
0.00.049.729 I llm_load_print_meta: n_expert_used    = 0
0.00.049.729 I llm_load_print_meta: causal attn      = 1
0.00.049.729 I llm_load_print_meta: pooling type     = 0
0.00.049.731 I llm_load_print_meta: rope type        = 2
0.00.049.732 I llm_load_print_meta: rope scaling     = linear
0.00.049.732 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.733 I llm_load_print_meta: freq_scale_train = 1
0.00.049.733 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.733 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.733 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.733 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.733 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.734 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.734 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.734 I llm_load_print_meta: model type       = 1.4B
0.00.049.734 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.738 I llm_load_print_meta: model params     = 1.41 B
0.00.049.739 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.739 I llm_load_print_meta: general.name     = 1.4B
0.00.049.739 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.740 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.740 I llm_load_print_meta: LF token         = 128 ''
0.00.049.740 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.740 I llm_load_print_meta: max token length = 1024
0.00.051.685 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.685 I llm_load_tensors: offloading output layer to GPU
0.00.051.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.696 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.697 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.703 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.704 I llama_new_context_with_model: n_ctx         = 128
0.00.052.704 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.705 I llama_new_context_with_model: n_batch       = 128
0.00.052.705 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.705 I llama_new_context_with_model: flash_attn    = 0
0.00.052.705 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.705 I llama_new_context_with_model: freq_scale    = 1
0.00.052.706 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.706 I ggml_metal_init: allocating
0.00.052.709 I ggml_metal_init: found device: Apple M4
0.00.052.711 I ggml_metal_init: picking default device: Apple M4
0.00.053.281 I ggml_metal_init: using embedded metal library
0.00.055.587 I ggml_metal_init: GPU name:   Apple M4
0.00.055.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.589 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.590 I ggml_metal_init: simdgroup reduction   = true
0.00.055.590 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.591 I ggml_metal_init: has bfloat            = true
0.00.055.591 I ggml_metal_init: use bfloat            = true
0.00.055.591 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.592 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.241 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.580 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.584 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.598 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.506 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.507 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.507 I llama_new_context_with_model: graph nodes  = 967
0.00.067.507 I llama_new_context_with_model: graph splits = 2
0.00.067.520 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.319 I 
0.00.479.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.479.370 I perplexity: tokenizing the input ..
0.00.487.498 I perplexity: tokenization took 8.127 ms
0.00.487.504 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.840 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.620.991 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.621.007 I llama_perf_context_print:        load time =     470.47 ms
0.00.621.009 I llama_perf_context_print: prompt eval time =     132.11 ms /   128 tokens (    1.03 ms per token,   968.90 tokens per second)
0.00.621.009 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.010 I llama_perf_context_print:       total time =     141.69 ms /   129 tokens
0.00.621.412 I ggml_metal_free: deallocating

real	0m0.635s
user	0m0.078s
sys	0m0.083s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.965 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.482 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.487 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.488 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.492 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.492 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.338 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.223 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.224 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.226 I llama_model_loader: - type  f32:  194 tensors
0.00.025.226 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.227 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.227 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.480 I llm_load_vocab: special tokens cache size = 25
0.00.052.503 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.506 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.506 I llm_load_print_meta: arch             = gptneox
0.00.052.507 I llm_load_print_meta: vocab type       = BPE
0.00.052.507 I llm_load_print_meta: n_vocab          = 50304
0.00.052.507 I llm_load_print_meta: n_merges         = 50009
0.00.052.507 I llm_load_print_meta: vocab_only       = 0
0.00.052.507 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.507 I llm_load_print_meta: n_embd           = 2048
0.00.052.508 I llm_load_print_meta: n_layer          = 24
0.00.052.510 I llm_load_print_meta: n_head           = 16
0.00.052.511 I llm_load_print_meta: n_head_kv        = 16
0.00.052.511 I llm_load_print_meta: n_rot            = 32
0.00.052.512 I llm_load_print_meta: n_swa            = 0
0.00.052.512 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.512 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.513 I llm_load_print_meta: n_gqa            = 1
0.00.052.513 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.515 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.516 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.516 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.516 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.517 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.518 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.518 I llm_load_print_meta: n_ff             = 8192
0.00.052.518 I llm_load_print_meta: n_expert         = 0
0.00.052.520 I llm_load_print_meta: n_expert_used    = 0
0.00.052.522 I llm_load_print_meta: causal attn      = 1
0.00.052.522 I llm_load_print_meta: pooling type     = 0
0.00.052.522 I llm_load_print_meta: rope type        = 2
0.00.052.522 I llm_load_print_meta: rope scaling     = linear
0.00.052.523 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.523 I llm_load_print_meta: freq_scale_train = 1
0.00.052.523 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.523 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.523 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.523 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.524 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.524 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.524 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.524 I llm_load_print_meta: model type       = 1.4B
0.00.052.525 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.525 I llm_load_print_meta: model params     = 1.41 B
0.00.052.526 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.526 I llm_load_print_meta: general.name     = 1.4B
0.00.052.526 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.526 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.526 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.527 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.527 I llm_load_print_meta: LF token         = 128 ''
0.00.052.527 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.528 I llm_load_print_meta: max token length = 1024
0.00.054.596 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.596 I llm_load_tensors: offloading output layer to GPU
0.00.054.596 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.607 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.608 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.499 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.500 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.500 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.500 I llama_new_context_with_model: n_batch       = 2048
0.00.055.500 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.501 I llama_new_context_with_model: flash_attn    = 0
0.00.055.501 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.501 I llama_new_context_with_model: freq_scale    = 1
0.00.055.502 I ggml_metal_init: allocating
0.00.055.508 I ggml_metal_init: found device: Apple M4
0.00.055.510 I ggml_metal_init: picking default device: Apple M4
0.00.056.107 I ggml_metal_init: using embedded metal library
0.00.058.416 I ggml_metal_init: GPU name:   Apple M4
0.00.058.418 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.418 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.419 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.420 I ggml_metal_init: simdgroup reduction   = true
0.00.058.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.421 I ggml_metal_init: has bfloat            = true
0.00.058.421 I ggml_metal_init: use bfloat            = true
0.00.058.421 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.426 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.664 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.030 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.035 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.055 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.009 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.010 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.010 I llama_new_context_with_model: graph nodes  = 967
0.00.090.011 I llama_new_context_with_model: graph splits = 2
0.00.090.026 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.168 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.169 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.378 I main: llama threadpool init, n_threads = 4
0.00.619.421 I 
0.00.619.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.441 I 
0.00.619.664 I sampler seed: 1234
0.00.619.668 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.619.711 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.619.715 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.619.715 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.380.649 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.380.650 I llama_perf_context_print:        load time =     609.41 ms
0.01.380.650 I llama_perf_context_print: prompt eval time =      47.21 ms /     7 tokens (    6.74 ms per token,   148.29 tokens per second)
0.01.380.652 I llama_perf_context_print:        eval time =     710.79 ms /    63 runs   (   11.28 ms per token,    88.63 tokens per second)
0.01.380.652 I llama_perf_context_print:       total time =     761.27 ms /    70 tokens
0.01.380.855 I ggml_metal_free: deallocating

real	0m1.399s
user	0m0.111s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.395 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.354 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.359 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.360 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.361 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.361 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.361 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.363 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.363 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.364 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.365 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.366 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.368 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.368 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.943 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.943 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.943 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.944 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.944 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.945 I llama_model_loader: - type  f32:  194 tensors
0.00.023.945 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.945 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.945 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.289 I llm_load_vocab: special tokens cache size = 25
0.00.050.278 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.281 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.281 I llm_load_print_meta: arch             = gptneox
0.00.050.281 I llm_load_print_meta: vocab type       = BPE
0.00.050.282 I llm_load_print_meta: n_vocab          = 50304
0.00.050.282 I llm_load_print_meta: n_merges         = 50009
0.00.050.282 I llm_load_print_meta: vocab_only       = 0
0.00.050.282 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.282 I llm_load_print_meta: n_embd           = 2048
0.00.050.283 I llm_load_print_meta: n_layer          = 24
0.00.050.285 I llm_load_print_meta: n_head           = 16
0.00.050.286 I llm_load_print_meta: n_head_kv        = 16
0.00.050.286 I llm_load_print_meta: n_rot            = 32
0.00.050.286 I llm_load_print_meta: n_swa            = 0
0.00.050.287 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.287 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.287 I llm_load_print_meta: n_gqa            = 1
0.00.050.289 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.289 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.290 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.290 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.290 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.290 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.291 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.292 I llm_load_print_meta: n_ff             = 8192
0.00.050.292 I llm_load_print_meta: n_expert         = 0
0.00.050.292 I llm_load_print_meta: n_expert_used    = 0
0.00.050.293 I llm_load_print_meta: causal attn      = 1
0.00.050.293 I llm_load_print_meta: pooling type     = 0
0.00.050.293 I llm_load_print_meta: rope type        = 2
0.00.050.293 I llm_load_print_meta: rope scaling     = linear
0.00.050.294 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.294 I llm_load_print_meta: freq_scale_train = 1
0.00.050.294 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.294 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.294 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.294 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.295 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.296 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.296 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.297 I llm_load_print_meta: model type       = 1.4B
0.00.050.297 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.297 I llm_load_print_meta: model params     = 1.41 B
0.00.050.298 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.298 I llm_load_print_meta: general.name     = 1.4B
0.00.050.298 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.299 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.299 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.299 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: LF token         = 128 ''
0.00.050.300 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: max token length = 1024
0.00.052.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.268 I llm_load_tensors: offloading output layer to GPU
0.00.052.268 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.279 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.280 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.170 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.170 I llama_new_context_with_model: n_ctx         = 128
0.00.053.170 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.171 I llama_new_context_with_model: n_batch       = 128
0.00.053.171 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.171 I llama_new_context_with_model: flash_attn    = 0
0.00.053.172 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.172 I llama_new_context_with_model: freq_scale    = 1
0.00.053.172 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.173 I ggml_metal_init: allocating
0.00.053.176 I ggml_metal_init: found device: Apple M4
0.00.053.178 I ggml_metal_init: picking default device: Apple M4
0.00.053.740 I ggml_metal_init: using embedded metal library
0.00.056.074 I ggml_metal_init: GPU name:   Apple M4
0.00.056.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.076 I ggml_metal_init: simdgroup reduction   = true
0.00.056.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.076 I ggml_metal_init: has bfloat            = true
0.00.056.076 I ggml_metal_init: use bfloat            = true
0.00.056.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.644 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.886 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.888 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.902 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.780 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.781 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.781 I llama_new_context_with_model: graph nodes  = 967
0.00.067.781 I llama_new_context_with_model: graph splits = 2
0.00.067.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.607 I 
0.00.574.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.653 I perplexity: tokenizing the input ..
0.00.582.399 I perplexity: tokenization took 7.745 ms
0.00.582.403 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.716.850 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.718.020 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.718.032 I llama_perf_context_print:        load time =     565.21 ms
0.00.718.033 I llama_perf_context_print: prompt eval time =     134.22 ms /   128 tokens (    1.05 ms per token,   953.67 tokens per second)
0.00.718.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.034 I llama_perf_context_print:       total time =     143.43 ms /   129 tokens
0.00.718.392 I ggml_metal_free: deallocating

real	0m0.733s
user	0m0.078s
sys	0m0.111s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.881 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.827 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.831 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.838 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.838 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.839 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.840 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.840 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.841 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.841 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.841 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.842 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.842 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.844 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.844 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.740 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.537 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.540 I llama_model_loader: - type  f32:  194 tensors
0.00.024.540 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.541 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.890 I llm_load_vocab: special tokens cache size = 25
0.00.050.683 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.685 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.685 I llm_load_print_meta: arch             = gptneox
0.00.050.686 I llm_load_print_meta: vocab type       = BPE
0.00.050.686 I llm_load_print_meta: n_vocab          = 50304
0.00.050.686 I llm_load_print_meta: n_merges         = 50009
0.00.050.686 I llm_load_print_meta: vocab_only       = 0
0.00.050.687 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.687 I llm_load_print_meta: n_embd           = 2048
0.00.050.687 I llm_load_print_meta: n_layer          = 24
0.00.050.689 I llm_load_print_meta: n_head           = 16
0.00.050.690 I llm_load_print_meta: n_head_kv        = 16
0.00.050.690 I llm_load_print_meta: n_rot            = 32
0.00.050.691 I llm_load_print_meta: n_swa            = 0
0.00.050.692 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.693 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.694 I llm_load_print_meta: n_gqa            = 1
0.00.050.695 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.696 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.696 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.697 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.697 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.697 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.697 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.698 I llm_load_print_meta: n_ff             = 8192
0.00.050.698 I llm_load_print_meta: n_expert         = 0
0.00.050.698 I llm_load_print_meta: n_expert_used    = 0
0.00.050.698 I llm_load_print_meta: causal attn      = 1
0.00.050.698 I llm_load_print_meta: pooling type     = 0
0.00.050.698 I llm_load_print_meta: rope type        = 2
0.00.050.699 I llm_load_print_meta: rope scaling     = linear
0.00.050.699 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.699 I llm_load_print_meta: freq_scale_train = 1
0.00.050.700 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.700 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.700 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.700 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.700 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.700 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.700 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.701 I llm_load_print_meta: model type       = 1.4B
0.00.050.701 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.702 I llm_load_print_meta: model params     = 1.41 B
0.00.050.702 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.702 I llm_load_print_meta: general.name     = 1.4B
0.00.050.703 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.703 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.703 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.703 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.703 I llm_load_print_meta: LF token         = 128 ''
0.00.050.704 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.704 I llm_load_print_meta: max token length = 1024
0.00.052.679 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.679 I llm_load_tensors: offloading output layer to GPU
0.00.052.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.689 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.690 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.602 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.603 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.603 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.603 I llama_new_context_with_model: n_batch       = 2048
0.00.053.603 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.603 I llama_new_context_with_model: flash_attn    = 0
0.00.053.604 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.604 I llama_new_context_with_model: freq_scale    = 1
0.00.053.604 I ggml_metal_init: allocating
0.00.053.610 I ggml_metal_init: found device: Apple M4
0.00.053.612 I ggml_metal_init: picking default device: Apple M4
0.00.054.197 I ggml_metal_init: using embedded metal library
0.00.056.542 I ggml_metal_init: GPU name:   Apple M4
0.00.056.544 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.544 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.545 I ggml_metal_init: simdgroup reduction   = true
0.00.056.545 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.545 I ggml_metal_init: has bfloat            = true
0.00.056.545 I ggml_metal_init: use bfloat            = true
0.00.056.545 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.125 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.778 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.783 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.802 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.670 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.671 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.671 I llama_new_context_with_model: graph nodes  = 967
0.00.086.672 I llama_new_context_with_model: graph splits = 2
0.00.086.686 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.819 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.819 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.380 I main: llama threadpool init, n_threads = 4
0.00.675.416 I 
0.00.675.456 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.457 I 
0.00.675.598 I sampler seed: 1234
0.00.675.607 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.675.652 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.675.661 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.675.662 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.525.002 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.525.003 I llama_perf_context_print:        load time =     666.50 ms
0.01.525.004 I llama_perf_context_print: prompt eval time =      51.54 ms /     7 tokens (    7.36 ms per token,   135.82 tokens per second)
0.01.525.004 I llama_perf_context_print:        eval time =     794.68 ms /    63 runs   (   12.61 ms per token,    79.28 tokens per second)
0.01.525.005 I llama_perf_context_print:       total time =     849.62 ms /    70 tokens
0.01.525.156 I ggml_metal_free: deallocating

real	0m1.544s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.894 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.832 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.837 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.839 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.839 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.840 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.840 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.842 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.843 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.848 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.849 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.849 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.585 I llama_model_loader: - type  f32:  194 tensors
0.00.023.585 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.585 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.685 I llm_load_vocab: special tokens cache size = 25
0.00.050.672 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.675 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.675 I llm_load_print_meta: arch             = gptneox
0.00.050.675 I llm_load_print_meta: vocab type       = BPE
0.00.050.676 I llm_load_print_meta: n_vocab          = 50304
0.00.050.676 I llm_load_print_meta: n_merges         = 50009
0.00.050.676 I llm_load_print_meta: vocab_only       = 0
0.00.050.676 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.676 I llm_load_print_meta: n_embd           = 2048
0.00.050.677 I llm_load_print_meta: n_layer          = 24
0.00.050.680 I llm_load_print_meta: n_head           = 16
0.00.050.680 I llm_load_print_meta: n_head_kv        = 16
0.00.050.680 I llm_load_print_meta: n_rot            = 32
0.00.050.681 I llm_load_print_meta: n_swa            = 0
0.00.050.682 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.682 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.684 I llm_load_print_meta: n_gqa            = 1
0.00.050.685 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.686 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.686 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.687 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.688 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.688 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.689 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.689 I llm_load_print_meta: n_ff             = 8192
0.00.050.689 I llm_load_print_meta: n_expert         = 0
0.00.050.690 I llm_load_print_meta: n_expert_used    = 0
0.00.050.690 I llm_load_print_meta: causal attn      = 1
0.00.050.690 I llm_load_print_meta: pooling type     = 0
0.00.050.690 I llm_load_print_meta: rope type        = 2
0.00.050.690 I llm_load_print_meta: rope scaling     = linear
0.00.050.691 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.691 I llm_load_print_meta: freq_scale_train = 1
0.00.050.691 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.695 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.695 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.695 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.696 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.696 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.696 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.696 I llm_load_print_meta: model type       = 1.4B
0.00.050.697 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.697 I llm_load_print_meta: model params     = 1.41 B
0.00.050.698 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.698 I llm_load_print_meta: general.name     = 1.4B
0.00.050.699 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.700 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.700 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.700 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.700 I llm_load_print_meta: LF token         = 128 ''
0.00.050.701 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.701 I llm_load_print_meta: max token length = 1024
0.00.052.715 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.716 I llm_load_tensors: offloading output layer to GPU
0.00.052.716 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.726 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.727 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.617 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.618 I llama_new_context_with_model: n_ctx         = 128
0.00.053.618 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.618 I llama_new_context_with_model: n_batch       = 128
0.00.053.619 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.619 I llama_new_context_with_model: flash_attn    = 0
0.00.053.619 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.620 I llama_new_context_with_model: freq_scale    = 1
0.00.053.620 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.620 I ggml_metal_init: allocating
0.00.053.626 I ggml_metal_init: found device: Apple M4
0.00.053.628 I ggml_metal_init: picking default device: Apple M4
0.00.054.212 I ggml_metal_init: using embedded metal library
0.00.056.550 I ggml_metal_init: GPU name:   Apple M4
0.00.056.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.552 I ggml_metal_init: simdgroup reduction   = true
0.00.056.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.553 I ggml_metal_init: has bfloat            = true
0.00.056.553 I ggml_metal_init: use bfloat            = true
0.00.056.553 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.133 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.397 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.400 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.423 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.295 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.296 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.297 I llama_new_context_with_model: graph nodes  = 967
0.00.068.297 I llama_new_context_with_model: graph splits = 2
0.00.068.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.310 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.134 I 
0.00.630.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.208 I perplexity: tokenizing the input ..
0.00.637.969 I perplexity: tokenization took 7.759 ms
0.00.637.972 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.171 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.779.529 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.779.542 I llama_perf_context_print:        load time =     621.24 ms
0.00.779.544 I llama_perf_context_print: prompt eval time =     139.97 ms /   128 tokens (    1.09 ms per token,   914.45 tokens per second)
0.00.779.545 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.546 I llama_perf_context_print:       total time =     149.41 ms /   129 tokens
0.00.779.955 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.078s
sys	0m0.105s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.058 I main: llama backend init
0.00.000.060 I main: load the model and apply lora adapter, if any
0.00.009.924 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.413 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.417 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.418 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.419 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.419 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.419 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.420 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.421 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.421 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.421 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.422 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.422 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.423 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.425 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.426 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.389 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.245 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.246 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.247 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.247 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.248 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.248 I llama_model_loader: - type  f32:  194 tensors
0.00.025.249 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.805 I llm_load_vocab: special tokens cache size = 25
0.00.051.703 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.705 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.706 I llm_load_print_meta: arch             = gptneox
0.00.051.706 I llm_load_print_meta: vocab type       = BPE
0.00.051.706 I llm_load_print_meta: n_vocab          = 50304
0.00.051.706 I llm_load_print_meta: n_merges         = 50009
0.00.051.707 I llm_load_print_meta: vocab_only       = 0
0.00.051.707 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.707 I llm_load_print_meta: n_embd           = 2048
0.00.051.707 I llm_load_print_meta: n_layer          = 24
0.00.051.710 I llm_load_print_meta: n_head           = 16
0.00.051.710 I llm_load_print_meta: n_head_kv        = 16
0.00.051.711 I llm_load_print_meta: n_rot            = 32
0.00.051.711 I llm_load_print_meta: n_swa            = 0
0.00.051.711 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.713 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.714 I llm_load_print_meta: n_gqa            = 1
0.00.051.715 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.715 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.716 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.716 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.716 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.717 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.717 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.717 I llm_load_print_meta: n_ff             = 8192
0.00.051.718 I llm_load_print_meta: n_expert         = 0
0.00.051.718 I llm_load_print_meta: n_expert_used    = 0
0.00.051.718 I llm_load_print_meta: causal attn      = 1
0.00.051.719 I llm_load_print_meta: pooling type     = 0
0.00.051.720 I llm_load_print_meta: rope type        = 2
0.00.051.724 I llm_load_print_meta: rope scaling     = linear
0.00.051.725 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.727 I llm_load_print_meta: freq_scale_train = 1
0.00.051.727 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.727 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.727 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.727 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.727 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.727 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.728 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.728 I llm_load_print_meta: model type       = 1.4B
0.00.051.728 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.729 I llm_load_print_meta: model params     = 1.41 B
0.00.051.729 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.729 I llm_load_print_meta: general.name     = 1.4B
0.00.051.729 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.729 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.730 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.730 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.730 I llm_load_print_meta: LF token         = 128 ''
0.00.051.730 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.730 I llm_load_print_meta: max token length = 1024
0.00.053.393 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.394 I llm_load_tensors: offloading output layer to GPU
0.00.053.394 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.404 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.405 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.258 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.259 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.260 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.260 I llama_new_context_with_model: n_batch       = 2048
0.00.054.260 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.260 I llama_new_context_with_model: flash_attn    = 0
0.00.054.261 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.261 I llama_new_context_with_model: freq_scale    = 1
0.00.054.262 I ggml_metal_init: allocating
0.00.054.268 I ggml_metal_init: found device: Apple M4
0.00.054.270 I ggml_metal_init: picking default device: Apple M4
0.00.054.853 I ggml_metal_init: using embedded metal library
0.00.057.190 I ggml_metal_init: GPU name:   Apple M4
0.00.057.191 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.192 I ggml_metal_init: simdgroup reduction   = true
0.00.057.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.193 I ggml_metal_init: has bfloat            = true
0.00.057.193 I ggml_metal_init: use bfloat            = true
0.00.057.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.022 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.298 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.305 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.334 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.297 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.298 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.299 I llama_new_context_with_model: graph nodes  = 967
0.00.087.299 I llama_new_context_with_model: graph splits = 2
0.00.087.313 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.454 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.454 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.428 I main: llama threadpool init, n_threads = 4
0.00.750.468 I 
0.00.750.489 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.489 I 
0.00.750.725 I sampler seed: 1234
0.00.750.730 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.778 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.780 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.780 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.648.214 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.648.215 I llama_perf_context_print:        load time =     740.50 ms
0.01.648.216 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.49 tokens per second)
0.01.648.216 I llama_perf_context_print:        eval time =     839.90 ms /    63 runs   (   13.33 ms per token,    75.01 tokens per second)
0.01.648.217 I llama_perf_context_print:       total time =     897.79 ms /    70 tokens
0.01.648.394 I ggml_metal_free: deallocating

real	0m1.665s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4398 (c250ecb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.888 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.632 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.637 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.640 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.640 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.640 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.641 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.642 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.642 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.642 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.643 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.643 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.643 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.644 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.646 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.646 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.668 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.713 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.714 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.715 I llama_model_loader: - type  f32:  194 tensors
0.00.024.716 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.834 I llm_load_vocab: special tokens cache size = 25
0.00.052.059 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.062 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.062 I llm_load_print_meta: arch             = gptneox
0.00.052.063 I llm_load_print_meta: vocab type       = BPE
0.00.052.063 I llm_load_print_meta: n_vocab          = 50304
0.00.052.063 I llm_load_print_meta: n_merges         = 50009
0.00.052.063 I llm_load_print_meta: vocab_only       = 0
0.00.052.064 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.064 I llm_load_print_meta: n_embd           = 2048
0.00.052.064 I llm_load_print_meta: n_layer          = 24
0.00.052.066 I llm_load_print_meta: n_head           = 16
0.00.052.067 I llm_load_print_meta: n_head_kv        = 16
0.00.052.067 I llm_load_print_meta: n_rot            = 32
0.00.052.067 I llm_load_print_meta: n_swa            = 0
0.00.052.068 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.068 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.068 I llm_load_print_meta: n_gqa            = 1
0.00.052.069 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.070 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.071 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.071 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.072 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.073 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.073 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.073 I llm_load_print_meta: n_ff             = 8192
0.00.052.075 I llm_load_print_meta: n_expert         = 0
0.00.052.075 I llm_load_print_meta: n_expert_used    = 0
0.00.052.075 I llm_load_print_meta: causal attn      = 1
0.00.052.076 I llm_load_print_meta: pooling type     = 0
0.00.052.076 I llm_load_print_meta: rope type        = 2
0.00.052.076 I llm_load_print_meta: rope scaling     = linear
0.00.052.076 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.077 I llm_load_print_meta: freq_scale_train = 1
0.00.052.078 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.078 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.078 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.078 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.079 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.079 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.079 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.079 I llm_load_print_meta: model type       = 1.4B
0.00.052.079 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.080 I llm_load_print_meta: model params     = 1.41 B
0.00.052.080 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.080 I llm_load_print_meta: general.name     = 1.4B
0.00.052.081 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.085 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.085 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.085 I llm_load_print_meta: LF token         = 128 ''
0.00.052.085 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.085 I llm_load_print_meta: max token length = 1024
0.00.054.157 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.157 I llm_load_tensors: offloading output layer to GPU
0.00.054.158 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.168 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.169 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.140 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.141 I llama_new_context_with_model: n_ctx         = 128
0.00.055.141 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.141 I llama_new_context_with_model: n_batch       = 128
0.00.055.142 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.142 I llama_new_context_with_model: flash_attn    = 0
0.00.055.142 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.143 I llama_new_context_with_model: freq_scale    = 1
0.00.055.143 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.143 I ggml_metal_init: allocating
0.00.055.150 I ggml_metal_init: found device: Apple M4
0.00.055.152 I ggml_metal_init: picking default device: Apple M4
0.00.055.710 I ggml_metal_init: using embedded metal library
0.00.058.015 I ggml_metal_init: GPU name:   Apple M4
0.00.058.016 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.017 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.017 I ggml_metal_init: simdgroup reduction   = true
0.00.058.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.017 I ggml_metal_init: has bfloat            = true
0.00.058.017 I ggml_metal_init: use bfloat            = true
0.00.058.018 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.018 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.315 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.573 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.575 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.589 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.456 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.457 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.458 I llama_new_context_with_model: graph nodes  = 967
0.00.069.458 I llama_new_context_with_model: graph splits = 2
0.00.069.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.469 I 
0.00.541.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.523 I perplexity: tokenizing the input ..
0.00.549.635 I perplexity: tokenization took 8.111 ms
0.00.549.643 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.689.675 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.690.834 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.690.852 I llama_perf_context_print:        load time =     531.57 ms
0.00.690.853 I llama_perf_context_print: prompt eval time =     139.81 ms /   128 tokens (    1.09 ms per token,   915.55 tokens per second)
0.00.690.854 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.690.854 I llama_perf_context_print:       total time =     149.39 ms /   129 tokens
0.00.691.295 I ggml_metal_free: deallocating

real	0m0.706s
user	0m0.079s
sys	0m0.104s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4398 (c250ecb3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12da0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12da0b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12da0bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12da0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12da0c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12da0cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12da0d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12da0da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12da0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12da0e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12da0ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12da0ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12da0fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12da101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12da109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12da11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12da11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12da11f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12da12670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12da12e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12da13560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12da13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12da143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12da14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12da15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12da15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12da15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12da168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12da16de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12da170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12da17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12da17800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12da18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12da185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12da18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12da18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12da191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12da19670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12da19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12da19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12da1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12da1a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12da1ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12da1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12da1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12da1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12da1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12da1ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12da1d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12da1d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12da1dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12da1e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12da1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12da1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12da1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12da1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12da1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12da20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12da20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12da21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12da21340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12da217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12da21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12da22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12da225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12da22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12da22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12da233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12da23840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12da23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12da24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12da24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12da24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12da25010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12da25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12da25ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12da26000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12da26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12da26aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12da26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12da27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12da27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12da27fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12da28530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12da28a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12da28fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12da29520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12da29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12da29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12da2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12da2aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12da2afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12da2b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12da2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12da2bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12da2c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12da2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12da1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12da2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12da2d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12da2dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12da2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12da2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12da2eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12da2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12da2f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12da2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12da300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12da30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12da30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12da310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12da31620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12da31b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12da32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12da324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12da32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12da32df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12da33290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12da33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12da33bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12da34070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12da34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12da349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12da34e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12da352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12da35790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12da35c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12da360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12da36570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12da36a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12da36eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12da37350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12da377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12da37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12da38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12da385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12da38a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12da38f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12da393b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12da39850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12da39cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12da3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12da3a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12da3aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12da3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12da3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12da3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12da3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12da3c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12da3c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12da3cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12da3cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12da3d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12da3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12da3ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12da3e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12da3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12da3eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12da3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12da3f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12da3f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12da3fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12da402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12da40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12da40bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12da41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12da41530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12da419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12da41e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12da42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12da427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12da42c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12da430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12da43590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12da43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12da43ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12da44370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12da44810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12da44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12da45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12da455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12da45a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12da45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12da463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12da46870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12da46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12da471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12da47650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12da47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12da47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12da48430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12da488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12da48d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12da492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12da49810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12da49d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12da4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12da4a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12da4ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12da4b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12da4b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12da4bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12da4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12da4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12da4cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12da4d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12da4db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12da4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12da4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12da4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12da4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12da4f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12da4fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12da50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12da505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12da50b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12da51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12da515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12da51b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12da52060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12da525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12da52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12da53050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12da535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12da53af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12da54040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12da54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12da54ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12da55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12da55580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12da55ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12da56020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12da56570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12da56ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12da57010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12da57560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12da57ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12da58000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12da58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12da58aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12da58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12da59540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12da59a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12da59fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12da5a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12da5aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12da5afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12da5b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12da5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12da5bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12da5c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12da5ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12da5cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12da5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12da5da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12da5dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12da5e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12da5ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12da5ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12da5f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12da5fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12da5ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12da604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12da60a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12da60f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12da614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12da61a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12da61eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12da62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12da627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12da62c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12da63130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12da635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12da63a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12da63f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12da643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12da64850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12da64cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12da65190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12da65630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12da65ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12da65f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12da664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12da66be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12da67300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12da67a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12da68140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12da68400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12da68bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12da68eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12da694c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.136.696 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.136.698 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12da18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12da18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12da19300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12da19770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12da19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12da1a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12da1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12da1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12da1ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12da1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12da1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12da1bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12da1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12da1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12da1d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12da1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12da1e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12da1e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12da1f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12da1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12da200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12da207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12da20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12da215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12da21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12da22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12da22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12da229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12da22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12da232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12da23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12da23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12da24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12da242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12da24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12da24bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12da25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12da254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12da25910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12da25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12da261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12da26660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12da26ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12da26f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12da273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12da27820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12da27c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12da28100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12da28570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12da289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12da28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12da292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12da29730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12da29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12da2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12da2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12da2a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12da2ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12da2b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12da2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12da2bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12da2bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12da2c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12da2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12da2cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12da2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12da2d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12da2d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12da2de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12da2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12da2e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12da2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12da2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12da2f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12da2f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12da2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12da301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12da30620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12da30a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12da30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12da31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12da317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12da31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12da320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12da32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12da329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12da32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12da33280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12da336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12da33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12da33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12da34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12da348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12da34d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12da35190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12da35600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12da35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12da35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12da36350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12da367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12da36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12da370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12da37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12da37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12da37df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12da38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12da386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12da38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12da38fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12da39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12da39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12da39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12da3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12da3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12da3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12da3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12da3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12da3b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12da3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12da3c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12da3c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12da3c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12da3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12da3d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12da3d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12da3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12da3df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12da3e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12da3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12da3ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12da3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12da3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12da3fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12da3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12da40310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12da40780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12da40bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12da41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12da414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12da41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12da41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12da42220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12da42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12da42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12da42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12da433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12da43850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12da43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12da44130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12da445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12da44a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12da44e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12da452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12da45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12da45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12da46040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12da464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12da46920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12da46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12da47200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12da47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12da47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146f04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146f044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146f04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146f04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146f05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146f056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146f05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146f05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146f06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146f06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146f06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146f07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146f075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146f07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146f07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146f08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146f08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146f08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146f09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146f094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146f09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146f09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146f0a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146f0a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146f0ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146f0af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146f0b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146f0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146f0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146f0c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146f0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146f0ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146f0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146f0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146f0d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146f0dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146f0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146f0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146f0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146f0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146f0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146f0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146f0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146f0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146f103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146f10830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146f10ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146f11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146f11cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146f11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146f12250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146f126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146f12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146f12fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146f13410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146f13880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146f13cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146f14160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146f145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146f14a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146f14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146f15320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146f15790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146f15c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146f16070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146f164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146f16950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146f16dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146f17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146f176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146f17b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146f17f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146f183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146f18860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146f18cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146f19140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146f195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146f19a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146f19e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146f1a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146f1a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146f1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146f1b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146f1b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146f1b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146f1bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146f1c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146f1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146f1caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146f1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146f1d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146f1d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146f1dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146f1e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146f1e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146f1ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146f1ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146f1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146f1f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146f1fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146f20030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146f204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146f20910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146f20d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146f211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146f21660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146f21ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146f21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146f223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146f22820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146f22c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146f23100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146f23570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146f239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146f23e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146f242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146f24730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146f24ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146f25010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146f25480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146f258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146f26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146f26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146f271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146f278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146f27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146f27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146f285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146f28c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12db04850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12db04cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12db05130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12db055a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12db05a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12db05fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12db06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12db06b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12db070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12db075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12db07ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12db07fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12db08b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12db092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12db09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12db0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12db0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12db0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12db0b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12db0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12db0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12db0cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12db0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12db0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12db0e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12db0e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12db0eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12db0f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12db0f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12db0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12db10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12db106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12db10f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12db114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12db11780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12db11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12db120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12db12560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12db12a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12db12ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12db13340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12db137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12db13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12db14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12db143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12db149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12db15000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12db15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12db15c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12db16230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12db16840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12db16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12db17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12db17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12db18260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12db18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12db18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12db18e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12db19470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12db19c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12db1a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12db1a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12db1aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12db1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12db1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12db1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12db1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12db1c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12db1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12db1caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12db1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12db1d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12db1d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12db1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12db1e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12db1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12db1edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12db1f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12db1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12db1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12db20300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12db20850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12db20da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12db212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12db21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12db21d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12db222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12db22830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12db22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12db232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12db23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12db23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12db242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12db24810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12db24d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12db252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12db25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12db25d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12db262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12db267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12db26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12db27290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12db277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12db27d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12db28280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12db287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12db28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12db29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12db297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12db29d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12db2a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12db2a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12db2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12db2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12db2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12db2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12db2bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12db2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12db2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12db2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12db2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12db2d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12db2db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12db2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12db2e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12db2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12db2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12db2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12db2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12db2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12db30040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12db304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12db30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12db30e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12db312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12db31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12db31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12db320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12db32540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12db329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12db32e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12db33320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12db337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12db33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12db34100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12db345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12db34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12db34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12db35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12db35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12db35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12db36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12db36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12db36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12db36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12db373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12db37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12db37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12db381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12db38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12db38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12db38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12db39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12db398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12db39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12db3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12db3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12db3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12db3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12db3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12db3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12db3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12db3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12db3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12db3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12db3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12db3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12db3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12db3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12db3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12db3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12db3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12db3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12db3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12db3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12db3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12db40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12db407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12db40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12db41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12db415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12db41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12db41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12db42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12db429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12db42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12db43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12db43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12db43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12db44320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12db44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12db45120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12db455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12db45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12db45e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12db464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12db46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12db47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12db475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12db47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12db48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12db48770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12db48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12db49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12db49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12db49cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12db4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12db4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12db4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12db4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12db4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12db4bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12db4c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12db4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12db4cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12db4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12db4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12db4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12db4e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12db4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12db4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12db4f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12db4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12db4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12db501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12db506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12db50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12db51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12db516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12db51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12db52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12db526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12db52c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12db53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12db536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12db53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12db54160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12db546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12db54c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12db55150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12db556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12db55bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12db56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12db56690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12db56be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12db57130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12db57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12db57bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12db58120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12db58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12db58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12db59110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12db59660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12db59bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12db5a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12db5a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12db5aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12db5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12db5b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12db5b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12db5be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12db5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12db5c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12db5cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12db5d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12db5d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12db5d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12db5de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12db5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12db5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12db5ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12db5f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12db5f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12db5fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12db60490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12db60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12db612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12db61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12db61d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12db62040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12db62650 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.852s
user	0m0.285s
sys	0m0.311s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4398 (c250ecb3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d70b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d70b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d70bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d70c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d70c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d70ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d70d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d70d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d70df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d70e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d70e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d70ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d70f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d710120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d710930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d711050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d711770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d711e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d7125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d712d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d7134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d713bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d7142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d714b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d7152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d715560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d715b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d7167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d716d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d716fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d717480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d717740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d718510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d7187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d718c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d719110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d7195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d719a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d719ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d71a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d71a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d71acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d71b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d71b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d71ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d71c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d71c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d71cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d71d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d71dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d71e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d71e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d71edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d71f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d71fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d71ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d7201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d7207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d720fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d721280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d721720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d722060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d722500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d7229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d722e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d7232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d723780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d723c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d7240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d724560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d724a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d724f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d7254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d7259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d725f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d726490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d7269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d726f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d727480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d7279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d727f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d728470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d7289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d728f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d729460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d7299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d729f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d72a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d72a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d72aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d72b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d72b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d72bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d72c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d72c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d71c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d72cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d72d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d72daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d72e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d72e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d72eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d72f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d72f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d72fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d730020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d730570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d730ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d731010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d731560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d731ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d731f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d7323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d732890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d732d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d7331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d733670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d733fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d734450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d7348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d734d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d735230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d7356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d735b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d736010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d7364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d736950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d736df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d737290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d737730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d737bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d738070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d738510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d7389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d738e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d7392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d739790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d739c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d73a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d73a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d73aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d73aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d73b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d73b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d73bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d73c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d73c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d73ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d73cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d73d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d73d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d73dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d73e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d73e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d73ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d73ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d73f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d73f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d73fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d7401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d740690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d740b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d740fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d741470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d741910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d741db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d742250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d7426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d742b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d743030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d7434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d743970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d743e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d7442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d744750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d744bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d745090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d745530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d7459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d745e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d746310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d7467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d746c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d7470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d747590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d747a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d747ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d748370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d748810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d748cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d749200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d749750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d749ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d74a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d74a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d74aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d74b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d74b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d74bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d74c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d74c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d74cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d74d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d74da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d74dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d74e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d74e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d74efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d74f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d74fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d74ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d750510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d750a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d750fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d751500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d751a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d751fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d7524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d752a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d752f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d7534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d753a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d753f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d7544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d754a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d754f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d7554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d755a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d755f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d7564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d756a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d756f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d7574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d7579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d757f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d758490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d7589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d758f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d759480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d7599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d759f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d75a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d75a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d75af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d75b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d75b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d75bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d75c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d75c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d75cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d75d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d75d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d75dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d75e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d75e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d75eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d75f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d75f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d75fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d760410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d760960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d760eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d761400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d761950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d761df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d762290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d762730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d762bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d763070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d763510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d7639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d763e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d7642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d764790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d764c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d7650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d765570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d765a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d765eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d766400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d766b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d767240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d767960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d768080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d768340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d768b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d768df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d769400 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.295 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13fa04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13fa05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13fa056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13fa05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13fa05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13fa06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13fa06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13fa06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13fa07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13fa075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13fa07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13fa08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13fa08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13fa093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13fa09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13fa0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13fa0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13fa0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13fa0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13fa0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13fa0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13fa0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13fa0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13fa0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13fa0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13fa0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13fa0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13fa0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13fa0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13fa0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13fa0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13fa0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13fa10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13fa106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13fa10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13fa10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13fa11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13fa118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13fa11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13fa12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13fa12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13fa12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13fa12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13fa13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13fa137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13fa13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13fa140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13fa14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13fa14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13fa14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13fa15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13fa156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13fa15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13fa15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13fa16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13fa16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13fa16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13fa17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13fa17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13fa17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13fa18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13fa184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13fa18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13fa18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13fa19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13fa19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13fa19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13fa19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13fa1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13fa1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13fa1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13fa1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13fa1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13fa1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13fa1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13fa1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13fa1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13fa1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13fa1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13fa1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13fa1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13fa1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13fa1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13fa1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13fa1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13fa1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13fa1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13fa1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13fa1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13fa20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13fa20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13fa209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13fa20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13fa212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13fa21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13fa21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13fa22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13fa22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13fa228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13fa22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13fa231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13fa23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13fa23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13fa23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13fa24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13fa24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13fa24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13fa250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13fa25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13fa259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13fa25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13fa262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13fa26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13fa26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13fa26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13fa27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13fa278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13fa27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13fa281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13fa28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13fa28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13fa28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13fa29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13fa297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13fa29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13fa2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13fa2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13fa2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13fa2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13fa2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13fa2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13fa2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13fa2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13fa2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13fa2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13fa2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13fa2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13fa2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13fa2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13fa2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13fa2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13fa2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13fa2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13fa2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13fa2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13fa2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13fa2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13fa30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13fa306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13fa30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13fa30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13fa31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13fa31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13fa31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13fa32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13fa325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13fa32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13fa32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13fa33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13fa337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13fa33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13fa34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13fa344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13fa34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13fa34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13fa35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13fa356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13fa35b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13fa35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13fa36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13fa36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13fa36ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13fa37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13fa375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13fa37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13fa37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13fa38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13fa38780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13fa38bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13fa39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13fa394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13fa39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13fa39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13fa3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13fa3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13fa3ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13fa3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13fa3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13fa3b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13fa3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13fa3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13fa3c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13fa3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13fa3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13fa3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13fa3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13fa3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13fa3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13fa3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13fa3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13fa3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13fa3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13fa3f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13fa3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13fa3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13fa403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13fa40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13fa40dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13fa41230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13fa416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13fa421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13fa424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13fa42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13fa42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13fa43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13fa434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13fa43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13fa43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13fa44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13fa44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13fa44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13fa44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13fa453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13fa45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13fa45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13fa46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13fa46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fa46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fa46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fa472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fa47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fa47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fa48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fa484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fa48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fa48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fa491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fa49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fa49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fa49f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fa4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fa4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fa4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fa4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fa4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fa4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fa4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fa4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fa4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fa4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fa4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fa4d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fa4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fa4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fa4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fa4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fa4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fa4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fa4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fa4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fa4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fa500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fa50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fa509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fa50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fa512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fa51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fa51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fa51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fa52460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fa528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fa52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fa531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fa53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fa53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fa53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fa54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fa547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fa54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fa550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fa55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fa559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13fa55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13fa56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13fa56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13fa576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13fa57de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fa580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fa58510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13fa58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13fa59120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d70c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d70bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d70c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d70cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d70d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d70d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d70da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d70dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d70b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d725960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d725c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d726090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d726980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d727100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d7278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d727fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d7286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d728db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d7294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d729e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d72a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d72ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d72b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d72b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d72c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d72c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d72c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d72ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d72d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d72d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d72db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d72dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d72e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d72e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d72eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d72eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d72f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d72f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d72fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d7301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d730620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d730a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d730f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d731370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d7317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d731c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d7320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d732530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d7329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d732e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d733280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d7336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d733b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d733fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d734440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d7348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d734d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d735190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d735600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d735a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d735ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d736350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d7367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d736c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d7370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d737510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d737980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d737df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d738260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d7386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d738b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d738fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d739420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d739890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d739d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d73a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d73a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d73aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d73aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d73b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d73b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d73bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d73c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d73c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d73c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d73cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d73d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d73d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d73db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d73df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d73e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d73e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d73ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d73f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d73f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d73fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d73fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d740310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d740780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d740bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d741060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d7414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d741940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d741db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d742220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d742690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d742b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d742f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d7433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d743850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d743cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d744130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d7445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d744a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d744e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d7452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d745760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d745bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d746040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d7464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d746920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d746d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d747200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d747670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d747ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d747f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d7483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d748830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d748ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d749110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d749580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d7499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d749e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d74a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d74a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d74abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d74b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d74b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d74b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d74bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d74c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d74c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d74cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d74cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d74d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d74d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d74dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d74e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d74e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d74e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d74ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d74f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d74f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d74fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d750000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d750470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d7508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d750d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d7511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d751630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d751aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d751f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d752380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d7527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d752c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d7530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d753540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d7539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d753e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d754290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d754700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d754b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d754fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d7558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d755d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d7561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d756610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d756a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d756ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d757360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d7577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d757c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d7580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d758520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d758990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d758e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d759270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d7596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d759b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d759fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d75a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d75a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d75ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d75b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d75b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d75ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d75bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d75c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d75c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d75cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d75d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d75d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d75d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d75dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d75e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d75e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d75eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d75efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d75f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d75fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d760000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d760470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d7608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d760d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d7611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d761630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d761aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d761f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d762380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d7627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d762c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d7630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d763540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d7639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d763e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d764290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d764700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d764b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d764fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d765450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d7658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d765d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d7661a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d766610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d766a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d766ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d767360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d7677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d767c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d7680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d768520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d768990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d768e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d769270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d718500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d718970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d718de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d719250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d7196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d719b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d719fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d71a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d71a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d71acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d71b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d71b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d71ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d71beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d71c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d71c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d71cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d71d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d71d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d71d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d71ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d71e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d71e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d71eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d71ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d71f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d71f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d71fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d720140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d7205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d720a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d720e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d721300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d721770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d721be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d722050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d7224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d722930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d723020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d723710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d723e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d7244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d724960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d724dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d725240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d716d10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.931s
user	0m0.244s
sys	0m0.141s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
