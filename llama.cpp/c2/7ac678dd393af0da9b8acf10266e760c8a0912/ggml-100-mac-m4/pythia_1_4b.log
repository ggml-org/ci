Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.665s
user	0m0.693s
sys	0m0.995s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target xxhash
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-cpu
[ 14%] Built target ggml-blas
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Linking CXX shared library libllama.dylib
[ 23%] Built target llama-gguf-hash
[ 23%] Built target llama-gguf
[ 23%] Built target llama
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 24%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Built target llava
[ 32%] Linking CXX static library libcommon.a
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 33%] Linking CXX static library libllava_static.a
[ 33%] Built target llama-quantize-stats
[ 33%] Built target test-c
[ 33%] Built target llama-simple
[ 33%] Built target llama-simple-chat
[ 33%] Built target common
[ 33%] Built target llava_static
[ 33%] Built target llava_shared
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 44%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-log
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-barrier
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Built target test-rope
[ 66%] Built target llama-batched-bench
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Built target llama-batched
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Built target llama-embedding
[ 74%] Built target llama-eval-callback
[ 74%] Built target llama-gbnf-validator
[ 74%] Built target llama-gguf-split
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Built target llama-imatrix
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-bench
[ 77%] Built target llama-infill
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-lookup
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Generating loading.html.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 84%] Linking CXX executable ../../bin/llama-passkey
[ 85%] Linking CXX executable ../../bin/llama-perplexity
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Generating index.html.hpp
[ 86%] Built target llama-lookup-merge
[ 86%] Built target llama-cli
[ 86%] Built target llama-lookup-create
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Built target llama-lookup-stats
[ 86%] Built target llama-parallel
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 88%] Built target llama-passkey
[ 88%] Built target llama-quantize
[ 88%] Built target llama-perplexity
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Built target llama-run
[ 95%] Built target llama-save-load-state
[ 95%] Built target llama-speculative-simple
[ 95%] Built target llama-speculative
[ 95%] Built target llama-tokenize
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 97%] Built target llama-gen-docs
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Built target llama-cvector-generator
[ 97%] Built target llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.463s
user	0m5.323s
sys	0m8.913s

main: quantize time =  2939.86 ms
main:    total time =  2939.86 ms

main: quantize time =  3409.43 ms
main:    total time =  3409.43 ms

main: quantize time =  2178.89 ms
main:    total time =  2178.89 ms

main: quantize time =  2011.27 ms
main:    total time =  2011.27 ms

main: quantize time =  2121.30 ms
main:    total time =  2121.30 ms

main: quantize time =  5036.41 ms
main:    total time =  5036.41 ms

main: quantize time =  5897.86 ms
main:    total time =  5897.86 ms

main: quantize time =  7077.25 ms
main:    total time =  7077.25 ms

main: quantize time =  5980.91 ms
main:    total time =  5980.91 ms

main: quantize time =  4734.97 ms
main:    total time =  4734.97 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.102 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.212 I main: llama backend init
0.00.000.218 I main: load the model and apply lora adapter, if any
0.00.040.830 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.052.185 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.052.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.052.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.052.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.052.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.052.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.052.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.052.212 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.052.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.052.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.052.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.052.215 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.052.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.052.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.052.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.052.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.052.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.061.600 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.070.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.070.926 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.070.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.070.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.070.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.070.929 I llama_model_loader: - type  f32:  194 tensors
0.00.070.930 I llama_model_loader: - type  f16:   98 tensors
0.00.104.524 I llm_load_vocab: special tokens cache size = 25
0.00.111.633 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.111.635 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.111.635 I llm_load_print_meta: arch             = gptneox
0.00.111.636 I llm_load_print_meta: vocab type       = BPE
0.00.111.636 I llm_load_print_meta: n_vocab          = 50304
0.00.111.636 I llm_load_print_meta: n_merges         = 50009
0.00.111.636 I llm_load_print_meta: vocab_only       = 0
0.00.111.637 I llm_load_print_meta: n_ctx_train      = 2048
0.00.111.637 I llm_load_print_meta: n_embd           = 2048
0.00.111.637 I llm_load_print_meta: n_layer          = 24
0.00.111.659 I llm_load_print_meta: n_head           = 16
0.00.111.660 I llm_load_print_meta: n_head_kv        = 16
0.00.111.661 I llm_load_print_meta: n_rot            = 32
0.00.111.661 I llm_load_print_meta: n_swa            = 0
0.00.111.661 I llm_load_print_meta: n_embd_head_k    = 128
0.00.111.661 I llm_load_print_meta: n_embd_head_v    = 128
0.00.111.662 I llm_load_print_meta: n_gqa            = 1
0.00.111.663 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.111.663 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.111.664 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.111.664 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.111.664 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.111.665 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.111.665 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.111.665 I llm_load_print_meta: n_ff             = 8192
0.00.111.666 I llm_load_print_meta: n_expert         = 0
0.00.111.666 I llm_load_print_meta: n_expert_used    = 0
0.00.111.666 I llm_load_print_meta: causal attn      = 1
0.00.111.666 I llm_load_print_meta: pooling type     = 0
0.00.111.666 I llm_load_print_meta: rope type        = 2
0.00.111.666 I llm_load_print_meta: rope scaling     = linear
0.00.111.667 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.111.667 I llm_load_print_meta: freq_scale_train = 1
0.00.111.667 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.111.667 I llm_load_print_meta: rope_finetuned   = unknown
0.00.111.667 I llm_load_print_meta: ssm_d_conv       = 0
0.00.111.669 I llm_load_print_meta: ssm_d_inner      = 0
0.00.111.669 I llm_load_print_meta: ssm_d_state      = 0
0.00.111.669 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.111.670 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.111.679 I llm_load_print_meta: model type       = 1.4B
0.00.111.680 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.111.680 I llm_load_print_meta: model params     = 1.41 B
0.00.111.681 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.111.681 I llm_load_print_meta: general.name     = 1.4B
0.00.111.681 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.111.681 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.111.681 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.111.682 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.111.682 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.111.682 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.111.682 I llm_load_print_meta: max token length = 1024
0.00.114.336 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.114.336 I llm_load_tensors: offloading output layer to GPU
0.00.114.336 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.114.355 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.114.356 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.115.336 I llama_new_context_with_model: n_seq_max     = 1
0.00.115.337 I llama_new_context_with_model: n_ctx         = 2048
0.00.115.337 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.115.337 I llama_new_context_with_model: n_batch       = 2048
0.00.115.338 I llama_new_context_with_model: n_ubatch      = 512
0.00.115.338 I llama_new_context_with_model: flash_attn    = 0
0.00.115.338 I llama_new_context_with_model: freq_base     = 10000.0
0.00.115.338 I llama_new_context_with_model: freq_scale    = 1
0.00.115.339 I ggml_metal_init: allocating
0.00.115.342 I ggml_metal_init: found device: Apple M4
0.00.115.344 I ggml_metal_init: picking default device: Apple M4
0.00.116.037 I ggml_metal_init: using embedded metal library
0.00.125.563 I ggml_metal_init: GPU name:   Apple M4
0.00.125.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.125.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.125.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.125.566 I ggml_metal_init: simdgroup reduction   = true
0.00.125.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.125.566 I ggml_metal_init: has bfloat            = true
0.00.125.566 I ggml_metal_init: use bfloat            = true
0.00.125.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.125.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.172.175 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.172.181 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.172.203 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.173.134 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.173.137 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.173.137 I llama_new_context_with_model: graph nodes  = 967
0.00.173.137 I llama_new_context_with_model: graph splits = 2
0.00.173.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.251.536 I main: llama threadpool init, n_threads = 4
0.00.251.569 I 
0.00.251.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.251.609 I 
0.00.251.693 I sampler seed: 1234
0.00.251.697 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.251.721 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.251.723 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.251.723 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.180.822 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.02.180.822 I llama_perf_context_print:        load time =     210.69 ms
0.02.180.823 I llama_perf_context_print: prompt eval time =      43.82 ms /     7 tokens (    6.26 ms per token,   159.75 tokens per second)
0.02.180.824 I llama_perf_context_print:        eval time =    1882.26 ms /    63 runs   (   29.88 ms per token,    33.47 tokens per second)
0.02.180.824 I llama_perf_context_print:       total time =    1929.29 ms /    70 tokens
0.02.181.020 I ggml_metal_free: deallocating

real	0m2.476s
user	0m0.146s
sys	0m0.099s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.014.356 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.527 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.535 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.536 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.536 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.536 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.539 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.540 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.541 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.541 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.541 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.542 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.542 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.545 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.545 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.685 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.800 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.889 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.889 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.890 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.891 I llama_model_loader: - type  f32:  194 tensors
0.00.037.891 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.651 I llm_load_vocab: special tokens cache size = 25
0.00.067.998 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.002 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.002 I llm_load_print_meta: arch             = gptneox
0.00.068.004 I llm_load_print_meta: vocab type       = BPE
0.00.068.005 I llm_load_print_meta: n_vocab          = 50304
0.00.068.005 I llm_load_print_meta: n_merges         = 50009
0.00.068.005 I llm_load_print_meta: vocab_only       = 0
0.00.068.005 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.005 I llm_load_print_meta: n_embd           = 2048
0.00.068.006 I llm_load_print_meta: n_layer          = 24
0.00.068.023 I llm_load_print_meta: n_head           = 16
0.00.068.025 I llm_load_print_meta: n_head_kv        = 16
0.00.068.025 I llm_load_print_meta: n_rot            = 32
0.00.068.025 I llm_load_print_meta: n_swa            = 0
0.00.068.025 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.026 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.026 I llm_load_print_meta: n_gqa            = 1
0.00.068.027 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.027 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.028 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.028 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.028 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.028 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.028 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.029 I llm_load_print_meta: n_ff             = 8192
0.00.068.030 I llm_load_print_meta: n_expert         = 0
0.00.068.032 I llm_load_print_meta: n_expert_used    = 0
0.00.068.032 I llm_load_print_meta: causal attn      = 1
0.00.068.032 I llm_load_print_meta: pooling type     = 0
0.00.068.033 I llm_load_print_meta: rope type        = 2
0.00.068.033 I llm_load_print_meta: rope scaling     = linear
0.00.068.033 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.033 I llm_load_print_meta: freq_scale_train = 1
0.00.068.034 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.034 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.034 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.034 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.034 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.034 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.034 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.044 I llm_load_print_meta: model type       = 1.4B
0.00.068.044 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.044 I llm_load_print_meta: model params     = 1.41 B
0.00.068.045 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.045 I llm_load_print_meta: general.name     = 1.4B
0.00.068.045 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.045 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.045 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.045 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.046 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.046 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.046 I llm_load_print_meta: max token length = 1024
0.00.069.952 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.952 I llm_load_tensors: offloading output layer to GPU
0.00.069.952 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.963 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.964 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.854 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.855 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.855 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.856 I llama_new_context_with_model: n_batch       = 2048
0.00.070.856 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.856 I llama_new_context_with_model: flash_attn    = 0
0.00.070.856 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.857 I llama_new_context_with_model: freq_scale    = 1
0.00.070.857 I ggml_metal_init: allocating
0.00.070.864 I ggml_metal_init: found device: Apple M4
0.00.070.867 I ggml_metal_init: picking default device: Apple M4
0.00.071.621 I ggml_metal_init: using embedded metal library
0.00.074.196 I ggml_metal_init: GPU name:   Apple M4
0.00.074.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.198 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.198 I ggml_metal_init: simdgroup reduction   = true
0.00.074.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.199 I ggml_metal_init: has bfloat            = true
0.00.074.199 I ggml_metal_init: use bfloat            = true
0.00.074.199 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.058 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.111.071 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.111.110 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.254 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.112.257 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.112.257 I llama_new_context_with_model: graph nodes  = 967
0.00.112.257 I llama_new_context_with_model: graph splits = 2
0.00.112.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.423.316 I main: llama threadpool init, n_threads = 4
0.01.423.358 I 
0.01.423.394 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.423.394 I 
0.01.423.576 I sampler seed: 1234
0.01.423.581 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.423.614 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.423.614 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.423.616 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.565.528 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60169.49 tokens per second)
0.02.565.529 I llama_perf_context_print:        load time =    1408.95 ms
0.02.565.530 I llama_perf_context_print: prompt eval time =      39.96 ms /     7 tokens (    5.71 ms per token,   175.19 tokens per second)
0.02.565.530 I llama_perf_context_print:        eval time =    1098.98 ms /    63 runs   (   17.44 ms per token,    57.33 tokens per second)
0.02.565.531 I llama_perf_context_print:       total time =    1142.21 ms /    70 tokens
0.02.565.717 I ggml_metal_free: deallocating

real	0m2.583s
user	0m0.117s
sys	0m0.241s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.015.227 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.953 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.965 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.966 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.967 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.357 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.823 I llama_model_loader: - type  f32:  194 tensors
0.00.040.823 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.823 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.598 I llm_load_vocab: special tokens cache size = 25
0.00.078.420 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.425 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.425 I llm_load_print_meta: arch             = gptneox
0.00.078.426 I llm_load_print_meta: vocab type       = BPE
0.00.078.426 I llm_load_print_meta: n_vocab          = 50304
0.00.078.426 I llm_load_print_meta: n_merges         = 50009
0.00.078.426 I llm_load_print_meta: vocab_only       = 0
0.00.078.427 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.429 I llm_load_print_meta: n_embd           = 2048
0.00.078.430 I llm_load_print_meta: n_layer          = 24
0.00.078.448 I llm_load_print_meta: n_head           = 16
0.00.078.449 I llm_load_print_meta: n_head_kv        = 16
0.00.078.450 I llm_load_print_meta: n_rot            = 32
0.00.078.450 I llm_load_print_meta: n_swa            = 0
0.00.078.450 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.451 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.452 I llm_load_print_meta: n_gqa            = 1
0.00.078.453 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.454 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.455 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.455 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.455 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.455 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.458 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.459 I llm_load_print_meta: n_ff             = 8192
0.00.078.459 I llm_load_print_meta: n_expert         = 0
0.00.078.459 I llm_load_print_meta: n_expert_used    = 0
0.00.078.460 I llm_load_print_meta: causal attn      = 1
0.00.078.462 I llm_load_print_meta: pooling type     = 0
0.00.078.462 I llm_load_print_meta: rope type        = 2
0.00.078.462 I llm_load_print_meta: rope scaling     = linear
0.00.078.463 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.463 I llm_load_print_meta: freq_scale_train = 1
0.00.078.464 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.464 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.464 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.464 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.464 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.465 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.465 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.475 I llm_load_print_meta: model type       = 1.4B
0.00.078.475 I llm_load_print_meta: model ftype      = Q4_0
0.00.078.476 I llm_load_print_meta: model params     = 1.41 B
0.00.078.477 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.078.477 I llm_load_print_meta: general.name     = 1.4B
0.00.078.477 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.478 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.478 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.478 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.479 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.078.479 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.479 I llm_load_print_meta: max token length = 1024
0.00.081.129 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.129 I llm_load_tensors: offloading output layer to GPU
0.00.081.130 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.141 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.081.143 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.082.560 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.561 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.561 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.562 I llama_new_context_with_model: n_batch       = 2048
0.00.082.562 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.562 I llama_new_context_with_model: flash_attn    = 0
0.00.082.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.563 I llama_new_context_with_model: freq_scale    = 1
0.00.082.564 I ggml_metal_init: allocating
0.00.082.569 I ggml_metal_init: found device: Apple M4
0.00.082.572 I ggml_metal_init: picking default device: Apple M4
0.00.083.598 I ggml_metal_init: using embedded metal library
0.00.087.508 I ggml_metal_init: GPU name:   Apple M4
0.00.087.510 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.512 I ggml_metal_init: simdgroup reduction   = true
0.00.087.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.512 I ggml_metal_init: has bfloat            = true
0.00.087.512 I ggml_metal_init: use bfloat            = true
0.00.087.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.127.969 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.127.983 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.128.008 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.129.180 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.129.182 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.129.182 I llama_new_context_with_model: graph nodes  = 967
0.00.129.182 I llama_new_context_with_model: graph splits = 2
0.00.129.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.410 I main: llama threadpool init, n_threads = 4
0.00.767.465 I 
0.00.767.500 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.502 I 
0.00.767.756 I sampler seed: 1234
0.00.767.762 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.805 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.806 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.806 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.449.585 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48530.42 tokens per second)
0.01.449.588 I llama_perf_context_print:        load time =     752.18 ms
0.01.449.590 I llama_perf_context_print: prompt eval time =      45.22 ms /     7 tokens (    6.46 ms per token,   154.79 tokens per second)
0.01.449.591 I llama_perf_context_print:        eval time =     633.74 ms /    63 runs   (   10.06 ms per token,    99.41 tokens per second)
0.01.449.592 I llama_perf_context_print:       total time =     682.18 ms /    70 tokens
0.01.449.823 I ggml_metal_free: deallocating

real	0m1.473s
user	0m0.135s
sys	0m0.178s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.897 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.046 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.049 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.012 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.013 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.014 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.014 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.014 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.015 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.015 I llama_model_loader: - type  f32:  194 tensors
0.00.033.016 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.016 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.866 I llm_load_vocab: special tokens cache size = 25
0.00.059.701 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.703 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.704 I llm_load_print_meta: arch             = gptneox
0.00.059.704 I llm_load_print_meta: vocab type       = BPE
0.00.059.704 I llm_load_print_meta: n_vocab          = 50304
0.00.059.705 I llm_load_print_meta: n_merges         = 50009
0.00.059.705 I llm_load_print_meta: vocab_only       = 0
0.00.059.705 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.705 I llm_load_print_meta: n_embd           = 2048
0.00.059.705 I llm_load_print_meta: n_layer          = 24
0.00.059.715 I llm_load_print_meta: n_head           = 16
0.00.059.716 I llm_load_print_meta: n_head_kv        = 16
0.00.059.716 I llm_load_print_meta: n_rot            = 32
0.00.059.716 I llm_load_print_meta: n_swa            = 0
0.00.059.716 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.716 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.719 I llm_load_print_meta: n_gqa            = 1
0.00.059.720 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.721 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.721 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.721 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.722 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.722 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.723 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.723 I llm_load_print_meta: n_ff             = 8192
0.00.059.723 I llm_load_print_meta: n_expert         = 0
0.00.059.724 I llm_load_print_meta: n_expert_used    = 0
0.00.059.725 I llm_load_print_meta: causal attn      = 1
0.00.059.725 I llm_load_print_meta: pooling type     = 0
0.00.059.726 I llm_load_print_meta: rope type        = 2
0.00.059.727 I llm_load_print_meta: rope scaling     = linear
0.00.059.727 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.727 I llm_load_print_meta: freq_scale_train = 1
0.00.059.727 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.727 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.727 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.728 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.728 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.728 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.728 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.733 I llm_load_print_meta: model type       = 1.4B
0.00.059.733 I llm_load_print_meta: model ftype      = Q4_1
0.00.059.733 I llm_load_print_meta: model params     = 1.41 B
0.00.059.734 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.059.734 I llm_load_print_meta: general.name     = 1.4B
0.00.059.734 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.734 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.735 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.735 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.736 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.736 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.736 I llm_load_print_meta: max token length = 1024
0.00.061.518 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.518 I llm_load_tensors: offloading output layer to GPU
0.00.061.518 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.523 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.061.525 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.062.470 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.471 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.471 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.471 I llama_new_context_with_model: n_batch       = 2048
0.00.062.471 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.472 I llama_new_context_with_model: flash_attn    = 0
0.00.062.472 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.472 I llama_new_context_with_model: freq_scale    = 1
0.00.062.473 I ggml_metal_init: allocating
0.00.062.476 I ggml_metal_init: found device: Apple M4
0.00.062.478 I ggml_metal_init: picking default device: Apple M4
0.00.063.067 I ggml_metal_init: using embedded metal library
0.00.065.369 I ggml_metal_init: GPU name:   Apple M4
0.00.065.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.371 I ggml_metal_init: simdgroup reduction   = true
0.00.065.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.371 I ggml_metal_init: has bfloat            = true
0.00.065.373 I ggml_metal_init: use bfloat            = true
0.00.065.373 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.374 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.531 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.536 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.558 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.605 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.607 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.607 I llama_new_context_with_model: graph nodes  = 967
0.00.095.608 I llama_new_context_with_model: graph splits = 2
0.00.095.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.861.185 I main: llama threadpool init, n_threads = 4
0.00.861.226 I 
0.00.861.254 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.861.256 I 
0.00.861.492 I sampler seed: 1234
0.00.861.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.861.508 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.861.508 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.861.510 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.591.513 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62226.12 tokens per second)
0.01.591.515 I llama_perf_context_print:        load time =     852.28 ms
0.01.591.518 I llama_perf_context_print: prompt eval time =      42.43 ms /     7 tokens (    6.06 ms per token,   164.97 tokens per second)
0.01.591.519 I llama_perf_context_print:        eval time =     684.59 ms /    63 runs   (   10.87 ms per token,    92.03 tokens per second)
0.01.591.520 I llama_perf_context_print:       total time =     730.33 ms /    70 tokens
0.01.591.717 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.109s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.099 I main: llama backend init
0.00.000.102 I main: load the model and apply lora adapter, if any
0.00.034.137 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.068.393 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.068.427 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.068.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.068.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.068.435 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.068.435 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.068.436 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.068.438 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.068.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.068.439 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.068.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.068.441 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.068.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.068.442 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.068.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.068.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.068.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.078.208 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.510 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.085.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.521 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.085.524 I llama_model_loader: - type  f32:  194 tensors
0.00.085.525 I llama_model_loader: - type q5_0:   97 tensors
0.00.085.525 I llama_model_loader: - type q6_K:    1 tensors
0.00.126.767 I llm_load_vocab: special tokens cache size = 25
0.00.135.928 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.135.932 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.135.933 I llm_load_print_meta: arch             = gptneox
0.00.135.933 I llm_load_print_meta: vocab type       = BPE
0.00.135.933 I llm_load_print_meta: n_vocab          = 50304
0.00.135.934 I llm_load_print_meta: n_merges         = 50009
0.00.135.934 I llm_load_print_meta: vocab_only       = 0
0.00.135.936 I llm_load_print_meta: n_ctx_train      = 2048
0.00.135.937 I llm_load_print_meta: n_embd           = 2048
0.00.135.937 I llm_load_print_meta: n_layer          = 24
0.00.135.954 I llm_load_print_meta: n_head           = 16
0.00.135.955 I llm_load_print_meta: n_head_kv        = 16
0.00.135.956 I llm_load_print_meta: n_rot            = 32
0.00.135.956 I llm_load_print_meta: n_swa            = 0
0.00.135.956 I llm_load_print_meta: n_embd_head_k    = 128
0.00.135.956 I llm_load_print_meta: n_embd_head_v    = 128
0.00.135.957 I llm_load_print_meta: n_gqa            = 1
0.00.135.958 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.135.958 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.135.959 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.135.959 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.135.960 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.135.960 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.135.960 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.135.962 I llm_load_print_meta: n_ff             = 8192
0.00.135.963 I llm_load_print_meta: n_expert         = 0
0.00.135.963 I llm_load_print_meta: n_expert_used    = 0
0.00.135.963 I llm_load_print_meta: causal attn      = 1
0.00.135.968 I llm_load_print_meta: pooling type     = 0
0.00.135.969 I llm_load_print_meta: rope type        = 2
0.00.135.969 I llm_load_print_meta: rope scaling     = linear
0.00.135.970 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.135.970 I llm_load_print_meta: freq_scale_train = 1
0.00.135.970 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.135.970 I llm_load_print_meta: rope_finetuned   = unknown
0.00.135.971 I llm_load_print_meta: ssm_d_conv       = 0
0.00.135.971 I llm_load_print_meta: ssm_d_inner      = 0
0.00.135.971 I llm_load_print_meta: ssm_d_state      = 0
0.00.135.971 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.135.971 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.135.982 I llm_load_print_meta: model type       = 1.4B
0.00.135.982 I llm_load_print_meta: model ftype      = Q5_0
0.00.135.983 I llm_load_print_meta: model params     = 1.41 B
0.00.135.984 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.135.984 I llm_load_print_meta: general.name     = 1.4B
0.00.135.984 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.135.984 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.135.985 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.135.985 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.135.985 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.135.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.135.986 I llm_load_print_meta: max token length = 1024
0.00.138.715 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.138.715 I llm_load_tensors: offloading output layer to GPU
0.00.138.715 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.138.726 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.138.727 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.139.897 I llama_new_context_with_model: n_seq_max     = 1
0.00.139.898 I llama_new_context_with_model: n_ctx         = 2048
0.00.139.898 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.139.899 I llama_new_context_with_model: n_batch       = 2048
0.00.139.899 I llama_new_context_with_model: n_ubatch      = 512
0.00.139.899 I llama_new_context_with_model: flash_attn    = 0
0.00.139.900 I llama_new_context_with_model: freq_base     = 10000.0
0.00.139.900 I llama_new_context_with_model: freq_scale    = 1
0.00.139.901 I ggml_metal_init: allocating
0.00.139.909 I ggml_metal_init: found device: Apple M4
0.00.139.912 I ggml_metal_init: picking default device: Apple M4
0.00.140.666 I ggml_metal_init: using embedded metal library
0.00.143.859 I ggml_metal_init: GPU name:   Apple M4
0.00.143.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.143.862 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.143.862 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.143.863 I ggml_metal_init: simdgroup reduction   = true
0.00.143.863 I ggml_metal_init: simdgroup matrix mul. = true
0.00.143.863 I ggml_metal_init: has bfloat            = true
0.00.143.863 I ggml_metal_init: use bfloat            = true
0.00.143.864 I ggml_metal_init: hasUnifiedMemory      = true
0.00.143.864 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.174.293 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.174.300 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.174.323 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.175.253 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.175.254 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.175.255 I llama_new_context_with_model: graph nodes  = 967
0.00.175.255 I llama_new_context_with_model: graph splits = 2
0.00.175.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.965.191 I main: llama threadpool init, n_threads = 4
0.00.965.282 I 
0.00.965.356 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.965.358 I 
0.00.965.713 I sampler seed: 1234
0.00.965.720 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.965.751 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.965.753 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.965.753 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.767.970 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.767.971 I llama_perf_context_print:        load time =     931.03 ms
0.01.767.972 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.01.767.974 I llama_perf_context_print:        eval time =     744.63 ms /    63 runs   (   11.82 ms per token,    84.61 tokens per second)
0.01.767.975 I llama_perf_context_print:       total time =     802.79 ms /    70 tokens
0.01.768.173 I ggml_metal_free: deallocating

real	0m1.859s
user	0m0.162s
sys	0m0.206s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.010.570 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.587 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.593 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.594 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.596 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.597 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.600 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.601 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.368 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.213 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.214 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.214 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.215 I llama_model_loader: - type  f32:  194 tensors
0.00.025.215 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.216 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.394 I llm_load_vocab: special tokens cache size = 25
0.00.051.425 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.428 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.428 I llm_load_print_meta: arch             = gptneox
0.00.051.429 I llm_load_print_meta: vocab type       = BPE
0.00.051.429 I llm_load_print_meta: n_vocab          = 50304
0.00.051.429 I llm_load_print_meta: n_merges         = 50009
0.00.051.429 I llm_load_print_meta: vocab_only       = 0
0.00.051.430 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.430 I llm_load_print_meta: n_embd           = 2048
0.00.051.430 I llm_load_print_meta: n_layer          = 24
0.00.051.443 I llm_load_print_meta: n_head           = 16
0.00.051.444 I llm_load_print_meta: n_head_kv        = 16
0.00.051.444 I llm_load_print_meta: n_rot            = 32
0.00.051.444 I llm_load_print_meta: n_swa            = 0
0.00.051.445 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.447 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.448 I llm_load_print_meta: n_gqa            = 1
0.00.051.448 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.449 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.450 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.450 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.452 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.452 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.452 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.453 I llm_load_print_meta: n_ff             = 8192
0.00.051.453 I llm_load_print_meta: n_expert         = 0
0.00.051.453 I llm_load_print_meta: n_expert_used    = 0
0.00.051.454 I llm_load_print_meta: causal attn      = 1
0.00.051.454 I llm_load_print_meta: pooling type     = 0
0.00.051.454 I llm_load_print_meta: rope type        = 2
0.00.051.455 I llm_load_print_meta: rope scaling     = linear
0.00.051.456 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.456 I llm_load_print_meta: freq_scale_train = 1
0.00.051.456 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.457 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.457 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.457 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.457 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.457 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.458 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.467 I llm_load_print_meta: model type       = 1.4B
0.00.051.467 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.467 I llm_load_print_meta: model params     = 1.41 B
0.00.051.468 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.468 I llm_load_print_meta: general.name     = 1.4B
0.00.051.468 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.468 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.468 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.469 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.470 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.470 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.470 I llm_load_print_meta: max token length = 1024
0.00.053.413 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.413 I llm_load_tensors: offloading output layer to GPU
0.00.053.414 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.424 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.425 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.339 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.340 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.340 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.340 I llama_new_context_with_model: n_batch       = 2048
0.00.054.340 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.341 I llama_new_context_with_model: flash_attn    = 0
0.00.054.341 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.341 I llama_new_context_with_model: freq_scale    = 1
0.00.054.342 I ggml_metal_init: allocating
0.00.054.344 I ggml_metal_init: found device: Apple M4
0.00.054.346 I ggml_metal_init: picking default device: Apple M4
0.00.054.936 I ggml_metal_init: using embedded metal library
0.00.057.243 I ggml_metal_init: GPU name:   Apple M4
0.00.057.244 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.245 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.245 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.245 I ggml_metal_init: simdgroup reduction   = true
0.00.057.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.247 I ggml_metal_init: has bfloat            = true
0.00.057.247 I ggml_metal_init: use bfloat            = true
0.00.057.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.531 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.537 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.555 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.555 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.556 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.556 I llama_new_context_with_model: graph nodes  = 967
0.00.086.557 I llama_new_context_with_model: graph splits = 2
0.00.086.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.283 I main: llama threadpool init, n_threads = 4
0.00.707.321 I 
0.00.707.375 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.376 I 
0.00.707.611 I sampler seed: 1234
0.00.707.616 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.658 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.658 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.658 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.548.045 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.01.548.045 I llama_perf_context_print:        load time =     696.71 ms
0.01.548.046 I llama_perf_context_print: prompt eval time =      42.33 ms /     7 tokens (    6.05 ms per token,   165.38 tokens per second)
0.01.548.047 I llama_perf_context_print:        eval time =     795.27 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.548.047 I llama_perf_context_print:       total time =     840.76 ms /    70 tokens
0.01.548.277 I ggml_metal_free: deallocating

real	0m1.565s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.011.042 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.737 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.742 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.744 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.744 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.745 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.745 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.745 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.747 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.748 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.748 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.748 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.749 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.751 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.751 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.751 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.828 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.829 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.829 I llama_model_loader: - type  f32:  194 tensors
0.00.025.830 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.830 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.830 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.746 I llm_load_vocab: special tokens cache size = 25
0.00.052.885 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.889 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.890 I llm_load_print_meta: arch             = gptneox
0.00.052.890 I llm_load_print_meta: vocab type       = BPE
0.00.052.890 I llm_load_print_meta: n_vocab          = 50304
0.00.052.891 I llm_load_print_meta: n_merges         = 50009
0.00.052.891 I llm_load_print_meta: vocab_only       = 0
0.00.052.891 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.891 I llm_load_print_meta: n_embd           = 2048
0.00.052.891 I llm_load_print_meta: n_layer          = 24
0.00.052.908 I llm_load_print_meta: n_head           = 16
0.00.052.909 I llm_load_print_meta: n_head_kv        = 16
0.00.052.909 I llm_load_print_meta: n_rot            = 32
0.00.052.910 I llm_load_print_meta: n_swa            = 0
0.00.052.910 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.910 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.910 I llm_load_print_meta: n_gqa            = 1
0.00.052.911 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.912 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.912 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.913 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.913 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.915 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.915 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.916 I llm_load_print_meta: n_ff             = 8192
0.00.052.916 I llm_load_print_meta: n_expert         = 0
0.00.052.916 I llm_load_print_meta: n_expert_used    = 0
0.00.052.916 I llm_load_print_meta: causal attn      = 1
0.00.052.916 I llm_load_print_meta: pooling type     = 0
0.00.052.916 I llm_load_print_meta: rope type        = 2
0.00.052.917 I llm_load_print_meta: rope scaling     = linear
0.00.052.917 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.917 I llm_load_print_meta: freq_scale_train = 1
0.00.052.917 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.918 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.918 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.918 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.918 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.918 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.918 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.928 I llm_load_print_meta: model type       = 1.4B
0.00.052.928 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.929 I llm_load_print_meta: model params     = 1.41 B
0.00.052.929 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.929 I llm_load_print_meta: general.name     = 1.4B
0.00.052.929 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.930 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.930 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.930 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.930 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.930 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.931 I llm_load_print_meta: max token length = 1024
0.00.054.789 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.789 I llm_load_tensors: offloading output layer to GPU
0.00.054.789 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.800 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.801 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.743 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.744 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.744 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.744 I llama_new_context_with_model: n_batch       = 2048
0.00.055.744 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.744 I llama_new_context_with_model: flash_attn    = 0
0.00.055.745 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.745 I llama_new_context_with_model: freq_scale    = 1
0.00.055.746 I ggml_metal_init: allocating
0.00.055.749 I ggml_metal_init: found device: Apple M4
0.00.055.751 I ggml_metal_init: picking default device: Apple M4
0.00.056.367 I ggml_metal_init: using embedded metal library
0.00.058.740 I ggml_metal_init: GPU name:   Apple M4
0.00.058.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.742 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.742 I ggml_metal_init: simdgroup reduction   = true
0.00.058.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.743 I ggml_metal_init: has bfloat            = true
0.00.058.743 I ggml_metal_init: use bfloat            = true
0.00.058.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.715 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.734 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.759 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.758 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.760 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.760 I llama_new_context_with_model: graph nodes  = 967
0.00.090.760 I llama_new_context_with_model: graph splits = 2
0.00.090.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.419.804 I main: llama threadpool init, n_threads = 4
0.00.419.844 I 
0.00.419.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.419.878 I 
0.00.420.115 I sampler seed: 1234
0.00.420.120 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.420.153 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.420.156 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.420.156 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.098.977 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.098.978 I llama_perf_context_print:        load time =     408.76 ms
0.01.098.979 I llama_perf_context_print: prompt eval time =      38.51 ms /     7 tokens (    5.50 ms per token,   181.77 tokens per second)
0.01.098.979 I llama_perf_context_print:        eval time =     637.51 ms /    63 runs   (   10.12 ms per token,    98.82 tokens per second)
0.01.098.980 I llama_perf_context_print:       total time =     679.17 ms /    70 tokens
0.01.099.164 I ggml_metal_free: deallocating

real	0m1.118s
user	0m0.109s
sys	0m0.094s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.171 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.477 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.478 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.478 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.479 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.481 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.488 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.490 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.380 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.439 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.439 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.440 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.440 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.440 I llama_model_loader: - type  f32:  194 tensors
0.00.026.441 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.441 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.441 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.441 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.658 I llm_load_vocab: special tokens cache size = 25
0.00.052.451 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.454 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.455 I llm_load_print_meta: arch             = gptneox
0.00.052.455 I llm_load_print_meta: vocab type       = BPE
0.00.052.455 I llm_load_print_meta: n_vocab          = 50304
0.00.052.456 I llm_load_print_meta: n_merges         = 50009
0.00.052.456 I llm_load_print_meta: vocab_only       = 0
0.00.052.456 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.456 I llm_load_print_meta: n_embd           = 2048
0.00.052.456 I llm_load_print_meta: n_layer          = 24
0.00.052.478 I llm_load_print_meta: n_head           = 16
0.00.052.479 I llm_load_print_meta: n_head_kv        = 16
0.00.052.479 I llm_load_print_meta: n_rot            = 32
0.00.052.479 I llm_load_print_meta: n_swa            = 0
0.00.052.479 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.480 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.480 I llm_load_print_meta: n_gqa            = 1
0.00.052.481 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.482 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.482 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.483 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.483 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.483 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.483 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.484 I llm_load_print_meta: n_ff             = 8192
0.00.052.484 I llm_load_print_meta: n_expert         = 0
0.00.052.484 I llm_load_print_meta: n_expert_used    = 0
0.00.052.485 I llm_load_print_meta: causal attn      = 1
0.00.052.485 I llm_load_print_meta: pooling type     = 0
0.00.052.485 I llm_load_print_meta: rope type        = 2
0.00.052.485 I llm_load_print_meta: rope scaling     = linear
0.00.052.486 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.486 I llm_load_print_meta: freq_scale_train = 1
0.00.052.488 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.488 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.488 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.489 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.489 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.489 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.489 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.499 I llm_load_print_meta: model type       = 1.4B
0.00.052.499 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.499 I llm_load_print_meta: model params     = 1.41 B
0.00.052.500 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.500 I llm_load_print_meta: general.name     = 1.4B
0.00.052.501 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.501 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.501 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.501 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.501 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.503 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.503 I llm_load_print_meta: max token length = 1024
0.00.054.415 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.415 I llm_load_tensors: offloading output layer to GPU
0.00.054.415 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.426 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.427 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.346 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.347 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.347 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.347 I llama_new_context_with_model: n_batch       = 2048
0.00.055.347 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.347 I llama_new_context_with_model: flash_attn    = 0
0.00.055.348 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.348 I llama_new_context_with_model: freq_scale    = 1
0.00.055.348 I ggml_metal_init: allocating
0.00.055.351 I ggml_metal_init: found device: Apple M4
0.00.055.353 I ggml_metal_init: picking default device: Apple M4
0.00.055.958 I ggml_metal_init: using embedded metal library
0.00.058.282 I ggml_metal_init: GPU name:   Apple M4
0.00.058.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.285 I ggml_metal_init: simdgroup reduction   = true
0.00.058.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.285 I ggml_metal_init: has bfloat            = true
0.00.058.285 I ggml_metal_init: use bfloat            = true
0.00.058.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.286 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.009 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.014 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.033 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.021 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.022 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.023 I llama_new_context_with_model: graph nodes  = 967
0.00.088.023 I llama_new_context_with_model: graph splits = 2
0.00.088.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.542.959 I main: llama threadpool init, n_threads = 4
0.00.543.007 I 
0.00.543.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.038 I 
0.00.543.269 I sampler seed: 1234
0.00.543.273 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.543.325 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.543.326 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.543.326 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.291.482 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61525.13 tokens per second)
0.01.291.483 I llama_perf_context_print:        load time =     531.78 ms
0.01.291.484 I llama_perf_context_print: prompt eval time =      43.02 ms /     7 tokens (    6.15 ms per token,   162.70 tokens per second)
0.01.291.485 I llama_perf_context_print:        eval time =     702.19 ms /    63 runs   (   11.15 ms per token,    89.72 tokens per second)
0.01.291.485 I llama_perf_context_print:       total time =     748.53 ms /    70 tokens
0.01.291.679 I ggml_metal_free: deallocating

real	0m1.307s
user	0m0.109s
sys	0m0.130s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.936 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.910 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.916 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.919 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.919 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.919 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.920 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.921 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.924 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.925 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.926 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.926 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.928 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.928 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.929 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.895 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.939 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.939 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.939 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.940 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.940 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.940 I llama_model_loader: - type  f32:  194 tensors
0.00.026.941 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.941 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.941 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.886 I llm_load_vocab: special tokens cache size = 25
0.00.053.834 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.837 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.837 I llm_load_print_meta: arch             = gptneox
0.00.053.838 I llm_load_print_meta: vocab type       = BPE
0.00.053.838 I llm_load_print_meta: n_vocab          = 50304
0.00.053.838 I llm_load_print_meta: n_merges         = 50009
0.00.053.838 I llm_load_print_meta: vocab_only       = 0
0.00.053.838 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.838 I llm_load_print_meta: n_embd           = 2048
0.00.053.839 I llm_load_print_meta: n_layer          = 24
0.00.053.853 I llm_load_print_meta: n_head           = 16
0.00.053.854 I llm_load_print_meta: n_head_kv        = 16
0.00.053.854 I llm_load_print_meta: n_rot            = 32
0.00.053.854 I llm_load_print_meta: n_swa            = 0
0.00.053.855 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.855 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.856 I llm_load_print_meta: n_gqa            = 1
0.00.053.860 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.861 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.863 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.863 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.863 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.863 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.863 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.864 I llm_load_print_meta: n_ff             = 8192
0.00.053.864 I llm_load_print_meta: n_expert         = 0
0.00.053.864 I llm_load_print_meta: n_expert_used    = 0
0.00.053.864 I llm_load_print_meta: causal attn      = 1
0.00.053.865 I llm_load_print_meta: pooling type     = 0
0.00.053.865 I llm_load_print_meta: rope type        = 2
0.00.053.865 I llm_load_print_meta: rope scaling     = linear
0.00.053.866 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.866 I llm_load_print_meta: freq_scale_train = 1
0.00.053.867 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.867 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.867 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.867 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.867 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.867 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.867 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.876 I llm_load_print_meta: model type       = 1.4B
0.00.053.877 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.877 I llm_load_print_meta: model params     = 1.41 B
0.00.053.879 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.879 I llm_load_print_meta: general.name     = 1.4B
0.00.053.879 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.879 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.880 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.881 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.881 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.881 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.881 I llm_load_print_meta: max token length = 1024
0.00.055.504 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.504 I llm_load_tensors: offloading output layer to GPU
0.00.055.504 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.514 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.515 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.381 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.381 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.382 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.382 I llama_new_context_with_model: n_batch       = 2048
0.00.056.382 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.382 I llama_new_context_with_model: flash_attn    = 0
0.00.056.383 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.383 I llama_new_context_with_model: freq_scale    = 1
0.00.056.383 I ggml_metal_init: allocating
0.00.056.387 I ggml_metal_init: found device: Apple M4
0.00.056.389 I ggml_metal_init: picking default device: Apple M4
0.00.057.028 I ggml_metal_init: using embedded metal library
0.00.059.385 I ggml_metal_init: GPU name:   Apple M4
0.00.059.386 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.387 I ggml_metal_init: simdgroup reduction   = true
0.00.059.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.388 I ggml_metal_init: has bfloat            = true
0.00.059.388 I ggml_metal_init: use bfloat            = true
0.00.059.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.472 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.478 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.498 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.616 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.617 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.618 I llama_new_context_with_model: graph nodes  = 967
0.00.091.618 I llama_new_context_with_model: graph splits = 2
0.00.091.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.688 I main: llama threadpool init, n_threads = 4
0.00.632.728 I 
0.00.632.760 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.761 I 
0.00.632.985 I sampler seed: 1234
0.00.632.990 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.633.030 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.633.031 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.633.031 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.392.706 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.392.707 I llama_perf_context_print:        load time =     621.75 ms
0.01.392.707 I llama_perf_context_print: prompt eval time =      47.14 ms /     7 tokens (    6.73 ms per token,   148.50 tokens per second)
0.01.392.708 I llama_perf_context_print:        eval time =     709.50 ms /    63 runs   (   11.26 ms per token,    88.79 tokens per second)
0.01.392.711 I llama_perf_context_print:       total time =     760.02 ms /    70 tokens
0.01.392.899 I ggml_metal_free: deallocating

real	0m1.413s
user	0m0.111s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.573 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.364 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.375 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.380 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.380 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.380 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.381 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.381 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.382 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.383 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.384 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.384 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.343 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.447 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.370 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.371 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.371 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.372 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.373 I llama_model_loader: - type  f32:  194 tensors
0.00.024.373 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.374 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.322 I llm_load_vocab: special tokens cache size = 25
0.00.051.377 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.380 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.380 I llm_load_print_meta: arch             = gptneox
0.00.051.381 I llm_load_print_meta: vocab type       = BPE
0.00.051.381 I llm_load_print_meta: n_vocab          = 50304
0.00.051.381 I llm_load_print_meta: n_merges         = 50009
0.00.051.381 I llm_load_print_meta: vocab_only       = 0
0.00.051.381 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.382 I llm_load_print_meta: n_embd           = 2048
0.00.051.382 I llm_load_print_meta: n_layer          = 24
0.00.051.396 I llm_load_print_meta: n_head           = 16
0.00.051.398 I llm_load_print_meta: n_head_kv        = 16
0.00.051.398 I llm_load_print_meta: n_rot            = 32
0.00.051.398 I llm_load_print_meta: n_swa            = 0
0.00.051.398 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.398 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.399 I llm_load_print_meta: n_gqa            = 1
0.00.051.405 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.406 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.407 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.407 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.407 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.407 I llm_load_print_meta: n_ff             = 8192
0.00.051.408 I llm_load_print_meta: n_expert         = 0
0.00.051.408 I llm_load_print_meta: n_expert_used    = 0
0.00.051.408 I llm_load_print_meta: causal attn      = 1
0.00.051.408 I llm_load_print_meta: pooling type     = 0
0.00.051.408 I llm_load_print_meta: rope type        = 2
0.00.051.408 I llm_load_print_meta: rope scaling     = linear
0.00.051.410 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.410 I llm_load_print_meta: freq_scale_train = 1
0.00.051.411 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.411 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.411 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.411 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.411 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.411 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.411 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.421 I llm_load_print_meta: model type       = 1.4B
0.00.051.421 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.422 I llm_load_print_meta: model params     = 1.41 B
0.00.051.422 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.423 I llm_load_print_meta: general.name     = 1.4B
0.00.051.423 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.423 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.423 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.423 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.423 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.424 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.424 I llm_load_print_meta: max token length = 1024
0.00.053.467 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.468 I llm_load_tensors: offloading output layer to GPU
0.00.053.468 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.478 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.479 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.351 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.352 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.352 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.353 I llama_new_context_with_model: n_batch       = 2048
0.00.054.353 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.353 I llama_new_context_with_model: flash_attn    = 0
0.00.054.354 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.354 I llama_new_context_with_model: freq_scale    = 1
0.00.054.354 I ggml_metal_init: allocating
0.00.054.360 I ggml_metal_init: found device: Apple M4
0.00.054.363 I ggml_metal_init: picking default device: Apple M4
0.00.054.937 I ggml_metal_init: using embedded metal library
0.00.057.245 I ggml_metal_init: GPU name:   Apple M4
0.00.057.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.248 I ggml_metal_init: simdgroup reduction   = true
0.00.057.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.248 I ggml_metal_init: has bfloat            = true
0.00.057.248 I ggml_metal_init: use bfloat            = true
0.00.057.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.876 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.883 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.902 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.843 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.845 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.845 I llama_new_context_with_model: graph nodes  = 967
0.00.085.845 I llama_new_context_with_model: graph splits = 2
0.00.085.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.894 I main: llama threadpool init, n_threads = 4
0.00.697.933 I 
0.00.697.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.988 I 
0.00.698.224 I sampler seed: 1234
0.00.698.228 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.698.239 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.698.240 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.698.240 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.547.779 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.547.780 I llama_perf_context_print:        load time =     689.32 ms
0.01.547.780 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.57 tokens per second)
0.01.547.781 I llama_perf_context_print:        eval time =     794.83 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.547.781 I llama_perf_context_print:       total time =     849.89 ms /    70 tokens
0.01.547.977 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.997 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.861 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.861 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.862 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.863 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.866 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.869 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.873 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.781 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.560 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.561 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.562 I llama_model_loader: - type  f32:  194 tensors
0.00.025.562 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.632 I llm_load_vocab: special tokens cache size = 25
0.00.051.692 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.695 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.696 I llm_load_print_meta: arch             = gptneox
0.00.051.696 I llm_load_print_meta: vocab type       = BPE
0.00.051.696 I llm_load_print_meta: n_vocab          = 50304
0.00.051.696 I llm_load_print_meta: n_merges         = 50009
0.00.051.697 I llm_load_print_meta: vocab_only       = 0
0.00.051.697 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.697 I llm_load_print_meta: n_embd           = 2048
0.00.051.697 I llm_load_print_meta: n_layer          = 24
0.00.051.706 I llm_load_print_meta: n_head           = 16
0.00.051.707 I llm_load_print_meta: n_head_kv        = 16
0.00.051.707 I llm_load_print_meta: n_rot            = 32
0.00.051.707 I llm_load_print_meta: n_swa            = 0
0.00.051.707 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.708 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.708 I llm_load_print_meta: n_gqa            = 1
0.00.051.709 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.710 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.710 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.711 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.711 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.711 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.711 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.712 I llm_load_print_meta: n_ff             = 8192
0.00.051.712 I llm_load_print_meta: n_expert         = 0
0.00.051.712 I llm_load_print_meta: n_expert_used    = 0
0.00.051.712 I llm_load_print_meta: causal attn      = 1
0.00.051.714 I llm_load_print_meta: pooling type     = 0
0.00.051.714 I llm_load_print_meta: rope type        = 2
0.00.051.714 I llm_load_print_meta: rope scaling     = linear
0.00.051.714 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.715 I llm_load_print_meta: freq_scale_train = 1
0.00.051.715 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.715 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.715 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.715 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.716 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.716 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.717 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.721 I llm_load_print_meta: model type       = 1.4B
0.00.051.721 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.722 I llm_load_print_meta: model params     = 1.41 B
0.00.051.722 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.722 I llm_load_print_meta: general.name     = 1.4B
0.00.051.723 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.723 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.723 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.724 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.724 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.725 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.725 I llm_load_print_meta: max token length = 1024
0.00.053.491 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.491 I llm_load_tensors: offloading output layer to GPU
0.00.053.492 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.497 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.498 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.395 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.396 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.396 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.396 I llama_new_context_with_model: n_batch       = 2048
0.00.054.397 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.397 I llama_new_context_with_model: flash_attn    = 0
0.00.054.397 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.398 I llama_new_context_with_model: freq_scale    = 1
0.00.054.398 I ggml_metal_init: allocating
0.00.054.401 I ggml_metal_init: found device: Apple M4
0.00.054.403 I ggml_metal_init: picking default device: Apple M4
0.00.055.011 I ggml_metal_init: using embedded metal library
0.00.057.297 I ggml_metal_init: GPU name:   Apple M4
0.00.057.298 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.299 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.299 I ggml_metal_init: simdgroup reduction   = true
0.00.057.301 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.301 I ggml_metal_init: has bfloat            = true
0.00.057.301 I ggml_metal_init: use bfloat            = true
0.00.057.302 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.927 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.933 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.950 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.048 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.049 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.050 I llama_new_context_with_model: graph nodes  = 967
0.00.087.050 I llama_new_context_with_model: graph splits = 2
0.00.087.063 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.812 I main: llama threadpool init, n_threads = 4
0.00.755.855 I 
0.00.755.879 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.879 I 
0.00.756.125 I sampler seed: 1234
0.00.756.129 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.159 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.159 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.650.533 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.650.534 I llama_perf_context_print:        load time =     745.81 ms
0.01.650.536 I llama_perf_context_print: prompt eval time =      58.29 ms /     7 tokens (    8.33 ms per token,   120.08 tokens per second)
0.01.650.537 I llama_perf_context_print:        eval time =     833.37 ms /    63 runs   (   13.23 ms per token,    75.60 tokens per second)
0.01.650.537 I llama_perf_context_print:       total time =     894.72 ms /    70 tokens
0.01.650.742 I ggml_metal_free: deallocating

real	0m1.671s
user	0m0.109s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.494 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.671 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.106 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.123 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.124 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.129 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.129 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.130 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.152 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.666 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.121 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.124 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.125 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.126 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.126 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.127 I llama_model_loader: - type  f32:  194 tensors
0.00.058.128 I llama_model_loader: - type  f16:   98 tensors
0.00.091.919 I llm_load_vocab: special tokens cache size = 25
0.00.099.448 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.451 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.451 I llm_load_print_meta: arch             = gptneox
0.00.099.452 I llm_load_print_meta: vocab type       = BPE
0.00.099.452 I llm_load_print_meta: n_vocab          = 50304
0.00.099.452 I llm_load_print_meta: n_merges         = 50009
0.00.099.452 I llm_load_print_meta: vocab_only       = 0
0.00.099.452 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.452 I llm_load_print_meta: n_embd           = 2048
0.00.099.453 I llm_load_print_meta: n_layer          = 24
0.00.099.465 I llm_load_print_meta: n_head           = 16
0.00.099.467 I llm_load_print_meta: n_head_kv        = 16
0.00.099.467 I llm_load_print_meta: n_rot            = 32
0.00.099.467 I llm_load_print_meta: n_swa            = 0
0.00.099.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.467 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.468 I llm_load_print_meta: n_gqa            = 1
0.00.099.470 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.471 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.471 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.472 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.472 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.472 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.472 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.473 I llm_load_print_meta: n_ff             = 8192
0.00.099.473 I llm_load_print_meta: n_expert         = 0
0.00.099.473 I llm_load_print_meta: n_expert_used    = 0
0.00.099.473 I llm_load_print_meta: causal attn      = 1
0.00.099.473 I llm_load_print_meta: pooling type     = 0
0.00.099.473 I llm_load_print_meta: rope type        = 2
0.00.099.474 I llm_load_print_meta: rope scaling     = linear
0.00.099.474 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.474 I llm_load_print_meta: freq_scale_train = 1
0.00.099.474 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.475 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.477 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.477 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.477 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.477 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.477 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.487 I llm_load_print_meta: model type       = 1.4B
0.00.099.487 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.488 I llm_load_print_meta: model params     = 1.41 B
0.00.099.488 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.488 I llm_load_print_meta: general.name     = 1.4B
0.00.099.489 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.489 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.489 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.489 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.490 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.099.490 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.490 I llm_load_print_meta: max token length = 1024
0.00.102.016 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.102.017 I llm_load_tensors: offloading output layer to GPU
0.00.102.017 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.102.028 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.102.029 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.022 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.023 I llama_new_context_with_model: n_ctx         = 128
0.00.103.023 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.103.023 I llama_new_context_with_model: n_batch       = 128
0.00.103.023 I llama_new_context_with_model: n_ubatch      = 128
0.00.103.023 I llama_new_context_with_model: flash_attn    = 0
0.00.103.024 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.024 I llama_new_context_with_model: freq_scale    = 1
0.00.103.025 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.103.025 I ggml_metal_init: allocating
0.00.103.032 I ggml_metal_init: found device: Apple M4
0.00.103.034 I ggml_metal_init: picking default device: Apple M4
0.00.103.705 I ggml_metal_init: using embedded metal library
0.00.106.399 I ggml_metal_init: GPU name:   Apple M4
0.00.106.400 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.106.401 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.106.401 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.106.402 I ggml_metal_init: simdgroup reduction   = true
0.00.106.402 I ggml_metal_init: simdgroup matrix mul. = true
0.00.106.402 I ggml_metal_init: has bfloat            = true
0.00.106.402 I ggml_metal_init: use bfloat            = true
0.00.106.402 I ggml_metal_init: hasUnifiedMemory      = true
0.00.106.403 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.800 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.117.803 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.117.815 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.752 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.118.753 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.118.753 I llama_new_context_with_model: graph nodes  = 967
0.00.118.754 I llama_new_context_with_model: graph splits = 2
0.00.118.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.465.973 I 
0.01.466.081 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.466.115 I perplexity: tokenizing the input ..
0.01.479.125 I perplexity: tokenization took 13.007 ms
0.01.479.156 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.600.991 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.602.853 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.602.878 I llama_perf_context_print:        load time =    1440.28 ms
0.01.602.882 I llama_perf_context_print: prompt eval time =     121.45 ms /   128 tokens (    0.95 ms per token,  1053.98 tokens per second)
0.01.602.883 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.602.884 I llama_perf_context_print:       total time =     136.92 ms /   129 tokens
0.01.603.663 I ggml_metal_free: deallocating

real	0m1.800s
user	0m0.131s
sys	0m0.259s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.853 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.377 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.832 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.043.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.866 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.870 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.875 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.876 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.876 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.877 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.567 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.222 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.226 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.226 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.227 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.227 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.228 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.062.229 I llama_model_loader: - type  f32:  194 tensors
0.00.062.229 I llama_model_loader: - type q8_0:   98 tensors
0.00.094.544 I llm_load_vocab: special tokens cache size = 25
0.00.101.609 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.101.612 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.101.612 I llm_load_print_meta: arch             = gptneox
0.00.101.613 I llm_load_print_meta: vocab type       = BPE
0.00.101.613 I llm_load_print_meta: n_vocab          = 50304
0.00.101.613 I llm_load_print_meta: n_merges         = 50009
0.00.101.613 I llm_load_print_meta: vocab_only       = 0
0.00.101.613 I llm_load_print_meta: n_ctx_train      = 2048
0.00.101.613 I llm_load_print_meta: n_embd           = 2048
0.00.101.614 I llm_load_print_meta: n_layer          = 24
0.00.101.628 I llm_load_print_meta: n_head           = 16
0.00.101.629 I llm_load_print_meta: n_head_kv        = 16
0.00.101.629 I llm_load_print_meta: n_rot            = 32
0.00.101.629 I llm_load_print_meta: n_swa            = 0
0.00.101.630 I llm_load_print_meta: n_embd_head_k    = 128
0.00.101.630 I llm_load_print_meta: n_embd_head_v    = 128
0.00.101.630 I llm_load_print_meta: n_gqa            = 1
0.00.101.631 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.101.632 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.101.633 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.101.633 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.101.633 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.101.633 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.101.633 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.101.634 I llm_load_print_meta: n_ff             = 8192
0.00.101.634 I llm_load_print_meta: n_expert         = 0
0.00.101.634 I llm_load_print_meta: n_expert_used    = 0
0.00.101.634 I llm_load_print_meta: causal attn      = 1
0.00.101.635 I llm_load_print_meta: pooling type     = 0
0.00.101.635 I llm_load_print_meta: rope type        = 2
0.00.101.637 I llm_load_print_meta: rope scaling     = linear
0.00.101.637 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.101.637 I llm_load_print_meta: freq_scale_train = 1
0.00.101.638 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.101.638 I llm_load_print_meta: rope_finetuned   = unknown
0.00.101.638 I llm_load_print_meta: ssm_d_conv       = 0
0.00.101.638 I llm_load_print_meta: ssm_d_inner      = 0
0.00.101.638 I llm_load_print_meta: ssm_d_state      = 0
0.00.101.640 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.101.640 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.101.650 I llm_load_print_meta: model type       = 1.4B
0.00.101.650 I llm_load_print_meta: model ftype      = Q8_0
0.00.101.650 I llm_load_print_meta: model params     = 1.41 B
0.00.101.651 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.101.651 I llm_load_print_meta: general.name     = 1.4B
0.00.101.651 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.101.651 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.101.652 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.101.652 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.101.652 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.101.652 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.101.652 I llm_load_print_meta: max token length = 1024
0.00.104.114 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.104.115 I llm_load_tensors: offloading output layer to GPU
0.00.104.115 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.104.126 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.104.127 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.105.130 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.130 I llama_new_context_with_model: n_ctx         = 128
0.00.105.131 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.105.131 I llama_new_context_with_model: n_batch       = 128
0.00.105.131 I llama_new_context_with_model: n_ubatch      = 128
0.00.105.131 I llama_new_context_with_model: flash_attn    = 0
0.00.105.132 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.132 I llama_new_context_with_model: freq_scale    = 1
0.00.105.132 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.105.133 I ggml_metal_init: allocating
0.00.105.135 I ggml_metal_init: found device: Apple M4
0.00.105.142 I ggml_metal_init: picking default device: Apple M4
0.00.105.871 I ggml_metal_init: using embedded metal library
0.00.108.645 I ggml_metal_init: GPU name:   Apple M4
0.00.108.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.108.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.108.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.108.647 I ggml_metal_init: simdgroup reduction   = true
0.00.108.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.108.648 I ggml_metal_init: has bfloat            = true
0.00.108.648 I ggml_metal_init: use bfloat            = true
0.00.108.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.108.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.997 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.120.003 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.120.018 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.950 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.120.951 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.120.951 I llama_new_context_with_model: graph nodes  = 967
0.00.120.951 I llama_new_context_with_model: graph splits = 2
0.00.120.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.023.687 I 
0.01.023.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.023.814 I perplexity: tokenizing the input ..
0.01.039.882 I perplexity: tokenization took 16.065 ms
0.01.039.900 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.167.456 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.168.705 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.168.728 I llama_perf_context_print:        load time =     998.29 ms
0.01.168.729 I llama_perf_context_print: prompt eval time =     126.59 ms /   128 tokens (    0.99 ms per token,  1011.11 tokens per second)
0.01.168.731 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.168.731 I llama_perf_context_print:       total time =     145.05 ms /   129 tokens
0.01.169.451 I ggml_metal_free: deallocating

real	0m1.205s
user	0m0.121s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.385 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.530 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.694 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.701 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.702 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.706 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.706 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.706 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.707 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.707 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.711 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.605 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.630 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.406 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.408 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.409 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.409 I llama_model_loader: - type  f32:  194 tensors
0.00.024.409 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.410 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.438 I llm_load_vocab: special tokens cache size = 25
0.00.050.498 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.501 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.501 I llm_load_print_meta: arch             = gptneox
0.00.050.502 I llm_load_print_meta: vocab type       = BPE
0.00.050.502 I llm_load_print_meta: n_vocab          = 50304
0.00.050.502 I llm_load_print_meta: n_merges         = 50009
0.00.050.502 I llm_load_print_meta: vocab_only       = 0
0.00.050.502 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.503 I llm_load_print_meta: n_embd           = 2048
0.00.050.503 I llm_load_print_meta: n_layer          = 24
0.00.050.517 I llm_load_print_meta: n_head           = 16
0.00.050.518 I llm_load_print_meta: n_head_kv        = 16
0.00.050.518 I llm_load_print_meta: n_rot            = 32
0.00.050.518 I llm_load_print_meta: n_swa            = 0
0.00.050.519 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.519 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.520 I llm_load_print_meta: n_gqa            = 1
0.00.050.520 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.521 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.522 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.523 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.524 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.524 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.524 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.525 I llm_load_print_meta: n_ff             = 8192
0.00.050.525 I llm_load_print_meta: n_expert         = 0
0.00.050.525 I llm_load_print_meta: n_expert_used    = 0
0.00.050.525 I llm_load_print_meta: causal attn      = 1
0.00.050.525 I llm_load_print_meta: pooling type     = 0
0.00.050.525 I llm_load_print_meta: rope type        = 2
0.00.050.525 I llm_load_print_meta: rope scaling     = linear
0.00.050.526 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.526 I llm_load_print_meta: freq_scale_train = 1
0.00.050.526 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.526 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.526 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.527 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.527 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.527 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.527 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.538 I llm_load_print_meta: model type       = 1.4B
0.00.050.538 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.539 I llm_load_print_meta: model params     = 1.41 B
0.00.050.539 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.539 I llm_load_print_meta: general.name     = 1.4B
0.00.050.540 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.540 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.540 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.540 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.540 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.541 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.541 I llm_load_print_meta: max token length = 1024
0.00.052.405 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.405 I llm_load_tensors: offloading output layer to GPU
0.00.052.405 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.415 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.416 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.289 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.290 I llama_new_context_with_model: n_ctx         = 128
0.00.053.290 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.290 I llama_new_context_with_model: n_batch       = 128
0.00.053.290 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.290 I llama_new_context_with_model: flash_attn    = 0
0.00.053.291 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.291 I llama_new_context_with_model: freq_scale    = 1
0.00.053.291 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.292 I ggml_metal_init: allocating
0.00.053.295 I ggml_metal_init: found device: Apple M4
0.00.053.297 I ggml_metal_init: picking default device: Apple M4
0.00.053.860 I ggml_metal_init: using embedded metal library
0.00.056.189 I ggml_metal_init: GPU name:   Apple M4
0.00.056.191 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.191 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.192 I ggml_metal_init: simdgroup reduction   = true
0.00.056.192 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.192 I ggml_metal_init: has bfloat            = true
0.00.056.192 I ggml_metal_init: use bfloat            = true
0.00.056.192 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.193 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.267 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.269 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.284 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.193 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.194 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.194 I llama_new_context_with_model: graph nodes  = 967
0.00.068.195 I llama_new_context_with_model: graph splits = 2
0.00.068.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.040 I 
0.00.632.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.091 I perplexity: tokenizing the input ..
0.00.640.005 I perplexity: tokenization took 7.912 ms
0.00.640.017 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.059 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.764.220 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.764.243 I llama_perf_context_print:        load time =     622.51 ms
0.00.764.244 I llama_perf_context_print: prompt eval time =     122.78 ms /   128 tokens (    0.96 ms per token,  1042.49 tokens per second)
0.00.764.245 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.245 I llama_perf_context_print:       total time =     132.20 ms /   129 tokens
0.00.764.698 I ggml_metal_free: deallocating

real	0m0.784s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.006 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.007 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.008 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.010 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.015 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.015 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.015 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.932 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.761 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.762 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.762 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.762 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.763 I llama_model_loader: - type  f32:  194 tensors
0.00.023.763 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.764 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.703 I llm_load_vocab: special tokens cache size = 25
0.00.050.696 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.699 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.699 I llm_load_print_meta: arch             = gptneox
0.00.050.699 I llm_load_print_meta: vocab type       = BPE
0.00.050.699 I llm_load_print_meta: n_vocab          = 50304
0.00.050.700 I llm_load_print_meta: n_merges         = 50009
0.00.050.700 I llm_load_print_meta: vocab_only       = 0
0.00.050.700 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.700 I llm_load_print_meta: n_embd           = 2048
0.00.050.700 I llm_load_print_meta: n_layer          = 24
0.00.050.715 I llm_load_print_meta: n_head           = 16
0.00.050.716 I llm_load_print_meta: n_head_kv        = 16
0.00.050.716 I llm_load_print_meta: n_rot            = 32
0.00.050.716 I llm_load_print_meta: n_swa            = 0
0.00.050.717 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.717 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.718 I llm_load_print_meta: n_gqa            = 1
0.00.050.718 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.719 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.720 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.720 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.720 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.720 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.720 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.722 I llm_load_print_meta: n_ff             = 8192
0.00.050.722 I llm_load_print_meta: n_expert         = 0
0.00.050.722 I llm_load_print_meta: n_expert_used    = 0
0.00.050.723 I llm_load_print_meta: causal attn      = 1
0.00.050.723 I llm_load_print_meta: pooling type     = 0
0.00.050.723 I llm_load_print_meta: rope type        = 2
0.00.050.723 I llm_load_print_meta: rope scaling     = linear
0.00.050.723 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.723 I llm_load_print_meta: freq_scale_train = 1
0.00.050.724 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.724 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.724 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.724 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.724 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.724 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.724 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.734 I llm_load_print_meta: model type       = 1.4B
0.00.050.734 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.734 I llm_load_print_meta: model params     = 1.41 B
0.00.050.735 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.735 I llm_load_print_meta: general.name     = 1.4B
0.00.050.735 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.735 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.736 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.736 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.736 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.736 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.738 I llm_load_print_meta: max token length = 1024
0.00.052.709 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.709 I llm_load_tensors: offloading output layer to GPU
0.00.052.709 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.719 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.720 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.623 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.624 I llama_new_context_with_model: n_ctx         = 128
0.00.053.625 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.625 I llama_new_context_with_model: n_batch       = 128
0.00.053.625 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.625 I llama_new_context_with_model: flash_attn    = 0
0.00.053.625 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.626 I llama_new_context_with_model: freq_scale    = 1
0.00.053.626 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.627 I ggml_metal_init: allocating
0.00.053.630 I ggml_metal_init: found device: Apple M4
0.00.053.632 I ggml_metal_init: picking default device: Apple M4
0.00.054.201 I ggml_metal_init: using embedded metal library
0.00.056.514 I ggml_metal_init: GPU name:   Apple M4
0.00.056.515 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.515 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.516 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.516 I ggml_metal_init: simdgroup reduction   = true
0.00.056.516 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.516 I ggml_metal_init: has bfloat            = true
0.00.056.516 I ggml_metal_init: use bfloat            = true
0.00.056.517 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.517 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.488 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.491 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.504 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.444 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.444 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.445 I llama_new_context_with_model: graph nodes  = 967
0.00.068.445 I llama_new_context_with_model: graph splits = 2
0.00.068.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.361 I 
0.00.653.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.433 I perplexity: tokenizing the input ..
0.00.661.382 I perplexity: tokenization took 7.947 ms
0.00.661.393 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.360 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.785.635 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.785.674 I llama_perf_context_print:        load time =     644.48 ms
0.00.785.675 I llama_perf_context_print: prompt eval time =     122.74 ms /   128 tokens (    0.96 ms per token,  1042.85 tokens per second)
0.00.785.676 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.676 I llama_perf_context_print:       total time =     132.32 ms /   129 tokens
0.00.786.178 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.079s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.156 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.382 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.389 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.395 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.395 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.395 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.396 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.396 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.398 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.398 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.400 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.400 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.119 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.121 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.122 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.122 I llama_model_loader: - type  f32:  194 tensors
0.00.025.123 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.123 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.152 I llm_load_vocab: special tokens cache size = 25
0.00.051.160 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.164 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.164 I llm_load_print_meta: arch             = gptneox
0.00.051.165 I llm_load_print_meta: vocab type       = BPE
0.00.051.165 I llm_load_print_meta: n_vocab          = 50304
0.00.051.166 I llm_load_print_meta: n_merges         = 50009
0.00.051.166 I llm_load_print_meta: vocab_only       = 0
0.00.051.167 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.167 I llm_load_print_meta: n_embd           = 2048
0.00.051.167 I llm_load_print_meta: n_layer          = 24
0.00.051.181 I llm_load_print_meta: n_head           = 16
0.00.051.182 I llm_load_print_meta: n_head_kv        = 16
0.00.051.182 I llm_load_print_meta: n_rot            = 32
0.00.051.182 I llm_load_print_meta: n_swa            = 0
0.00.051.182 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.183 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.183 I llm_load_print_meta: n_gqa            = 1
0.00.051.184 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.185 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.185 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.185 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.186 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.186 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.186 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.186 I llm_load_print_meta: n_ff             = 8192
0.00.051.187 I llm_load_print_meta: n_expert         = 0
0.00.051.187 I llm_load_print_meta: n_expert_used    = 0
0.00.051.187 I llm_load_print_meta: causal attn      = 1
0.00.051.187 I llm_load_print_meta: pooling type     = 0
0.00.051.187 I llm_load_print_meta: rope type        = 2
0.00.051.187 I llm_load_print_meta: rope scaling     = linear
0.00.051.189 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.189 I llm_load_print_meta: freq_scale_train = 1
0.00.051.189 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.189 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.189 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.190 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.190 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.190 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.190 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.199 I llm_load_print_meta: model type       = 1.4B
0.00.051.200 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.200 I llm_load_print_meta: model params     = 1.41 B
0.00.051.201 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.201 I llm_load_print_meta: general.name     = 1.4B
0.00.051.201 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.201 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.201 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.201 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.202 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.202 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.202 I llm_load_print_meta: max token length = 1024
0.00.053.221 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.221 I llm_load_tensors: offloading output layer to GPU
0.00.053.221 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.231 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.232 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.108 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.109 I llama_new_context_with_model: n_ctx         = 128
0.00.054.109 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.109 I llama_new_context_with_model: n_batch       = 128
0.00.054.109 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.109 I llama_new_context_with_model: flash_attn    = 0
0.00.054.110 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.110 I llama_new_context_with_model: freq_scale    = 1
0.00.054.110 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.111 I ggml_metal_init: allocating
0.00.054.116 I ggml_metal_init: found device: Apple M4
0.00.054.118 I ggml_metal_init: picking default device: Apple M4
0.00.054.667 I ggml_metal_init: using embedded metal library
0.00.057.021 I ggml_metal_init: GPU name:   Apple M4
0.00.057.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.023 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.024 I ggml_metal_init: simdgroup reduction   = true
0.00.057.024 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.024 I ggml_metal_init: has bfloat            = true
0.00.057.024 I ggml_metal_init: use bfloat            = true
0.00.057.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.025 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.559 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.562 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.576 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.434 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.435 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.435 I llama_new_context_with_model: graph nodes  = 967
0.00.068.436 I llama_new_context_with_model: graph splits = 2
0.00.068.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.119 I 
0.00.731.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.168 I perplexity: tokenizing the input ..
0.00.739.275 I perplexity: tokenization took 8.105 ms
0.00.739.285 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.874.485 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.875.641 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.875.665 I llama_perf_context_print:        load time =     720.96 ms
0.00.875.666 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.37 tokens per second)
0.00.875.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.875.669 I llama_perf_context_print:       total time =     144.55 ms /   129 tokens
0.00.876.136 I ggml_metal_free: deallocating

real	0m0.892s
user	0m0.077s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.835 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.566 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.489 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.295 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.296 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.296 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.297 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.297 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.297 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.298 I llama_model_loader: - type  f32:  194 tensors
0.00.023.298 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.298 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.080 I llm_load_vocab: special tokens cache size = 25
0.00.050.204 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.207 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.207 I llm_load_print_meta: arch             = gptneox
0.00.050.208 I llm_load_print_meta: vocab type       = BPE
0.00.050.208 I llm_load_print_meta: n_vocab          = 50304
0.00.050.208 I llm_load_print_meta: n_merges         = 50009
0.00.050.208 I llm_load_print_meta: vocab_only       = 0
0.00.050.209 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.209 I llm_load_print_meta: n_embd           = 2048
0.00.050.209 I llm_load_print_meta: n_layer          = 24
0.00.050.223 I llm_load_print_meta: n_head           = 16
0.00.050.224 I llm_load_print_meta: n_head_kv        = 16
0.00.050.224 I llm_load_print_meta: n_rot            = 32
0.00.050.224 I llm_load_print_meta: n_swa            = 0
0.00.050.225 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.225 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.225 I llm_load_print_meta: n_gqa            = 1
0.00.050.226 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.229 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.229 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.230 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.230 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.230 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.230 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.231 I llm_load_print_meta: n_ff             = 8192
0.00.050.231 I llm_load_print_meta: n_expert         = 0
0.00.050.232 I llm_load_print_meta: n_expert_used    = 0
0.00.050.232 I llm_load_print_meta: causal attn      = 1
0.00.050.232 I llm_load_print_meta: pooling type     = 0
0.00.050.232 I llm_load_print_meta: rope type        = 2
0.00.050.232 I llm_load_print_meta: rope scaling     = linear
0.00.050.232 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.233 I llm_load_print_meta: freq_scale_train = 1
0.00.050.233 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.233 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.233 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.235 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.235 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.235 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.235 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.244 I llm_load_print_meta: model type       = 1.4B
0.00.050.244 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.245 I llm_load_print_meta: model params     = 1.41 B
0.00.050.245 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.245 I llm_load_print_meta: general.name     = 1.4B
0.00.050.246 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.246 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.246 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.246 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.246 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.247 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.247 I llm_load_print_meta: max token length = 1024
0.00.051.801 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.801 I llm_load_tensors: offloading output layer to GPU
0.00.051.801 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.811 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.813 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.644 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.645 I llama_new_context_with_model: n_ctx         = 128
0.00.052.645 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.645 I llama_new_context_with_model: n_batch       = 128
0.00.052.645 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.645 I llama_new_context_with_model: flash_attn    = 0
0.00.052.646 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.646 I llama_new_context_with_model: freq_scale    = 1
0.00.052.647 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.647 I ggml_metal_init: allocating
0.00.052.652 I ggml_metal_init: found device: Apple M4
0.00.052.657 I ggml_metal_init: picking default device: Apple M4
0.00.053.227 I ggml_metal_init: using embedded metal library
0.00.055.576 I ggml_metal_init: GPU name:   Apple M4
0.00.055.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.578 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.578 I ggml_metal_init: simdgroup reduction   = true
0.00.055.578 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.578 I ggml_metal_init: has bfloat            = true
0.00.055.579 I ggml_metal_init: use bfloat            = true
0.00.055.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.084 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.087 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.100 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.939 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.939 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.940 I llama_new_context_with_model: graph nodes  = 967
0.00.066.940 I llama_new_context_with_model: graph splits = 2
0.00.066.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.839 I 
0.00.670.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.896 I perplexity: tokenizing the input ..
0.00.678.000 I perplexity: tokenization took 7.102 ms
0.00.678.010 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.002 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.813.417 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.813.436 I llama_perf_context_print:        load time =     662.00 ms
0.00.813.437 I llama_perf_context_print: prompt eval time =     133.77 ms /   128 tokens (    1.05 ms per token,   956.89 tokens per second)
0.00.813.437 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.437 I llama_perf_context_print:       total time =     142.60 ms /   129 tokens
0.00.813.782 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.078s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.048 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.043 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.052 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.053 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.054 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.058 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.744 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.745 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.745 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.746 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.746 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.746 I llama_model_loader: - type  f32:  194 tensors
0.00.023.747 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.747 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.747 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.188 I llm_load_vocab: special tokens cache size = 25
0.00.051.260 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.264 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.264 I llm_load_print_meta: arch             = gptneox
0.00.051.265 I llm_load_print_meta: vocab type       = BPE
0.00.051.265 I llm_load_print_meta: n_vocab          = 50304
0.00.051.265 I llm_load_print_meta: n_merges         = 50009
0.00.051.266 I llm_load_print_meta: vocab_only       = 0
0.00.051.266 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.266 I llm_load_print_meta: n_embd           = 2048
0.00.051.271 I llm_load_print_meta: n_layer          = 24
0.00.051.288 I llm_load_print_meta: n_head           = 16
0.00.051.290 I llm_load_print_meta: n_head_kv        = 16
0.00.051.290 I llm_load_print_meta: n_rot            = 32
0.00.051.290 I llm_load_print_meta: n_swa            = 0
0.00.051.290 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.290 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.295 I llm_load_print_meta: n_gqa            = 1
0.00.051.295 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.296 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.297 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.297 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.299 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.299 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.299 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.299 I llm_load_print_meta: n_ff             = 8192
0.00.051.300 I llm_load_print_meta: n_expert         = 0
0.00.051.300 I llm_load_print_meta: n_expert_used    = 0
0.00.051.300 I llm_load_print_meta: causal attn      = 1
0.00.051.300 I llm_load_print_meta: pooling type     = 0
0.00.051.300 I llm_load_print_meta: rope type        = 2
0.00.051.300 I llm_load_print_meta: rope scaling     = linear
0.00.051.301 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.301 I llm_load_print_meta: freq_scale_train = 1
0.00.051.301 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.301 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.303 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.303 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.303 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.303 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.303 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.313 I llm_load_print_meta: model type       = 1.4B
0.00.051.314 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.314 I llm_load_print_meta: model params     = 1.41 B
0.00.051.314 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.315 I llm_load_print_meta: general.name     = 1.4B
0.00.051.315 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.315 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.315 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.315 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.316 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.316 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.316 I llm_load_print_meta: max token length = 1024
0.00.053.081 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.081 I llm_load_tensors: offloading output layer to GPU
0.00.053.082 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.093 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.094 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.101 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.102 I llama_new_context_with_model: n_ctx         = 128
0.00.054.103 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.103 I llama_new_context_with_model: n_batch       = 128
0.00.054.103 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.103 I llama_new_context_with_model: flash_attn    = 0
0.00.054.103 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.104 I llama_new_context_with_model: freq_scale    = 1
0.00.054.104 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.105 I ggml_metal_init: allocating
0.00.054.109 I ggml_metal_init: found device: Apple M4
0.00.054.111 I ggml_metal_init: picking default device: Apple M4
0.00.054.721 I ggml_metal_init: using embedded metal library
0.00.057.161 I ggml_metal_init: GPU name:   Apple M4
0.00.057.163 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.163 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.164 I ggml_metal_init: simdgroup reduction   = true
0.00.057.164 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.164 I ggml_metal_init: has bfloat            = true
0.00.057.164 I ggml_metal_init: use bfloat            = true
0.00.057.165 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.166 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.697 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.702 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.717 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.657 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.658 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.658 I llama_new_context_with_model: graph nodes  = 967
0.00.068.658 I llama_new_context_with_model: graph splits = 2
0.00.068.671 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.391.157 I 
0.00.391.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.391.204 I perplexity: tokenizing the input ..
0.00.398.494 I perplexity: tokenization took 7.289 ms
0.00.398.505 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.530.110 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.531.346 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.531.359 I llama_perf_context_print:        load time =     382.11 ms
0.00.531.360 I llama_perf_context_print: prompt eval time =     131.34 ms /   128 tokens (    1.03 ms per token,   974.53 tokens per second)
0.00.531.361 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.531.361 I llama_perf_context_print:       total time =     140.20 ms /   129 tokens
0.00.531.749 I ggml_metal_free: deallocating

real	0m0.549s
user	0m0.078s
sys	0m0.065s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.625 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.628 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.629 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.631 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.385 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.337 I llama_model_loader: - type  f32:  194 tensors
0.00.023.337 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.337 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.338 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.338 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.176 I llm_load_vocab: special tokens cache size = 25
0.00.050.257 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.260 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.260 I llm_load_print_meta: arch             = gptneox
0.00.050.261 I llm_load_print_meta: vocab type       = BPE
0.00.050.261 I llm_load_print_meta: n_vocab          = 50304
0.00.050.261 I llm_load_print_meta: n_merges         = 50009
0.00.050.261 I llm_load_print_meta: vocab_only       = 0
0.00.050.261 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.262 I llm_load_print_meta: n_embd           = 2048
0.00.050.262 I llm_load_print_meta: n_layer          = 24
0.00.050.276 I llm_load_print_meta: n_head           = 16
0.00.050.278 I llm_load_print_meta: n_head_kv        = 16
0.00.050.278 I llm_load_print_meta: n_rot            = 32
0.00.050.278 I llm_load_print_meta: n_swa            = 0
0.00.050.278 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.278 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.279 I llm_load_print_meta: n_gqa            = 1
0.00.050.280 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.281 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.281 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.282 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.282 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.282 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.282 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.283 I llm_load_print_meta: n_ff             = 8192
0.00.050.283 I llm_load_print_meta: n_expert         = 0
0.00.050.283 I llm_load_print_meta: n_expert_used    = 0
0.00.050.283 I llm_load_print_meta: causal attn      = 1
0.00.050.283 I llm_load_print_meta: pooling type     = 0
0.00.050.283 I llm_load_print_meta: rope type        = 2
0.00.050.284 I llm_load_print_meta: rope scaling     = linear
0.00.050.284 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.284 I llm_load_print_meta: freq_scale_train = 1
0.00.050.284 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.285 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.285 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.285 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.285 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.285 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.285 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.295 I llm_load_print_meta: model type       = 1.4B
0.00.050.295 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.295 I llm_load_print_meta: model params     = 1.41 B
0.00.050.296 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.296 I llm_load_print_meta: general.name     = 1.4B
0.00.050.296 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.296 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.297 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.297 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.297 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.300 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: max token length = 1024
0.00.052.152 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.152 I llm_load_tensors: offloading output layer to GPU
0.00.052.152 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.163 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.164 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.028 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.029 I llama_new_context_with_model: n_ctx         = 128
0.00.053.029 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.029 I llama_new_context_with_model: n_batch       = 128
0.00.053.029 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.030 I llama_new_context_with_model: flash_attn    = 0
0.00.053.030 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.030 I llama_new_context_with_model: freq_scale    = 1
0.00.053.031 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.031 I ggml_metal_init: allocating
0.00.053.034 I ggml_metal_init: found device: Apple M4
0.00.053.036 I ggml_metal_init: picking default device: Apple M4
0.00.053.582 I ggml_metal_init: using embedded metal library
0.00.055.995 I ggml_metal_init: GPU name:   Apple M4
0.00.055.996 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.997 I ggml_metal_init: simdgroup reduction   = true
0.00.055.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.998 I ggml_metal_init: has bfloat            = true
0.00.055.998 I ggml_metal_init: use bfloat            = true
0.00.055.998 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.509 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.515 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.532 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.398 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.398 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.399 I llama_new_context_with_model: graph nodes  = 967
0.00.068.399 I llama_new_context_with_model: graph splits = 2
0.00.068.412 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.437 I 
0.00.484.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.484.498 I perplexity: tokenizing the input ..
0.00.492.449 I perplexity: tokenization took 7.949 ms
0.00.492.460 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.624.588 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.625.889 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.625.905 I llama_perf_context_print:        load time =     475.70 ms
0.00.625.906 I llama_perf_context_print: prompt eval time =     131.90 ms /   128 tokens (    1.03 ms per token,   970.43 tokens per second)
0.00.625.907 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.625.907 I llama_perf_context_print:       total time =     141.47 ms /   129 tokens
0.00.626.503 I ggml_metal_free: deallocating

real	0m0.640s
user	0m0.079s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.125 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.157 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.161 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.163 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.164 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.164 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.165 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.166 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.166 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.167 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.167 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.167 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.171 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.050 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.966 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.967 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.967 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.968 I llama_model_loader: - type  f32:  194 tensors
0.00.023.968 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.968 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.969 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.096 I llm_load_vocab: special tokens cache size = 25
0.00.049.882 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.884 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.884 I llm_load_print_meta: arch             = gptneox
0.00.049.884 I llm_load_print_meta: vocab type       = BPE
0.00.049.885 I llm_load_print_meta: n_vocab          = 50304
0.00.049.885 I llm_load_print_meta: n_merges         = 50009
0.00.049.885 I llm_load_print_meta: vocab_only       = 0
0.00.049.885 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.885 I llm_load_print_meta: n_embd           = 2048
0.00.049.886 I llm_load_print_meta: n_layer          = 24
0.00.049.900 I llm_load_print_meta: n_head           = 16
0.00.049.901 I llm_load_print_meta: n_head_kv        = 16
0.00.049.901 I llm_load_print_meta: n_rot            = 32
0.00.049.901 I llm_load_print_meta: n_swa            = 0
0.00.049.901 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.902 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.902 I llm_load_print_meta: n_gqa            = 1
0.00.049.903 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.904 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.904 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.905 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.905 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.905 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.905 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.906 I llm_load_print_meta: n_ff             = 8192
0.00.049.906 I llm_load_print_meta: n_expert         = 0
0.00.049.906 I llm_load_print_meta: n_expert_used    = 0
0.00.049.906 I llm_load_print_meta: causal attn      = 1
0.00.049.907 I llm_load_print_meta: pooling type     = 0
0.00.049.907 I llm_load_print_meta: rope type        = 2
0.00.049.909 I llm_load_print_meta: rope scaling     = linear
0.00.049.911 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.911 I llm_load_print_meta: freq_scale_train = 1
0.00.049.911 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.911 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.913 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.913 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.914 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.914 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.914 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.923 I llm_load_print_meta: model type       = 1.4B
0.00.049.925 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.925 I llm_load_print_meta: model params     = 1.41 B
0.00.049.926 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.926 I llm_load_print_meta: general.name     = 1.4B
0.00.049.926 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.926 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.928 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.928 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.928 I llm_load_print_meta: max token length = 1024
0.00.051.872 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.873 I llm_load_tensors: offloading output layer to GPU
0.00.051.873 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.883 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.884 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.839 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.840 I llama_new_context_with_model: n_ctx         = 128
0.00.052.840 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.840 I llama_new_context_with_model: n_batch       = 128
0.00.052.840 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.841 I llama_new_context_with_model: flash_attn    = 0
0.00.052.841 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.841 I llama_new_context_with_model: freq_scale    = 1
0.00.052.842 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.842 I ggml_metal_init: allocating
0.00.052.845 I ggml_metal_init: found device: Apple M4
0.00.052.847 I ggml_metal_init: picking default device: Apple M4
0.00.053.413 I ggml_metal_init: using embedded metal library
0.00.055.735 I ggml_metal_init: GPU name:   Apple M4
0.00.055.736 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.737 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.737 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.737 I ggml_metal_init: simdgroup reduction   = true
0.00.055.738 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.738 I ggml_metal_init: has bfloat            = true
0.00.055.739 I ggml_metal_init: use bfloat            = true
0.00.055.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.740 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.539 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.542 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.555 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.494 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.495 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.495 I llama_new_context_with_model: graph nodes  = 967
0.00.067.495 I llama_new_context_with_model: graph splits = 2
0.00.067.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.427 I 
0.00.571.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.466 I perplexity: tokenizing the input ..
0.00.579.446 I perplexity: tokenization took 7.979 ms
0.00.579.459 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.713.554 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.714.707 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.714.723 I llama_perf_context_print:        load time =     562.30 ms
0.00.714.724 I llama_perf_context_print: prompt eval time =     133.86 ms /   128 tokens (    1.05 ms per token,   956.22 tokens per second)
0.00.714.727 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.714.728 I llama_perf_context_print:       total time =     143.30 ms /   129 tokens
0.00.715.127 I ggml_metal_free: deallocating

real	0m0.728s
user	0m0.077s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.963 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.104 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.108 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.111 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.112 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.116 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.026 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.134 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.971 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.971 I llama_model_loader: - type  f32:  194 tensors
0.00.024.972 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.972 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.676 I llm_load_vocab: special tokens cache size = 25
0.00.051.576 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.579 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.579 I llm_load_print_meta: arch             = gptneox
0.00.051.579 I llm_load_print_meta: vocab type       = BPE
0.00.051.579 I llm_load_print_meta: n_vocab          = 50304
0.00.051.579 I llm_load_print_meta: n_merges         = 50009
0.00.051.580 I llm_load_print_meta: vocab_only       = 0
0.00.051.580 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.580 I llm_load_print_meta: n_embd           = 2048
0.00.051.580 I llm_load_print_meta: n_layer          = 24
0.00.051.594 I llm_load_print_meta: n_head           = 16
0.00.051.594 I llm_load_print_meta: n_head_kv        = 16
0.00.051.595 I llm_load_print_meta: n_rot            = 32
0.00.051.595 I llm_load_print_meta: n_swa            = 0
0.00.051.595 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.595 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.596 I llm_load_print_meta: n_gqa            = 1
0.00.051.597 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.597 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.598 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.598 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.599 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.599 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.599 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.600 I llm_load_print_meta: n_ff             = 8192
0.00.051.600 I llm_load_print_meta: n_expert         = 0
0.00.051.600 I llm_load_print_meta: n_expert_used    = 0
0.00.051.600 I llm_load_print_meta: causal attn      = 1
0.00.051.600 I llm_load_print_meta: pooling type     = 0
0.00.051.600 I llm_load_print_meta: rope type        = 2
0.00.051.601 I llm_load_print_meta: rope scaling     = linear
0.00.051.601 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.601 I llm_load_print_meta: freq_scale_train = 1
0.00.051.601 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.601 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.601 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.602 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.602 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.604 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.604 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.612 I llm_load_print_meta: model type       = 1.4B
0.00.051.613 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.613 I llm_load_print_meta: model params     = 1.41 B
0.00.051.613 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.614 I llm_load_print_meta: general.name     = 1.4B
0.00.051.614 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.614 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.614 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.614 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.614 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.615 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.615 I llm_load_print_meta: max token length = 1024
0.00.053.243 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.243 I llm_load_tensors: offloading output layer to GPU
0.00.053.243 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.253 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.254 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.106 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.107 I llama_new_context_with_model: n_ctx         = 128
0.00.054.107 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.107 I llama_new_context_with_model: n_batch       = 128
0.00.054.107 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.108 I llama_new_context_with_model: flash_attn    = 0
0.00.054.108 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.108 I llama_new_context_with_model: freq_scale    = 1
0.00.054.109 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.109 I ggml_metal_init: allocating
0.00.054.112 I ggml_metal_init: found device: Apple M4
0.00.054.114 I ggml_metal_init: picking default device: Apple M4
0.00.054.672 I ggml_metal_init: using embedded metal library
0.00.056.979 I ggml_metal_init: GPU name:   Apple M4
0.00.056.980 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.981 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.981 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.981 I ggml_metal_init: simdgroup reduction   = true
0.00.056.981 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.981 I ggml_metal_init: has bfloat            = true
0.00.056.982 I ggml_metal_init: use bfloat            = true
0.00.056.982 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.972 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.974 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.988 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.872 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.873 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.874 I llama_new_context_with_model: graph nodes  = 967
0.00.068.874 I llama_new_context_with_model: graph splits = 2
0.00.068.886 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.092 I 
0.00.664.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.135 I perplexity: tokenizing the input ..
0.00.672.595 I perplexity: tokenization took 8.458 ms
0.00.672.606 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.258 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.814.428 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.814.449 I llama_perf_context_print:        load time =     654.13 ms
0.00.814.450 I llama_perf_context_print: prompt eval time =     140.43 ms /   128 tokens (    1.10 ms per token,   911.51 tokens per second)
0.00.814.451 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.451 I llama_perf_context_print:       total time =     150.36 ms /   129 tokens
0.00.814.865 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.079s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.077 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.859 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.753 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.755 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.755 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.755 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.756 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.756 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.757 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.757 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.761 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.592 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.673 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.519 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.521 I llama_model_loader: - type  f32:  194 tensors
0.00.023.521 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.446 I llm_load_vocab: special tokens cache size = 25
0.00.049.363 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.365 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.366 I llm_load_print_meta: arch             = gptneox
0.00.049.366 I llm_load_print_meta: vocab type       = BPE
0.00.049.366 I llm_load_print_meta: n_vocab          = 50304
0.00.049.366 I llm_load_print_meta: n_merges         = 50009
0.00.049.367 I llm_load_print_meta: vocab_only       = 0
0.00.049.367 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.367 I llm_load_print_meta: n_embd           = 2048
0.00.049.367 I llm_load_print_meta: n_layer          = 24
0.00.049.382 I llm_load_print_meta: n_head           = 16
0.00.049.382 I llm_load_print_meta: n_head_kv        = 16
0.00.049.383 I llm_load_print_meta: n_rot            = 32
0.00.049.383 I llm_load_print_meta: n_swa            = 0
0.00.049.383 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.383 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.384 I llm_load_print_meta: n_gqa            = 1
0.00.049.385 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.385 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.386 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.386 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.387 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.387 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.387 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.388 I llm_load_print_meta: n_ff             = 8192
0.00.049.390 I llm_load_print_meta: n_expert         = 0
0.00.049.390 I llm_load_print_meta: n_expert_used    = 0
0.00.049.390 I llm_load_print_meta: causal attn      = 1
0.00.049.390 I llm_load_print_meta: pooling type     = 0
0.00.049.390 I llm_load_print_meta: rope type        = 2
0.00.049.391 I llm_load_print_meta: rope scaling     = linear
0.00.049.391 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.391 I llm_load_print_meta: freq_scale_train = 1
0.00.049.391 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.391 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.392 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.392 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.392 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.392 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.392 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.407 I llm_load_print_meta: model type       = 1.4B
0.00.049.408 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.408 I llm_load_print_meta: model params     = 1.41 B
0.00.049.408 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.410 I llm_load_print_meta: general.name     = 1.4B
0.00.049.410 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.410 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.410 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.410 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.411 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.411 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.411 I llm_load_print_meta: max token length = 1024
0.00.051.349 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.349 I llm_load_tensors: offloading output layer to GPU
0.00.051.349 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.359 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.361 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.252 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.253 I llama_new_context_with_model: n_ctx         = 128
0.00.052.253 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.253 I llama_new_context_with_model: n_batch       = 128
0.00.052.253 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.254 I llama_new_context_with_model: flash_attn    = 0
0.00.052.254 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.254 I llama_new_context_with_model: freq_scale    = 1
0.00.052.255 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.255 I ggml_metal_init: allocating
0.00.052.258 I ggml_metal_init: found device: Apple M4
0.00.052.260 I ggml_metal_init: picking default device: Apple M4
0.00.052.824 I ggml_metal_init: using embedded metal library
0.00.055.104 I ggml_metal_init: GPU name:   Apple M4
0.00.055.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.107 I ggml_metal_init: simdgroup reduction   = true
0.00.055.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.107 I ggml_metal_init: has bfloat            = true
0.00.055.107 I ggml_metal_init: use bfloat            = true
0.00.055.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.751 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.753 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.767 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.708 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.709 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.709 I llama_new_context_with_model: graph nodes  = 967
0.00.066.709 I llama_new_context_with_model: graph splits = 2
0.00.066.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.051 I 
0.00.522.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.096 I perplexity: tokenizing the input ..
0.00.530.225 I perplexity: tokenization took 8.127 ms
0.00.530.240 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.670.712 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.671.869 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.671.884 I llama_perf_context_print:        load time =     513.19 ms
0.00.671.885 I llama_perf_context_print: prompt eval time =     140.23 ms /   128 tokens (    1.10 ms per token,   912.77 tokens per second)
0.00.671.886 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.671.887 I llama_perf_context_print:       total time =     149.84 ms /   129 tokens
0.00.672.362 I ggml_metal_free: deallocating

real	0m0.685s
user	0m0.077s
sys	0m0.103s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.390 I build: 4324 (c27ac678) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.057 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.017 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.032 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.033 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.034 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.034 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.036 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.036 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.037 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.037 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.039 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.039 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.067 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.076 I llama_model_loader: - type  f32:  194 tensors
0.00.051.077 I llama_model_loader: - type  f16:   98 tensors
0.00.079.797 I llm_load_vocab: special tokens cache size = 25
0.00.086.408 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.411 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.411 I llm_load_print_meta: arch             = gptneox
0.00.086.411 I llm_load_print_meta: vocab type       = BPE
0.00.086.412 I llm_load_print_meta: n_vocab          = 50304
0.00.086.412 I llm_load_print_meta: n_merges         = 50009
0.00.086.412 I llm_load_print_meta: vocab_only       = 0
0.00.086.412 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.412 I llm_load_print_meta: n_embd           = 2048
0.00.086.412 I llm_load_print_meta: n_layer          = 24
0.00.086.427 I llm_load_print_meta: n_head           = 16
0.00.086.428 I llm_load_print_meta: n_head_kv        = 16
0.00.086.428 I llm_load_print_meta: n_rot            = 32
0.00.086.428 I llm_load_print_meta: n_swa            = 0
0.00.086.429 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.429 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.429 I llm_load_print_meta: n_gqa            = 1
0.00.086.430 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.431 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.431 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.432 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.432 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.432 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.432 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.433 I llm_load_print_meta: n_ff             = 8192
0.00.086.433 I llm_load_print_meta: n_expert         = 0
0.00.086.435 I llm_load_print_meta: n_expert_used    = 0
0.00.086.435 I llm_load_print_meta: causal attn      = 1
0.00.086.435 I llm_load_print_meta: pooling type     = 0
0.00.086.435 I llm_load_print_meta: rope type        = 2
0.00.086.435 I llm_load_print_meta: rope scaling     = linear
0.00.086.436 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.437 I llm_load_print_meta: freq_scale_train = 1
0.00.086.437 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.437 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.438 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.438 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.439 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.439 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.439 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.448 I llm_load_print_meta: model type       = 1.4B
0.00.086.449 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.449 I llm_load_print_meta: model params     = 1.41 B
0.00.086.450 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.450 I llm_load_print_meta: general.name     = 1.4B
0.00.086.450 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.450 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.450 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.451 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.451 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.451 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.451 I llm_load_print_meta: max token length = 1024
0.00.088.926 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.926 I llm_load_tensors: offloading output layer to GPU
0.00.088.927 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.937 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.938 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.830 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.831 I llama_new_context_with_model: n_ctx         = 128
0.00.089.831 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.831 I llama_new_context_with_model: n_batch       = 128
0.00.089.831 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.831 I llama_new_context_with_model: flash_attn    = 0
0.00.089.832 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.832 I llama_new_context_with_model: freq_scale    = 1
0.00.089.832 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.833 I ggml_metal_init: allocating
0.00.089.836 I ggml_metal_init: found device: Apple M4
0.00.089.838 I ggml_metal_init: picking default device: Apple M4
0.00.090.432 I ggml_metal_init: using embedded metal library
0.00.092.950 I ggml_metal_init: GPU name:   Apple M4
0.00.092.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.953 I ggml_metal_init: simdgroup reduction   = true
0.00.092.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.953 I ggml_metal_init: has bfloat            = true
0.00.092.953 I ggml_metal_init: use bfloat            = true
0.00.092.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.407 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.410 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.424 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.282 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.283 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.283 I llama_new_context_with_model: graph nodes  = 967
0.00.104.283 I llama_new_context_with_model: graph splits = 2
0.00.104.296 I 
0.00.104.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.328 I compute_imatrix: tokenizing the input ..
0.00.111.209 I compute_imatrix: tokenization took 6.88 ms
0.00.111.211 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.526.915 I compute_imatrix: 1.42 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.529.278 I llama_perf_context_print:        load time =    1505.85 ms
0.01.529.279 I llama_perf_context_print: prompt eval time =    1415.08 ms /   128 tokens (   11.06 ms per token,    90.45 tokens per second)
0.01.529.280 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.529.280 I llama_perf_context_print:       total time =    1508.21 ms /   129 tokens
0.01.529.811 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.164s
sys	0m0.249s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4324 (c27ac678)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12110b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12110be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12110c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12110c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12110cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12110d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12110dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12110e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12110e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12110eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12110f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12110f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121110040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1211107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121111000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121111720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121111e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121112560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121112c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121113450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121113b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121114290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1211149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121115250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121115970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121115c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121116240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121116eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1211173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1211176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121117b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121117e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1211186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121118be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121118ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121119340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1211197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121119c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12111a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12111a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12111aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12111af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12111b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12111b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12111bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12111c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12111c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12111d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12111d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12111dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12111e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12111e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12111ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12111f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12111fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121120130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1211205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121120890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121120ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121121690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121121950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121121df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121122290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121122730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121122bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121123070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121123510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1211239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121123e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1211242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121124790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121124c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1211250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121125620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121125b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1211260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121126610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121126b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1211270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121127600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121127b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1211280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1211285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121128b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121129090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1211295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121129b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12112a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12112a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12112ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12112b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12112b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12112bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12112c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12112c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12112cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12112d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12111cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12112d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12112dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12112e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12112e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12112ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12112f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12112f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12112fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1211301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1211306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121130c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121131190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1211316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121131c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121132180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121132620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121132ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121132f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121133400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1211338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121133d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1211341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121134680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121134b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121134fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121135460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121135900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121135da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121136240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1211366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121136b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121137020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1211374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121137960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121137e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1211382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121138740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121138be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121139080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121139520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1211399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121139e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12113a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12113a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12113ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12113b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12113b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12113ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12113bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12113c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12113c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12113cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12113d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12113d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12113da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12113df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12113e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12113e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12113ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12113f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12113f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12113fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12113ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121140420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1211408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121140d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121141200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1211416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121141b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121141fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121142480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121142920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121142dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121143260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121143700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121143ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121144040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1211444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121144980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121144e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1211452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121145760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121145c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1211460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121146540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1211469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121146e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121147320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1211477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121147c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121148100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1211485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121148a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121148ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121149380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1211498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121149e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12114a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12114a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12114ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12114b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12114b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12114bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12114c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12114ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12114cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12114d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12114d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12114e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12114e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12114ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12114eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12114f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12114fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121150140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121150690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121150be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121151130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121151680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121151bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121152120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121152670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121152bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121153110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121153660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121153bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121154100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121154650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121154ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1211550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121155640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121155b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1211560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121156630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121156b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1211570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121157620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121157b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1211580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121158610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121158b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1211590b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121159600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121159b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12115a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12115a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12115ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12115b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12115b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12115bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12115c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12115c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12115cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12115d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12115d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12115db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12115e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12115e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12115eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12115f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12115f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12115faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121160040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121160590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121160ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121161030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121161580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121161ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121162020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1211624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121162960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121162e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1211632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121163740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121163be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121164080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121164520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1211649c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121164e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121165300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1211657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121165c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1211660e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121166580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121166ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1211671f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121167910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121168030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121168750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121168a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121169200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1211694c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121169ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.140.610 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12110fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1211101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121110620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121110a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121110f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121111370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1211117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121111c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1211120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121112530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1211129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121112e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121113700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121113e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121114660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121114d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121115440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121115b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121116220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121116ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121117290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121117980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121118070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121118760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121118e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1211192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121119730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121119ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12111a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12111a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12111a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12111ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12111b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12111b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12111b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12111bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12111c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12111c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12111cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12111cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12111d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12111d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12111dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12111e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12111e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12111e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12111ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12111f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12111f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12111fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121120000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121120470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1211208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121120d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1211211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121121630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121121aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121121f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121122380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1211227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121122c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1211230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121123540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1211239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121123e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121124290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121124700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121124b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121124fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121125450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1211258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121125d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1211261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121126610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121126a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121126ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121127360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1211277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121127c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1211280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121128520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121128990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121128e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121129270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1211296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121129b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121129fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12112a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12112a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12112ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12112b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12112b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12112ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12112bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12112c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12112c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12112cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12112d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12112d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12112d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12112dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12112e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12112e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12112eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12112efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12112f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12112f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12112fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121130160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1211305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121130a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121130eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121131320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121131790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121131c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121132070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1211324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121132950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121132dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121133230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1211336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121133b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121133f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1211343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121134860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121134cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121135140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1211355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121135a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121135e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121136300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121136770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121136be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121137050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1211374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121137930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121137da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121138210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121138680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121138af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121138f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1211393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121139840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121139cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12113a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12113a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12113aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12113ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12113b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12113b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12113bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12113c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12113c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12113c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12113cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12113d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12113d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12113dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12113df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12113e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12113e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12113ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12113f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12113f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12113f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12113fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1211402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121140730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121140ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121141010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121141480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1211418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121141d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1211421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121142640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121142ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121142f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121143390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121143800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121143c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1211440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121144550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1211449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121144e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1211452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121145710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121145b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121145ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121146460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1211468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121146d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1211471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121147620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121147a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121147f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121148370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1211487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121148c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1211490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121149530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1211499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121149e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12114a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12114a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12114ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12114afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12114b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12114b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12114bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12114c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12114c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12114cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12114d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12114d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12114dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12114df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12114e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12114e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12114ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12114f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12114f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12114f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12114fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1211502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121150730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121150ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121151010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121151480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1211518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121151d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1211521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121152640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121152ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121152f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121153390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121153800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121153c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1211540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121154550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1211549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121154e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1211552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121155710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121155b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121155ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121156460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1211568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121156d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1211571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121157620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121157a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121157f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121158370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1211587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121158c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1211590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121159530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1211599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121159e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12115a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12115a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12115ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12115afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12115b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12115b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12115bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12115c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12115c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12115ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12115cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12115d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12115d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12115dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12115e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12115e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12115e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12115edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12115f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12115f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12115fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12115ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121160420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121160890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121160f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121161670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121161d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121162450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1211628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121162d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1211631a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121163610 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12110fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1211101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121110620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121110a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121110f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121111370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1211117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121111c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1211120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121112530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1211129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121112e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121113700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121113e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121114660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121114d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121115440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121115b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121116220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121116ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121117290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121117980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121118070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121118760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121118e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1211192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121119730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121119ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12111a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12111a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12111a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12111ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12111b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12111b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12111b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12111bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12111c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12111c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12111cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12111cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12111d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12111d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12111dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12111e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12111e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12111e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12111ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12111f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12111f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12111fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121120000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121120470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1211208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121120d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1211211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121121630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121121aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121121f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121122380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1211227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121122c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1211230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121123540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1211239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121123e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121124290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121124700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121124b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121124fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121125450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121204d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121205170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1212055e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121205a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121205ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121206330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1212067a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121206c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121207080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1212074f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121207960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121207dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121208240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1212086b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121208b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121208f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121209400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121209870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121209ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12120a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1211258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121125d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1211261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121126610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121126a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121126ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121127360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1211277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121127c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1211280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121128520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121128990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121128e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121129270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1211296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121129b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121129fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12112a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12112a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12112ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12112b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12112b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12112ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12112bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12112c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12112c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12112cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12112d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12112d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12112d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12112dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12112e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12112e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12112eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12112efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12112f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12112f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12112fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121130160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1211305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121130a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121130eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121131320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121131790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121131c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121132070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1211324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121132950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121132dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121133230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1211336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121133b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121133f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1211343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121134860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121134cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121135140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1211355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121135a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121135e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121136300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121136770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121136be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121137050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1211374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121137930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121137da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121138210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121138680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121138af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121138f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1211393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121139840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121139cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12113a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12113a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12113aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12113ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12113b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12113b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12113bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12113c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12113c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12113c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12113cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10fd04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10fd044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10fd04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12120a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12120ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12120afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12120b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12120b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12120bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12120c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12120c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12120ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12120cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12120d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12120d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12120dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12120e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12120e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12120e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12120ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12120f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12120f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12120fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12120ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121210430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1212108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121210d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121211240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121211750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121211bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121212030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1212124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121212910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121212e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121213340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121213eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121214170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121214730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121214cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1212152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121215870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121215e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1212163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1212169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121216f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121217530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121217af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1212180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121218670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121218c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1212191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1212197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121219d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12121a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12121a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12121aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12121b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12121ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12121bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12121c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12121cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12121d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12121d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12121dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12121e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12121e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12121edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12121f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12121f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12121ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1212204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121220ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121221070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121221630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121221bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1212221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121222770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121222d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1212232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1212238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121223e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121224430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1212249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121224fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121225570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121225b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1212260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1212266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121226c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121227230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1212277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121227db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121228370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121228870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121228d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121229270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121229770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121229c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12122a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12122a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12122ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12122b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12122b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12122ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12122bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12122c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12122c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12122ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12122d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12122dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12122e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12122ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12122f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12122f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12122fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121230160 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.831s
user	0m0.293s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4324 (c27ac678)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139f0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139f0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139f0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139f0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139f0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139f104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139f10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139f11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139f115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139f11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139f11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139f124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139f12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139f137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139f13fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139f14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139f15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139f15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139f16400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139f17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139f17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139f18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139f18920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139f18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139f191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139f19e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139f1a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139f1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139f1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139f1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139f1b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139f1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139f1be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139f1c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139f1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139f1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139f1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139f1d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139f1deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139f1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139f1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139f1eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139f1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139f1f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139f1fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139f20600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139f20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139f21220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139f21e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139f22450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139f22c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139f230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139f23580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139f23840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139f23e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139f24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139f24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139f24da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139f25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139f256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139f26020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139f264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139f26960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139f26e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139f272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139f27740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139f27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139f28080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139f285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139f28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139f29070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139f295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139f29b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139f2a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139f2a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139f2ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139f2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139f2b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139f2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139f2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139f2c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139f2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139f2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139f2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139f2dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139f2e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139f2e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139f2eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139f2f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139f2f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139f2fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139f30000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139f1fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139f30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139f30c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139f316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139f31c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139f32160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139f326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139f32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139f33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139f336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139f33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139f34140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139f34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139f34be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139f35130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139f355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139f35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139f35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139f363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139f36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139f36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139f37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139f37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139f37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139f37f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139f38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139f388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139f38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139f391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139f39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139f39b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139f39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139f3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139f3a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139f3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139f3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139f3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139f3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139f3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139f3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139f3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139f3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139f3d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139f3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139f3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139f3e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139f3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139f3e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139f3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139f3f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139f3f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139f3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139f400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139f40590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139f40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139f40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139f41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139f41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139f41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139f42150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139f425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139f42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139f42f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139f433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139f43870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139f43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139f441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139f44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139f44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139f44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139f45430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139f458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139f45d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139f46210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139f466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139f46b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139f46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139f47490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139f47930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139f47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139f48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139f48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139f48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139f49050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139f494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139f49990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139f49e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139f4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139f4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139f4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139f4b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139f4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139f4b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139f4be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139f4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139f4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139f4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139f4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139f4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139f4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139f4e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139f4e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139f4ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139f4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139f4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139f4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139f508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139f510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139f51560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139f51a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139f51ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139f52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139f52ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139f530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139f53640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139f53b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139f540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139f54630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139f54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139f550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139f55620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139f55b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139f560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139f56610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139f56b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139f570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139f57600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139f57b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139f580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139f585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139f58b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139f59090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139f595e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139f59b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139f5a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139f5a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139f5ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139f5b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139f5b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139f5bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139f5c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139f5c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139f5cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139f5d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139f5d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139f5daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139f5e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139f5e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139f5eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139f5f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139f5f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139f5fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139f60020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139f60570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139f60ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139f61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139f61560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139f61ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139f62000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139f62550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139f62aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139f62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139f63540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139f63a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139f63fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139f64530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139f64a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139f64fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139f65470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139f65910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139f65db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139f66250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139f666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139f66b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139f67030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139f674d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139f67970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139f67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139f682b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139f68750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139f68bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139f69090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139f69530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139f69a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139f6a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139f6a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139f6afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139f6b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139f6b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139f6c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139f6c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139f6ca80 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b007700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b0079c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b007e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b0082a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b008710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b008b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b008ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b009460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b0098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b009d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b00a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b00a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b00b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b00bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b00c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b00cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b00d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b00d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b00e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b00e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b00ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b00f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b00fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b0103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b010b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b010dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b011080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b0114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b011960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b011dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b012240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b012770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b012be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b012ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b013310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b013780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b013bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b014060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b0144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b014940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b014db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b015220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b015690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b015b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b015f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b0163e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b016850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b016cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b017130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b0175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b017a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b017e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b0182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b018760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b018bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b019040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b0195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b019ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b019f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b01a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b01a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b01ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b01b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b01b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b01b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b01be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b01c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b01c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b01cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b01cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b01d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b01d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b01dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b01e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b01e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b01ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b01ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b01f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b01f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b01fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b0200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b020530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b0209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b020e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b021280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b0216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b021b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b021fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b022440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b0228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b022d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b023190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b023600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b023a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b023ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b024350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b0247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b024c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b0250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b025510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b025980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b025df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b026260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b0266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b026b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b026fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b027420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b027890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b027d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b028170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b0285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b028a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b028ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b029330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b0297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b029c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b02a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b02a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b02a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b02add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b02b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b02b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b02bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b02bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b02c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b02c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b02cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b02d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b02d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b02da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b02dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b02e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b02e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b02ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b02f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b02f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b02f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b02fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b030220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b030690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b030b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b030f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b0313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b031850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b031cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b032130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b0325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b032a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b032e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b0332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b033760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b033bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b034040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b0344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b034920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b034d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b035200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b035670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b035ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b035f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b0363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b036830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b036ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b037110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b037580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b0379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b037e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b0382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b038740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b038bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b039020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b039490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b039900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b039d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b03a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b03a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b03aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b03af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b03b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b03b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b03bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b03c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b03c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b03c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b03ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b03d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b03d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b03db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b03e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b03e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b03e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b03ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b03f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b03f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b03faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b03ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b040380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b0407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b040c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b0410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b041540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b0419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b041e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b042290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b042700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b042b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b042fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b043570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b0439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b043e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b0449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b044c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b044f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b045390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b045800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b045c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b0460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b046550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b0469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b046e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b0472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b047710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b047b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b047ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b048460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b0488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b048d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b0491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b049620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b049a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b049f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b04a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b04a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b04ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b04b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b04b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b04b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b04be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b04c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b04c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b04cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b04cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b04d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b04d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b04dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b04e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b04e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b04ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b04eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b04f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b04f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b04fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b0500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b050510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b050980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b050df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b051260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b0516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b051b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b051fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b052420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b052890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b052d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b053170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b0535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b053a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b053ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b054330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b0547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b054c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b055080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b0554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b055960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b055dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b056240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b0566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b056b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b057400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b057870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b057ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b058150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b0585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b059030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b059750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b059e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b05a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b05a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b05acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b05b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b05b8d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139e04870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139e04ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139e05150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139e055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139e05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139e05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139e06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139e06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139e06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139e07130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139e075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139e07c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139e08740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139e08ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139e09700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139e09e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139e0a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139e0ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139e0b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139e0bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139e0c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139e0c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139e0d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139e0d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139e0def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139e0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139e0e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139e0e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139e0ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139e0f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139e0f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139e0fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139e0ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139e10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139e10700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139e10b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139e10fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139e11450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139e118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139e11d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139e121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139e12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139e12a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139e12ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139e13360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139e137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139e13c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139e140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139e14520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139e14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139e14e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139e15270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139e156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139e15b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139e15fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139e16430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139e169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139e16ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139e17310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139e17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139e17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139e18060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139e184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139e18940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139e18db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139e19220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139e19690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139e19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139e19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139e1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139e1a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139e1acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139e1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139e1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139e1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139e1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139e1c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139e1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139e1cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139e1d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139e1d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139e1d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139e1dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139e1e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139e1e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139e1eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139e1ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139e1f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139e1f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139e1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139e20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139e20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139e209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139e20e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139e212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139e21740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139e21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139e22020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139e22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139e22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139e22d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139e231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139e23650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139e23ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139e23f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139e243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139e24810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139e24c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139e250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139e25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139e259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139e25e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139e262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139e26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139e26b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139e27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139e27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139e278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139e27d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139e281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139e28630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139e28aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139e28f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139e29380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139e297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139e29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139e2a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139e2a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139e2a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139e2ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139e2b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139e2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139e2bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139e2bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139e2c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139e2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139e2cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139e2d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139e2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139e2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139e2def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139e2e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139e2e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139e2ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139e2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139e2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139e2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139e2fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139e30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139e306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139e30b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139e30fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139e31430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139e318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139e31d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139e32180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139e325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139e32a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139e32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139e33340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139e337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139e33c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139e34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139e34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139e34970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139e34de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139e35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139e356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139e35b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139e35fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139e36410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139e36880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139e36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139e37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139e375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139e37a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139e37eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139e38320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139e38790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139e38c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139e39070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139e394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139e39950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139e39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139e3a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139e3a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139e3ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139e3af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139e3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139e3b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139e3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139e3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139e3c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139e3ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139e3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139e3d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139e3d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139e3dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139e3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139e3e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139e3e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139e3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139e3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139e3f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139e3faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139e3ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139e403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139e40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139e40dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139e41240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139e41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139e42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139e42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139e42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139e42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139e43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139e43940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139e43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139e44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139e44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139e44b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139e44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139e453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139e45850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139e45cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139e46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139e465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139e46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139e46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139e472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139e47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139e47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139e48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139e484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139e48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139e48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139e49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139e49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139e49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139e49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139e4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139e4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139e4b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139e4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139e4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139e4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139e4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139e4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139e4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139e4d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139e4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139e4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139e4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139e4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139e4e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139e4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139e4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139e4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139e4fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139e4ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139e50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139e508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139e50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139e51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139e51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139e51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139e51ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139e52350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139e527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139e52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139e530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139e53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139e53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139e53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139e54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139e546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139e54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139e54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139e55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139e55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139e55d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139e56770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139e56e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139e575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139e57cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139e57f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139e58400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139e58a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139e59010 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.936s
user	0m0.244s
sys	0m0.145s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
