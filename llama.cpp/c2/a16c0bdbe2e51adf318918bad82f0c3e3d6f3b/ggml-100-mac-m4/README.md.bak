### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.60 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.36 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.19 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  178.82 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    1.07 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.70 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.21 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 221.49 sec*proc (27 tests)

Total Test time (real) = 221.50 sec

real	3m41.519s
user	7m35.926s
sys	0m6.103s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.43 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.20 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.90 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.29 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.30 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.34 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.37 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.05 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.12 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.43 sec*proc (27 tests)

Total Test time (real) =  51.44 sec

real	0m51.451s
user	1m11.845s
sys	0m5.524s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.136 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.399 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.708 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.715 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.718 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.719 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.720 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.721 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.722 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.723 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.723 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.724 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.724 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.728 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.729 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.729 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.730 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.730 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.731 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.732 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.931 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.934 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.934 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.935 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.935 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.030.936 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.936 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.030.937 I llama_model_loader: - type  f32:  124 tensors
0.00.030.937 I llama_model_loader: - type  f16:   73 tensors
0.00.035.460 I llm_load_vocab: special tokens cache size = 5
0.00.037.733 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.037.738 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.037.738 I llm_load_print_meta: arch             = bert
0.00.037.739 I llm_load_print_meta: vocab type       = WPM
0.00.037.739 I llm_load_print_meta: n_vocab          = 30522
0.00.037.739 I llm_load_print_meta: n_merges         = 0
0.00.037.739 I llm_load_print_meta: vocab_only       = 0
0.00.037.740 I llm_load_print_meta: n_ctx_train      = 512
0.00.037.740 I llm_load_print_meta: n_embd           = 384
0.00.037.740 I llm_load_print_meta: n_layer          = 12
0.00.037.744 I llm_load_print_meta: n_head           = 12
0.00.037.745 I llm_load_print_meta: n_head_kv        = 12
0.00.037.769 I llm_load_print_meta: n_rot            = 32
0.00.037.770 I llm_load_print_meta: n_swa            = 0
0.00.037.770 I llm_load_print_meta: n_embd_head_k    = 32
0.00.037.771 I llm_load_print_meta: n_embd_head_v    = 32
0.00.037.772 I llm_load_print_meta: n_gqa            = 1
0.00.037.773 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.037.773 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.037.774 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.037.775 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.037.775 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.037.775 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.037.775 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.037.776 I llm_load_print_meta: n_ff             = 1536
0.00.037.776 I llm_load_print_meta: n_expert         = 0
0.00.037.777 I llm_load_print_meta: n_expert_used    = 0
0.00.037.777 I llm_load_print_meta: causal attn      = 0
0.00.037.777 I llm_load_print_meta: pooling type     = 2
0.00.037.777 I llm_load_print_meta: rope type        = 2
0.00.037.778 I llm_load_print_meta: rope scaling     = linear
0.00.037.778 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.037.779 I llm_load_print_meta: freq_scale_train = 1
0.00.037.780 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.037.780 I llm_load_print_meta: rope_finetuned   = unknown
0.00.037.780 I llm_load_print_meta: ssm_d_conv       = 0
0.00.037.780 I llm_load_print_meta: ssm_d_inner      = 0
0.00.037.781 I llm_load_print_meta: ssm_d_state      = 0
0.00.037.781 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.037.781 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.037.793 I llm_load_print_meta: model type       = 33M
0.00.037.793 I llm_load_print_meta: model ftype      = F16
0.00.037.794 I llm_load_print_meta: model params     = 33.21 M
0.00.037.794 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.037.795 I llm_load_print_meta: general.name     = Bge Small
0.00.037.795 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.037.795 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.037.796 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.037.796 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.037.796 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.037.797 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.037.797 I llm_load_print_meta: max token length = 21
0.00.039.862 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.862 I llm_load_tensors: offloading output layer to GPU
0.00.039.865 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.893 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.895 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.522 I llama_new_context_with_model: n_seq_max     = 1
0.00.040.524 I llama_new_context_with_model: n_ctx         = 512
0.00.040.524 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.040.524 I llama_new_context_with_model: n_batch       = 2048
0.00.040.525 I llama_new_context_with_model: n_ubatch      = 2048
0.00.040.525 I llama_new_context_with_model: flash_attn    = 0
0.00.040.526 I llama_new_context_with_model: freq_base     = 10000.0
0.00.040.526 I llama_new_context_with_model: freq_scale    = 1
0.00.040.527 I ggml_metal_init: allocating
0.00.040.531 I ggml_metal_init: found device: Apple M4
0.00.040.534 I ggml_metal_init: picking default device: Apple M4
0.00.041.451 I ggml_metal_init: using embedded metal library
0.00.045.879 I ggml_metal_init: GPU name:   Apple M4
0.00.045.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.883 I ggml_metal_init: simdgroup reduction   = true
0.00.045.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.883 I ggml_metal_init: has bfloat            = true
0.00.045.883 I ggml_metal_init: use bfloat            = true
0.00.045.884 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.884 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.932 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.934 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.935 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.059.731 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.059.732 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.059.732 I llama_new_context_with_model: graph nodes  = 429
0.00.059.733 I llama_new_context_with_model: graph splits = 2
0.00.059.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.070.546 I 
0.00.070.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.071.268 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.089 I llama_perf_context_print:        load time =      50.14 ms
0.00.076.090 I llama_perf_context_print: prompt eval time =       4.66 ms /     9 tokens (    0.52 ms per token,  1929.67 tokens per second)
0.00.076.090 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.091 I llama_perf_context_print:       total time =       5.54 ms /    10 tokens
0.00.076.236 I ggml_metal_free: deallocating

real	0m0.256s
user	0m0.063s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.582 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.678 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.682 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.683 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.683 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.684 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.684 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.685 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.685 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.685 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.685 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.687 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.688 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.688 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.688 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.689 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.689 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.690 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.159 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.160 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.161 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.161 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.161 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.162 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.162 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.162 I llama_model_loader: - type  f32:  124 tensors
0.00.015.163 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.567 I llm_load_vocab: special tokens cache size = 5
0.00.018.866 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.876 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.877 I llm_load_print_meta: arch             = bert
0.00.018.877 I llm_load_print_meta: vocab type       = WPM
0.00.018.877 I llm_load_print_meta: n_vocab          = 30522
0.00.018.877 I llm_load_print_meta: n_merges         = 0
0.00.018.878 I llm_load_print_meta: vocab_only       = 0
0.00.018.878 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.878 I llm_load_print_meta: n_embd           = 384
0.00.018.878 I llm_load_print_meta: n_layer          = 12
0.00.018.882 I llm_load_print_meta: n_head           = 12
0.00.018.882 I llm_load_print_meta: n_head_kv        = 12
0.00.018.890 I llm_load_print_meta: n_rot            = 32
0.00.018.890 I llm_load_print_meta: n_swa            = 0
0.00.018.890 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.893 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.893 I llm_load_print_meta: n_gqa            = 1
0.00.018.894 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.894 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.896 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.896 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.896 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.896 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.896 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.897 I llm_load_print_meta: n_ff             = 1536
0.00.018.897 I llm_load_print_meta: n_expert         = 0
0.00.018.897 I llm_load_print_meta: n_expert_used    = 0
0.00.018.898 I llm_load_print_meta: causal attn      = 0
0.00.018.898 I llm_load_print_meta: pooling type     = 2
0.00.018.898 I llm_load_print_meta: rope type        = 2
0.00.018.898 I llm_load_print_meta: rope scaling     = linear
0.00.018.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.899 I llm_load_print_meta: freq_scale_train = 1
0.00.018.899 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.899 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.900 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.905 I llm_load_print_meta: model type       = 33M
0.00.018.905 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.906 I llm_load_print_meta: model params     = 33.21 M
0.00.018.906 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.906 I llm_load_print_meta: general.name     = Bge Small
0.00.018.907 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.907 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.907 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.907 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.907 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.907 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.908 I llm_load_print_meta: max token length = 21
0.00.020.285 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.286 I llm_load_tensors: offloading output layer to GPU
0.00.020.287 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.294 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.295 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.670 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.671 I llama_new_context_with_model: n_ctx         = 512
0.00.020.671 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.672 I llama_new_context_with_model: n_batch       = 2048
0.00.020.672 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.672 I llama_new_context_with_model: flash_attn    = 0
0.00.020.672 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.673 I llama_new_context_with_model: freq_scale    = 1
0.00.020.673 I ggml_metal_init: allocating
0.00.020.676 I ggml_metal_init: found device: Apple M4
0.00.020.678 I ggml_metal_init: picking default device: Apple M4
0.00.021.307 I ggml_metal_init: using embedded metal library
0.00.023.851 I ggml_metal_init: GPU name:   Apple M4
0.00.023.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.853 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.853 I ggml_metal_init: simdgroup reduction   = true
0.00.023.853 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.854 I ggml_metal_init: has bfloat            = true
0.00.023.854 I ggml_metal_init: use bfloat            = true
0.00.023.854 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.674 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.678 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.679 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.299 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.300 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.300 I llama_new_context_with_model: graph nodes  = 429
0.00.035.300 I llama_new_context_with_model: graph splits = 2
0.00.035.314 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.615 I 
0.00.040.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.188 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.681 I llama_perf_context_print:        load time =      31.03 ms
0.00.045.682 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2068.97 tokens per second)
0.00.045.683 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.683 I llama_perf_context_print:       total time =       5.07 ms /    10 tokens
0.00.045.873 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.030s
sys	0m0.018s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.116 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.156 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.927 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.935 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.936 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.941 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.942 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.943 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.943 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.944 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.945 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.945 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.949 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.949 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.950 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.951 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.616 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.616 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.616 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.617 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.617 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.617 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.618 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.618 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.619 I llama_model_loader: - type  f32:   41 tensors
0.00.049.619 I llama_model_loader: - type  f16:   29 tensors
0.00.067.490 W llm_load_vocab: empty token at index 5
0.00.072.049 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.073.308 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.333 I llm_load_vocab: special tokens cache size = 5
0.00.333.027 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.333.033 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.333.034 I llm_load_print_meta: arch             = jina-bert-v2
0.00.333.035 I llm_load_print_meta: vocab type       = BPE
0.00.333.035 I llm_load_print_meta: n_vocab          = 61056
0.00.333.036 I llm_load_print_meta: n_merges         = 39382
0.00.333.036 I llm_load_print_meta: vocab_only       = 0
0.00.333.038 I llm_load_print_meta: n_ctx_train      = 8192
0.00.333.038 I llm_load_print_meta: n_embd           = 384
0.00.333.038 I llm_load_print_meta: n_layer          = 4
0.00.333.044 I llm_load_print_meta: n_head           = 12
0.00.333.045 I llm_load_print_meta: n_head_kv        = 12
0.00.333.067 I llm_load_print_meta: n_rot            = 32
0.00.333.068 I llm_load_print_meta: n_swa            = 0
0.00.333.068 I llm_load_print_meta: n_embd_head_k    = 32
0.00.333.068 I llm_load_print_meta: n_embd_head_v    = 32
0.00.333.069 I llm_load_print_meta: n_gqa            = 1
0.00.333.070 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.333.070 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.333.071 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.333.071 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.333.072 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.333.072 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.333.072 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.333.078 I llm_load_print_meta: n_ff             = 1536
0.00.333.078 I llm_load_print_meta: n_expert         = 0
0.00.333.078 I llm_load_print_meta: n_expert_used    = 0
0.00.333.078 I llm_load_print_meta: causal attn      = 0
0.00.333.078 I llm_load_print_meta: pooling type     = -1
0.00.333.078 I llm_load_print_meta: rope type        = -1
0.00.333.079 I llm_load_print_meta: rope scaling     = linear
0.00.333.079 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.333.080 I llm_load_print_meta: freq_scale_train = 1
0.00.333.080 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.333.080 I llm_load_print_meta: rope_finetuned   = unknown
0.00.333.080 I llm_load_print_meta: ssm_d_conv       = 0
0.00.333.080 I llm_load_print_meta: ssm_d_inner      = 0
0.00.333.080 I llm_load_print_meta: ssm_d_state      = 0
0.00.333.080 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.333.081 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.333.101 I llm_load_print_meta: model type       = 33M
0.00.333.102 I llm_load_print_meta: model ftype      = F16
0.00.333.102 I llm_load_print_meta: model params     = 32.90 M
0.00.333.103 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.333.103 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.333.103 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.333.103 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.333.104 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.333.104 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.333.105 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.333.106 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.333.106 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.333.106 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.333.106 I llm_load_print_meta: max token length = 45
0.00.334.127 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.334.127 I llm_load_tensors: offloading output layer to GPU
0.00.334.128 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.334.151 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.334.152 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.335.002 I llama_new_context_with_model: n_seq_max     = 1
0.00.335.003 I llama_new_context_with_model: n_ctx         = 8192
0.00.335.003 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.335.003 I llama_new_context_with_model: n_batch       = 2048
0.00.335.003 I llama_new_context_with_model: n_ubatch      = 2048
0.00.335.004 I llama_new_context_with_model: flash_attn    = 0
0.00.335.004 I llama_new_context_with_model: freq_base     = 10000.0
0.00.335.004 I llama_new_context_with_model: freq_scale    = 1
0.00.335.005 I ggml_metal_init: allocating
0.00.335.012 I ggml_metal_init: found device: Apple M4
0.00.335.014 I ggml_metal_init: picking default device: Apple M4
0.00.336.069 I ggml_metal_init: using embedded metal library
0.00.338.737 I ggml_metal_init: GPU name:   Apple M4
0.00.338.739 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.338.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.338.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.338.740 I ggml_metal_init: simdgroup reduction   = true
0.00.338.740 I ggml_metal_init: simdgroup matrix mul. = true
0.00.338.740 I ggml_metal_init: has bfloat            = true
0.00.338.740 I ggml_metal_init: use bfloat            = true
0.00.338.741 I ggml_metal_init: hasUnifiedMemory      = true
0.00.338.741 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.632 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.350.635 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.350.636 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.351.181 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.351.182 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.351.182 I llama_new_context_with_model: graph nodes  = 154
0.00.351.183 I llama_new_context_with_model: graph splits = 2
0.00.351.200 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.364.258 I 
0.00.364.293 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.364.447 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.364.448 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.364.450 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.364.450 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.364.454 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.364.454 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.364.984 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.368.559 I llama_perf_context_print:        load time =     340.09 ms
0.00.368.560 I llama_perf_context_print: prompt eval time =       3.57 ms /    62 tokens (    0.06 ms per token, 17381.55 tokens per second)
0.00.368.561 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.368.562 I llama_perf_context_print:       total time =       4.30 ms /    63 tokens
0.00.368.767 I ggml_metal_free: deallocating

real	0m1.048s
user	0m0.340s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.107 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.220 I main: llama backend init
0.00.000.226 I main: load the model and apply lora adapter, if any
0.00.030.008 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.912 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.927 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.928 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.929 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.930 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.930 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.932 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.933 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.933 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.934 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.934 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.935 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.942 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.746 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.060.749 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.750 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.752 I llama_model_loader: - type  f32:  194 tensors
0.00.060.752 I llama_model_loader: - type  f16:   98 tensors
0.00.091.357 I llm_load_vocab: special tokens cache size = 25
0.00.098.250 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.253 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.254 I llm_load_print_meta: arch             = gptneox
0.00.098.254 I llm_load_print_meta: vocab type       = BPE
0.00.098.254 I llm_load_print_meta: n_vocab          = 50304
0.00.098.254 I llm_load_print_meta: n_merges         = 50009
0.00.098.254 I llm_load_print_meta: vocab_only       = 0
0.00.098.254 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.255 I llm_load_print_meta: n_embd           = 2048
0.00.098.255 I llm_load_print_meta: n_layer          = 24
0.00.098.259 I llm_load_print_meta: n_head           = 16
0.00.098.260 I llm_load_print_meta: n_head_kv        = 16
0.00.098.279 I llm_load_print_meta: n_rot            = 32
0.00.098.280 I llm_load_print_meta: n_swa            = 0
0.00.098.280 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.280 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.281 I llm_load_print_meta: n_gqa            = 1
0.00.098.282 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.282 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.283 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.283 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.283 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.283 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.283 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.284 I llm_load_print_meta: n_ff             = 8192
0.00.098.288 I llm_load_print_meta: n_expert         = 0
0.00.098.288 I llm_load_print_meta: n_expert_used    = 0
0.00.098.288 I llm_load_print_meta: causal attn      = 1
0.00.098.289 I llm_load_print_meta: pooling type     = 0
0.00.098.289 I llm_load_print_meta: rope type        = 2
0.00.098.290 I llm_load_print_meta: rope scaling     = linear
0.00.098.291 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.291 I llm_load_print_meta: freq_scale_train = 1
0.00.098.291 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.291 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.291 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.292 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.292 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.292 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.292 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.302 I llm_load_print_meta: model type       = 1.4B
0.00.098.302 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.302 I llm_load_print_meta: model params     = 1.41 B
0.00.098.304 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.304 I llm_load_print_meta: general.name     = 1.4B
0.00.098.304 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.304 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.304 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.304 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.311 I llm_load_print_meta: LF token         = 128 ''
0.00.098.312 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.312 I llm_load_print_meta: max token length = 1024
0.00.100.920 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.920 I llm_load_tensors: offloading output layer to GPU
0.00.100.920 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.939 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.940 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.882 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.883 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.883 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.883 I llama_new_context_with_model: n_batch       = 2048
0.00.101.883 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.883 I llama_new_context_with_model: flash_attn    = 0
0.00.101.884 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.884 I llama_new_context_with_model: freq_scale    = 1
0.00.101.884 I ggml_metal_init: allocating
0.00.101.891 I ggml_metal_init: found device: Apple M4
0.00.101.893 I ggml_metal_init: picking default device: Apple M4
0.00.102.548 I ggml_metal_init: using embedded metal library
0.00.111.732 I ggml_metal_init: GPU name:   Apple M4
0.00.111.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.111.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.111.734 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.111.734 I ggml_metal_init: simdgroup reduction   = true
0.00.111.734 I ggml_metal_init: simdgroup matrix mul. = true
0.00.111.734 I ggml_metal_init: has bfloat            = true
0.00.111.735 I ggml_metal_init: use bfloat            = true
0.00.111.735 I ggml_metal_init: hasUnifiedMemory      = true
0.00.111.735 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.154.519 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.154.525 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.154.546 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.155.432 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.155.434 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.155.434 I llama_new_context_with_model: graph nodes  = 967
0.00.155.435 I llama_new_context_with_model: graph splits = 2
0.00.155.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.231.371 I main: llama threadpool init, n_threads = 4
0.00.231.405 I 
0.00.231.442 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.231.444 I 
0.00.231.524 I sampler seed: 1234
0.00.231.528 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.231.552 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.231.554 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.231.554 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.082.614 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.02.082.615 I llama_perf_context_print:        load time =     201.35 ms
0.02.082.616 I llama_perf_context_print: prompt eval time =      43.84 ms /     7 tokens (    6.26 ms per token,   159.66 tokens per second)
0.02.082.616 I llama_perf_context_print:        eval time =    1804.24 ms /    63 runs   (   28.64 ms per token,    34.92 tokens per second)
0.02.082.617 I llama_perf_context_print:       total time =    1851.25 ms /    70 tokens
0.02.082.809 I ggml_metal_free: deallocating

real	0m2.388s
user	0m0.143s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.601 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.725 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.489 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.514 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.517 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.518 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.524 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.313 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.317 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.317 I llama_model_loader: - type  f32:  194 tensors
0.00.056.318 I llama_model_loader: - type  f16:   98 tensors
0.00.085.242 I llm_load_vocab: special tokens cache size = 25
0.00.091.985 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.988 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.988 I llm_load_print_meta: arch             = gptneox
0.00.091.988 I llm_load_print_meta: vocab type       = BPE
0.00.091.988 I llm_load_print_meta: n_vocab          = 50304
0.00.091.989 I llm_load_print_meta: n_merges         = 50009
0.00.091.989 I llm_load_print_meta: vocab_only       = 0
0.00.091.989 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.989 I llm_load_print_meta: n_embd           = 2048
0.00.091.989 I llm_load_print_meta: n_layer          = 24
0.00.091.992 I llm_load_print_meta: n_head           = 16
0.00.091.993 I llm_load_print_meta: n_head_kv        = 16
0.00.091.999 I llm_load_print_meta: n_rot            = 32
0.00.091.999 I llm_load_print_meta: n_swa            = 0
0.00.092.000 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.000 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.000 I llm_load_print_meta: n_gqa            = 1
0.00.092.001 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.001 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.002 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.002 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.002 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.003 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.003 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.003 I llm_load_print_meta: n_ff             = 8192
0.00.092.004 I llm_load_print_meta: n_expert         = 0
0.00.092.004 I llm_load_print_meta: n_expert_used    = 0
0.00.092.004 I llm_load_print_meta: causal attn      = 1
0.00.092.004 I llm_load_print_meta: pooling type     = 0
0.00.092.004 I llm_load_print_meta: rope type        = 2
0.00.092.004 I llm_load_print_meta: rope scaling     = linear
0.00.092.005 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.005 I llm_load_print_meta: freq_scale_train = 1
0.00.092.005 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.005 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.006 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.006 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.006 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.006 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.006 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.010 I llm_load_print_meta: model type       = 1.4B
0.00.092.011 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.011 I llm_load_print_meta: model params     = 1.41 B
0.00.092.012 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.012 I llm_load_print_meta: general.name     = 1.4B
0.00.092.012 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.012 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.012 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.013 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.013 I llm_load_print_meta: LF token         = 128 ''
0.00.092.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.013 I llm_load_print_meta: max token length = 1024
0.00.094.030 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.030 I llm_load_tensors: offloading output layer to GPU
0.00.094.031 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.036 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.037 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.023 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.024 I llama_new_context_with_model: n_ctx         = 128
0.00.095.024 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.024 I llama_new_context_with_model: n_batch       = 128
0.00.095.024 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.024 I llama_new_context_with_model: flash_attn    = 0
0.00.095.025 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.025 I llama_new_context_with_model: freq_scale    = 1
0.00.095.025 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.026 I ggml_metal_init: allocating
0.00.095.035 I ggml_metal_init: found device: Apple M4
0.00.095.037 I ggml_metal_init: picking default device: Apple M4
0.00.095.699 I ggml_metal_init: using embedded metal library
0.00.098.235 I ggml_metal_init: GPU name:   Apple M4
0.00.098.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.237 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.237 I ggml_metal_init: simdgroup reduction   = true
0.00.098.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.238 I ggml_metal_init: has bfloat            = true
0.00.098.238 I ggml_metal_init: use bfloat            = true
0.00.098.238 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.239 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.043 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.051 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.066 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.899 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.900 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.900 I llama_new_context_with_model: graph nodes  = 967
0.00.109.900 I llama_new_context_with_model: graph splits = 2
0.00.109.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.276 I 
0.00.790.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.334 I perplexity: tokenizing the input ..
0.00.802.823 I perplexity: tokenization took 12.485 ms
0.00.802.851 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.925.914 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.927.837 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.927.863 I llama_perf_context_print:        load time =     764.53 ms
0.00.927.865 I llama_perf_context_print: prompt eval time =     122.11 ms /   128 tokens (    0.95 ms per token,  1048.20 tokens per second)
0.00.927.867 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.927.868 I llama_perf_context_print:       total time =     137.59 ms /   129 tokens
0.00.928.710 I ggml_metal_free: deallocating

real	0m1.121s
user	0m0.125s
sys	0m0.182s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.663 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.319 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.323 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.326 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.326 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.327 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.327 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.327 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.328 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.329 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.329 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.330 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.331 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.331 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.331 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.333 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.202 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.119 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.120 I llama_model_loader: - type  f32:  194 tensors
0.00.025.120 I llama_model_loader: - type q8_0:   98 tensors
0.00.046.269 I llm_load_vocab: special tokens cache size = 25
0.00.052.209 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.212 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.213 I llm_load_print_meta: arch             = gptneox
0.00.052.213 I llm_load_print_meta: vocab type       = BPE
0.00.052.214 I llm_load_print_meta: n_vocab          = 50304
0.00.052.214 I llm_load_print_meta: n_merges         = 50009
0.00.052.214 I llm_load_print_meta: vocab_only       = 0
0.00.052.214 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.214 I llm_load_print_meta: n_embd           = 2048
0.00.052.214 I llm_load_print_meta: n_layer          = 24
0.00.052.219 I llm_load_print_meta: n_head           = 16
0.00.052.220 I llm_load_print_meta: n_head_kv        = 16
0.00.052.234 I llm_load_print_meta: n_rot            = 32
0.00.052.235 I llm_load_print_meta: n_swa            = 0
0.00.052.235 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.235 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.236 I llm_load_print_meta: n_gqa            = 1
0.00.052.236 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.237 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.238 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.239 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.239 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.239 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.239 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.240 I llm_load_print_meta: n_ff             = 8192
0.00.052.240 I llm_load_print_meta: n_expert         = 0
0.00.052.240 I llm_load_print_meta: n_expert_used    = 0
0.00.052.240 I llm_load_print_meta: causal attn      = 1
0.00.052.240 I llm_load_print_meta: pooling type     = 0
0.00.052.241 I llm_load_print_meta: rope type        = 2
0.00.052.241 I llm_load_print_meta: rope scaling     = linear
0.00.052.241 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.241 I llm_load_print_meta: freq_scale_train = 1
0.00.052.241 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.242 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.242 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.242 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.242 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.242 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.242 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.252 I llm_load_print_meta: model type       = 1.4B
0.00.052.253 I llm_load_print_meta: model ftype      = Q8_0
0.00.052.253 I llm_load_print_meta: model params     = 1.41 B
0.00.052.254 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.052.254 I llm_load_print_meta: general.name     = 1.4B
0.00.052.254 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.254 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.254 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.254 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.255 I llm_load_print_meta: LF token         = 128 ''
0.00.052.255 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.255 I llm_load_print_meta: max token length = 1024
0.00.054.729 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.729 I llm_load_tensors: offloading output layer to GPU
0.00.054.729 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.741 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.742 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.055.751 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.752 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.752 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.753 I llama_new_context_with_model: n_batch       = 2048
0.00.055.753 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.753 I llama_new_context_with_model: flash_attn    = 0
0.00.055.754 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.754 I llama_new_context_with_model: freq_scale    = 1
0.00.055.755 I ggml_metal_init: allocating
0.00.055.762 I ggml_metal_init: found device: Apple M4
0.00.055.765 I ggml_metal_init: picking default device: Apple M4
0.00.056.513 I ggml_metal_init: using embedded metal library
0.00.059.073 I ggml_metal_init: GPU name:   Apple M4
0.00.059.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.076 I ggml_metal_init: simdgroup reduction   = true
0.00.059.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.077 I ggml_metal_init: has bfloat            = true
0.00.059.077 I ggml_metal_init: use bfloat            = true
0.00.059.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.080 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.870 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.887 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.913 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.067 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.069 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.070 I llama_new_context_with_model: graph nodes  = 967
0.00.094.070 I llama_new_context_with_model: graph splits = 2
0.00.094.086 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.045.786 I main: llama threadpool init, n_threads = 4
0.01.045.833 I 
0.01.045.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.045.861 I 
0.01.046.030 I sampler seed: 1234
0.01.046.036 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.046.046 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.046.048 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.046.048 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.185.585 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.02.185.586 I llama_perf_context_print:        load time =    1036.12 ms
0.02.185.586 I llama_perf_context_print: prompt eval time =      39.90 ms /     7 tokens (    5.70 ms per token,   175.45 tokens per second)
0.02.185.587 I llama_perf_context_print:        eval time =    1096.60 ms /    63 runs   (   17.41 ms per token,    57.45 tokens per second)
0.02.185.589 I llama_perf_context_print:       total time =    1139.80 ms /    70 tokens
0.02.185.785 I ggml_metal_free: deallocating

real	0m2.203s
user	0m0.112s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.876 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.243 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.245 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.246 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.246 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.246 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.248 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.250 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.362 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.362 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.363 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.363 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.364 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.364 I llama_model_loader: - type  f32:  194 tensors
0.00.033.365 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.139 I llm_load_vocab: special tokens cache size = 25
0.00.066.699 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.702 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.702 I llm_load_print_meta: arch             = gptneox
0.00.066.702 I llm_load_print_meta: vocab type       = BPE
0.00.066.702 I llm_load_print_meta: n_vocab          = 50304
0.00.066.702 I llm_load_print_meta: n_merges         = 50009
0.00.066.703 I llm_load_print_meta: vocab_only       = 0
0.00.066.703 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.703 I llm_load_print_meta: n_embd           = 2048
0.00.066.703 I llm_load_print_meta: n_layer          = 24
0.00.066.706 I llm_load_print_meta: n_head           = 16
0.00.066.707 I llm_load_print_meta: n_head_kv        = 16
0.00.066.719 I llm_load_print_meta: n_rot            = 32
0.00.066.719 I llm_load_print_meta: n_swa            = 0
0.00.066.719 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.719 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.720 I llm_load_print_meta: n_gqa            = 1
0.00.066.721 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.721 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.722 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.722 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.727 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.727 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.728 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.728 I llm_load_print_meta: n_ff             = 8192
0.00.066.728 I llm_load_print_meta: n_expert         = 0
0.00.066.729 I llm_load_print_meta: n_expert_used    = 0
0.00.066.729 I llm_load_print_meta: causal attn      = 1
0.00.066.729 I llm_load_print_meta: pooling type     = 0
0.00.066.729 I llm_load_print_meta: rope type        = 2
0.00.066.730 I llm_load_print_meta: rope scaling     = linear
0.00.066.731 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.731 I llm_load_print_meta: freq_scale_train = 1
0.00.066.731 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.731 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.731 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.732 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.732 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.733 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.733 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.743 I llm_load_print_meta: model type       = 1.4B
0.00.066.744 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.744 I llm_load_print_meta: model params     = 1.41 B
0.00.066.745 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.745 I llm_load_print_meta: general.name     = 1.4B
0.00.066.745 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.745 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.745 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.746 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.746 I llm_load_print_meta: LF token         = 128 ''
0.00.066.746 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.747 I llm_load_print_meta: max token length = 1024
0.00.069.022 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.022 I llm_load_tensors: offloading output layer to GPU
0.00.069.022 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.033 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.034 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.939 I llama_new_context_with_model: n_ctx         = 128
0.00.069.939 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.939 I llama_new_context_with_model: n_batch       = 128
0.00.069.939 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.939 I llama_new_context_with_model: flash_attn    = 0
0.00.069.940 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.940 I llama_new_context_with_model: freq_scale    = 1
0.00.069.940 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.941 I ggml_metal_init: allocating
0.00.069.944 I ggml_metal_init: found device: Apple M4
0.00.069.946 I ggml_metal_init: picking default device: Apple M4
0.00.070.525 I ggml_metal_init: using embedded metal library
0.00.073.022 I ggml_metal_init: GPU name:   Apple M4
0.00.073.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.024 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.024 I ggml_metal_init: simdgroup reduction   = true
0.00.073.024 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.025 I ggml_metal_init: has bfloat            = true
0.00.073.025 I ggml_metal_init: use bfloat            = true
0.00.073.025 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.938 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.940 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.955 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.905 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.906 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.906 I llama_new_context_with_model: graph nodes  = 967
0.00.084.906 I llama_new_context_with_model: graph splits = 2
0.00.084.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.611 I 
0.00.787.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.787.672 I perplexity: tokenizing the input ..
0.00.795.689 I perplexity: tokenization took 8.016 ms
0.00.795.699 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.920.105 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.921.347 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.921.361 I llama_perf_context_print:        load time =     775.73 ms
0.00.921.362 I llama_perf_context_print: prompt eval time =     124.18 ms /   128 tokens (    0.97 ms per token,  1030.76 tokens per second)
0.00.921.365 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.921.365 I llama_perf_context_print:       total time =     133.75 ms /   129 tokens
0.00.921.787 I ggml_metal_free: deallocating

real	0m0.941s
user	0m0.095s
sys	0m0.144s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.957 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.672 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.576 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.577 I llama_model_loader: - type  f32:  194 tensors
0.00.026.577 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.578 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.634 I llm_load_vocab: special tokens cache size = 25
0.00.053.667 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.671 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.671 I llm_load_print_meta: arch             = gptneox
0.00.053.671 I llm_load_print_meta: vocab type       = BPE
0.00.053.672 I llm_load_print_meta: n_vocab          = 50304
0.00.053.672 I llm_load_print_meta: n_merges         = 50009
0.00.053.672 I llm_load_print_meta: vocab_only       = 0
0.00.053.672 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.674 I llm_load_print_meta: n_embd           = 2048
0.00.053.674 I llm_load_print_meta: n_layer          = 24
0.00.053.680 I llm_load_print_meta: n_head           = 16
0.00.053.686 I llm_load_print_meta: n_head_kv        = 16
0.00.053.699 I llm_load_print_meta: n_rot            = 32
0.00.053.700 I llm_load_print_meta: n_swa            = 0
0.00.053.700 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.700 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.701 I llm_load_print_meta: n_gqa            = 1
0.00.053.701 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.702 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.702 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.703 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.703 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.706 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.706 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.707 I llm_load_print_meta: n_ff             = 8192
0.00.053.707 I llm_load_print_meta: n_expert         = 0
0.00.053.707 I llm_load_print_meta: n_expert_used    = 0
0.00.053.707 I llm_load_print_meta: causal attn      = 1
0.00.053.708 I llm_load_print_meta: pooling type     = 0
0.00.053.708 I llm_load_print_meta: rope type        = 2
0.00.053.708 I llm_load_print_meta: rope scaling     = linear
0.00.053.709 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.709 I llm_load_print_meta: freq_scale_train = 1
0.00.053.709 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.709 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.709 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.709 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.709 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.710 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.710 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.719 I llm_load_print_meta: model type       = 1.4B
0.00.053.720 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.720 I llm_load_print_meta: model params     = 1.41 B
0.00.053.720 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.720 I llm_load_print_meta: general.name     = 1.4B
0.00.053.721 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.721 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.721 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.721 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.722 I llm_load_print_meta: LF token         = 128 ''
0.00.053.722 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.722 I llm_load_print_meta: max token length = 1024
0.00.055.582 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.582 I llm_load_tensors: offloading output layer to GPU
0.00.055.582 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.593 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.595 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.490 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.491 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.491 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.491 I llama_new_context_with_model: n_batch       = 2048
0.00.056.492 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.492 I llama_new_context_with_model: flash_attn    = 0
0.00.056.492 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.493 I llama_new_context_with_model: freq_scale    = 1
0.00.056.493 I ggml_metal_init: allocating
0.00.056.500 I ggml_metal_init: found device: Apple M4
0.00.056.502 I ggml_metal_init: picking default device: Apple M4
0.00.057.235 I ggml_metal_init: using embedded metal library
0.00.059.774 I ggml_metal_init: GPU name:   Apple M4
0.00.059.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.776 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.776 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.777 I ggml_metal_init: simdgroup reduction   = true
0.00.059.777 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.777 I ggml_metal_init: has bfloat            = true
0.00.059.777 I ggml_metal_init: use bfloat            = true
0.00.059.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.778 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.637 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.645 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.673 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.828 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.829 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.830 I llama_new_context_with_model: graph nodes  = 967
0.00.093.830 I llama_new_context_with_model: graph splits = 2
0.00.093.847 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.585 I main: llama threadpool init, n_threads = 4
0.00.673.630 I 
0.00.673.671 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.672 I 
0.00.673.872 I sampler seed: 1234
0.00.673.878 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.673.922 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.673.924 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.673.924 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.356.058 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.356.061 I llama_perf_context_print:        load time =     662.62 ms
0.01.356.061 I llama_perf_context_print: prompt eval time =      44.98 ms /     7 tokens (    6.43 ms per token,   155.64 tokens per second)
0.01.356.062 I llama_perf_context_print:        eval time =     634.13 ms /    63 runs   (   10.07 ms per token,    99.35 tokens per second)
0.01.356.063 I llama_perf_context_print:       total time =     682.48 ms /    70 tokens
0.01.356.252 I ggml_metal_free: deallocating

real	0m1.375s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.815 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.383 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.387 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.394 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.394 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.395 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.395 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.395 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.396 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.397 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.397 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.130 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.023 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.024 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.024 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.025 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.025 I llama_model_loader: - type  f32:  194 tensors
0.00.024.026 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.026 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.905 I llm_load_vocab: special tokens cache size = 25
0.00.049.839 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.842 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.842 I llm_load_print_meta: arch             = gptneox
0.00.049.843 I llm_load_print_meta: vocab type       = BPE
0.00.049.843 I llm_load_print_meta: n_vocab          = 50304
0.00.049.843 I llm_load_print_meta: n_merges         = 50009
0.00.049.843 I llm_load_print_meta: vocab_only       = 0
0.00.049.843 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.844 I llm_load_print_meta: n_embd           = 2048
0.00.049.844 I llm_load_print_meta: n_layer          = 24
0.00.049.846 I llm_load_print_meta: n_head           = 16
0.00.049.847 I llm_load_print_meta: n_head_kv        = 16
0.00.049.859 I llm_load_print_meta: n_rot            = 32
0.00.049.859 I llm_load_print_meta: n_swa            = 0
0.00.049.859 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.859 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.860 I llm_load_print_meta: n_gqa            = 1
0.00.049.861 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.861 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.862 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.862 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.862 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.863 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.863 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.863 I llm_load_print_meta: n_ff             = 8192
0.00.049.864 I llm_load_print_meta: n_expert         = 0
0.00.049.864 I llm_load_print_meta: n_expert_used    = 0
0.00.049.864 I llm_load_print_meta: causal attn      = 1
0.00.049.864 I llm_load_print_meta: pooling type     = 0
0.00.049.864 I llm_load_print_meta: rope type        = 2
0.00.049.865 I llm_load_print_meta: rope scaling     = linear
0.00.049.865 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.868 I llm_load_print_meta: freq_scale_train = 1
0.00.049.868 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.868 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.868 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.868 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.868 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.869 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.878 I llm_load_print_meta: model type       = 1.4B
0.00.049.879 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.879 I llm_load_print_meta: model params     = 1.41 B
0.00.049.879 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.880 I llm_load_print_meta: general.name     = 1.4B
0.00.049.880 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.880 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.880 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.880 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.881 I llm_load_print_meta: LF token         = 128 ''
0.00.049.881 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.881 I llm_load_print_meta: max token length = 1024
0.00.051.865 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.866 I llm_load_tensors: offloading output layer to GPU
0.00.051.866 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.877 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.878 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.768 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.769 I llama_new_context_with_model: n_ctx         = 128
0.00.052.769 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.769 I llama_new_context_with_model: n_batch       = 128
0.00.052.769 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.770 I llama_new_context_with_model: flash_attn    = 0
0.00.052.770 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.770 I llama_new_context_with_model: freq_scale    = 1
0.00.052.770 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.771 I ggml_metal_init: allocating
0.00.052.774 I ggml_metal_init: found device: Apple M4
0.00.052.776 I ggml_metal_init: picking default device: Apple M4
0.00.053.339 I ggml_metal_init: using embedded metal library
0.00.055.767 I ggml_metal_init: GPU name:   Apple M4
0.00.055.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.769 I ggml_metal_init: simdgroup reduction   = true
0.00.055.769 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.769 I ggml_metal_init: has bfloat            = true
0.00.055.769 I ggml_metal_init: use bfloat            = true
0.00.055.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.709 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.711 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.726 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.648 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.649 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.650 I llama_new_context_with_model: graph nodes  = 967
0.00.067.650 I llama_new_context_with_model: graph splits = 2
0.00.067.662 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.595.992 I 
0.00.596.030 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.039 I perplexity: tokenizing the input ..
0.00.603.683 I perplexity: tokenization took 7.642 ms
0.00.603.697 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.846 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.726.981 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.726.998 I llama_perf_context_print:        load time =     586.17 ms
0.00.726.999 I llama_perf_context_print: prompt eval time =     121.92 ms /   128 tokens (    0.95 ms per token,  1049.84 tokens per second)
0.00.727.000 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.001 I llama_perf_context_print:       total time =     131.01 ms /   129 tokens
0.00.727.446 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.077s
sys	0m0.099s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.739 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.708 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.704 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.705 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.706 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.706 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.706 I llama_model_loader: - type  f32:  194 tensors
0.00.026.707 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.707 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.792 I llm_load_vocab: special tokens cache size = 25
0.00.052.776 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.778 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.779 I llm_load_print_meta: arch             = gptneox
0.00.052.779 I llm_load_print_meta: vocab type       = BPE
0.00.052.779 I llm_load_print_meta: n_vocab          = 50304
0.00.052.780 I llm_load_print_meta: n_merges         = 50009
0.00.052.780 I llm_load_print_meta: vocab_only       = 0
0.00.052.780 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.780 I llm_load_print_meta: n_embd           = 2048
0.00.052.780 I llm_load_print_meta: n_layer          = 24
0.00.052.783 I llm_load_print_meta: n_head           = 16
0.00.052.784 I llm_load_print_meta: n_head_kv        = 16
0.00.052.796 I llm_load_print_meta: n_rot            = 32
0.00.052.798 I llm_load_print_meta: n_swa            = 0
0.00.052.798 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.798 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.799 I llm_load_print_meta: n_gqa            = 1
0.00.052.800 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.801 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.801 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.801 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.801 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.802 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.802 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.802 I llm_load_print_meta: n_ff             = 8192
0.00.052.803 I llm_load_print_meta: n_expert         = 0
0.00.052.803 I llm_load_print_meta: n_expert_used    = 0
0.00.052.803 I llm_load_print_meta: causal attn      = 1
0.00.052.803 I llm_load_print_meta: pooling type     = 0
0.00.052.803 I llm_load_print_meta: rope type        = 2
0.00.052.803 I llm_load_print_meta: rope scaling     = linear
0.00.052.804 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.804 I llm_load_print_meta: freq_scale_train = 1
0.00.052.805 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.805 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.805 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.805 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.805 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.805 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.805 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.815 I llm_load_print_meta: model type       = 1.4B
0.00.052.815 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.815 I llm_load_print_meta: model params     = 1.41 B
0.00.052.816 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.817 I llm_load_print_meta: general.name     = 1.4B
0.00.052.817 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.817 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.817 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.817 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.818 I llm_load_print_meta: LF token         = 128 ''
0.00.052.818 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.818 I llm_load_print_meta: max token length = 1024
0.00.054.795 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.795 I llm_load_tensors: offloading output layer to GPU
0.00.054.796 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.806 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.807 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.758 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.759 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.759 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.759 I llama_new_context_with_model: n_batch       = 2048
0.00.055.760 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.760 I llama_new_context_with_model: flash_attn    = 0
0.00.055.760 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.760 I llama_new_context_with_model: freq_scale    = 1
0.00.055.761 I ggml_metal_init: allocating
0.00.055.764 I ggml_metal_init: found device: Apple M4
0.00.055.768 I ggml_metal_init: picking default device: Apple M4
0.00.056.376 I ggml_metal_init: using embedded metal library
0.00.058.686 I ggml_metal_init: GPU name:   Apple M4
0.00.058.688 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.688 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.689 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.689 I ggml_metal_init: simdgroup reduction   = true
0.00.058.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.689 I ggml_metal_init: has bfloat            = true
0.00.058.689 I ggml_metal_init: use bfloat            = true
0.00.058.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.690 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.211 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.219 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.237 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.358 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.359 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.360 I llama_new_context_with_model: graph nodes  = 967
0.00.088.360 I llama_new_context_with_model: graph splits = 2
0.00.088.374 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.114 I main: llama threadpool init, n_threads = 4
0.00.700.155 I 
0.00.700.188 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.190 I 
0.00.700.416 I sampler seed: 1234
0.00.700.420 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.431 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.431 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.431 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.429.754 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 66981.13 tokens per second)
0.01.429.754 I llama_perf_context_print:        load time =     689.37 ms
0.01.429.755 I llama_perf_context_print: prompt eval time =      44.26 ms /     7 tokens (    6.32 ms per token,   158.14 tokens per second)
0.01.429.756 I llama_perf_context_print:        eval time =     682.18 ms /    63 runs   (   10.83 ms per token,    92.35 tokens per second)
0.01.429.756 I llama_perf_context_print:       total time =     729.64 ms /    70 tokens
0.01.429.941 I ggml_metal_free: deallocating

real	0m1.448s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.448 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.275 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.276 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.276 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.277 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.278 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.280 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.280 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.280 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.281 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.281 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.909 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.910 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.911 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.911 I llama_model_loader: - type  f32:  194 tensors
0.00.022.912 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.912 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.823 I llm_load_vocab: special tokens cache size = 25
0.00.048.708 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.710 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.711 I llm_load_print_meta: arch             = gptneox
0.00.048.711 I llm_load_print_meta: vocab type       = BPE
0.00.048.711 I llm_load_print_meta: n_vocab          = 50304
0.00.048.711 I llm_load_print_meta: n_merges         = 50009
0.00.048.712 I llm_load_print_meta: vocab_only       = 0
0.00.048.712 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.712 I llm_load_print_meta: n_embd           = 2048
0.00.048.712 I llm_load_print_meta: n_layer          = 24
0.00.048.715 I llm_load_print_meta: n_head           = 16
0.00.048.717 I llm_load_print_meta: n_head_kv        = 16
0.00.048.729 I llm_load_print_meta: n_rot            = 32
0.00.048.729 I llm_load_print_meta: n_swa            = 0
0.00.048.729 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.729 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.730 I llm_load_print_meta: n_gqa            = 1
0.00.048.731 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.731 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.732 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.732 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.732 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.732 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.732 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.733 I llm_load_print_meta: n_ff             = 8192
0.00.048.733 I llm_load_print_meta: n_expert         = 0
0.00.048.733 I llm_load_print_meta: n_expert_used    = 0
0.00.048.733 I llm_load_print_meta: causal attn      = 1
0.00.048.734 I llm_load_print_meta: pooling type     = 0
0.00.048.734 I llm_load_print_meta: rope type        = 2
0.00.048.734 I llm_load_print_meta: rope scaling     = linear
0.00.048.735 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.736 I llm_load_print_meta: freq_scale_train = 1
0.00.048.736 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.736 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.736 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.736 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.737 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.737 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.737 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.746 I llm_load_print_meta: model type       = 1.4B
0.00.048.746 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.747 I llm_load_print_meta: model params     = 1.41 B
0.00.048.747 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.747 I llm_load_print_meta: general.name     = 1.4B
0.00.048.748 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.748 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.748 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.748 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.748 I llm_load_print_meta: LF token         = 128 ''
0.00.048.749 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.749 I llm_load_print_meta: max token length = 1024
0.00.050.721 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.722 I llm_load_tensors: offloading output layer to GPU
0.00.050.722 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.732 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.733 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.672 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.673 I llama_new_context_with_model: n_ctx         = 128
0.00.051.674 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.674 I llama_new_context_with_model: n_batch       = 128
0.00.051.674 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.674 I llama_new_context_with_model: flash_attn    = 0
0.00.051.674 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.675 I llama_new_context_with_model: freq_scale    = 1
0.00.051.675 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.675 I ggml_metal_init: allocating
0.00.051.678 I ggml_metal_init: found device: Apple M4
0.00.051.680 I ggml_metal_init: picking default device: Apple M4
0.00.052.258 I ggml_metal_init: using embedded metal library
0.00.054.561 I ggml_metal_init: GPU name:   Apple M4
0.00.054.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.564 I ggml_metal_init: simdgroup reduction   = true
0.00.054.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.564 I ggml_metal_init: has bfloat            = true
0.00.054.564 I ggml_metal_init: use bfloat            = true
0.00.054.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.341 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.343 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.356 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.289 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.290 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.290 I llama_new_context_with_model: graph nodes  = 967
0.00.066.290 I llama_new_context_with_model: graph splits = 2
0.00.066.303 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.840 I 
0.00.661.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.881 I perplexity: tokenizing the input ..
0.00.669.803 I perplexity: tokenization took 7.921 ms
0.00.669.814 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.755 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.793.938 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.793.956 I llama_perf_context_print:        load time =     653.39 ms
0.00.793.958 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.07 tokens per second)
0.00.793.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.960 I llama_perf_context_print:       total time =     132.12 ms /   129 tokens
0.00.794.429 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.077s
sys	0m0.116s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.026 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.696 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.697 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.702 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.703 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.703 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.703 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.704 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.704 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.707 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.708 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.578 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.617 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.427 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.429 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.429 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.430 I llama_model_loader: - type  f32:  194 tensors
0.00.025.430 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.430 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.472 I llm_load_vocab: special tokens cache size = 25
0.00.051.277 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.280 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.281 I llm_load_print_meta: arch             = gptneox
0.00.051.281 I llm_load_print_meta: vocab type       = BPE
0.00.051.281 I llm_load_print_meta: n_vocab          = 50304
0.00.051.281 I llm_load_print_meta: n_merges         = 50009
0.00.051.282 I llm_load_print_meta: vocab_only       = 0
0.00.051.282 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.282 I llm_load_print_meta: n_embd           = 2048
0.00.051.282 I llm_load_print_meta: n_layer          = 24
0.00.051.285 I llm_load_print_meta: n_head           = 16
0.00.051.285 I llm_load_print_meta: n_head_kv        = 16
0.00.051.292 I llm_load_print_meta: n_rot            = 32
0.00.051.292 I llm_load_print_meta: n_swa            = 0
0.00.051.292 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.293 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.293 I llm_load_print_meta: n_gqa            = 1
0.00.051.296 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.297 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.298 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.298 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.298 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.298 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.298 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.299 I llm_load_print_meta: n_ff             = 8192
0.00.051.300 I llm_load_print_meta: n_expert         = 0
0.00.051.300 I llm_load_print_meta: n_expert_used    = 0
0.00.051.301 I llm_load_print_meta: causal attn      = 1
0.00.051.303 I llm_load_print_meta: pooling type     = 0
0.00.051.303 I llm_load_print_meta: rope type        = 2
0.00.051.303 I llm_load_print_meta: rope scaling     = linear
0.00.051.303 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.304 I llm_load_print_meta: freq_scale_train = 1
0.00.051.308 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.309 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.309 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.309 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.309 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.309 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.309 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.315 I llm_load_print_meta: model type       = 1.4B
0.00.051.315 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.316 I llm_load_print_meta: model params     = 1.41 B
0.00.051.316 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.317 I llm_load_print_meta: general.name     = 1.4B
0.00.051.318 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.318 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.318 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.318 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.318 I llm_load_print_meta: LF token         = 128 ''
0.00.051.319 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.319 I llm_load_print_meta: max token length = 1024
0.00.053.090 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.090 I llm_load_tensors: offloading output layer to GPU
0.00.053.090 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.095 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.096 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.028 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.029 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.029 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.029 I llama_new_context_with_model: n_batch       = 2048
0.00.054.029 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.029 I llama_new_context_with_model: flash_attn    = 0
0.00.054.030 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.030 I llama_new_context_with_model: freq_scale    = 1
0.00.054.031 I ggml_metal_init: allocating
0.00.054.034 I ggml_metal_init: found device: Apple M4
0.00.054.036 I ggml_metal_init: picking default device: Apple M4
0.00.054.631 I ggml_metal_init: using embedded metal library
0.00.056.948 I ggml_metal_init: GPU name:   Apple M4
0.00.056.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.951 I ggml_metal_init: simdgroup reduction   = true
0.00.056.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.951 I ggml_metal_init: has bfloat            = true
0.00.056.951 I ggml_metal_init: use bfloat            = true
0.00.056.952 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.654 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.660 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.733 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.734 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.734 I llama_new_context_with_model: graph nodes  = 967
0.00.087.735 I llama_new_context_with_model: graph splits = 2
0.00.087.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.401 I main: llama threadpool init, n_threads = 4
0.00.776.446 I 
0.00.776.486 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.488 I 
0.00.776.711 I sampler seed: 1234
0.00.776.730 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.767 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.769 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.769 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.569.075 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.569.076 I llama_perf_context_print:        load time =     766.37 ms
0.01.569.076 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.76 tokens per second)
0.01.569.077 I llama_perf_context_print:        eval time =     742.31 ms /    63 runs   (   11.78 ms per token,    84.87 tokens per second)
0.01.569.078 I llama_perf_context_print:       total time =     792.68 ms /    70 tokens
0.01.569.264 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.109s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.811 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.781 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.782 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.782 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.782 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.787 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.787 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.792 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.792 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.563 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.580 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.385 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.387 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.387 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.388 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.388 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.389 I llama_model_loader: - type  f32:  194 tensors
0.00.024.389 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.389 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.086 I llm_load_vocab: special tokens cache size = 25
0.00.051.169 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.172 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.172 I llm_load_print_meta: arch             = gptneox
0.00.051.173 I llm_load_print_meta: vocab type       = BPE
0.00.051.173 I llm_load_print_meta: n_vocab          = 50304
0.00.051.173 I llm_load_print_meta: n_merges         = 50009
0.00.051.173 I llm_load_print_meta: vocab_only       = 0
0.00.051.173 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.174 I llm_load_print_meta: n_embd           = 2048
0.00.051.174 I llm_load_print_meta: n_layer          = 24
0.00.051.177 I llm_load_print_meta: n_head           = 16
0.00.051.177 I llm_load_print_meta: n_head_kv        = 16
0.00.051.189 I llm_load_print_meta: n_rot            = 32
0.00.051.190 I llm_load_print_meta: n_swa            = 0
0.00.051.190 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.190 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.191 I llm_load_print_meta: n_gqa            = 1
0.00.051.192 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.193 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.194 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.194 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.194 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.194 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.194 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.195 I llm_load_print_meta: n_ff             = 8192
0.00.051.195 I llm_load_print_meta: n_expert         = 0
0.00.051.195 I llm_load_print_meta: n_expert_used    = 0
0.00.051.195 I llm_load_print_meta: causal attn      = 1
0.00.051.196 I llm_load_print_meta: pooling type     = 0
0.00.051.196 I llm_load_print_meta: rope type        = 2
0.00.051.196 I llm_load_print_meta: rope scaling     = linear
0.00.051.196 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.197 I llm_load_print_meta: freq_scale_train = 1
0.00.051.197 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.197 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.198 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.198 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.198 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.198 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.199 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.208 I llm_load_print_meta: model type       = 1.4B
0.00.051.208 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.209 I llm_load_print_meta: model params     = 1.41 B
0.00.051.210 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.210 I llm_load_print_meta: general.name     = 1.4B
0.00.051.211 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: LF token         = 128 ''
0.00.051.212 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.212 I llm_load_print_meta: max token length = 1024
0.00.053.186 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.186 I llm_load_tensors: offloading output layer to GPU
0.00.053.186 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.197 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.198 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.108 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.108 I llama_new_context_with_model: n_ctx         = 128
0.00.054.109 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.109 I llama_new_context_with_model: n_batch       = 128
0.00.054.109 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.109 I llama_new_context_with_model: flash_attn    = 0
0.00.054.109 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.110 I llama_new_context_with_model: freq_scale    = 1
0.00.054.110 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.111 I ggml_metal_init: allocating
0.00.054.114 I ggml_metal_init: found device: Apple M4
0.00.054.116 I ggml_metal_init: picking default device: Apple M4
0.00.054.689 I ggml_metal_init: using embedded metal library
0.00.056.982 I ggml_metal_init: GPU name:   Apple M4
0.00.056.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.984 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.984 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.984 I ggml_metal_init: simdgroup reduction   = true
0.00.056.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.985 I ggml_metal_init: has bfloat            = true
0.00.056.985 I ggml_metal_init: use bfloat            = true
0.00.056.985 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.797 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.800 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.813 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.726 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.728 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.728 I llama_new_context_with_model: graph nodes  = 967
0.00.068.728 I llama_new_context_with_model: graph splits = 2
0.00.068.740 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.543 I 
0.00.716.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.584 I perplexity: tokenizing the input ..
0.00.724.869 I perplexity: tokenization took 8.284 ms
0.00.724.880 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.743 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.860.889 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.860.904 I llama_perf_context_print:        load time =     706.73 ms
0.00.860.905 I llama_perf_context_print: prompt eval time =     134.64 ms /   128 tokens (    1.05 ms per token,   950.71 tokens per second)
0.00.860.906 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.860.906 I llama_perf_context_print:       total time =     144.36 ms /   129 tokens
0.00.861.335 I ggml_metal_free: deallocating

real	0m0.876s
user	0m0.078s
sys	0m0.122s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.940 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.776 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.781 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.782 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.786 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.645 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.646 I llama_model_loader: - type  f32:  194 tensors
0.00.024.646 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.646 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.870 I llm_load_vocab: special tokens cache size = 25
0.00.050.772 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.774 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.775 I llm_load_print_meta: arch             = gptneox
0.00.050.775 I llm_load_print_meta: vocab type       = BPE
0.00.050.775 I llm_load_print_meta: n_vocab          = 50304
0.00.050.776 I llm_load_print_meta: n_merges         = 50009
0.00.050.776 I llm_load_print_meta: vocab_only       = 0
0.00.050.776 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.776 I llm_load_print_meta: n_embd           = 2048
0.00.050.776 I llm_load_print_meta: n_layer          = 24
0.00.050.779 I llm_load_print_meta: n_head           = 16
0.00.050.779 I llm_load_print_meta: n_head_kv        = 16
0.00.050.794 I llm_load_print_meta: n_rot            = 32
0.00.050.794 I llm_load_print_meta: n_swa            = 0
0.00.050.794 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.794 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.795 I llm_load_print_meta: n_gqa            = 1
0.00.050.796 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.796 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.797 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.797 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.797 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.797 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.798 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.798 I llm_load_print_meta: n_ff             = 8192
0.00.050.798 I llm_load_print_meta: n_expert         = 0
0.00.050.798 I llm_load_print_meta: n_expert_used    = 0
0.00.050.800 I llm_load_print_meta: causal attn      = 1
0.00.050.801 I llm_load_print_meta: pooling type     = 0
0.00.050.801 I llm_load_print_meta: rope type        = 2
0.00.050.801 I llm_load_print_meta: rope scaling     = linear
0.00.050.802 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.802 I llm_load_print_meta: freq_scale_train = 1
0.00.050.802 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.802 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.802 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.802 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.802 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.803 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.803 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.812 I llm_load_print_meta: model type       = 1.4B
0.00.050.812 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.813 I llm_load_print_meta: model params     = 1.41 B
0.00.050.813 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.814 I llm_load_print_meta: general.name     = 1.4B
0.00.050.815 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.815 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.815 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.815 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.815 I llm_load_print_meta: LF token         = 128 ''
0.00.050.816 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.816 I llm_load_print_meta: max token length = 1024
0.00.052.773 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.773 I llm_load_tensors: offloading output layer to GPU
0.00.052.773 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.783 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.785 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.685 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.685 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.686 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.686 I llama_new_context_with_model: n_batch       = 2048
0.00.053.686 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.686 I llama_new_context_with_model: flash_attn    = 0
0.00.053.686 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.687 I llama_new_context_with_model: freq_scale    = 1
0.00.053.687 I ggml_metal_init: allocating
0.00.053.690 I ggml_metal_init: found device: Apple M4
0.00.053.692 I ggml_metal_init: picking default device: Apple M4
0.00.054.310 I ggml_metal_init: using embedded metal library
0.00.056.571 I ggml_metal_init: GPU name:   Apple M4
0.00.056.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.574 I ggml_metal_init: simdgroup reduction   = true
0.00.056.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.575 I ggml_metal_init: has bfloat            = true
0.00.056.576 I ggml_metal_init: use bfloat            = true
0.00.056.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.663 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.668 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.685 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.602 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.603 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.604 I llama_new_context_with_model: graph nodes  = 967
0.00.085.604 I llama_new_context_with_model: graph splits = 2
0.00.085.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.452 I main: llama threadpool init, n_threads = 4
0.00.811.494 I 
0.00.811.519 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.519 I 
0.00.811.749 I sampler seed: 1234
0.00.811.753 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.795 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.796 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.796 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.649.816 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.649.817 I llama_perf_context_print:        load time =     802.51 ms
0.01.649.817 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.67 tokens per second)
0.01.649.818 I llama_perf_context_print:        eval time =     792.92 ms /    63 runs   (   12.59 ms per token,    79.45 tokens per second)
0.01.649.818 I llama_perf_context_print:       total time =     838.37 ms /    70 tokens
0.01.650.021 I ggml_metal_free: deallocating

real	0m1.669s
user	0m0.109s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.344 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.144 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.151 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.152 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.152 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.155 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.155 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.156 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.157 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.931 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.998 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.804 I llama_model_loader: - type  f32:  194 tensors
0.00.023.804 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.805 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.597 I llm_load_vocab: special tokens cache size = 25
0.00.049.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.500 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.500 I llm_load_print_meta: arch             = gptneox
0.00.049.500 I llm_load_print_meta: vocab type       = BPE
0.00.049.500 I llm_load_print_meta: n_vocab          = 50304
0.00.049.501 I llm_load_print_meta: n_merges         = 50009
0.00.049.501 I llm_load_print_meta: vocab_only       = 0
0.00.049.501 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.501 I llm_load_print_meta: n_embd           = 2048
0.00.049.501 I llm_load_print_meta: n_layer          = 24
0.00.049.504 I llm_load_print_meta: n_head           = 16
0.00.049.505 I llm_load_print_meta: n_head_kv        = 16
0.00.049.517 I llm_load_print_meta: n_rot            = 32
0.00.049.517 I llm_load_print_meta: n_swa            = 0
0.00.049.517 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.517 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.518 I llm_load_print_meta: n_gqa            = 1
0.00.049.519 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.519 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.521 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.521 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.521 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.522 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.522 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.522 I llm_load_print_meta: n_ff             = 8192
0.00.049.522 I llm_load_print_meta: n_expert         = 0
0.00.049.523 I llm_load_print_meta: n_expert_used    = 0
0.00.049.523 I llm_load_print_meta: causal attn      = 1
0.00.049.523 I llm_load_print_meta: pooling type     = 0
0.00.049.523 I llm_load_print_meta: rope type        = 2
0.00.049.523 I llm_load_print_meta: rope scaling     = linear
0.00.049.523 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.524 I llm_load_print_meta: freq_scale_train = 1
0.00.049.524 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.524 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.524 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.524 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.524 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.525 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.525 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.534 I llm_load_print_meta: model type       = 1.4B
0.00.049.535 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.535 I llm_load_print_meta: model params     = 1.41 B
0.00.049.535 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.535 I llm_load_print_meta: general.name     = 1.4B
0.00.049.536 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.536 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.536 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.536 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.537 I llm_load_print_meta: LF token         = 128 ''
0.00.049.537 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.537 I llm_load_print_meta: max token length = 1024
0.00.051.484 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.484 I llm_load_tensors: offloading output layer to GPU
0.00.051.484 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.494 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.495 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.381 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.382 I llama_new_context_with_model: n_ctx         = 128
0.00.052.382 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.382 I llama_new_context_with_model: n_batch       = 128
0.00.052.382 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.383 I llama_new_context_with_model: flash_attn    = 0
0.00.052.383 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.383 I llama_new_context_with_model: freq_scale    = 1
0.00.052.384 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.384 I ggml_metal_init: allocating
0.00.052.390 I ggml_metal_init: found device: Apple M4
0.00.052.392 I ggml_metal_init: picking default device: Apple M4
0.00.052.933 I ggml_metal_init: using embedded metal library
0.00.055.248 I ggml_metal_init: GPU name:   Apple M4
0.00.055.250 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.250 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.251 I ggml_metal_init: simdgroup reduction   = true
0.00.055.251 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.251 I ggml_metal_init: has bfloat            = true
0.00.055.251 I ggml_metal_init: use bfloat            = true
0.00.055.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.147 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.152 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.177 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.116 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.117 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.118 I llama_new_context_with_model: graph nodes  = 967
0.00.067.118 I llama_new_context_with_model: graph splits = 2
0.00.067.130 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.779 I 
0.00.790.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.877 I perplexity: tokenizing the input ..
0.00.799.058 I perplexity: tokenization took 8.179 ms
0.00.799.069 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.933.912 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.935.097 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.935.114 I llama_perf_context_print:        load time =     781.42 ms
0.00.935.115 I llama_perf_context_print: prompt eval time =     134.62 ms /   128 tokens (    1.05 ms per token,   950.84 tokens per second)
0.00.935.116 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.935.116 I llama_perf_context_print:       total time =     144.34 ms /   129 tokens
0.00.935.584 I ggml_metal_free: deallocating

real	0m0.949s
user	0m0.077s
sys	0m0.129s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.699 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.147 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.148 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.148 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.150 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.151 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.155 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.156 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.957 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.846 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.847 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.848 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.848 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.848 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.849 I llama_model_loader: - type  f32:  194 tensors
0.00.023.849 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.849 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.850 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.904 I llm_load_vocab: special tokens cache size = 25
0.00.050.699 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.701 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.702 I llm_load_print_meta: arch             = gptneox
0.00.050.702 I llm_load_print_meta: vocab type       = BPE
0.00.050.702 I llm_load_print_meta: n_vocab          = 50304
0.00.050.703 I llm_load_print_meta: n_merges         = 50009
0.00.050.703 I llm_load_print_meta: vocab_only       = 0
0.00.050.703 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.703 I llm_load_print_meta: n_embd           = 2048
0.00.050.703 I llm_load_print_meta: n_layer          = 24
0.00.050.707 I llm_load_print_meta: n_head           = 16
0.00.050.707 I llm_load_print_meta: n_head_kv        = 16
0.00.050.719 I llm_load_print_meta: n_rot            = 32
0.00.050.719 I llm_load_print_meta: n_swa            = 0
0.00.050.720 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.720 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.721 I llm_load_print_meta: n_gqa            = 1
0.00.050.721 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.722 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.723 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.723 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.723 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.723 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.723 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.724 I llm_load_print_meta: n_ff             = 8192
0.00.050.724 I llm_load_print_meta: n_expert         = 0
0.00.050.726 I llm_load_print_meta: n_expert_used    = 0
0.00.050.727 I llm_load_print_meta: causal attn      = 1
0.00.050.727 I llm_load_print_meta: pooling type     = 0
0.00.050.727 I llm_load_print_meta: rope type        = 2
0.00.050.727 I llm_load_print_meta: rope scaling     = linear
0.00.050.728 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.728 I llm_load_print_meta: freq_scale_train = 1
0.00.050.728 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.729 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.730 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.730 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.730 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.730 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.730 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.740 I llm_load_print_meta: model type       = 1.4B
0.00.050.740 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.740 I llm_load_print_meta: model params     = 1.41 B
0.00.050.741 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.741 I llm_load_print_meta: general.name     = 1.4B
0.00.050.741 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.741 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.741 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.742 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.742 I llm_load_print_meta: LF token         = 128 ''
0.00.050.742 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.742 I llm_load_print_meta: max token length = 1024
0.00.052.685 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.685 I llm_load_tensors: offloading output layer to GPU
0.00.052.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.696 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.697 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.741 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.742 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.742 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.742 I llama_new_context_with_model: n_batch       = 2048
0.00.053.742 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.743 I llama_new_context_with_model: flash_attn    = 0
0.00.053.743 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.743 I llama_new_context_with_model: freq_scale    = 1
0.00.053.744 I ggml_metal_init: allocating
0.00.053.747 I ggml_metal_init: found device: Apple M4
0.00.053.749 I ggml_metal_init: picking default device: Apple M4
0.00.054.352 I ggml_metal_init: using embedded metal library
0.00.056.676 I ggml_metal_init: GPU name:   Apple M4
0.00.056.678 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.679 I ggml_metal_init: simdgroup reduction   = true
0.00.056.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.680 I ggml_metal_init: has bfloat            = true
0.00.056.680 I ggml_metal_init: use bfloat            = true
0.00.056.681 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.596 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.601 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.619 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.697 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.698 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.698 I llama_new_context_with_model: graph nodes  = 967
0.00.087.699 I llama_new_context_with_model: graph splits = 2
0.00.087.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.158 I main: llama threadpool init, n_threads = 4
0.00.521.199 I 
0.00.521.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.228 I 
0.00.521.467 I sampler seed: 1234
0.00.521.472 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.521.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.521.497 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.521.497 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.206.156 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.206.157 I llama_perf_context_print:        load time =     511.45 ms
0.01.206.157 I llama_perf_context_print: prompt eval time =      39.67 ms /     7 tokens (    5.67 ms per token,   176.46 tokens per second)
0.01.206.158 I llama_perf_context_print:        eval time =     641.96 ms /    63 runs   (   10.19 ms per token,    98.14 tokens per second)
0.01.206.158 I llama_perf_context_print:       total time =     685.00 ms /    70 tokens
0.01.206.334 I ggml_metal_free: deallocating

real	0m1.226s
user	0m0.110s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.822 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.888 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.892 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.897 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.898 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.898 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.900 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.900 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.901 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.901 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.903 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.683 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.528 I llama_model_loader: - type  f32:  194 tensors
0.00.027.528 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.528 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.528 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.192 I llm_load_vocab: special tokens cache size = 25
0.00.054.103 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.106 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.106 I llm_load_print_meta: arch             = gptneox
0.00.054.106 I llm_load_print_meta: vocab type       = BPE
0.00.054.107 I llm_load_print_meta: n_vocab          = 50304
0.00.054.107 I llm_load_print_meta: n_merges         = 50009
0.00.054.107 I llm_load_print_meta: vocab_only       = 0
0.00.054.107 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.107 I llm_load_print_meta: n_embd           = 2048
0.00.054.107 I llm_load_print_meta: n_layer          = 24
0.00.054.110 I llm_load_print_meta: n_head           = 16
0.00.054.111 I llm_load_print_meta: n_head_kv        = 16
0.00.054.122 I llm_load_print_meta: n_rot            = 32
0.00.054.123 I llm_load_print_meta: n_swa            = 0
0.00.054.123 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.123 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.124 I llm_load_print_meta: n_gqa            = 1
0.00.054.124 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.125 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.126 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.126 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.126 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.126 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.126 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.127 I llm_load_print_meta: n_ff             = 8192
0.00.054.127 I llm_load_print_meta: n_expert         = 0
0.00.054.127 I llm_load_print_meta: n_expert_used    = 0
0.00.054.128 I llm_load_print_meta: causal attn      = 1
0.00.054.128 I llm_load_print_meta: pooling type     = 0
0.00.054.129 I llm_load_print_meta: rope type        = 2
0.00.054.129 I llm_load_print_meta: rope scaling     = linear
0.00.054.130 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.130 I llm_load_print_meta: freq_scale_train = 1
0.00.054.130 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.130 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.130 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.130 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.131 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.131 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.131 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.140 I llm_load_print_meta: model type       = 1.4B
0.00.054.141 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.054.141 I llm_load_print_meta: model params     = 1.41 B
0.00.054.142 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.054.143 I llm_load_print_meta: general.name     = 1.4B
0.00.054.144 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.144 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.144 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.144 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.145 I llm_load_print_meta: LF token         = 128 ''
0.00.054.146 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.146 I llm_load_print_meta: max token length = 1024
0.00.055.997 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.997 I llm_load_tensors: offloading output layer to GPU
0.00.055.998 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.008 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.056.009 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.958 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.959 I llama_new_context_with_model: n_ctx         = 128
0.00.056.959 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.960 I llama_new_context_with_model: n_batch       = 128
0.00.056.960 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.960 I llama_new_context_with_model: flash_attn    = 0
0.00.056.960 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.961 I llama_new_context_with_model: freq_scale    = 1
0.00.056.961 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.961 I ggml_metal_init: allocating
0.00.056.965 I ggml_metal_init: found device: Apple M4
0.00.056.967 I ggml_metal_init: picking default device: Apple M4
0.00.057.525 I ggml_metal_init: using embedded metal library
0.00.059.864 I ggml_metal_init: GPU name:   Apple M4
0.00.059.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.867 I ggml_metal_init: simdgroup reduction   = true
0.00.059.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.867 I ggml_metal_init: has bfloat            = true
0.00.059.867 I ggml_metal_init: use bfloat            = true
0.00.059.867 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.868 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.732 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.736 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.749 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.682 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.683 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.684 I llama_new_context_with_model: graph nodes  = 967
0.00.071.684 I llama_new_context_with_model: graph splits = 2
0.00.071.696 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.464.226 I 
0.00.464.259 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.464.267 I perplexity: tokenizing the input ..
0.00.472.246 I perplexity: tokenization took 7.978 ms
0.00.472.257 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.604.763 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.605.928 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.605.953 I llama_perf_context_print:        load time =     452.40 ms
0.00.605.955 I llama_perf_context_print: prompt eval time =     132.28 ms /   128 tokens (    1.03 ms per token,   967.68 tokens per second)
0.00.605.957 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.605.958 I llama_perf_context_print:       total time =     141.73 ms /   129 tokens
0.00.606.491 I ggml_metal_free: deallocating

real	0m0.622s
user	0m0.078s
sys	0m0.077s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.009.162 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.486 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.491 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.497 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.498 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.498 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.499 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.499 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.500 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.500 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.501 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.490 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.587 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.463 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.464 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.464 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.465 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.465 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.465 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.466 I llama_model_loader: - type  f32:  194 tensors
0.00.024.466 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.467 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.467 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.467 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.238 I llm_load_vocab: special tokens cache size = 25
0.00.051.315 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.318 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.318 I llm_load_print_meta: arch             = gptneox
0.00.051.318 I llm_load_print_meta: vocab type       = BPE
0.00.051.319 I llm_load_print_meta: n_vocab          = 50304
0.00.051.319 I llm_load_print_meta: n_merges         = 50009
0.00.051.319 I llm_load_print_meta: vocab_only       = 0
0.00.051.319 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.319 I llm_load_print_meta: n_embd           = 2048
0.00.051.320 I llm_load_print_meta: n_layer          = 24
0.00.051.322 I llm_load_print_meta: n_head           = 16
0.00.051.323 I llm_load_print_meta: n_head_kv        = 16
0.00.051.334 I llm_load_print_meta: n_rot            = 32
0.00.051.334 I llm_load_print_meta: n_swa            = 0
0.00.051.335 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.335 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.335 I llm_load_print_meta: n_gqa            = 1
0.00.051.336 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.337 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.337 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.338 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.338 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.338 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.338 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.339 I llm_load_print_meta: n_ff             = 8192
0.00.051.339 I llm_load_print_meta: n_expert         = 0
0.00.051.339 I llm_load_print_meta: n_expert_used    = 0
0.00.051.339 I llm_load_print_meta: causal attn      = 1
0.00.051.340 I llm_load_print_meta: pooling type     = 0
0.00.051.340 I llm_load_print_meta: rope type        = 2
0.00.051.340 I llm_load_print_meta: rope scaling     = linear
0.00.051.340 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.341 I llm_load_print_meta: freq_scale_train = 1
0.00.051.341 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.341 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.341 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.341 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.341 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.341 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.342 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.350 I llm_load_print_meta: model type       = 1.4B
0.00.051.351 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.351 I llm_load_print_meta: model params     = 1.41 B
0.00.051.352 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.352 I llm_load_print_meta: general.name     = 1.4B
0.00.051.352 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.352 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.352 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.356 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.356 I llm_load_print_meta: LF token         = 128 ''
0.00.051.357 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.357 I llm_load_print_meta: max token length = 1024
0.00.052.969 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.970 I llm_load_tensors: offloading output layer to GPU
0.00.052.970 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.980 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.981 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.829 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.830 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.830 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.830 I llama_new_context_with_model: n_batch       = 2048
0.00.053.830 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.831 I llama_new_context_with_model: flash_attn    = 0
0.00.053.831 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.831 I llama_new_context_with_model: freq_scale    = 1
0.00.053.832 I ggml_metal_init: allocating
0.00.053.837 I ggml_metal_init: found device: Apple M4
0.00.053.839 I ggml_metal_init: picking default device: Apple M4
0.00.054.406 I ggml_metal_init: using embedded metal library
0.00.056.707 I ggml_metal_init: GPU name:   Apple M4
0.00.056.708 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.709 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.709 I ggml_metal_init: simdgroup reduction   = true
0.00.056.709 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.709 I ggml_metal_init: has bfloat            = true
0.00.056.710 I ggml_metal_init: use bfloat            = true
0.00.056.710 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.261 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.269 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.286 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.324 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.326 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.326 I llama_new_context_with_model: graph nodes  = 967
0.00.087.326 I llama_new_context_with_model: graph splits = 2
0.00.087.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.036 I main: llama threadpool init, n_threads = 4
0.00.616.074 I 
0.00.616.106 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.106 I 
0.00.616.337 I sampler seed: 1234
0.00.616.341 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.616.365 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.616.367 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.616.367 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.362.561 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.362.562 I llama_perf_context_print:        load time =     606.87 ms
0.01.362.563 I llama_perf_context_print: prompt eval time =      40.45 ms /     7 tokens (    5.78 ms per token,   173.07 tokens per second)
0.01.362.565 I llama_perf_context_print:        eval time =     702.75 ms /    63 runs   (   11.15 ms per token,    89.65 tokens per second)
0.01.362.565 I llama_perf_context_print:       total time =     746.53 ms /    70 tokens
0.01.362.756 I ggml_metal_free: deallocating

real	0m1.381s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.480 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.217 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.225 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.228 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.236 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.924 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.945 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.728 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.729 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.729 I llama_model_loader: - type  f32:  194 tensors
0.00.022.730 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.730 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.730 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.730 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.652 I llm_load_vocab: special tokens cache size = 25
0.00.048.582 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.585 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.585 I llm_load_print_meta: arch             = gptneox
0.00.048.586 I llm_load_print_meta: vocab type       = BPE
0.00.048.586 I llm_load_print_meta: n_vocab          = 50304
0.00.048.586 I llm_load_print_meta: n_merges         = 50009
0.00.048.586 I llm_load_print_meta: vocab_only       = 0
0.00.048.586 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.587 I llm_load_print_meta: n_embd           = 2048
0.00.048.587 I llm_load_print_meta: n_layer          = 24
0.00.048.589 I llm_load_print_meta: n_head           = 16
0.00.048.590 I llm_load_print_meta: n_head_kv        = 16
0.00.048.602 I llm_load_print_meta: n_rot            = 32
0.00.048.605 I llm_load_print_meta: n_swa            = 0
0.00.048.605 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.605 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.606 I llm_load_print_meta: n_gqa            = 1
0.00.048.607 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.608 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.608 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.608 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.608 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.609 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.610 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.611 I llm_load_print_meta: n_ff             = 8192
0.00.048.611 I llm_load_print_meta: n_expert         = 0
0.00.048.611 I llm_load_print_meta: n_expert_used    = 0
0.00.048.611 I llm_load_print_meta: causal attn      = 1
0.00.048.611 I llm_load_print_meta: pooling type     = 0
0.00.048.611 I llm_load_print_meta: rope type        = 2
0.00.048.612 I llm_load_print_meta: rope scaling     = linear
0.00.048.612 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.612 I llm_load_print_meta: freq_scale_train = 1
0.00.048.612 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.612 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.613 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.613 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.613 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.613 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.613 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.623 I llm_load_print_meta: model type       = 1.4B
0.00.048.623 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.623 I llm_load_print_meta: model params     = 1.41 B
0.00.048.624 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.624 I llm_load_print_meta: general.name     = 1.4B
0.00.048.624 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.625 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.625 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.625 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.625 I llm_load_print_meta: LF token         = 128 ''
0.00.048.626 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.626 I llm_load_print_meta: max token length = 1024
0.00.050.517 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.517 I llm_load_tensors: offloading output layer to GPU
0.00.050.518 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.528 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.529 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.445 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.446 I llama_new_context_with_model: n_ctx         = 128
0.00.051.446 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.446 I llama_new_context_with_model: n_batch       = 128
0.00.051.446 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.447 I llama_new_context_with_model: flash_attn    = 0
0.00.051.447 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.447 I llama_new_context_with_model: freq_scale    = 1
0.00.051.448 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.448 I ggml_metal_init: allocating
0.00.051.451 I ggml_metal_init: found device: Apple M4
0.00.051.453 I ggml_metal_init: picking default device: Apple M4
0.00.052.017 I ggml_metal_init: using embedded metal library
0.00.054.343 I ggml_metal_init: GPU name:   Apple M4
0.00.054.345 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.345 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.345 I ggml_metal_init: simdgroup reduction   = true
0.00.054.346 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.346 I ggml_metal_init: has bfloat            = true
0.00.054.346 I ggml_metal_init: use bfloat            = true
0.00.054.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.996 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.999 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.011 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.915 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.916 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.916 I llama_new_context_with_model: graph nodes  = 967
0.00.065.916 I llama_new_context_with_model: graph splits = 2
0.00.065.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.548.028 I 
0.00.548.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.548.079 I perplexity: tokenizing the input ..
0.00.556.183 I perplexity: tokenization took 8.102 ms
0.00.556.193 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.688.618 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.689.865 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.689.887 I llama_perf_context_print:        load time =     539.54 ms
0.00.689.889 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.27 tokens per second)
0.00.689.890 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.890 I llama_perf_context_print:       total time =     141.86 ms /   129 tokens
0.00.690.418 I ggml_metal_free: deallocating

real	0m0.704s
user	0m0.077s
sys	0m0.090s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.965 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.975 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.978 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.979 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.981 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.982 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.853 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.924 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.662 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.663 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.664 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.664 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.664 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.665 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.665 I llama_model_loader: - type  f32:  194 tensors
0.00.023.665 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.666 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.666 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.743 I llm_load_vocab: special tokens cache size = 25
0.00.049.630 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.633 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.633 I llm_load_print_meta: arch             = gptneox
0.00.049.634 I llm_load_print_meta: vocab type       = BPE
0.00.049.634 I llm_load_print_meta: n_vocab          = 50304
0.00.049.634 I llm_load_print_meta: n_merges         = 50009
0.00.049.634 I llm_load_print_meta: vocab_only       = 0
0.00.049.635 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.635 I llm_load_print_meta: n_embd           = 2048
0.00.049.635 I llm_load_print_meta: n_layer          = 24
0.00.049.638 I llm_load_print_meta: n_head           = 16
0.00.049.638 I llm_load_print_meta: n_head_kv        = 16
0.00.049.650 I llm_load_print_meta: n_rot            = 32
0.00.049.650 I llm_load_print_meta: n_swa            = 0
0.00.049.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.653 I llm_load_print_meta: n_gqa            = 1
0.00.049.654 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.656 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.656 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.657 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.657 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.657 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.657 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.658 I llm_load_print_meta: n_ff             = 8192
0.00.049.658 I llm_load_print_meta: n_expert         = 0
0.00.049.658 I llm_load_print_meta: n_expert_used    = 0
0.00.049.658 I llm_load_print_meta: causal attn      = 1
0.00.049.658 I llm_load_print_meta: pooling type     = 0
0.00.049.658 I llm_load_print_meta: rope type        = 2
0.00.049.659 I llm_load_print_meta: rope scaling     = linear
0.00.049.659 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.659 I llm_load_print_meta: freq_scale_train = 1
0.00.049.659 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.660 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.661 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.661 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.661 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.661 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.661 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.671 I llm_load_print_meta: model type       = 1.4B
0.00.049.671 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.671 I llm_load_print_meta: model params     = 1.41 B
0.00.049.672 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.672 I llm_load_print_meta: general.name     = 1.4B
0.00.049.672 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.672 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.672 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.673 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.673 I llm_load_print_meta: LF token         = 128 ''
0.00.049.673 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.673 I llm_load_print_meta: max token length = 1024
0.00.051.615 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.616 I llm_load_tensors: offloading output layer to GPU
0.00.051.616 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.626 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.627 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.535 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.536 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.536 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.536 I llama_new_context_with_model: n_batch       = 2048
0.00.052.536 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.537 I llama_new_context_with_model: flash_attn    = 0
0.00.052.537 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.537 I llama_new_context_with_model: freq_scale    = 1
0.00.052.538 I ggml_metal_init: allocating
0.00.052.541 I ggml_metal_init: found device: Apple M4
0.00.052.543 I ggml_metal_init: picking default device: Apple M4
0.00.053.125 I ggml_metal_init: using embedded metal library
0.00.055.396 I ggml_metal_init: GPU name:   Apple M4
0.00.055.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.398 I ggml_metal_init: simdgroup reduction   = true
0.00.055.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.398 I ggml_metal_init: has bfloat            = true
0.00.055.398 I ggml_metal_init: use bfloat            = true
0.00.055.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.399 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.923 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.929 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.948 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.954 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.956 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.956 I llama_new_context_with_model: graph nodes  = 967
0.00.084.957 I llama_new_context_with_model: graph splits = 2
0.00.084.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.790 I main: llama threadpool init, n_threads = 4
0.00.620.836 I 
0.00.620.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.876 I 
0.00.621.125 I sampler seed: 1234
0.00.621.129 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.621.166 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.621.183 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.621.183 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.385.310 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.385.310 I llama_perf_context_print:        load time =     612.02 ms
0.01.385.311 I llama_perf_context_print: prompt eval time =      47.15 ms /     7 tokens (    6.74 ms per token,   148.46 tokens per second)
0.01.385.312 I llama_perf_context_print:        eval time =     713.88 ms /    63 runs   (   11.33 ms per token,    88.25 tokens per second)
0.01.385.312 I llama_perf_context_print:       total time =     764.53 ms /    70 tokens
0.01.385.514 I ggml_metal_free: deallocating

real	0m1.405s
user	0m0.109s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.576 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.294 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.295 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.295 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.296 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.297 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.297 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.298 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.299 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.043 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.864 I llama_model_loader: - type  f32:  194 tensors
0.00.023.864 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.864 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.865 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.829 I llm_load_vocab: special tokens cache size = 25
0.00.050.899 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.902 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.902 I llm_load_print_meta: arch             = gptneox
0.00.050.902 I llm_load_print_meta: vocab type       = BPE
0.00.050.903 I llm_load_print_meta: n_vocab          = 50304
0.00.050.903 I llm_load_print_meta: n_merges         = 50009
0.00.050.903 I llm_load_print_meta: vocab_only       = 0
0.00.050.903 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.903 I llm_load_print_meta: n_embd           = 2048
0.00.050.904 I llm_load_print_meta: n_layer          = 24
0.00.050.906 I llm_load_print_meta: n_head           = 16
0.00.050.909 I llm_load_print_meta: n_head_kv        = 16
0.00.050.921 I llm_load_print_meta: n_rot            = 32
0.00.050.921 I llm_load_print_meta: n_swa            = 0
0.00.050.921 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.922 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.923 I llm_load_print_meta: n_gqa            = 1
0.00.050.924 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.924 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.925 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.925 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.925 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.925 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.926 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.926 I llm_load_print_meta: n_ff             = 8192
0.00.050.926 I llm_load_print_meta: n_expert         = 0
0.00.050.927 I llm_load_print_meta: n_expert_used    = 0
0.00.050.927 I llm_load_print_meta: causal attn      = 1
0.00.050.927 I llm_load_print_meta: pooling type     = 0
0.00.050.927 I llm_load_print_meta: rope type        = 2
0.00.050.927 I llm_load_print_meta: rope scaling     = linear
0.00.050.927 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.928 I llm_load_print_meta: freq_scale_train = 1
0.00.050.928 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.928 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.928 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.928 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.928 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.928 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.928 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.938 I llm_load_print_meta: model type       = 1.4B
0.00.050.938 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.939 I llm_load_print_meta: model params     = 1.41 B
0.00.050.939 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.939 I llm_load_print_meta: general.name     = 1.4B
0.00.050.939 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: LF token         = 128 ''
0.00.050.940 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.941 I llm_load_print_meta: max token length = 1024
0.00.052.931 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.931 I llm_load_tensors: offloading output layer to GPU
0.00.052.931 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.942 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.943 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.863 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.864 I llama_new_context_with_model: n_ctx         = 128
0.00.053.864 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.864 I llama_new_context_with_model: n_batch       = 128
0.00.053.864 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.865 I llama_new_context_with_model: flash_attn    = 0
0.00.053.865 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.865 I llama_new_context_with_model: freq_scale    = 1
0.00.053.866 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.866 I ggml_metal_init: allocating
0.00.053.869 I ggml_metal_init: found device: Apple M4
0.00.053.871 I ggml_metal_init: picking default device: Apple M4
0.00.054.431 I ggml_metal_init: using embedded metal library
0.00.056.729 I ggml_metal_init: GPU name:   Apple M4
0.00.056.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.731 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.732 I ggml_metal_init: simdgroup reduction   = true
0.00.056.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.732 I ggml_metal_init: has bfloat            = true
0.00.056.732 I ggml_metal_init: use bfloat            = true
0.00.056.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.656 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.658 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.672 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.628 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.629 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.630 I llama_new_context_with_model: graph nodes  = 967
0.00.068.630 I llama_new_context_with_model: graph splits = 2
0.00.068.644 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.535 I 
0.00.564.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.611 I perplexity: tokenizing the input ..
0.00.572.332 I perplexity: tokenization took 7.72 ms
0.00.572.343 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.706.079 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.707.308 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.707.317 I llama_perf_context_print:        load time =     554.95 ms
0.00.707.318 I llama_perf_context_print: prompt eval time =     133.51 ms /   128 tokens (    1.04 ms per token,   958.73 tokens per second)
0.00.707.319 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.320 I llama_perf_context_print:       total time =     142.78 ms /   129 tokens
0.00.707.834 I ggml_metal_free: deallocating

real	0m0.723s
user	0m0.078s
sys	0m0.102s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.015.790 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.864 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.033.869 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.875 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.875 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.876 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.877 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.879 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.883 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.415 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.417 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.417 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.417 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.418 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.418 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.045.419 I llama_model_loader: - type  f32:  194 tensors
0.00.045.419 I llama_model_loader: - type q5_K:   61 tensors
0.00.045.419 I llama_model_loader: - type q6_K:   37 tensors
0.00.078.778 I llm_load_vocab: special tokens cache size = 25
0.00.089.189 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.193 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.193 I llm_load_print_meta: arch             = gptneox
0.00.089.194 I llm_load_print_meta: vocab type       = BPE
0.00.089.194 I llm_load_print_meta: n_vocab          = 50304
0.00.089.194 I llm_load_print_meta: n_merges         = 50009
0.00.089.195 I llm_load_print_meta: vocab_only       = 0
0.00.089.195 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.195 I llm_load_print_meta: n_embd           = 2048
0.00.089.195 I llm_load_print_meta: n_layer          = 24
0.00.089.199 I llm_load_print_meta: n_head           = 16
0.00.089.200 I llm_load_print_meta: n_head_kv        = 16
0.00.089.213 I llm_load_print_meta: n_rot            = 32
0.00.089.213 I llm_load_print_meta: n_swa            = 0
0.00.089.213 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.213 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.214 I llm_load_print_meta: n_gqa            = 1
0.00.089.215 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.216 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.216 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.217 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.217 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.217 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.218 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.218 I llm_load_print_meta: n_ff             = 8192
0.00.089.219 I llm_load_print_meta: n_expert         = 0
0.00.089.219 I llm_load_print_meta: n_expert_used    = 0
0.00.089.219 I llm_load_print_meta: causal attn      = 1
0.00.089.219 I llm_load_print_meta: pooling type     = 0
0.00.089.219 I llm_load_print_meta: rope type        = 2
0.00.089.222 I llm_load_print_meta: rope scaling     = linear
0.00.089.222 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.223 I llm_load_print_meta: freq_scale_train = 1
0.00.089.223 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.223 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.223 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.224 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.224 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.224 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.234 I llm_load_print_meta: model type       = 1.4B
0.00.089.235 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.089.235 I llm_load_print_meta: model params     = 1.41 B
0.00.089.236 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.089.236 I llm_load_print_meta: general.name     = 1.4B
0.00.089.236 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.236 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.237 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.237 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.237 I llm_load_print_meta: LF token         = 128 ''
0.00.089.238 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.238 I llm_load_print_meta: max token length = 1024
0.00.091.997 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.997 I llm_load_tensors: offloading output layer to GPU
0.00.091.998 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.009 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.092.010 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.093.414 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.415 I llama_new_context_with_model: n_ctx         = 2048
0.00.093.416 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.093.416 I llama_new_context_with_model: n_batch       = 2048
0.00.093.416 I llama_new_context_with_model: n_ubatch      = 512
0.00.093.416 I llama_new_context_with_model: flash_attn    = 0
0.00.093.417 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.418 I llama_new_context_with_model: freq_scale    = 1
0.00.093.418 I ggml_metal_init: allocating
0.00.093.428 I ggml_metal_init: found device: Apple M4
0.00.093.431 I ggml_metal_init: picking default device: Apple M4
0.00.094.316 I ggml_metal_init: using embedded metal library
0.00.097.869 I ggml_metal_init: GPU name:   Apple M4
0.00.097.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.873 I ggml_metal_init: simdgroup reduction   = true
0.00.097.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.873 I ggml_metal_init: has bfloat            = true
0.00.097.873 I ggml_metal_init: use bfloat            = true
0.00.097.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.133.999 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.134.006 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.134.025 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.135.059 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.135.061 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.135.061 I llama_new_context_with_model: graph nodes  = 967
0.00.135.061 I llama_new_context_with_model: graph splits = 2
0.00.135.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.965.223 I main: llama threadpool init, n_threads = 4
0.00.965.302 I 
0.00.965.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.965.378 I 
0.00.965.888 I sampler seed: 1234
0.00.965.895 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.965.927 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.965.929 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.965.929 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.818.579 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.818.580 I llama_perf_context_print:        load time =     949.42 ms
0.01.818.581 I llama_perf_context_print: prompt eval time =      52.24 ms /     7 tokens (    7.46 ms per token,   134.01 tokens per second)
0.01.818.581 I llama_perf_context_print:        eval time =     797.40 ms /    63 runs   (   12.66 ms per token,    79.01 tokens per second)
0.01.818.582 I llama_perf_context_print:       total time =     853.37 ms /    70 tokens
0.01.818.775 I ggml_metal_free: deallocating

real	0m1.847s
user	0m0.143s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.626 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.424 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.434 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.171 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.196 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.016 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.017 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.018 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.018 I llama_model_loader: - type  f32:  194 tensors
0.00.023.019 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.019 I llama_model_loader: - type q6_K:   37 tensors
0.00.042.945 I llm_load_vocab: special tokens cache size = 25
0.00.048.907 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.910 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.910 I llm_load_print_meta: arch             = gptneox
0.00.048.911 I llm_load_print_meta: vocab type       = BPE
0.00.048.911 I llm_load_print_meta: n_vocab          = 50304
0.00.048.911 I llm_load_print_meta: n_merges         = 50009
0.00.048.911 I llm_load_print_meta: vocab_only       = 0
0.00.048.911 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.911 I llm_load_print_meta: n_embd           = 2048
0.00.048.912 I llm_load_print_meta: n_layer          = 24
0.00.048.914 I llm_load_print_meta: n_head           = 16
0.00.048.915 I llm_load_print_meta: n_head_kv        = 16
0.00.048.927 I llm_load_print_meta: n_rot            = 32
0.00.048.927 I llm_load_print_meta: n_swa            = 0
0.00.048.927 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.927 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.928 I llm_load_print_meta: n_gqa            = 1
0.00.048.929 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.929 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.930 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.930 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.931 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.931 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.931 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.933 I llm_load_print_meta: n_ff             = 8192
0.00.048.935 I llm_load_print_meta: n_expert         = 0
0.00.048.936 I llm_load_print_meta: n_expert_used    = 0
0.00.048.936 I llm_load_print_meta: causal attn      = 1
0.00.048.936 I llm_load_print_meta: pooling type     = 0
0.00.048.936 I llm_load_print_meta: rope type        = 2
0.00.048.936 I llm_load_print_meta: rope scaling     = linear
0.00.048.937 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.937 I llm_load_print_meta: freq_scale_train = 1
0.00.048.937 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.937 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.938 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.938 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.938 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.939 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.940 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.949 I llm_load_print_meta: model type       = 1.4B
0.00.048.950 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.048.950 I llm_load_print_meta: model params     = 1.41 B
0.00.048.951 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.048.951 I llm_load_print_meta: general.name     = 1.4B
0.00.048.951 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.952 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.952 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.952 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.952 I llm_load_print_meta: LF token         = 128 ''
0.00.048.953 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.953 I llm_load_print_meta: max token length = 1024
0.00.050.933 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.933 I llm_load_tensors: offloading output layer to GPU
0.00.050.934 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.944 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.945 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.861 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.861 I llama_new_context_with_model: n_ctx         = 128
0.00.051.861 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.861 I llama_new_context_with_model: n_batch       = 128
0.00.051.862 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.862 I llama_new_context_with_model: flash_attn    = 0
0.00.051.862 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.862 I llama_new_context_with_model: freq_scale    = 1
0.00.051.863 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.863 I ggml_metal_init: allocating
0.00.051.869 I ggml_metal_init: found device: Apple M4
0.00.051.873 I ggml_metal_init: picking default device: Apple M4
0.00.052.459 I ggml_metal_init: using embedded metal library
0.00.054.787 I ggml_metal_init: GPU name:   Apple M4
0.00.054.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.789 I ggml_metal_init: simdgroup reduction   = true
0.00.054.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.790 I ggml_metal_init: has bfloat            = true
0.00.054.790 I ggml_metal_init: use bfloat            = true
0.00.054.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.665 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.667 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.620 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.621 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.622 I llama_new_context_with_model: graph nodes  = 967
0.00.066.622 I llama_new_context_with_model: graph splits = 2
0.00.066.636 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.534 I 
0.00.667.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.574 I perplexity: tokenizing the input ..
0.00.675.250 I perplexity: tokenization took 7.674 ms
0.00.675.265 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.561 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.816.719 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.816.739 I llama_perf_context_print:        load time =     658.90 ms
0.00.816.740 I llama_perf_context_print: prompt eval time =     140.07 ms /   128 tokens (    1.09 ms per token,   913.82 tokens per second)
0.00.816.741 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.741 I llama_perf_context_print:       total time =     149.20 ms /   129 tokens
0.00.817.120 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.077s
sys	0m0.128s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.012.517 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.020.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.706 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.711 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.713 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.714 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.714 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.734 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.638 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.639 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.640 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.640 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.640 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.640 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.029.641 I llama_model_loader: - type  f32:  194 tensors
0.00.029.641 I llama_model_loader: - type q6_K:   98 tensors
0.00.049.970 I llm_load_vocab: special tokens cache size = 25
0.00.055.890 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.892 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.893 I llm_load_print_meta: arch             = gptneox
0.00.055.893 I llm_load_print_meta: vocab type       = BPE
0.00.055.893 I llm_load_print_meta: n_vocab          = 50304
0.00.055.894 I llm_load_print_meta: n_merges         = 50009
0.00.055.894 I llm_load_print_meta: vocab_only       = 0
0.00.055.894 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.894 I llm_load_print_meta: n_embd           = 2048
0.00.055.894 I llm_load_print_meta: n_layer          = 24
0.00.055.897 I llm_load_print_meta: n_head           = 16
0.00.055.897 I llm_load_print_meta: n_head_kv        = 16
0.00.055.909 I llm_load_print_meta: n_rot            = 32
0.00.055.909 I llm_load_print_meta: n_swa            = 0
0.00.055.909 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.909 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.911 I llm_load_print_meta: n_gqa            = 1
0.00.055.912 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.912 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.913 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.913 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.914 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.914 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.914 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.915 I llm_load_print_meta: n_ff             = 8192
0.00.055.915 I llm_load_print_meta: n_expert         = 0
0.00.055.915 I llm_load_print_meta: n_expert_used    = 0
0.00.055.915 I llm_load_print_meta: causal attn      = 1
0.00.055.916 I llm_load_print_meta: pooling type     = 0
0.00.055.917 I llm_load_print_meta: rope type        = 2
0.00.055.917 I llm_load_print_meta: rope scaling     = linear
0.00.055.918 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.918 I llm_load_print_meta: freq_scale_train = 1
0.00.055.918 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.918 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.918 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.918 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.919 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.919 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.920 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.929 I llm_load_print_meta: model type       = 1.4B
0.00.055.929 I llm_load_print_meta: model ftype      = Q6_K
0.00.055.930 I llm_load_print_meta: model params     = 1.41 B
0.00.055.931 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.055.931 I llm_load_print_meta: general.name     = 1.4B
0.00.055.931 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.931 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.931 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.932 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.932 I llm_load_print_meta: LF token         = 128 ''
0.00.055.932 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.932 I llm_load_print_meta: max token length = 1024
0.00.057.442 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.443 I llm_load_tensors: offloading output layer to GPU
0.00.057.443 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.453 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.057.454 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.058.280 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.280 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.281 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.281 I llama_new_context_with_model: n_batch       = 2048
0.00.058.281 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.281 I llama_new_context_with_model: flash_attn    = 0
0.00.058.282 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.282 I llama_new_context_with_model: freq_scale    = 1
0.00.058.282 I ggml_metal_init: allocating
0.00.058.288 I ggml_metal_init: found device: Apple M4
0.00.058.292 I ggml_metal_init: picking default device: Apple M4
0.00.058.894 I ggml_metal_init: using embedded metal library
0.00.061.262 I ggml_metal_init: GPU name:   Apple M4
0.00.061.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.264 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.264 I ggml_metal_init: simdgroup reduction   = true
0.00.061.265 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.266 I ggml_metal_init: has bfloat            = true
0.00.061.266 I ggml_metal_init: use bfloat            = true
0.00.061.266 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.267 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.769 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.774 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.792 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.959 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.961 I llama_new_context_with_model: graph nodes  = 967
0.00.092.961 I llama_new_context_with_model: graph splits = 2
0.00.092.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.132 I main: llama threadpool init, n_threads = 4
0.00.879.172 I 
0.00.879.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.200 I 
0.00.879.436 I sampler seed: 1234
0.00.879.441 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.879.480 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.879.480 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.879.480 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.762.946 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.762.947 I llama_perf_context_print:        load time =     866.61 ms
0.01.762.948 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.62 tokens per second)
0.01.762.948 I llama_perf_context_print:        eval time =     825.99 ms /    63 runs   (   13.11 ms per token,    76.27 tokens per second)
0.01.762.949 I llama_perf_context_print:       total time =     883.82 ms /    70 tokens
0.01.763.138 I ggml_metal_free: deallocating

real	0m1.784s
user	0m0.110s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4281 (c2a16c0b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.443 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.886 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.665 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.665 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.666 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.666 I llama_model_loader: - type  f32:  194 tensors
0.00.024.667 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.509 I llm_load_vocab: special tokens cache size = 25
0.00.050.475 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.478 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.478 I llm_load_print_meta: arch             = gptneox
0.00.050.478 I llm_load_print_meta: vocab type       = BPE
0.00.050.479 I llm_load_print_meta: n_vocab          = 50304
0.00.050.479 I llm_load_print_meta: n_merges         = 50009
0.00.050.479 I llm_load_print_meta: vocab_only       = 0
0.00.050.479 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.479 I llm_load_print_meta: n_embd           = 2048
0.00.050.480 I llm_load_print_meta: n_layer          = 24
0.00.050.483 I llm_load_print_meta: n_head           = 16
0.00.050.484 I llm_load_print_meta: n_head_kv        = 16
0.00.050.490 I llm_load_print_meta: n_rot            = 32
0.00.050.491 I llm_load_print_meta: n_swa            = 0
0.00.050.491 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.491 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.492 I llm_load_print_meta: n_gqa            = 1
0.00.050.492 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.493 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.494 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.494 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.494 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.494 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.495 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.495 I llm_load_print_meta: n_ff             = 8192
0.00.050.496 I llm_load_print_meta: n_expert         = 0
0.00.050.496 I llm_load_print_meta: n_expert_used    = 0
0.00.050.496 I llm_load_print_meta: causal attn      = 1
0.00.050.496 I llm_load_print_meta: pooling type     = 0
0.00.050.496 I llm_load_print_meta: rope type        = 2
0.00.050.496 I llm_load_print_meta: rope scaling     = linear
0.00.050.497 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.497 I llm_load_print_meta: freq_scale_train = 1
0.00.050.497 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.497 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.498 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.498 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.498 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.498 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.498 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.503 I llm_load_print_meta: model type       = 1.4B
0.00.050.503 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.503 I llm_load_print_meta: model params     = 1.41 B
0.00.050.504 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.506 I llm_load_print_meta: general.name     = 1.4B
0.00.050.506 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.507 I llm_load_print_meta: LF token         = 128 ''
0.00.050.507 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.507 I llm_load_print_meta: max token length = 1024
0.00.052.279 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.279 I llm_load_tensors: offloading output layer to GPU
0.00.052.280 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.285 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.285 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.182 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.183 I llama_new_context_with_model: n_ctx         = 128
0.00.053.184 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.184 I llama_new_context_with_model: n_batch       = 128
0.00.053.184 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.184 I llama_new_context_with_model: flash_attn    = 0
0.00.053.184 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.185 I llama_new_context_with_model: freq_scale    = 1
0.00.053.185 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.186 I ggml_metal_init: allocating
0.00.053.189 I ggml_metal_init: found device: Apple M4
0.00.053.191 I ggml_metal_init: picking default device: Apple M4
0.00.053.745 I ggml_metal_init: using embedded metal library
0.00.056.022 I ggml_metal_init: GPU name:   Apple M4
0.00.056.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.023 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.024 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.024 I ggml_metal_init: simdgroup reduction   = true
0.00.056.024 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.024 I ggml_metal_init: has bfloat            = true
0.00.056.025 I ggml_metal_init: use bfloat            = true
0.00.056.025 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.672 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.677 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.691 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.610 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.611 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.611 I llama_new_context_with_model: graph nodes  = 967
0.00.067.611 I llama_new_context_with_model: graph splits = 2
0.00.067.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.142.601 I 
0.00.142.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.142.642 I perplexity: tokenizing the input ..
0.00.149.844 I perplexity: tokenization took 7.201 ms
0.00.149.855 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.289.321 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.290.611 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.290.624 I llama_perf_context_print:        load time =     132.16 ms
0.00.290.625 I llama_perf_context_print: prompt eval time =     139.24 ms /   128 tokens (    1.09 ms per token,   919.28 tokens per second)
0.00.290.626 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.290.626 I llama_perf_context_print:       total time =     148.02 ms /   129 tokens
0.00.291.022 I ggml_metal_free: deallocating

real	0m0.305s
user	0m0.076s
sys	0m0.040s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4281 (c2a16c0b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f70a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f70a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f70ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f70b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f70b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f70bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f70c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f70cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f70d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f70d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f70dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f70dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f70ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f70f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f70fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f7101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f7108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f710ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f711710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f711ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f712600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f712d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f713440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f713ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f714400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f7146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f714cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f715940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f715e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f716140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f7165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f7168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f717130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f717670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f717930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f717dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f718270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f718710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f718bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f719050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f7194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f719990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f719e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f71a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f71a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f71aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f71b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f71bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f71c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f71c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f71cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f71d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f71d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f71df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f71e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f71ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f71f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f71f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f71f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f720120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f7203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f720880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f720d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f7211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f721660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f721b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f721fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f722440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f7228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f722d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f723220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f7236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f7240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f724600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f724b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f7250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f7255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f725b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f726090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f7265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f726b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f727080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f7275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f727b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f728070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f7285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f728b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f729060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f7295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f729b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f72a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f72a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f72aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f72b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f72b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f72bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f71b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f72bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f72c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f72cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f72d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f72d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f72dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f72e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f72e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f72ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f72f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f72f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f72fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f730170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f7306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f730c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f7310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f731550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f7319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f731e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f732330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f7327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f732c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f733110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f7335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f733a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f733ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f734390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f734830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f735170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f735610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f735ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f735f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f7363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f736890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f736d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f7371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f737670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f737b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f737fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f738450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f7388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f738d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f739230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f7396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f739b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f73a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f73a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f73a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f73adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f73b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f73b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f73bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f73c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f73c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f73c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f73ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f73d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f73d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f73dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f73e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f73e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f73ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f73eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f73f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f73f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f73fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f740130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f7405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f740a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f740f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f7413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f741850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f741cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f742190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f742630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f742ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f742f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f743410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f7438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f743d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f7441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f744690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f744b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f744fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f745470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f745910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f745db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f746250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f7466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f746b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f747030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f7474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f747970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f747e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f748360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f7488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f748e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f749350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f749610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f749c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f74a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f74a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f74b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f74b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f74b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f74bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f74c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f74cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f74d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f74d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f74d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f74e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f74e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f74ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f74f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f74f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f74fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f750110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f750660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f750bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f751100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f751650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f751ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f7520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f752640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f752b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f7530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f753630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f753b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f7540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f754620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f754b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f7550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f755610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f755b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f7560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f756600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f756b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f7570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f7575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f757b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f758090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f7585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f758b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f759080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f7595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f759b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f75a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f75a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f75ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f75b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f75b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f75bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f75c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f75c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f75caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f75d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f75d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f75dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f75e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f75e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f75ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f75f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f75f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f75fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f760010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f760560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f760ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f760f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f7613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f761890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f761d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f7621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f762670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f762b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f762fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f763450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f7638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f763d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f764230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f7646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f764b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f765010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f765560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f765c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f7663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f766ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f7671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f7674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f767c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f767f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f768560 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.169.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f604ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f605150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f6055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f605a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f605ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f606310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f606780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f606bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f607060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f6074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f607940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f607fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f608ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f609270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f609a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f60a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f60a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f60afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f60b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f60bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f60c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f60cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f60d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f60db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f60e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f60e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f60e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f60ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f60f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f60f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f60f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f60fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f610350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f610610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f610a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f610ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f611360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f6117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f611c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f6120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f612520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f612990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f612e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f613270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f6136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f613b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f614430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f6148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f6155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f615a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f615ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f616340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f6167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f616d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f617220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f617f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f6183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f618850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f618cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f619130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f6195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f619a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f619e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f61a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f61a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f61abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f61b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f61b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f61b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f61bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f61c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f61c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f61cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f61cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f61d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f61d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f61dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f61e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f61e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f61e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f61ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f61f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f61f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f61fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f620020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f620490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f620900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f620d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f6211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f621650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f621ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f621f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f6223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f622810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f6230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f623560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f6239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f623e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f6242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f624720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f625000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f625470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f6258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f625d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f6261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f626630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f626aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f626f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f627380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f6277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f627c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f6280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f628540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f6289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f628e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f629290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f629700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f629b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f629fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f62a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f62a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f62ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f62b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f62b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f62ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f62bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f62c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f62c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f62cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f62d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f62d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f62d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f62de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f62e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f62e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f62eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f62efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f62f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f62f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f62fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f630180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f6305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f630a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f630ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f631340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f6317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f631c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f632090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f632500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f632970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f632de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f633250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f6336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f633b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f633fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f634410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f634880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f634cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f635160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f6355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f635a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f635eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f636320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f636790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f636c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f637070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f6374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f637950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f637dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f638230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f6386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f638b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f638f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f6393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f639860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f63a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f63a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f63aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f63ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f63b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f63b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f63bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f63c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f63c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f63c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f63cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f63d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f63d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f63daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f63df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f63e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f63e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f63ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f63f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f63f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f63fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f63fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f6402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f640750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f640ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f641150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f6415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f642110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f6423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f642b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f642f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f6433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f643850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f643cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f644130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f6445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f644a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f644e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f6452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f645760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f645bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f646040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f6464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f646920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f646d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f647200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f647670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f647ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f647f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f6483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f648830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f648ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f649110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f649580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f6499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f649e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f64a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f64a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f64abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f64b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f64b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f64b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f64bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f64c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f64c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f64cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f64cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f64d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f64d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f64dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f64e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f64e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f64e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f64ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f64f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f64f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f64fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f650000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f650470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f6508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f650d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f6511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f651630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f651aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f651f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f652380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f6527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f652c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f6530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f653540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f6539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f653e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f654290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f654700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f654b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f654fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f655450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f6558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f655d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f6567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f656ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f6575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f657d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f657fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f658430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f658a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f659040 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f70b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f70bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f70c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f70c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f70ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f70ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f70d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f70d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f70dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f70e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f70e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f70ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f70f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f70faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f7102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f7109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f7110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f7117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f711e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f712810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f712f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f7135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f713ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f7143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f714ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f714f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f7153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f715810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f715c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f7160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f7169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f716e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f717100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f717570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f7179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f717e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f7182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f718730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f718ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f719010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f719480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f7198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f719d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f71a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f71a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f71aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f71af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f71b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f71b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f71bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f71c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f71c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f71c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f71ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f71d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f71d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f71db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f71dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f71e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f71e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f71ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f71f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f71f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f71fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f71ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f720370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f7207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f720c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f7210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f721530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f7219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f721e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f722280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f7226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f722b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f722fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f723440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f7238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f723d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f724190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f724600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f724a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f724ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f725350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f7257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f725c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f7260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f726510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f726980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f726df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f727260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f7276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f727b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f727fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f728420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f728890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f728d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f729170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f7295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f729a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f729ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f72a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f72a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f72ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f72b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f72b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f72b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f72bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f72c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f72c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f72cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f72cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f72d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f72d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f72dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f72e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f72e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f72ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f72eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f72f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f72f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f72fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f730060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f7304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f730940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f730db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f731220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f731690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f731b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f731f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f7323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f732850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f732cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f733130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f7335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f733a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f733e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f7342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f734760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f734bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f735040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f7354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f735920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f735d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f736200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f736670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f736ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f736f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f7373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f737830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f737ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f738110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f738580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f7389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f738e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f7392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f739740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f73a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f73a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f73a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f73ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f73b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f73b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f73bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f73bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f73c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f73c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f73cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f73d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f73d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f73d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f73de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f73e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f73e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f73eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f73f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f73f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f73f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f73fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f7401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f740630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f740aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f740f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f7417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f741c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f7420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f7429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f742e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f743290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f743700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f743b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f744450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f7448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f744d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f7451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f745610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f745a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f745ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f746360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f7467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f746c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f7470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f747520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f747990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f747e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f748580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f7489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f748e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f7492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f749740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f749bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f74a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f74a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f74a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f74ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f74b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f74b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f74bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f74bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f74c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f74c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f74cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f74d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f74d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f74d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f74de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f74e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f74e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f74eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f74f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f74f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f74f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f74fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f7501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f750630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f750aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f750f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f751380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f7517f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f751c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f7520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f752540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f7529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f752e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f753290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f753700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f753b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f753fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f754450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f7548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f754d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f7551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f755610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f755a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f756360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f7567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f756c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f7570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f757520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f757990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f757e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f758270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f7586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f758b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f758fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f759430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f7598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f759d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f75a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f75a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f75aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f75aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f75b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f75b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f75bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f75c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f75c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f75cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f75d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f75db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f75e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f75e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f75eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f75ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f75f3f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.852s
user	0m0.293s
sys	0m0.306s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4281 (c2a16c0b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e60e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e60ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e60f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e60f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e60fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e610340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e6108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e610ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e611450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e611950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e611e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e612350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e612e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e613620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e613e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e614550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e614c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e615390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e615ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e616280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e6169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e6170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e6177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e618080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e6187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e618a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e619070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e619ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e61a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e61a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e61a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e61ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e61b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e61ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e61bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e61c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e61c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e61cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e61cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e61d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e61d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e61dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e61e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e61e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e61e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e61ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e61f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e61fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e620480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e6210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e6216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e621cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e6222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e622ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e622f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e623400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e6236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e623cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e6244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e624780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e624c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e6250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e625560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e625a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e625ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e626340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e6267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e626c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e627120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e6275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e627a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e627f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e628450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e6289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e629440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e629990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e629ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e62a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e62a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e62aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e62b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e62b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e62bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e62c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e62c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e62ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e62d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e62d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e62dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e62e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e62e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e62ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e62f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e62f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e62fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e61fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e6302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e630aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e630ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e631540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e631a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e631fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e632530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e632a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e632fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e633520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e633a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e634510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e634a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e634fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e635450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e6358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e635d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e636230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e6366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e636b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e637010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e6374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e637950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e637df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e638290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e638730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e638bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e639070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e639510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e6399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e639e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e63a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e63a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e63ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e63b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e63b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e63ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e63beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e63c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e63c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e63cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e63d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e63d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e63da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e63df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e63e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e63e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e63ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e63f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e63f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e63fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e63ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e640410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e6408b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e640d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e6411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e641690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e641b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e641fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e642470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e642910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e642db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e643250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e6436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e643b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e644030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e6444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e644970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e644e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e6452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e645750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e645bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e646090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e646530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e6469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e646e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e647310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e6477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e647c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e6480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e648590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e648a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e648ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e649370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e649810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e649cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e64a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e64a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e64aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e64af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e64b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e64b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e64bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e64c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e64c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e64cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e64d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e64d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e64d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e64dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e64e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e64ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e64f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e64f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e64fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e650140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e650750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e650f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e6513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e651880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e651d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e6524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e652a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e652f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e6534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e653a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e653f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e6544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e654a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e654f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e6554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e6559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e655f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e656490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e6569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e656f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e657480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e6579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e657f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e658470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e6589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e658f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e659460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e6599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e659f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e65a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e65a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e65aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e65b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e65b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e65bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e65c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e65c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e65ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e65d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e65d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e65dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e65e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e65e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e65eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e65f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e65f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e65fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e6603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e660940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e660e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e6613e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e661930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e661e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e6623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e662920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e662e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e6633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e663910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e663e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e6643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e664900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e664e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e6652f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e665790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e665c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e6660d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e666570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e666a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e666eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e667350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e6677f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e667c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e668130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e6685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e668a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e668f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e6693b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e669900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e66a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e66a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e66ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e66b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e66b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e66c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e66c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e66c900 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e707810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e707c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e7080f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e708560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e7089d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e708e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e7092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e709720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e709b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e70a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e70a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e70aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e70b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e70bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e70c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e70cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e70d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e70db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e70e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e70ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e70f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e70f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e70ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e710690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e710db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e711070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e711330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e7117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e711c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e712080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e7124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e712a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e712e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e713150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e7135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e713a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e713ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e714310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e714780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e714bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e715060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e7154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e715940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e715db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e716220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e716690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e716b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e716f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e7173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e717850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e717cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e718130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e7185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e718a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e718e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e7192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e719860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e719d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e71a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e71a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e71aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e71af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e71b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e71b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e71bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e71c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e71c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e71c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e71ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e71d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e71d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e71db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e71dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e71e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e71e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e71ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e71f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e71f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e71fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e71ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e720370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e7207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e720c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e7210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e721530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e7219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e721e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e722280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e7226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e722b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e722fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e723440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e7238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e723d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e724190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e724600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e724a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e724ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e725350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e7257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e725c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e7260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e726510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e726980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e726df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e727260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e7276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e727b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e727fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e728420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e728890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e728d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e729170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e7295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e729a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e729ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e72a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e72a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e72ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e72b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e72b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e72b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e72bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e72c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e72c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e72cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e72cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e72d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e72d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e72dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e72e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e72e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e72ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e72eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e72f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e72f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e72fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e730060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e7304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e730940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e730db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e731220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e731690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e731b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e731f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e7323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e732850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e732cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e733130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e7335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e733a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e733e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e7342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e734760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e734bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e735040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e7354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e735920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e735d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e736200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e736670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e736ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e736f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e7373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e737830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e737ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e738110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e738580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e7389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e738e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e7392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e739740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e73a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e73a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e73a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e73ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e73b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e73b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e73bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e73bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e73c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e73c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e73cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e73d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e73d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e73d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e73de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e73e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e73e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e73eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e73f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e73f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e73f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e73fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e7401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e740630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e740aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e740f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e7417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e741c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e7420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e7429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e742e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e743290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e743820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e743c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e744100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e744c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e744f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e7451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e745640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e745ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e745f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e746390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e746800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e746c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e7470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e747550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e7479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e747e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e7482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e748710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e748b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e748ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e749460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e7498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e749d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e74a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e74a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e74aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e74af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e74b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e74b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e74bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e74c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e74c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e74c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e74ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e74d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e74d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e74db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e74dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e74e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e74e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e74ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e74f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e74f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e74fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e74fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e750350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e7507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e750c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e7510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e751510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e751980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e751df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e752260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e7526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e752b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e752fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e753420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e753890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e753d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e754170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e7545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e754a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e754ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e755330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e7557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e755c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e756080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e7564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e756960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e756dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e757240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e7576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e757b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e757f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e758400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e758870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e7592e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e759a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e75a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e75a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e75ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e75af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e75b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e75bb80 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e7046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e704b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e704fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e705430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e7058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e705d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e706180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e7065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e706a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e706ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e707340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e7079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e708500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e708cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e7094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e709be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e70a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e70aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e70b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e70b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e70c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e70c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e70ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e70d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e70dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e70df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e70e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e70e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e70eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e70ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e70f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e70f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e70fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e710050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e7104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e710930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e710da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e711210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e711680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e711af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e711f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e7123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e712840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e712cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e713120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e713590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e713a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e713e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e7142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e714750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e714bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e715030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e7154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e715910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e715d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e7161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e716760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e716c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e7170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e717540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e7179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e717e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e718290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e718700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e718b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e718fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e719450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e7198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e719d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e71a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e71a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e71aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e71aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e71b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e71b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e71bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e71c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e71c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e71c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e71ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e71d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e71d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e71db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e71dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e71e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e71e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e71ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e71f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e71f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e71fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e71fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e720340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e7207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e720c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e721090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e721500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e721970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e721de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e722250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e7226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e722b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e722fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e723410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e723880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e723cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e724160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e7245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e724a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e724eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e725320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e725790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e725c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e726070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e7264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e726950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e726dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e727230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e7276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e727b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e727f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e7283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e728860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e728cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e729140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e7295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e729a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e72a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e72a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e72abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e72b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e72b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e72b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e72bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e72c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e72c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e72caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e72cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e72d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e72d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e72dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e72e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e72e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e72ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e72ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e72f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e72f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e72fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e730030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e7304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e730910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e730d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e7311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e731660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e731ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e731f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e7323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e732820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e732c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e733100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e733570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e7339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e733e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e7342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e734730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e734ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e735010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e735480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e7358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e735d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e7361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e736640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e736ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e736f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e737390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e737800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e737c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e7380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e738550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e7389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e738e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e7392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e739b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e739ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e73a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e73a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e73ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e73b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e73b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e73ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e73bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e73c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e73c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e73cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e73d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e73d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e73d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e73de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e73e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e73e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e73eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e73efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e73f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e73f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e73fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e740190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e740720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e740b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e741000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e741b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e741e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e7420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e7429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e742e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e743290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e743700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e743b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e744450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e7448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e744d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e7451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e745610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e745a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e745ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e746360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e7467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e746c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e7470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e747520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e747990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e747e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e748270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e7486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e748b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e748fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e749430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e7498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e749d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e74a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e74a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e74aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e74b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e74b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e74bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e74bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e74c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e74c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e74ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e74d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e74d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e74da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e74de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e74e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e74e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e74ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e74f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e74f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e74f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e74fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e750200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e750670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e750ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e750f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e7513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e751830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e751ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e752110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e752580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e7529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e752e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e7532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e753740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e753bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e754020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e754490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e754900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e754d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e7551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e755ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e756530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e756c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e757370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e757a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e757d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e7581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e7587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e758dd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.933s
user	0m0.244s
sys	0m0.140s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.38 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.63 sec*proc (2 tests)

Total Test time (real) =   0.64 sec
        0.64 real         0.15 user         0.05 sys
```
