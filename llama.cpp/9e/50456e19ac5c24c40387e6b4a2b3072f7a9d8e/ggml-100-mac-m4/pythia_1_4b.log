Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.665s
user	0m0.843s
sys	0m1.297s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target sha1
[  5%] Built target build_info
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target xxhash
[  6%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target test-c
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-chat
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-sampling
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-log
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Built target test-chat
[ 54%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-arg-parser
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Built target test-quantize-perf
[ 67%] Built target test-rope
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-infill
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-bench
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-parallel
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookahead
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-passkey
[ 83%] Built target llama-perplexity
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-run
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-gen-docs
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.321s
user	0m6.614s
sys	0m10.101s

main: quantize time =  5831.69 ms
main:    total time =  5831.69 ms

main: quantize time =  3530.23 ms
main:    total time =  3530.23 ms

main: quantize time =  3920.64 ms
main:    total time =  3920.64 ms

main: quantize time =  3307.95 ms
main:    total time =  3307.95 ms

main: quantize time =  2501.08 ms
main:    total time =  2501.08 ms

main: quantize time =  5437.67 ms
main:    total time =  5437.67 ms

main: quantize time =  5656.03 ms
main:    total time =  5656.03 ms

main: quantize time =  6866.29 ms
main:    total time =  6866.29 ms

main: quantize time =  6042.54 ms
main:    total time =  6042.54 ms

main: quantize time =  4509.61 ms
main:    total time =  4509.61 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.147 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.328 I main: llama backend init
0.00.000.334 I main: load the model and apply lora adapter, if any
0.00.054.169 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.066.547 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.563 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.566 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.569 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.569 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.570 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.577 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.581 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.073.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.075.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.345 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.082.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.351 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.352 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.352 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.354 I llama_model_loader: - type  f32:  194 tensors
0.00.082.354 I llama_model_loader: - type  f16:   98 tensors
0.00.082.356 I print_info: file format = GGUF V3 (latest)
0.00.082.357 I print_info: file type   = all F32 (guessed)
0.00.082.360 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.100.889 I load: special tokens cache size = 25
0.00.111.326 I load: token to piece cache size = 0.2984 MB
0.00.111.356 I print_info: arch             = gptneox
0.00.111.357 I print_info: vocab_only       = 0
0.00.111.358 I print_info: n_ctx_train      = 2048
0.00.111.358 I print_info: n_embd           = 2048
0.00.111.358 I print_info: n_layer          = 24
0.00.111.364 I print_info: n_head           = 16
0.00.111.365 I print_info: n_head_kv        = 16
0.00.111.365 I print_info: n_rot            = 32
0.00.111.366 I print_info: n_swa            = 0
0.00.111.366 I print_info: n_embd_head_k    = 128
0.00.111.366 I print_info: n_embd_head_v    = 128
0.00.111.367 I print_info: n_gqa            = 1
0.00.111.368 I print_info: n_embd_k_gqa     = 2048
0.00.111.369 I print_info: n_embd_v_gqa     = 2048
0.00.111.372 I print_info: f_norm_eps       = 1.0e-05
0.00.111.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.111.373 I print_info: f_clamp_kqv      = 0.0e+00
0.00.111.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.111.384 I print_info: f_logit_scale    = 0.0e+00
0.00.111.394 I print_info: n_ff             = 8192
0.00.111.397 I print_info: n_expert         = 0
0.00.111.397 I print_info: n_expert_used    = 0
0.00.111.397 I print_info: causal attn      = 1
0.00.111.397 I print_info: pooling type     = 0
0.00.111.397 I print_info: rope type        = 2
0.00.111.398 I print_info: rope scaling     = linear
0.00.111.398 I print_info: freq_base_train  = 10000.0
0.00.111.399 I print_info: freq_scale_train = 1
0.00.111.399 I print_info: n_ctx_orig_yarn  = 2048
0.00.111.399 I print_info: rope_finetuned   = unknown
0.00.111.400 I print_info: ssm_d_conv       = 0
0.00.111.400 I print_info: ssm_d_inner      = 0
0.00.111.400 I print_info: ssm_d_state      = 0
0.00.111.400 I print_info: ssm_dt_rank      = 0
0.00.111.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.111.401 I print_info: model type       = 1.4B
0.00.111.401 I print_info: model params     = 1.41 B
0.00.111.401 I print_info: general.name     = 1.4B
0.00.111.402 I print_info: vocab type       = BPE
0.00.111.402 I print_info: n_vocab          = 50304
0.00.111.402 I print_info: n_merges         = 50009
0.00.111.403 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.111.403 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.111.403 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.111.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.111.404 I print_info: LF token         = 187 'Ċ'
0.00.111.404 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.111.404 I print_info: max token length = 1024
0.00.111.405 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.150.140 I load_tensors: offloading 24 repeating layers to GPU
0.00.150.144 I load_tensors: offloading output layer to GPU
0.00.150.145 I load_tensors: offloaded 25/25 layers to GPU
0.00.150.167 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.150.168 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.150.553 I llama_context: n_seq_max     = 1
0.00.150.554 I llama_context: n_ctx         = 2048
0.00.150.554 I llama_context: n_ctx_per_seq = 2048
0.00.150.555 I llama_context: n_batch       = 2048
0.00.150.555 I llama_context: n_ubatch      = 512
0.00.150.555 I llama_context: flash_attn    = 0
0.00.150.556 I llama_context: freq_base     = 10000.0
0.00.150.556 I llama_context: freq_scale    = 1
0.00.150.557 I ggml_metal_init: allocating
0.00.150.583 I ggml_metal_init: found device: Apple M4
0.00.150.589 I ggml_metal_init: picking default device: Apple M4
0.00.151.240 I ggml_metal_init: using embedded metal library
0.00.367.833 I ggml_metal_init: GPU name:   Apple M4
0.00.367.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.367.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.367.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.367.847 I ggml_metal_init: simdgroup reduction   = true
0.00.367.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.367.847 I ggml_metal_init: has residency sets    = true
0.00.367.847 I ggml_metal_init: has bfloat            = true
0.00.367.848 I ggml_metal_init: use bfloat            = true
0.00.367.853 I ggml_metal_init: hasUnifiedMemory      = true
0.00.367.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.405.907 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.405.912 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.445.853 I init:      Metal KV buffer size =   384.00 MiB
0.00.445.865 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.449.757 I init:      Metal compute buffer size =   102.25 MiB
0.00.449.759 I init:        CPU compute buffer size =     8.01 MiB
0.00.449.759 I init: graph nodes  = 967
0.00.449.759 I init: graph splits = 2
0.00.449.763 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.449.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.449.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.150 I main: llama threadpool init, n_threads = 4
0.00.519.187 I 
0.00.519.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.519.201 I 
0.00.519.242 I sampler seed: 1234
0.00.519.247 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.519.272 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.519.274 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.519.274 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.349.644 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56349.21 tokens per second)
0.02.349.644 I llama_perf_context_print:        load time =     464.13 ms
0.02.349.645 I llama_perf_context_print: prompt eval time =      44.03 ms /     7 tokens (    6.29 ms per token,   158.99 tokens per second)
0.02.349.646 I llama_perf_context_print:        eval time =    1783.34 ms /    63 runs   (   28.31 ms per token,    35.33 tokens per second)
0.02.349.646 I llama_perf_context_print:       total time =    1831.34 ms /    70 tokens
0.02.353.565 I ggml_metal_free: deallocating

real	0m2.698s
user	0m0.144s
sys	0m0.154s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.362 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.363 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.363 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.364 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.365 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.366 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.367 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.367 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.369 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.369 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.370 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.283 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.469 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.699 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.703 I llama_model_loader: - type  f32:  194 tensors
0.00.038.703 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.704 I print_info: file format = GGUF V3 (latest)
0.00.038.704 I print_info: file type   = Q8_0
0.00.038.705 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.817 I load: special tokens cache size = 25
0.00.055.082 I load: token to piece cache size = 0.2984 MB
0.00.055.097 I print_info: arch             = gptneox
0.00.055.098 I print_info: vocab_only       = 0
0.00.055.099 I print_info: n_ctx_train      = 2048
0.00.055.099 I print_info: n_embd           = 2048
0.00.055.099 I print_info: n_layer          = 24
0.00.055.104 I print_info: n_head           = 16
0.00.055.105 I print_info: n_head_kv        = 16
0.00.055.105 I print_info: n_rot            = 32
0.00.055.105 I print_info: n_swa            = 0
0.00.055.105 I print_info: n_embd_head_k    = 128
0.00.055.105 I print_info: n_embd_head_v    = 128
0.00.055.108 I print_info: n_gqa            = 1
0.00.055.109 I print_info: n_embd_k_gqa     = 2048
0.00.055.110 I print_info: n_embd_v_gqa     = 2048
0.00.055.111 I print_info: f_norm_eps       = 1.0e-05
0.00.055.111 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.111 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.112 I print_info: f_logit_scale    = 0.0e+00
0.00.055.112 I print_info: n_ff             = 8192
0.00.055.113 I print_info: n_expert         = 0
0.00.055.113 I print_info: n_expert_used    = 0
0.00.055.113 I print_info: causal attn      = 1
0.00.055.113 I print_info: pooling type     = 0
0.00.055.113 I print_info: rope type        = 2
0.00.055.114 I print_info: rope scaling     = linear
0.00.055.116 I print_info: freq_base_train  = 10000.0
0.00.055.116 I print_info: freq_scale_train = 1
0.00.055.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.117 I print_info: rope_finetuned   = unknown
0.00.055.117 I print_info: ssm_d_conv       = 0
0.00.055.117 I print_info: ssm_d_inner      = 0
0.00.055.117 I print_info: ssm_d_state      = 0
0.00.055.117 I print_info: ssm_dt_rank      = 0
0.00.055.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.118 I print_info: model type       = 1.4B
0.00.055.119 I print_info: model params     = 1.41 B
0.00.055.120 I print_info: general.name     = 1.4B
0.00.055.120 I print_info: vocab type       = BPE
0.00.055.121 I print_info: n_vocab          = 50304
0.00.055.121 I print_info: n_merges         = 50009
0.00.055.121 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.121 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.121 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.122 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.122 I print_info: LF token         = 187 'Ċ'
0.00.055.122 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.122 I print_info: max token length = 1024
0.00.055.123 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.999.752 I load_tensors: offloading 24 repeating layers to GPU
0.00.999.757 I load_tensors: offloading output layer to GPU
0.00.999.758 I load_tensors: offloaded 25/25 layers to GPU
0.00.999.780 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.999.781 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.001.089 I llama_context: n_seq_max     = 1
0.01.001.091 I llama_context: n_ctx         = 2048
0.01.001.091 I llama_context: n_ctx_per_seq = 2048
0.01.001.092 I llama_context: n_batch       = 2048
0.01.001.092 I llama_context: n_ubatch      = 512
0.01.001.092 I llama_context: flash_attn    = 0
0.01.001.093 I llama_context: freq_base     = 10000.0
0.01.001.094 I llama_context: freq_scale    = 1
0.01.001.094 I ggml_metal_init: allocating
0.01.001.102 I ggml_metal_init: found device: Apple M4
0.01.001.109 I ggml_metal_init: picking default device: Apple M4
0.01.002.437 I ggml_metal_init: using embedded metal library
0.01.008.199 I ggml_metal_init: GPU name:   Apple M4
0.01.008.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.008.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.008.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.008.204 I ggml_metal_init: simdgroup reduction   = true
0.01.008.204 I ggml_metal_init: simdgroup matrix mul. = true
0.01.008.205 I ggml_metal_init: has residency sets    = true
0.01.008.205 I ggml_metal_init: has bfloat            = true
0.01.008.205 I ggml_metal_init: use bfloat            = true
0.01.008.206 I ggml_metal_init: hasUnifiedMemory      = true
0.01.008.207 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.027.230 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.027.234 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.080.721 I init:      Metal KV buffer size =   384.00 MiB
0.01.080.728 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.085.732 I init:      Metal compute buffer size =   102.25 MiB
0.01.085.734 I init:        CPU compute buffer size =     8.01 MiB
0.01.085.734 I init: graph nodes  = 967
0.01.085.735 I init: graph splits = 2
0.01.085.742 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.085.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.085.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.139.211 I main: llama threadpool init, n_threads = 4
0.01.139.253 I 
0.01.139.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.139.267 I 
0.01.139.436 I sampler seed: 1234
0.01.139.441 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.139.453 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.139.453 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.139.453 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.248.663 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48763.74 tokens per second)
0.02.248.664 I llama_perf_context_print:        load time =    1128.67 ms
0.02.248.665 I llama_perf_context_print: prompt eval time =      48.93 ms /     7 tokens (    6.99 ms per token,   143.05 tokens per second)
0.02.248.666 I llama_perf_context_print:        eval time =    1057.78 ms /    63 runs   (   16.79 ms per token,    59.56 tokens per second)
0.02.248.666 I llama_perf_context_print:       total time =    1110.15 ms /    70 tokens
0.02.251.507 I ggml_metal_free: deallocating

real	0m2.267s
user	0m0.111s
sys	0m0.249s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.981 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.651 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.654 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.655 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.655 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.656 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.656 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.657 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.657 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.658 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.681 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.585 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.585 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.586 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.586 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.587 I llama_model_loader: - type  f32:  194 tensors
0.00.027.587 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.588 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.588 I print_info: file format = GGUF V3 (latest)
0.00.027.589 I print_info: file type   = Q4_0
0.00.027.590 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.121 I load: special tokens cache size = 25
0.00.042.187 I load: token to piece cache size = 0.2984 MB
0.00.042.208 I print_info: arch             = gptneox
0.00.042.209 I print_info: vocab_only       = 0
0.00.042.209 I print_info: n_ctx_train      = 2048
0.00.042.210 I print_info: n_embd           = 2048
0.00.042.210 I print_info: n_layer          = 24
0.00.042.214 I print_info: n_head           = 16
0.00.042.215 I print_info: n_head_kv        = 16
0.00.042.215 I print_info: n_rot            = 32
0.00.042.215 I print_info: n_swa            = 0
0.00.042.216 I print_info: n_embd_head_k    = 128
0.00.042.216 I print_info: n_embd_head_v    = 128
0.00.042.236 I print_info: n_gqa            = 1
0.00.042.238 I print_info: n_embd_k_gqa     = 2048
0.00.042.238 I print_info: n_embd_v_gqa     = 2048
0.00.042.239 I print_info: f_norm_eps       = 1.0e-05
0.00.042.239 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.240 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.240 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.240 I print_info: f_logit_scale    = 0.0e+00
0.00.042.240 I print_info: n_ff             = 8192
0.00.042.241 I print_info: n_expert         = 0
0.00.042.244 I print_info: n_expert_used    = 0
0.00.042.245 I print_info: causal attn      = 1
0.00.042.245 I print_info: pooling type     = 0
0.00.042.245 I print_info: rope type        = 2
0.00.042.247 I print_info: rope scaling     = linear
0.00.042.247 I print_info: freq_base_train  = 10000.0
0.00.042.247 I print_info: freq_scale_train = 1
0.00.042.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.248 I print_info: rope_finetuned   = unknown
0.00.042.248 I print_info: ssm_d_conv       = 0
0.00.042.248 I print_info: ssm_d_inner      = 0
0.00.042.248 I print_info: ssm_d_state      = 0
0.00.042.248 I print_info: ssm_dt_rank      = 0
0.00.042.248 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.249 I print_info: model type       = 1.4B
0.00.042.249 I print_info: model params     = 1.41 B
0.00.042.249 I print_info: general.name     = 1.4B
0.00.042.249 I print_info: vocab type       = BPE
0.00.042.250 I print_info: n_vocab          = 50304
0.00.042.250 I print_info: n_merges         = 50009
0.00.042.250 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.250 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.250 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.250 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.251 I print_info: LF token         = 187 'Ċ'
0.00.042.251 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.251 I print_info: max token length = 1024
0.00.042.251 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.548.932 I load_tensors: offloading 24 repeating layers to GPU
0.00.548.947 I load_tensors: offloading output layer to GPU
0.00.548.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.548.983 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.548.984 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.550.284 I llama_context: n_seq_max     = 1
0.00.550.294 I llama_context: n_ctx         = 2048
0.00.550.294 I llama_context: n_ctx_per_seq = 2048
0.00.550.295 I llama_context: n_batch       = 2048
0.00.550.295 I llama_context: n_ubatch      = 512
0.00.550.296 I llama_context: flash_attn    = 0
0.00.550.298 I llama_context: freq_base     = 10000.0
0.00.550.298 I llama_context: freq_scale    = 1
0.00.550.300 I ggml_metal_init: allocating
0.00.550.419 I ggml_metal_init: found device: Apple M4
0.00.550.437 I ggml_metal_init: picking default device: Apple M4
0.00.552.341 I ggml_metal_init: using embedded metal library
0.00.558.631 I ggml_metal_init: GPU name:   Apple M4
0.00.558.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.641 I ggml_metal_init: simdgroup reduction   = true
0.00.558.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.642 I ggml_metal_init: has residency sets    = true
0.00.558.642 I ggml_metal_init: has bfloat            = true
0.00.558.643 I ggml_metal_init: use bfloat            = true
0.00.558.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.577.370 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.577.375 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.857 I init:      Metal KV buffer size =   384.00 MiB
0.00.634.866 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.639.569 I init:      Metal compute buffer size =   102.25 MiB
0.00.639.571 I init:        CPU compute buffer size =     8.01 MiB
0.00.639.572 I init: graph nodes  = 967
0.00.639.572 I init: graph splits = 2
0.00.639.580 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.639.709 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.639.709 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.128 I main: llama threadpool init, n_threads = 4
0.00.694.171 I 
0.00.694.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.184 I 
0.00.694.302 I sampler seed: 1234
0.00.694.306 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.345 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.349 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.349 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.378.036 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.378.037 I llama_perf_context_print:        load time =     683.47 ms
0.01.378.038 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.33 tokens per second)
0.01.378.038 I llama_perf_context_print:        eval time =     637.11 ms /    63 runs   (   10.11 ms per token,    98.88 tokens per second)
0.01.378.040 I llama_perf_context_print:       total time =     684.59 ms /    70 tokens
0.01.381.858 I ggml_metal_free: deallocating

real	0m1.399s
user	0m0.110s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.172 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.741 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.759 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.597 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.390 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.391 I llama_model_loader: - type  f32:  194 tensors
0.00.025.391 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.391 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.392 I print_info: file format = GGUF V3 (latest)
0.00.025.392 I print_info: file type   = Q4_1
0.00.025.393 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.311 I load: special tokens cache size = 25
0.00.039.346 I load: token to piece cache size = 0.2984 MB
0.00.039.361 I print_info: arch             = gptneox
0.00.039.362 I print_info: vocab_only       = 0
0.00.039.362 I print_info: n_ctx_train      = 2048
0.00.039.362 I print_info: n_embd           = 2048
0.00.039.362 I print_info: n_layer          = 24
0.00.039.365 I print_info: n_head           = 16
0.00.039.366 I print_info: n_head_kv        = 16
0.00.039.366 I print_info: n_rot            = 32
0.00.039.366 I print_info: n_swa            = 0
0.00.039.366 I print_info: n_embd_head_k    = 128
0.00.039.367 I print_info: n_embd_head_v    = 128
0.00.039.367 I print_info: n_gqa            = 1
0.00.039.368 I print_info: n_embd_k_gqa     = 2048
0.00.039.371 I print_info: n_embd_v_gqa     = 2048
0.00.039.372 I print_info: f_norm_eps       = 1.0e-05
0.00.039.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.373 I print_info: f_logit_scale    = 0.0e+00
0.00.039.373 I print_info: n_ff             = 8192
0.00.039.373 I print_info: n_expert         = 0
0.00.039.373 I print_info: n_expert_used    = 0
0.00.039.374 I print_info: causal attn      = 1
0.00.039.374 I print_info: pooling type     = 0
0.00.039.375 I print_info: rope type        = 2
0.00.039.376 I print_info: rope scaling     = linear
0.00.039.377 I print_info: freq_base_train  = 10000.0
0.00.039.377 I print_info: freq_scale_train = 1
0.00.039.377 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.377 I print_info: rope_finetuned   = unknown
0.00.039.377 I print_info: ssm_d_conv       = 0
0.00.039.377 I print_info: ssm_d_inner      = 0
0.00.039.378 I print_info: ssm_d_state      = 0
0.00.039.379 I print_info: ssm_dt_rank      = 0
0.00.039.379 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.379 I print_info: model type       = 1.4B
0.00.039.379 I print_info: model params     = 1.41 B
0.00.039.380 I print_info: general.name     = 1.4B
0.00.039.380 I print_info: vocab type       = BPE
0.00.039.380 I print_info: n_vocab          = 50304
0.00.039.380 I print_info: n_merges         = 50009
0.00.039.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.381 I print_info: LF token         = 187 'Ċ'
0.00.039.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.382 I print_info: max token length = 1024
0.00.039.382 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.066 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.081 I load_tensors: offloading output layer to GPU
0.00.639.082 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.118 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.639.119 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.640.707 I llama_context: n_seq_max     = 1
0.00.640.710 I llama_context: n_ctx         = 2048
0.00.640.711 I llama_context: n_ctx_per_seq = 2048
0.00.640.712 I llama_context: n_batch       = 2048
0.00.640.712 I llama_context: n_ubatch      = 512
0.00.640.713 I llama_context: flash_attn    = 0
0.00.640.715 I llama_context: freq_base     = 10000.0
0.00.640.715 I llama_context: freq_scale    = 1
0.00.640.717 I ggml_metal_init: allocating
0.00.640.795 I ggml_metal_init: found device: Apple M4
0.00.640.808 I ggml_metal_init: picking default device: Apple M4
0.00.642.803 I ggml_metal_init: using embedded metal library
0.00.649.342 I ggml_metal_init: GPU name:   Apple M4
0.00.649.346 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.347 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.347 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.348 I ggml_metal_init: simdgroup reduction   = true
0.00.649.348 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.348 I ggml_metal_init: has residency sets    = true
0.00.649.348 I ggml_metal_init: has bfloat            = true
0.00.649.349 I ggml_metal_init: use bfloat            = true
0.00.649.350 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.351 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.946 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.666.950 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.200 I init:      Metal KV buffer size =   384.00 MiB
0.00.721.206 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.725.344 I init:      Metal compute buffer size =   102.25 MiB
0.00.725.347 I init:        CPU compute buffer size =     8.01 MiB
0.00.725.347 I init: graph nodes  = 967
0.00.725.348 I init: graph splits = 2
0.00.725.352 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.725.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.267 I main: llama threadpool init, n_threads = 4
0.00.781.310 I 
0.00.781.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.324 I 
0.00.781.467 I sampler seed: 1234
0.00.781.472 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.483 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.483 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.483 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.517.924 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.517.925 I llama_perf_context_print:        load time =     771.36 ms
0.01.517.926 I llama_perf_context_print: prompt eval time =      46.80 ms /     7 tokens (    6.69 ms per token,   149.58 tokens per second)
0.01.517.926 I llama_perf_context_print:        eval time =     686.83 ms /    63 runs   (   10.90 ms per token,    91.73 tokens per second)
0.01.517.927 I llama_perf_context_print:       total time =     737.39 ms /    70 tokens
0.01.521.786 I ggml_metal_free: deallocating

real	0m1.540s
user	0m0.108s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.409 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.385 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.389 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.392 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.393 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.400 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.198 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.995 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.996 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.997 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.998 I llama_model_loader: - type  f32:  194 tensors
0.00.025.998 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.999 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.999 I print_info: file format = GGUF V3 (latest)
0.00.026.000 I print_info: file type   = Q5_0
0.00.026.001 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.850 I load: special tokens cache size = 25
0.00.040.105 I load: token to piece cache size = 0.2984 MB
0.00.040.119 I print_info: arch             = gptneox
0.00.040.120 I print_info: vocab_only       = 0
0.00.040.121 I print_info: n_ctx_train      = 2048
0.00.040.121 I print_info: n_embd           = 2048
0.00.040.121 I print_info: n_layer          = 24
0.00.040.124 I print_info: n_head           = 16
0.00.040.125 I print_info: n_head_kv        = 16
0.00.040.125 I print_info: n_rot            = 32
0.00.040.125 I print_info: n_swa            = 0
0.00.040.125 I print_info: n_embd_head_k    = 128
0.00.040.125 I print_info: n_embd_head_v    = 128
0.00.040.126 I print_info: n_gqa            = 1
0.00.040.127 I print_info: n_embd_k_gqa     = 2048
0.00.040.128 I print_info: n_embd_v_gqa     = 2048
0.00.040.128 I print_info: f_norm_eps       = 1.0e-05
0.00.040.129 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.129 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.129 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.129 I print_info: f_logit_scale    = 0.0e+00
0.00.040.130 I print_info: n_ff             = 8192
0.00.040.130 I print_info: n_expert         = 0
0.00.040.130 I print_info: n_expert_used    = 0
0.00.040.130 I print_info: causal attn      = 1
0.00.040.130 I print_info: pooling type     = 0
0.00.040.131 I print_info: rope type        = 2
0.00.040.131 I print_info: rope scaling     = linear
0.00.040.131 I print_info: freq_base_train  = 10000.0
0.00.040.131 I print_info: freq_scale_train = 1
0.00.040.131 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.132 I print_info: rope_finetuned   = unknown
0.00.040.132 I print_info: ssm_d_conv       = 0
0.00.040.132 I print_info: ssm_d_inner      = 0
0.00.040.132 I print_info: ssm_d_state      = 0
0.00.040.134 I print_info: ssm_dt_rank      = 0
0.00.040.134 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.134 I print_info: model type       = 1.4B
0.00.040.135 I print_info: model params     = 1.41 B
0.00.040.135 I print_info: general.name     = 1.4B
0.00.040.135 I print_info: vocab type       = BPE
0.00.040.135 I print_info: n_vocab          = 50304
0.00.040.135 I print_info: n_merges         = 50009
0.00.040.136 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.136 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.136 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.136 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.137 I print_info: LF token         = 187 'Ċ'
0.00.040.138 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.138 I print_info: max token length = 1024
0.00.040.138 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.779 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.793 I load_tensors: offloading output layer to GPU
0.00.653.794 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.832 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.653.833 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.655.582 I llama_context: n_seq_max     = 1
0.00.655.585 I llama_context: n_ctx         = 2048
0.00.655.586 I llama_context: n_ctx_per_seq = 2048
0.00.655.586 I llama_context: n_batch       = 2048
0.00.655.587 I llama_context: n_ubatch      = 512
0.00.655.587 I llama_context: flash_attn    = 0
0.00.655.588 I llama_context: freq_base     = 10000.0
0.00.655.589 I llama_context: freq_scale    = 1
0.00.655.590 I ggml_metal_init: allocating
0.00.655.649 I ggml_metal_init: found device: Apple M4
0.00.655.667 I ggml_metal_init: picking default device: Apple M4
0.00.657.264 I ggml_metal_init: using embedded metal library
0.00.663.580 I ggml_metal_init: GPU name:   Apple M4
0.00.663.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.586 I ggml_metal_init: simdgroup reduction   = true
0.00.663.586 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.586 I ggml_metal_init: has residency sets    = true
0.00.663.586 I ggml_metal_init: has bfloat            = true
0.00.663.587 I ggml_metal_init: use bfloat            = true
0.00.663.587 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.589 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.330 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.680.334 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.381 I init:      Metal KV buffer size =   384.00 MiB
0.00.733.389 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.940 I init:      Metal compute buffer size =   102.25 MiB
0.00.737.943 I init:        CPU compute buffer size =     8.01 MiB
0.00.737.943 I init: graph nodes  = 967
0.00.737.943 I init: graph splits = 2
0.00.737.949 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.085 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.628 I main: llama threadpool init, n_threads = 4
0.00.796.666 I 
0.00.796.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.679 I 
0.00.796.828 I sampler seed: 1234
0.00.796.833 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.876 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.880 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.880 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.582.548 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51976.57 tokens per second)
0.01.582.550 I llama_perf_context_print:        load time =     786.41 ms
0.01.582.550 I llama_perf_context_print: prompt eval time =      47.40 ms /     7 tokens (    6.77 ms per token,   147.67 tokens per second)
0.01.582.551 I llama_perf_context_print:        eval time =     735.35 ms /    63 runs   (   11.67 ms per token,    85.67 tokens per second)
0.01.582.551 I llama_perf_context_print:       total time =     786.72 ms /    70 tokens
0.01.586.677 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.108s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.191 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.162 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.164 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.165 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.166 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.166 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.167 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.167 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.167 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.168 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.168 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.168 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.169 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.032 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.799 I llama_model_loader: - type  f32:  194 tensors
0.00.026.799 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.799 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.800 I print_info: file format = GGUF V3 (latest)
0.00.026.800 I print_info: file type   = Q5_1
0.00.026.802 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.724 I load: special tokens cache size = 25
0.00.040.889 I load: token to piece cache size = 0.2984 MB
0.00.040.903 I print_info: arch             = gptneox
0.00.040.904 I print_info: vocab_only       = 0
0.00.040.904 I print_info: n_ctx_train      = 2048
0.00.040.905 I print_info: n_embd           = 2048
0.00.040.905 I print_info: n_layer          = 24
0.00.040.907 I print_info: n_head           = 16
0.00.040.912 I print_info: n_head_kv        = 16
0.00.040.912 I print_info: n_rot            = 32
0.00.040.912 I print_info: n_swa            = 0
0.00.040.913 I print_info: n_embd_head_k    = 128
0.00.040.913 I print_info: n_embd_head_v    = 128
0.00.040.914 I print_info: n_gqa            = 1
0.00.040.914 I print_info: n_embd_k_gqa     = 2048
0.00.040.915 I print_info: n_embd_v_gqa     = 2048
0.00.040.916 I print_info: f_norm_eps       = 1.0e-05
0.00.040.916 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.916 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.916 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.916 I print_info: f_logit_scale    = 0.0e+00
0.00.040.917 I print_info: n_ff             = 8192
0.00.040.917 I print_info: n_expert         = 0
0.00.040.918 I print_info: n_expert_used    = 0
0.00.040.918 I print_info: causal attn      = 1
0.00.040.918 I print_info: pooling type     = 0
0.00.040.918 I print_info: rope type        = 2
0.00.040.918 I print_info: rope scaling     = linear
0.00.040.918 I print_info: freq_base_train  = 10000.0
0.00.040.919 I print_info: freq_scale_train = 1
0.00.040.919 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.919 I print_info: rope_finetuned   = unknown
0.00.040.919 I print_info: ssm_d_conv       = 0
0.00.040.919 I print_info: ssm_d_inner      = 0
0.00.040.919 I print_info: ssm_d_state      = 0
0.00.040.920 I print_info: ssm_dt_rank      = 0
0.00.040.920 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.920 I print_info: model type       = 1.4B
0.00.040.920 I print_info: model params     = 1.41 B
0.00.040.920 I print_info: general.name     = 1.4B
0.00.040.921 I print_info: vocab type       = BPE
0.00.040.921 I print_info: n_vocab          = 50304
0.00.040.921 I print_info: n_merges         = 50009
0.00.040.922 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.922 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.922 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.922 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.922 I print_info: LF token         = 187 'Ċ'
0.00.040.923 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.923 I print_info: max token length = 1024
0.00.040.923 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.686.595 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.599 I load_tensors: offloading output layer to GPU
0.00.686.600 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.621 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.686.625 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.687.877 I llama_context: n_seq_max     = 1
0.00.687.879 I llama_context: n_ctx         = 2048
0.00.687.880 I llama_context: n_ctx_per_seq = 2048
0.00.687.880 I llama_context: n_batch       = 2048
0.00.687.881 I llama_context: n_ubatch      = 512
0.00.687.881 I llama_context: flash_attn    = 0
0.00.687.882 I llama_context: freq_base     = 10000.0
0.00.687.882 I llama_context: freq_scale    = 1
0.00.687.884 I ggml_metal_init: allocating
0.00.687.893 I ggml_metal_init: found device: Apple M4
0.00.687.901 I ggml_metal_init: picking default device: Apple M4
0.00.689.374 I ggml_metal_init: using embedded metal library
0.00.695.417 I ggml_metal_init: GPU name:   Apple M4
0.00.695.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.422 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.422 I ggml_metal_init: simdgroup reduction   = true
0.00.695.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.423 I ggml_metal_init: has residency sets    = true
0.00.695.423 I ggml_metal_init: has bfloat            = true
0.00.695.423 I ggml_metal_init: use bfloat            = true
0.00.695.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.795 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.711.799 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.769.447 I init:      Metal KV buffer size =   384.00 MiB
0.00.769.454 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.773.706 I init:      Metal compute buffer size =   102.25 MiB
0.00.773.708 I init:        CPU compute buffer size =     8.01 MiB
0.00.773.709 I init: graph nodes  = 967
0.00.773.709 I init: graph splits = 2
0.00.773.716 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.773.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.773.846 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.980 I main: llama threadpool init, n_threads = 4
0.00.829.030 I 
0.00.829.047 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.829.048 I 
0.00.829.204 I sampler seed: 1234
0.00.829.208 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.258 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.261 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.261 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.660.482 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.660.483 I llama_perf_context_print:        load time =     818.08 ms
0.01.660.484 I llama_perf_context_print: prompt eval time =      42.21 ms /     7 tokens (    6.03 ms per token,   165.85 tokens per second)
0.01.660.484 I llama_perf_context_print:        eval time =     786.07 ms /    63 runs   (   12.48 ms per token,    80.15 tokens per second)
0.01.660.484 I llama_perf_context_print:       total time =     832.21 ms /    70 tokens
0.01.663.912 I ggml_metal_free: deallocating

real	0m1.680s
user	0m0.108s
sys	0m0.223s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.028 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.545 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.550 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.551 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.553 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.349 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.190 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.191 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.192 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.192 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.192 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.193 I llama_model_loader: - type  f32:  194 tensors
0.00.025.194 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.194 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.194 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.195 I print_info: file format = GGUF V3 (latest)
0.00.025.195 I print_info: file type   = Q2_K - Medium
0.00.025.196 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.044 I load: special tokens cache size = 25
0.00.039.098 I load: token to piece cache size = 0.2984 MB
0.00.039.112 I print_info: arch             = gptneox
0.00.039.113 I print_info: vocab_only       = 0
0.00.039.114 I print_info: n_ctx_train      = 2048
0.00.039.114 I print_info: n_embd           = 2048
0.00.039.114 I print_info: n_layer          = 24
0.00.039.116 I print_info: n_head           = 16
0.00.039.117 I print_info: n_head_kv        = 16
0.00.039.117 I print_info: n_rot            = 32
0.00.039.118 I print_info: n_swa            = 0
0.00.039.118 I print_info: n_embd_head_k    = 128
0.00.039.118 I print_info: n_embd_head_v    = 128
0.00.039.119 I print_info: n_gqa            = 1
0.00.039.121 I print_info: n_embd_k_gqa     = 2048
0.00.039.121 I print_info: n_embd_v_gqa     = 2048
0.00.039.122 I print_info: f_norm_eps       = 1.0e-05
0.00.039.122 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.123 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.123 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.123 I print_info: f_logit_scale    = 0.0e+00
0.00.039.123 I print_info: n_ff             = 8192
0.00.039.124 I print_info: n_expert         = 0
0.00.039.124 I print_info: n_expert_used    = 0
0.00.039.124 I print_info: causal attn      = 1
0.00.039.124 I print_info: pooling type     = 0
0.00.039.124 I print_info: rope type        = 2
0.00.039.125 I print_info: rope scaling     = linear
0.00.039.126 I print_info: freq_base_train  = 10000.0
0.00.039.126 I print_info: freq_scale_train = 1
0.00.039.126 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.126 I print_info: rope_finetuned   = unknown
0.00.039.126 I print_info: ssm_d_conv       = 0
0.00.039.126 I print_info: ssm_d_inner      = 0
0.00.039.127 I print_info: ssm_d_state      = 0
0.00.039.127 I print_info: ssm_dt_rank      = 0
0.00.039.128 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.128 I print_info: model type       = 1.4B
0.00.039.129 I print_info: model params     = 1.41 B
0.00.039.129 I print_info: general.name     = 1.4B
0.00.039.129 I print_info: vocab type       = BPE
0.00.039.129 I print_info: n_vocab          = 50304
0.00.039.130 I print_info: n_merges         = 50009
0.00.039.130 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.130 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: LF token         = 187 'Ċ'
0.00.039.131 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: max token length = 1024
0.00.039.132 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.405.818 I load_tensors: offloading 24 repeating layers to GPU
0.00.405.831 I load_tensors: offloading output layer to GPU
0.00.405.831 I load_tensors: offloaded 25/25 layers to GPU
0.00.405.865 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.405.867 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.407.401 I llama_context: n_seq_max     = 1
0.00.407.405 I llama_context: n_ctx         = 2048
0.00.407.406 I llama_context: n_ctx_per_seq = 2048
0.00.407.406 I llama_context: n_batch       = 2048
0.00.407.407 I llama_context: n_ubatch      = 512
0.00.407.407 I llama_context: flash_attn    = 0
0.00.407.408 I llama_context: freq_base     = 10000.0
0.00.407.409 I llama_context: freq_scale    = 1
0.00.407.410 I ggml_metal_init: allocating
0.00.407.479 I ggml_metal_init: found device: Apple M4
0.00.407.494 I ggml_metal_init: picking default device: Apple M4
0.00.409.350 I ggml_metal_init: using embedded metal library
0.00.415.667 I ggml_metal_init: GPU name:   Apple M4
0.00.415.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.415.677 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.415.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.415.678 I ggml_metal_init: simdgroup reduction   = true
0.00.415.678 I ggml_metal_init: simdgroup matrix mul. = true
0.00.415.679 I ggml_metal_init: has residency sets    = true
0.00.415.679 I ggml_metal_init: has bfloat            = true
0.00.415.679 I ggml_metal_init: use bfloat            = true
0.00.415.681 I ggml_metal_init: hasUnifiedMemory      = true
0.00.415.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.437.240 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.437.245 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.497.589 I init:      Metal KV buffer size =   384.00 MiB
0.00.497.595 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.501.907 I init:      Metal compute buffer size =   102.25 MiB
0.00.501.909 I init:        CPU compute buffer size =     8.01 MiB
0.00.501.909 I init: graph nodes  = 967
0.00.501.909 I init: graph splits = 2
0.00.501.915 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.502.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.502.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.561.347 I main: llama threadpool init, n_threads = 4
0.00.561.400 I 
0.00.561.417 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.561.417 I 
0.00.561.596 I sampler seed: 1234
0.00.561.600 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.561.648 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.561.649 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.561.652 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.245.695 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.245.696 I llama_perf_context_print:        load time =     550.63 ms
0.01.245.697 I llama_perf_context_print: prompt eval time =      44.41 ms /     7 tokens (    6.34 ms per token,   157.60 tokens per second)
0.01.245.698 I llama_perf_context_print:        eval time =     636.81 ms /    63 runs   (   10.11 ms per token,    98.93 tokens per second)
0.01.245.698 I llama_perf_context_print:       total time =     685.04 ms /    70 tokens
0.01.249.392 I ggml_metal_free: deallocating

real	0m1.265s
user	0m0.112s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.601 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.286 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.294 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.301 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.916 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.570 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.572 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.572 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.572 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.573 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.573 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.574 I llama_model_loader: - type  f32:  194 tensors
0.00.024.574 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.574 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.574 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.575 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.575 I print_info: file format = GGUF V3 (latest)
0.00.024.576 I print_info: file type   = Q3_K - Medium
0.00.024.577 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.376 I load: special tokens cache size = 25
0.00.038.406 I load: token to piece cache size = 0.2984 MB
0.00.038.421 I print_info: arch             = gptneox
0.00.038.422 I print_info: vocab_only       = 0
0.00.038.422 I print_info: n_ctx_train      = 2048
0.00.038.422 I print_info: n_embd           = 2048
0.00.038.422 I print_info: n_layer          = 24
0.00.038.425 I print_info: n_head           = 16
0.00.038.426 I print_info: n_head_kv        = 16
0.00.038.426 I print_info: n_rot            = 32
0.00.038.426 I print_info: n_swa            = 0
0.00.038.427 I print_info: n_embd_head_k    = 128
0.00.038.427 I print_info: n_embd_head_v    = 128
0.00.038.428 I print_info: n_gqa            = 1
0.00.038.428 I print_info: n_embd_k_gqa     = 2048
0.00.038.431 I print_info: n_embd_v_gqa     = 2048
0.00.038.431 I print_info: f_norm_eps       = 1.0e-05
0.00.038.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.432 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.432 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.432 I print_info: f_logit_scale    = 0.0e+00
0.00.038.432 I print_info: n_ff             = 8192
0.00.038.433 I print_info: n_expert         = 0
0.00.038.433 I print_info: n_expert_used    = 0
0.00.038.433 I print_info: causal attn      = 1
0.00.038.433 I print_info: pooling type     = 0
0.00.038.433 I print_info: rope type        = 2
0.00.038.433 I print_info: rope scaling     = linear
0.00.038.435 I print_info: freq_base_train  = 10000.0
0.00.038.435 I print_info: freq_scale_train = 1
0.00.038.435 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.435 I print_info: rope_finetuned   = unknown
0.00.038.436 I print_info: ssm_d_conv       = 0
0.00.038.436 I print_info: ssm_d_inner      = 0
0.00.038.436 I print_info: ssm_d_state      = 0
0.00.038.436 I print_info: ssm_dt_rank      = 0
0.00.038.437 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.437 I print_info: model type       = 1.4B
0.00.038.438 I print_info: model params     = 1.41 B
0.00.038.438 I print_info: general.name     = 1.4B
0.00.038.438 I print_info: vocab type       = BPE
0.00.038.438 I print_info: n_vocab          = 50304
0.00.038.438 I print_info: n_merges         = 50009
0.00.038.439 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.439 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.440 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.440 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.440 I print_info: LF token         = 187 'Ċ'
0.00.038.441 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.441 I print_info: max token length = 1024
0.00.038.441 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.437.852 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.869 I load_tensors: offloading output layer to GPU
0.00.437.870 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.926 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.930 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.438.986 I llama_context: n_seq_max     = 1
0.00.438.989 I llama_context: n_ctx         = 2048
0.00.438.989 I llama_context: n_ctx_per_seq = 2048
0.00.438.990 I llama_context: n_batch       = 2048
0.00.438.990 I llama_context: n_ubatch      = 512
0.00.438.991 I llama_context: flash_attn    = 0
0.00.438.993 I llama_context: freq_base     = 10000.0
0.00.438.994 I llama_context: freq_scale    = 1
0.00.438.996 I ggml_metal_init: allocating
0.00.439.083 I ggml_metal_init: found device: Apple M4
0.00.439.097 I ggml_metal_init: picking default device: Apple M4
0.00.441.055 I ggml_metal_init: using embedded metal library
0.00.446.716 I ggml_metal_init: GPU name:   Apple M4
0.00.446.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.734 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.735 I ggml_metal_init: simdgroup reduction   = true
0.00.446.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.735 I ggml_metal_init: has residency sets    = true
0.00.446.736 I ggml_metal_init: has bfloat            = true
0.00.446.736 I ggml_metal_init: use bfloat            = true
0.00.446.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.719 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.466.724 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.527.636 I init:      Metal KV buffer size =   384.00 MiB
0.00.527.645 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.363 I init:      Metal compute buffer size =   102.25 MiB
0.00.532.365 I init:        CPU compute buffer size =     8.01 MiB
0.00.532.366 I init: graph nodes  = 967
0.00.532.366 I init: graph splits = 2
0.00.532.375 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.532.499 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.532.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.182 I main: llama threadpool init, n_threads = 4
0.00.582.223 I 
0.00.582.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.238 I 
0.00.582.367 I sampler seed: 1234
0.00.582.372 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.582.392 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.582.392 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.582.392 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.334.593 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49100.97 tokens per second)
0.01.334.594 I llama_perf_context_print:        load time =     572.89 ms
0.01.334.595 I llama_perf_context_print: prompt eval time =      50.58 ms /     7 tokens (    7.23 ms per token,   138.40 tokens per second)
0.01.334.596 I llama_perf_context_print:        eval time =     698.72 ms /    63 runs   (   11.09 ms per token,    90.17 tokens per second)
0.01.334.597 I llama_perf_context_print:       total time =     753.10 ms /    70 tokens
0.01.338.296 I ggml_metal_free: deallocating

real	0m1.353s
user	0m0.110s
sys	0m0.179s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.181 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.708 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.715 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.715 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.716 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.717 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.721 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.722 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.504 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.469 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.471 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.471 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.472 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.472 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.473 I llama_model_loader: - type  f32:  194 tensors
0.00.025.473 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.474 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.474 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.475 I print_info: file format = GGUF V3 (latest)
0.00.025.475 I print_info: file type   = Q4_K - Medium
0.00.025.476 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.586 I load: special tokens cache size = 25
0.00.039.924 I load: token to piece cache size = 0.2984 MB
0.00.039.942 I print_info: arch             = gptneox
0.00.039.943 I print_info: vocab_only       = 0
0.00.039.943 I print_info: n_ctx_train      = 2048
0.00.039.943 I print_info: n_embd           = 2048
0.00.039.943 I print_info: n_layer          = 24
0.00.039.947 I print_info: n_head           = 16
0.00.039.948 I print_info: n_head_kv        = 16
0.00.039.948 I print_info: n_rot            = 32
0.00.039.948 I print_info: n_swa            = 0
0.00.039.948 I print_info: n_embd_head_k    = 128
0.00.039.948 I print_info: n_embd_head_v    = 128
0.00.039.949 I print_info: n_gqa            = 1
0.00.039.949 I print_info: n_embd_k_gqa     = 2048
0.00.039.950 I print_info: n_embd_v_gqa     = 2048
0.00.039.950 I print_info: f_norm_eps       = 1.0e-05
0.00.039.951 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.951 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.951 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.951 I print_info: f_logit_scale    = 0.0e+00
0.00.039.952 I print_info: n_ff             = 8192
0.00.039.952 I print_info: n_expert         = 0
0.00.039.952 I print_info: n_expert_used    = 0
0.00.039.952 I print_info: causal attn      = 1
0.00.039.954 I print_info: pooling type     = 0
0.00.039.956 I print_info: rope type        = 2
0.00.039.956 I print_info: rope scaling     = linear
0.00.039.956 I print_info: freq_base_train  = 10000.0
0.00.039.956 I print_info: freq_scale_train = 1
0.00.039.957 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.957 I print_info: rope_finetuned   = unknown
0.00.039.957 I print_info: ssm_d_conv       = 0
0.00.039.957 I print_info: ssm_d_inner      = 0
0.00.039.957 I print_info: ssm_d_state      = 0
0.00.039.957 I print_info: ssm_dt_rank      = 0
0.00.039.957 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.959 I print_info: model type       = 1.4B
0.00.039.959 I print_info: model params     = 1.41 B
0.00.039.959 I print_info: general.name     = 1.4B
0.00.039.960 I print_info: vocab type       = BPE
0.00.039.960 I print_info: n_vocab          = 50304
0.00.039.960 I print_info: n_merges         = 50009
0.00.039.960 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.961 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.961 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.961 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.961 I print_info: LF token         = 187 'Ċ'
0.00.039.961 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.961 I print_info: max token length = 1024
0.00.039.962 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.521.397 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.419 I load_tensors: offloading output layer to GPU
0.00.521.420 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.454 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.468 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.522.672 I llama_context: n_seq_max     = 1
0.00.522.675 I llama_context: n_ctx         = 2048
0.00.522.675 I llama_context: n_ctx_per_seq = 2048
0.00.522.676 I llama_context: n_batch       = 2048
0.00.522.676 I llama_context: n_ubatch      = 512
0.00.522.677 I llama_context: flash_attn    = 0
0.00.522.679 I llama_context: freq_base     = 10000.0
0.00.522.681 I llama_context: freq_scale    = 1
0.00.522.683 I ggml_metal_init: allocating
0.00.522.771 I ggml_metal_init: found device: Apple M4
0.00.522.784 I ggml_metal_init: picking default device: Apple M4
0.00.524.716 I ggml_metal_init: using embedded metal library
0.00.531.259 I ggml_metal_init: GPU name:   Apple M4
0.00.531.269 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.270 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.272 I ggml_metal_init: simdgroup reduction   = true
0.00.531.272 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.272 I ggml_metal_init: has residency sets    = true
0.00.531.273 I ggml_metal_init: has bfloat            = true
0.00.531.273 I ggml_metal_init: use bfloat            = true
0.00.531.274 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.722 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.549.728 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.605.855 I init:      Metal KV buffer size =   384.00 MiB
0.00.605.864 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.610.776 I init:      Metal compute buffer size =   102.25 MiB
0.00.610.780 I init:        CPU compute buffer size =     8.01 MiB
0.00.610.780 I init: graph nodes  = 967
0.00.610.781 I init: graph splits = 2
0.00.610.787 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.610.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.610.916 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.290 I main: llama threadpool init, n_threads = 4
0.00.669.340 I 
0.00.669.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.358 I 
0.00.669.517 I sampler seed: 1234
0.00.669.522 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.669.546 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.669.547 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.669.547 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.438.202 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50070.52 tokens per second)
0.01.438.203 I llama_perf_context_print:        load time =     659.42 ms
0.01.438.204 I llama_perf_context_print: prompt eval time =      54.01 ms /     7 tokens (    7.72 ms per token,   129.62 tokens per second)
0.01.438.205 I llama_perf_context_print:        eval time =     711.66 ms /    63 runs   (   11.30 ms per token,    88.53 tokens per second)
0.01.438.206 I llama_perf_context_print:       total time =     769.60 ms /    70 tokens
0.01.441.974 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.435 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.448 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.449 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.450 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.451 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.454 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.074 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.075 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.075 I llama_model_loader: - type  f32:  194 tensors
0.00.026.076 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.076 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.076 I print_info: file format = GGUF V3 (latest)
0.00.026.077 I print_info: file type   = Q5_K - Medium
0.00.026.078 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.981 I load: special tokens cache size = 25
0.00.040.234 I load: token to piece cache size = 0.2984 MB
0.00.040.247 I print_info: arch             = gptneox
0.00.040.248 I print_info: vocab_only       = 0
0.00.040.249 I print_info: n_ctx_train      = 2048
0.00.040.249 I print_info: n_embd           = 2048
0.00.040.249 I print_info: n_layer          = 24
0.00.040.252 I print_info: n_head           = 16
0.00.040.253 I print_info: n_head_kv        = 16
0.00.040.253 I print_info: n_rot            = 32
0.00.040.253 I print_info: n_swa            = 0
0.00.040.253 I print_info: n_embd_head_k    = 128
0.00.040.253 I print_info: n_embd_head_v    = 128
0.00.040.254 I print_info: n_gqa            = 1
0.00.040.255 I print_info: n_embd_k_gqa     = 2048
0.00.040.257 I print_info: n_embd_v_gqa     = 2048
0.00.040.257 I print_info: f_norm_eps       = 1.0e-05
0.00.040.258 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.258 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.258 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.258 I print_info: f_logit_scale    = 0.0e+00
0.00.040.259 I print_info: n_ff             = 8192
0.00.040.259 I print_info: n_expert         = 0
0.00.040.259 I print_info: n_expert_used    = 0
0.00.040.259 I print_info: causal attn      = 1
0.00.040.260 I print_info: pooling type     = 0
0.00.040.260 I print_info: rope type        = 2
0.00.040.260 I print_info: rope scaling     = linear
0.00.040.260 I print_info: freq_base_train  = 10000.0
0.00.040.261 I print_info: freq_scale_train = 1
0.00.040.261 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.261 I print_info: rope_finetuned   = unknown
0.00.040.261 I print_info: ssm_d_conv       = 0
0.00.040.261 I print_info: ssm_d_inner      = 0
0.00.040.261 I print_info: ssm_d_state      = 0
0.00.040.261 I print_info: ssm_dt_rank      = 0
0.00.040.261 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.262 I print_info: model type       = 1.4B
0.00.040.262 I print_info: model params     = 1.41 B
0.00.040.262 I print_info: general.name     = 1.4B
0.00.040.262 I print_info: vocab type       = BPE
0.00.040.263 I print_info: n_vocab          = 50304
0.00.040.263 I print_info: n_merges         = 50009
0.00.040.263 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: LF token         = 187 'Ċ'
0.00.040.264 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.265 I print_info: max token length = 1024
0.00.040.265 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.581.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.581.518 I load_tensors: offloading output layer to GPU
0.00.581.519 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.543 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.581.548 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.582.881 I llama_context: n_seq_max     = 1
0.00.582.884 I llama_context: n_ctx         = 2048
0.00.582.885 I llama_context: n_ctx_per_seq = 2048
0.00.582.885 I llama_context: n_batch       = 2048
0.00.582.886 I llama_context: n_ubatch      = 512
0.00.582.886 I llama_context: flash_attn    = 0
0.00.582.887 I llama_context: freq_base     = 10000.0
0.00.582.887 I llama_context: freq_scale    = 1
0.00.582.888 I ggml_metal_init: allocating
0.00.582.919 I ggml_metal_init: found device: Apple M4
0.00.582.930 I ggml_metal_init: picking default device: Apple M4
0.00.584.454 I ggml_metal_init: using embedded metal library
0.00.590.786 I ggml_metal_init: GPU name:   Apple M4
0.00.590.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.792 I ggml_metal_init: simdgroup reduction   = true
0.00.590.792 I ggml_metal_init: simdgroup matrix mul. = true
0.00.590.792 I ggml_metal_init: has residency sets    = true
0.00.590.793 I ggml_metal_init: has bfloat            = true
0.00.590.793 I ggml_metal_init: use bfloat            = true
0.00.590.794 I ggml_metal_init: hasUnifiedMemory      = true
0.00.590.795 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.608.131 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.608.136 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.590 I init:      Metal KV buffer size =   384.00 MiB
0.00.662.599 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.667.344 I init:      Metal compute buffer size =   102.25 MiB
0.00.667.347 I init:        CPU compute buffer size =     8.01 MiB
0.00.667.347 I init: graph nodes  = 967
0.00.667.347 I init: graph splits = 2
0.00.667.355 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.667.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.667.477 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.728 I main: llama threadpool init, n_threads = 4
0.00.731.778 I 
0.00.731.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.792 I 
0.00.731.945 I sampler seed: 1234
0.00.731.950 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.998 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.998 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.584.842 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.584.844 I llama_perf_context_print:        load time =     721.21 ms
0.01.584.845 I llama_perf_context_print: prompt eval time =      52.92 ms /     7 tokens (    7.56 ms per token,   132.28 tokens per second)
0.01.584.845 I llama_perf_context_print:        eval time =     796.96 ms /    63 runs   (   12.65 ms per token,    79.05 tokens per second)
0.01.584.846 I llama_perf_context_print:       total time =     853.82 ms /    70 tokens
0.01.587.598 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.108s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.224 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.894 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.147 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.149 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.149 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.150 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.150 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.150 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.151 I llama_model_loader: - type  f32:  194 tensors
0.00.027.152 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.152 I print_info: file format = GGUF V3 (latest)
0.00.027.157 I print_info: file type   = Q6_K
0.00.027.158 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.611 I load: special tokens cache size = 25
0.00.041.575 I load: token to piece cache size = 0.2984 MB
0.00.041.592 I print_info: arch             = gptneox
0.00.041.592 I print_info: vocab_only       = 0
0.00.041.593 I print_info: n_ctx_train      = 2048
0.00.041.593 I print_info: n_embd           = 2048
0.00.041.593 I print_info: n_layer          = 24
0.00.041.597 I print_info: n_head           = 16
0.00.041.598 I print_info: n_head_kv        = 16
0.00.041.598 I print_info: n_rot            = 32
0.00.041.598 I print_info: n_swa            = 0
0.00.041.598 I print_info: n_embd_head_k    = 128
0.00.041.599 I print_info: n_embd_head_v    = 128
0.00.041.599 I print_info: n_gqa            = 1
0.00.041.602 I print_info: n_embd_k_gqa     = 2048
0.00.041.603 I print_info: n_embd_v_gqa     = 2048
0.00.041.603 I print_info: f_norm_eps       = 1.0e-05
0.00.041.604 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.606 I print_info: f_logit_scale    = 0.0e+00
0.00.041.606 I print_info: n_ff             = 8192
0.00.041.606 I print_info: n_expert         = 0
0.00.041.606 I print_info: n_expert_used    = 0
0.00.041.607 I print_info: causal attn      = 1
0.00.041.608 I print_info: pooling type     = 0
0.00.041.608 I print_info: rope type        = 2
0.00.041.608 I print_info: rope scaling     = linear
0.00.041.609 I print_info: freq_base_train  = 10000.0
0.00.041.609 I print_info: freq_scale_train = 1
0.00.041.609 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.609 I print_info: rope_finetuned   = unknown
0.00.041.610 I print_info: ssm_d_conv       = 0
0.00.041.610 I print_info: ssm_d_inner      = 0
0.00.041.610 I print_info: ssm_d_state      = 0
0.00.041.610 I print_info: ssm_dt_rank      = 0
0.00.041.610 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.610 I print_info: model type       = 1.4B
0.00.041.611 I print_info: model params     = 1.41 B
0.00.041.614 I print_info: general.name     = 1.4B
0.00.041.614 I print_info: vocab type       = BPE
0.00.041.616 I print_info: n_vocab          = 50304
0.00.041.616 I print_info: n_merges         = 50009
0.00.041.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.617 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.617 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.617 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.617 I print_info: LF token         = 187 'Ċ'
0.00.041.617 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.618 I print_info: max token length = 1024
0.00.041.618 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.929 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.937 I load_tensors: offloading output layer to GPU
0.00.634.938 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.957 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.634.958 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.635.734 I llama_context: n_seq_max     = 1
0.00.635.740 I llama_context: n_ctx         = 2048
0.00.635.740 I llama_context: n_ctx_per_seq = 2048
0.00.635.740 I llama_context: n_batch       = 2048
0.00.635.741 I llama_context: n_ubatch      = 512
0.00.635.741 I llama_context: flash_attn    = 0
0.00.635.742 I llama_context: freq_base     = 10000.0
0.00.635.743 I llama_context: freq_scale    = 1
0.00.635.744 I ggml_metal_init: allocating
0.00.635.785 I ggml_metal_init: found device: Apple M4
0.00.635.796 I ggml_metal_init: picking default device: Apple M4
0.00.636.929 I ggml_metal_init: using embedded metal library
0.00.641.132 I ggml_metal_init: GPU name:   Apple M4
0.00.641.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.139 I ggml_metal_init: simdgroup reduction   = true
0.00.641.139 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.140 I ggml_metal_init: has residency sets    = true
0.00.641.140 I ggml_metal_init: has bfloat            = true
0.00.641.140 I ggml_metal_init: use bfloat            = true
0.00.641.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.144 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.294 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.656.297 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.929 I init:      Metal KV buffer size =   384.00 MiB
0.00.686.935 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.691.307 I init:      Metal compute buffer size =   102.25 MiB
0.00.691.309 I init:        CPU compute buffer size =     8.01 MiB
0.00.691.309 I init: graph nodes  = 967
0.00.691.310 I init: graph splits = 2
0.00.691.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.691.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.691.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.975 I main: llama threadpool init, n_threads = 4
0.00.759.016 I 
0.00.759.040 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.040 I 
0.00.759.214 I sampler seed: 1234
0.00.759.219 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.229 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.230 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.230 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.644.178 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.01.644.179 I llama_perf_context_print:        load time =     748.03 ms
0.01.644.179 I llama_perf_context_print: prompt eval time =      57.65 ms /     7 tokens (    8.24 ms per token,   121.43 tokens per second)
0.01.644.180 I llama_perf_context_print:        eval time =     824.82 ms /    63 runs   (   13.09 ms per token,    76.38 tokens per second)
0.01.644.180 I llama_perf_context_print:       total time =     885.92 ms /    70 tokens
0.01.647.007 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.105s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.699 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.957 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.893 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.900 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.902 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.903 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.919 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.926 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.926 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.932 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.933 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.934 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.939 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.942 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.943 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.943 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.380 I llama_model_loader: - type  f32:  194 tensors
0.00.056.380 I llama_model_loader: - type  f16:   98 tensors
0.00.056.381 I print_info: file format = GGUF V3 (latest)
0.00.056.382 I print_info: file type   = all F32 (guessed)
0.00.056.383 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.962 I load: special tokens cache size = 25
0.00.077.230 I load: token to piece cache size = 0.2984 MB
0.00.077.244 I print_info: arch             = gptneox
0.00.077.245 I print_info: vocab_only       = 0
0.00.077.246 I print_info: n_ctx_train      = 2048
0.00.077.246 I print_info: n_embd           = 2048
0.00.077.246 I print_info: n_layer          = 24
0.00.077.249 I print_info: n_head           = 16
0.00.077.250 I print_info: n_head_kv        = 16
0.00.077.250 I print_info: n_rot            = 32
0.00.077.250 I print_info: n_swa            = 0
0.00.077.251 I print_info: n_embd_head_k    = 128
0.00.077.251 I print_info: n_embd_head_v    = 128
0.00.077.251 I print_info: n_gqa            = 1
0.00.077.252 I print_info: n_embd_k_gqa     = 2048
0.00.077.253 I print_info: n_embd_v_gqa     = 2048
0.00.077.254 I print_info: f_norm_eps       = 1.0e-05
0.00.077.254 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.254 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.254 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.262 I print_info: f_logit_scale    = 0.0e+00
0.00.077.271 I print_info: n_ff             = 8192
0.00.077.271 I print_info: n_expert         = 0
0.00.077.271 I print_info: n_expert_used    = 0
0.00.077.272 I print_info: causal attn      = 1
0.00.077.272 I print_info: pooling type     = 0
0.00.077.272 I print_info: rope type        = 2
0.00.077.272 I print_info: rope scaling     = linear
0.00.077.273 I print_info: freq_base_train  = 10000.0
0.00.077.273 I print_info: freq_scale_train = 1
0.00.077.273 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.274 I print_info: rope_finetuned   = unknown
0.00.077.274 I print_info: ssm_d_conv       = 0
0.00.077.274 I print_info: ssm_d_inner      = 0
0.00.077.274 I print_info: ssm_d_state      = 0
0.00.077.274 I print_info: ssm_dt_rank      = 0
0.00.077.274 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.275 I print_info: model type       = 1.4B
0.00.077.275 I print_info: model params     = 1.41 B
0.00.077.276 I print_info: general.name     = 1.4B
0.00.077.276 I print_info: vocab type       = BPE
0.00.077.276 I print_info: n_vocab          = 50304
0.00.077.276 I print_info: n_merges         = 50009
0.00.077.277 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.277 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.278 I print_info: LF token         = 187 'Ċ'
0.00.077.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.278 I print_info: max token length = 1024
0.00.077.278 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.957.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.957.999 I load_tensors: offloading output layer to GPU
0.00.958.000 I load_tensors: offloaded 25/25 layers to GPU
0.00.958.024 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.958.026 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.959.251 I llama_context: n_seq_max     = 1
0.00.959.252 I llama_context: n_ctx         = 128
0.00.959.253 I llama_context: n_ctx_per_seq = 128
0.00.959.253 I llama_context: n_batch       = 128
0.00.959.253 I llama_context: n_ubatch      = 128
0.00.959.253 I llama_context: flash_attn    = 0
0.00.959.254 I llama_context: freq_base     = 10000.0
0.00.959.254 I llama_context: freq_scale    = 1
0.00.959.255 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.959.255 I ggml_metal_init: allocating
0.00.959.316 I ggml_metal_init: found device: Apple M4
0.00.959.322 I ggml_metal_init: picking default device: Apple M4
0.00.960.501 I ggml_metal_init: using embedded metal library
0.00.964.516 I ggml_metal_init: GPU name:   Apple M4
0.00.964.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.964.519 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.964.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.964.520 I ggml_metal_init: simdgroup reduction   = true
0.00.964.520 I ggml_metal_init: simdgroup matrix mul. = true
0.00.964.520 I ggml_metal_init: has residency sets    = true
0.00.964.520 I ggml_metal_init: has bfloat            = true
0.00.964.521 I ggml_metal_init: use bfloat            = true
0.00.964.521 I ggml_metal_init: hasUnifiedMemory      = true
0.00.964.522 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.976.316 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.976.318 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.978.090 I init:      Metal KV buffer size =    24.00 MiB
0.00.978.092 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.979.722 I init:      Metal compute buffer size =    25.56 MiB
0.00.979.724 I init:        CPU compute buffer size =     1.06 MiB
0.00.979.724 I init: graph nodes  = 967
0.00.979.724 I init: graph splits = 2
0.00.979.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.979.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.013.664 I 
0.01.013.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.013.702 I perplexity: tokenizing the input ..
0.01.019.047 I perplexity: tokenization took 5.343 ms
0.01.019.052 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.137.611 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.138.961 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.138.993 I llama_perf_context_print:        load time =     989.70 ms
0.01.138.994 I llama_perf_context_print: prompt eval time =     118.30 ms /   128 tokens (    0.92 ms per token,  1082.04 tokens per second)
0.01.138.995 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.138.995 I llama_perf_context_print:       total time =     125.33 ms /   129 tokens
0.01.139.538 I ggml_metal_free: deallocating

real	0m1.329s
user	0m0.099s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.611 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.181 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.191 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.195 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.027 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.858 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.860 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.861 I llama_model_loader: - type  f32:  194 tensors
0.00.025.862 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.863 I print_info: file format = GGUF V3 (latest)
0.00.025.863 I print_info: file type   = Q8_0
0.00.025.864 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.986 I load: special tokens cache size = 25
0.00.040.153 I load: token to piece cache size = 0.2984 MB
0.00.040.170 I print_info: arch             = gptneox
0.00.040.171 I print_info: vocab_only       = 0
0.00.040.171 I print_info: n_ctx_train      = 2048
0.00.040.171 I print_info: n_embd           = 2048
0.00.040.171 I print_info: n_layer          = 24
0.00.040.175 I print_info: n_head           = 16
0.00.040.176 I print_info: n_head_kv        = 16
0.00.040.176 I print_info: n_rot            = 32
0.00.040.176 I print_info: n_swa            = 0
0.00.040.176 I print_info: n_embd_head_k    = 128
0.00.040.176 I print_info: n_embd_head_v    = 128
0.00.040.177 I print_info: n_gqa            = 1
0.00.040.178 I print_info: n_embd_k_gqa     = 2048
0.00.040.178 I print_info: n_embd_v_gqa     = 2048
0.00.040.179 I print_info: f_norm_eps       = 1.0e-05
0.00.040.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.179 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.180 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.180 I print_info: f_logit_scale    = 0.0e+00
0.00.040.180 I print_info: n_ff             = 8192
0.00.040.180 I print_info: n_expert         = 0
0.00.040.181 I print_info: n_expert_used    = 0
0.00.040.181 I print_info: causal attn      = 1
0.00.040.181 I print_info: pooling type     = 0
0.00.040.181 I print_info: rope type        = 2
0.00.040.181 I print_info: rope scaling     = linear
0.00.040.182 I print_info: freq_base_train  = 10000.0
0.00.040.182 I print_info: freq_scale_train = 1
0.00.040.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.182 I print_info: rope_finetuned   = unknown
0.00.040.182 I print_info: ssm_d_conv       = 0
0.00.040.182 I print_info: ssm_d_inner      = 0
0.00.040.183 I print_info: ssm_d_state      = 0
0.00.040.183 I print_info: ssm_dt_rank      = 0
0.00.040.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.183 I print_info: model type       = 1.4B
0.00.040.184 I print_info: model params     = 1.41 B
0.00.040.184 I print_info: general.name     = 1.4B
0.00.040.184 I print_info: vocab type       = BPE
0.00.040.184 I print_info: n_vocab          = 50304
0.00.040.184 I print_info: n_merges         = 50009
0.00.040.185 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.185 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.185 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.185 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.185 I print_info: LF token         = 187 'Ċ'
0.00.040.186 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.186 I print_info: max token length = 1024
0.00.040.186 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.823.875 I load_tensors: offloading 24 repeating layers to GPU
0.00.823.884 I load_tensors: offloading output layer to GPU
0.00.823.885 I load_tensors: offloaded 25/25 layers to GPU
0.00.823.912 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.823.916 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.825.211 I llama_context: n_seq_max     = 1
0.00.825.213 I llama_context: n_ctx         = 128
0.00.825.213 I llama_context: n_ctx_per_seq = 128
0.00.825.213 I llama_context: n_batch       = 128
0.00.825.214 I llama_context: n_ubatch      = 128
0.00.825.214 I llama_context: flash_attn    = 0
0.00.825.215 I llama_context: freq_base     = 10000.0
0.00.825.215 I llama_context: freq_scale    = 1
0.00.825.216 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.825.218 I ggml_metal_init: allocating
0.00.825.257 I ggml_metal_init: found device: Apple M4
0.00.825.266 I ggml_metal_init: picking default device: Apple M4
0.00.826.558 I ggml_metal_init: using embedded metal library
0.00.831.799 I ggml_metal_init: GPU name:   Apple M4
0.00.831.803 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.831.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.831.804 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.831.805 I ggml_metal_init: simdgroup reduction   = true
0.00.831.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.831.805 I ggml_metal_init: has residency sets    = true
0.00.831.806 I ggml_metal_init: has bfloat            = true
0.00.831.806 I ggml_metal_init: use bfloat            = true
0.00.831.807 I ggml_metal_init: hasUnifiedMemory      = true
0.00.831.808 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.848.058 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.848.062 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.850.852 I init:      Metal KV buffer size =    24.00 MiB
0.00.850.859 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.853.488 I init:      Metal compute buffer size =    25.56 MiB
0.00.853.490 I init:        CPU compute buffer size =     1.06 MiB
0.00.853.491 I init: graph nodes  = 967
0.00.853.491 I init: graph splits = 2
0.00.853.493 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.853.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.521 I 
0.00.879.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.592 I perplexity: tokenizing the input ..
0.00.887.147 I perplexity: tokenization took 7.553 ms
0.00.887.154 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.025.348 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.026.674 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.026.696 I llama_perf_context_print:        load time =     869.90 ms
0.01.026.697 I llama_perf_context_print: prompt eval time =     137.32 ms /   128 tokens (    1.07 ms per token,   932.14 tokens per second)
0.01.026.698 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.026.699 I llama_perf_context_print:       total time =     147.18 ms /   129 tokens
0.01.027.303 I ggml_metal_free: deallocating

real	0m1.042s
user	0m0.078s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.104 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.341 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.352 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.353 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.353 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.353 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.354 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.355 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.355 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.356 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.356 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.356 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.359 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.173 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.232 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.235 I llama_model_loader: - type  f32:  194 tensors
0.00.026.236 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.236 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.237 I print_info: file format = GGUF V3 (latest)
0.00.026.237 I print_info: file type   = Q4_0
0.00.026.238 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.319 I load: special tokens cache size = 25
0.00.040.437 I load: token to piece cache size = 0.2984 MB
0.00.040.453 I print_info: arch             = gptneox
0.00.040.454 I print_info: vocab_only       = 0
0.00.040.454 I print_info: n_ctx_train      = 2048
0.00.040.454 I print_info: n_embd           = 2048
0.00.040.455 I print_info: n_layer          = 24
0.00.040.459 I print_info: n_head           = 16
0.00.040.459 I print_info: n_head_kv        = 16
0.00.040.460 I print_info: n_rot            = 32
0.00.040.460 I print_info: n_swa            = 0
0.00.040.460 I print_info: n_embd_head_k    = 128
0.00.040.460 I print_info: n_embd_head_v    = 128
0.00.040.461 I print_info: n_gqa            = 1
0.00.040.461 I print_info: n_embd_k_gqa     = 2048
0.00.040.462 I print_info: n_embd_v_gqa     = 2048
0.00.040.462 I print_info: f_norm_eps       = 1.0e-05
0.00.040.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.463 I print_info: f_logit_scale    = 0.0e+00
0.00.040.464 I print_info: n_ff             = 8192
0.00.040.464 I print_info: n_expert         = 0
0.00.040.464 I print_info: n_expert_used    = 0
0.00.040.464 I print_info: causal attn      = 1
0.00.040.464 I print_info: pooling type     = 0
0.00.040.464 I print_info: rope type        = 2
0.00.040.465 I print_info: rope scaling     = linear
0.00.040.465 I print_info: freq_base_train  = 10000.0
0.00.040.465 I print_info: freq_scale_train = 1
0.00.040.465 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.466 I print_info: rope_finetuned   = unknown
0.00.040.466 I print_info: ssm_d_conv       = 0
0.00.040.466 I print_info: ssm_d_inner      = 0
0.00.040.466 I print_info: ssm_d_state      = 0
0.00.040.466 I print_info: ssm_dt_rank      = 0
0.00.040.466 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.467 I print_info: model type       = 1.4B
0.00.040.469 I print_info: model params     = 1.41 B
0.00.040.469 I print_info: general.name     = 1.4B
0.00.040.470 I print_info: vocab type       = BPE
0.00.040.470 I print_info: n_vocab          = 50304
0.00.040.470 I print_info: n_merges         = 50009
0.00.040.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.472 I print_info: LF token         = 187 'Ċ'
0.00.040.473 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.473 I print_info: max token length = 1024
0.00.040.473 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.544.339 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.352 I load_tensors: offloading output layer to GPU
0.00.544.353 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.386 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.544.388 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.546.042 I llama_context: n_seq_max     = 1
0.00.546.045 I llama_context: n_ctx         = 128
0.00.546.046 I llama_context: n_ctx_per_seq = 128
0.00.546.046 I llama_context: n_batch       = 128
0.00.546.047 I llama_context: n_ubatch      = 128
0.00.546.047 I llama_context: flash_attn    = 0
0.00.546.049 I llama_context: freq_base     = 10000.0
0.00.546.050 I llama_context: freq_scale    = 1
0.00.546.051 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.546.053 I ggml_metal_init: allocating
0.00.546.131 I ggml_metal_init: found device: Apple M4
0.00.546.146 I ggml_metal_init: picking default device: Apple M4
0.00.548.005 I ggml_metal_init: using embedded metal library
0.00.553.550 I ggml_metal_init: GPU name:   Apple M4
0.00.553.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.553.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.553.559 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.553.559 I ggml_metal_init: simdgroup reduction   = true
0.00.553.560 I ggml_metal_init: simdgroup matrix mul. = true
0.00.553.560 I ggml_metal_init: has residency sets    = true
0.00.553.560 I ggml_metal_init: has bfloat            = true
0.00.553.561 I ggml_metal_init: use bfloat            = true
0.00.553.562 I ggml_metal_init: hasUnifiedMemory      = true
0.00.553.566 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.572.506 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.572.510 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.576.176 I init:      Metal KV buffer size =    24.00 MiB
0.00.576.180 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.579.414 I init:      Metal compute buffer size =    25.56 MiB
0.00.579.416 I init:        CPU compute buffer size =     1.06 MiB
0.00.579.417 I init: graph nodes  = 967
0.00.579.418 I init: graph splits = 2
0.00.579.421 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.579.421 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.112 I 
0.00.603.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.169 I perplexity: tokenizing the input ..
0.00.609.334 I perplexity: tokenization took 6.164 ms
0.00.609.338 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.162 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.732.500 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.732.520 I llama_perf_context_print:        load time =     593.00 ms
0.00.732.522 I llama_perf_context_print: prompt eval time =     121.59 ms /   128 tokens (    0.95 ms per token,  1052.69 tokens per second)
0.00.732.522 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.523 I llama_perf_context_print:       total time =     129.41 ms /   129 tokens
0.00.733.062 I ggml_metal_free: deallocating

real	0m0.749s
user	0m0.078s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.760 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.320 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.326 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.334 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.335 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.338 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.339 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.339 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.343 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.343 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.999 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.001 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.001 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.002 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.003 I llama_model_loader: - type  f32:  194 tensors
0.00.025.003 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.003 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.004 I print_info: file format = GGUF V3 (latest)
0.00.025.005 I print_info: file type   = Q4_1
0.00.025.006 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.184 I load: special tokens cache size = 25
0.00.039.353 I load: token to piece cache size = 0.2984 MB
0.00.039.370 I print_info: arch             = gptneox
0.00.039.371 I print_info: vocab_only       = 0
0.00.039.371 I print_info: n_ctx_train      = 2048
0.00.039.371 I print_info: n_embd           = 2048
0.00.039.371 I print_info: n_layer          = 24
0.00.039.375 I print_info: n_head           = 16
0.00.039.376 I print_info: n_head_kv        = 16
0.00.039.376 I print_info: n_rot            = 32
0.00.039.376 I print_info: n_swa            = 0
0.00.039.376 I print_info: n_embd_head_k    = 128
0.00.039.376 I print_info: n_embd_head_v    = 128
0.00.039.377 I print_info: n_gqa            = 1
0.00.039.377 I print_info: n_embd_k_gqa     = 2048
0.00.039.378 I print_info: n_embd_v_gqa     = 2048
0.00.039.379 I print_info: f_norm_eps       = 1.0e-05
0.00.039.379 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.379 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.379 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.379 I print_info: f_logit_scale    = 0.0e+00
0.00.039.380 I print_info: n_ff             = 8192
0.00.039.380 I print_info: n_expert         = 0
0.00.039.380 I print_info: n_expert_used    = 0
0.00.039.380 I print_info: causal attn      = 1
0.00.039.380 I print_info: pooling type     = 0
0.00.039.384 I print_info: rope type        = 2
0.00.039.385 I print_info: rope scaling     = linear
0.00.039.385 I print_info: freq_base_train  = 10000.0
0.00.039.385 I print_info: freq_scale_train = 1
0.00.039.385 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.386 I print_info: rope_finetuned   = unknown
0.00.039.386 I print_info: ssm_d_conv       = 0
0.00.039.386 I print_info: ssm_d_inner      = 0
0.00.039.386 I print_info: ssm_d_state      = 0
0.00.039.386 I print_info: ssm_dt_rank      = 0
0.00.039.387 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.387 I print_info: model type       = 1.4B
0.00.039.387 I print_info: model params     = 1.41 B
0.00.039.387 I print_info: general.name     = 1.4B
0.00.039.388 I print_info: vocab type       = BPE
0.00.039.388 I print_info: n_vocab          = 50304
0.00.039.388 I print_info: n_merges         = 50009
0.00.039.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.389 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.389 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.389 I print_info: LF token         = 187 'Ċ'
0.00.039.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.389 I print_info: max token length = 1024
0.00.039.390 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.636.429 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.442 I load_tensors: offloading output layer to GPU
0.00.636.443 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.482 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.636.483 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.638.175 I llama_context: n_seq_max     = 1
0.00.638.177 I llama_context: n_ctx         = 128
0.00.638.178 I llama_context: n_ctx_per_seq = 128
0.00.638.178 I llama_context: n_batch       = 128
0.00.638.179 I llama_context: n_ubatch      = 128
0.00.638.179 I llama_context: flash_attn    = 0
0.00.638.182 I llama_context: freq_base     = 10000.0
0.00.638.182 I llama_context: freq_scale    = 1
0.00.638.183 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.638.185 I ggml_metal_init: allocating
0.00.638.271 I ggml_metal_init: found device: Apple M4
0.00.638.284 I ggml_metal_init: picking default device: Apple M4
0.00.640.067 I ggml_metal_init: using embedded metal library
0.00.646.065 I ggml_metal_init: GPU name:   Apple M4
0.00.646.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.079 I ggml_metal_init: simdgroup reduction   = true
0.00.646.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.080 I ggml_metal_init: has residency sets    = true
0.00.646.080 I ggml_metal_init: has bfloat            = true
0.00.646.080 I ggml_metal_init: use bfloat            = true
0.00.646.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.089 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.838 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.664.843 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.668.337 I init:      Metal KV buffer size =    24.00 MiB
0.00.668.341 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.671.463 I init:      Metal compute buffer size =    25.56 MiB
0.00.671.464 I init:        CPU compute buffer size =     1.06 MiB
0.00.671.465 I init: graph nodes  = 967
0.00.671.465 I init: graph splits = 2
0.00.671.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.671.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.235 I 
0.00.699.293 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.299 I perplexity: tokenizing the input ..
0.00.707.025 I perplexity: tokenization took 7.722 ms
0.00.707.037 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.842.651 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.843.996 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.844.023 I llama_perf_context_print:        load time =     690.47 ms
0.00.844.024 I llama_perf_context_print: prompt eval time =     134.68 ms /   128 tokens (    1.05 ms per token,   950.43 tokens per second)
0.00.844.025 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.844.025 I llama_perf_context_print:       total time =     144.79 ms /   129 tokens
0.00.844.593 I ggml_metal_free: deallocating

real	0m0.859s
user	0m0.080s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.344 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.959 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.965 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.969 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.969 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.970 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.970 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.970 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.972 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.972 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.973 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.973 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.973 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.974 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.976 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.976 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.816 I llama_model_loader: - type  f32:  194 tensors
0.00.026.816 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.817 I print_info: file format = GGUF V3 (latest)
0.00.026.817 I print_info: file type   = Q5_0
0.00.026.820 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.261 I load: special tokens cache size = 25
0.00.041.399 I load: token to piece cache size = 0.2984 MB
0.00.041.417 I print_info: arch             = gptneox
0.00.041.418 I print_info: vocab_only       = 0
0.00.041.418 I print_info: n_ctx_train      = 2048
0.00.041.418 I print_info: n_embd           = 2048
0.00.041.418 I print_info: n_layer          = 24
0.00.041.423 I print_info: n_head           = 16
0.00.041.423 I print_info: n_head_kv        = 16
0.00.041.424 I print_info: n_rot            = 32
0.00.041.424 I print_info: n_swa            = 0
0.00.041.424 I print_info: n_embd_head_k    = 128
0.00.041.424 I print_info: n_embd_head_v    = 128
0.00.041.425 I print_info: n_gqa            = 1
0.00.041.425 I print_info: n_embd_k_gqa     = 2048
0.00.041.426 I print_info: n_embd_v_gqa     = 2048
0.00.041.426 I print_info: f_norm_eps       = 1.0e-05
0.00.041.427 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.427 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.427 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.427 I print_info: f_logit_scale    = 0.0e+00
0.00.041.428 I print_info: n_ff             = 8192
0.00.041.428 I print_info: n_expert         = 0
0.00.041.428 I print_info: n_expert_used    = 0
0.00.041.428 I print_info: causal attn      = 1
0.00.041.428 I print_info: pooling type     = 0
0.00.041.429 I print_info: rope type        = 2
0.00.041.429 I print_info: rope scaling     = linear
0.00.041.432 I print_info: freq_base_train  = 10000.0
0.00.041.433 I print_info: freq_scale_train = 1
0.00.041.433 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.433 I print_info: rope_finetuned   = unknown
0.00.041.433 I print_info: ssm_d_conv       = 0
0.00.041.433 I print_info: ssm_d_inner      = 0
0.00.041.433 I print_info: ssm_d_state      = 0
0.00.041.433 I print_info: ssm_dt_rank      = 0
0.00.041.433 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.434 I print_info: model type       = 1.4B
0.00.041.434 I print_info: model params     = 1.41 B
0.00.041.435 I print_info: general.name     = 1.4B
0.00.041.435 I print_info: vocab type       = BPE
0.00.041.435 I print_info: n_vocab          = 50304
0.00.041.436 I print_info: n_merges         = 50009
0.00.041.436 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: LF token         = 187 'Ċ'
0.00.041.437 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.437 I print_info: max token length = 1024
0.00.041.437 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.670.992 I load_tensors: offloading 24 repeating layers to GPU
0.00.671.013 I load_tensors: offloading output layer to GPU
0.00.671.014 I load_tensors: offloaded 25/25 layers to GPU
0.00.671.052 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.671.053 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.672.523 I llama_context: n_seq_max     = 1
0.00.672.527 I llama_context: n_ctx         = 128
0.00.672.528 I llama_context: n_ctx_per_seq = 128
0.00.672.528 I llama_context: n_batch       = 128
0.00.672.529 I llama_context: n_ubatch      = 128
0.00.672.529 I llama_context: flash_attn    = 0
0.00.672.532 I llama_context: freq_base     = 10000.0
0.00.672.532 I llama_context: freq_scale    = 1
0.00.672.533 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.672.535 I ggml_metal_init: allocating
0.00.672.626 I ggml_metal_init: found device: Apple M4
0.00.672.641 I ggml_metal_init: picking default device: Apple M4
0.00.674.620 I ggml_metal_init: using embedded metal library
0.00.680.568 I ggml_metal_init: GPU name:   Apple M4
0.00.680.578 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.680.579 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.680.580 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.680.580 I ggml_metal_init: simdgroup reduction   = true
0.00.680.581 I ggml_metal_init: simdgroup matrix mul. = true
0.00.680.581 I ggml_metal_init: has residency sets    = true
0.00.680.581 I ggml_metal_init: has bfloat            = true
0.00.680.582 I ggml_metal_init: use bfloat            = true
0.00.680.584 I ggml_metal_init: hasUnifiedMemory      = true
0.00.680.589 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.700.158 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.700.163 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.707 I init:      Metal KV buffer size =    24.00 MiB
0.00.703.710 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.707.036 I init:      Metal compute buffer size =    25.56 MiB
0.00.707.038 I init:        CPU compute buffer size =     1.06 MiB
0.00.707.039 I init: graph nodes  = 967
0.00.707.040 I init: graph splits = 2
0.00.707.043 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.707.044 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.719 I 
0.00.737.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.766 I perplexity: tokenizing the input ..
0.00.744.022 I perplexity: tokenization took 6.254 ms
0.00.744.026 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.887.974 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.889.327 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.889.350 I llama_perf_context_print:        load time =     727.37 ms
0.00.889.351 I llama_perf_context_print: prompt eval time =     143.64 ms /   128 tokens (    1.12 ms per token,   891.14 tokens per second)
0.00.889.352 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.353 I llama_perf_context_print:       total time =     151.63 ms /   129 tokens
0.00.889.929 I ggml_metal_free: deallocating

real	0m0.905s
user	0m0.080s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.311 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.224 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.226 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.227 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.152 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.152 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.152 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.153 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.153 I llama_model_loader: - type  f32:  194 tensors
0.00.026.154 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.154 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.155 I print_info: file format = GGUF V3 (latest)
0.00.026.155 I print_info: file type   = Q5_1
0.00.026.156 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.563 I load: special tokens cache size = 25
0.00.040.494 I load: token to piece cache size = 0.2984 MB
0.00.040.512 I print_info: arch             = gptneox
0.00.040.513 I print_info: vocab_only       = 0
0.00.040.513 I print_info: n_ctx_train      = 2048
0.00.040.513 I print_info: n_embd           = 2048
0.00.040.514 I print_info: n_layer          = 24
0.00.040.518 I print_info: n_head           = 16
0.00.040.518 I print_info: n_head_kv        = 16
0.00.040.518 I print_info: n_rot            = 32
0.00.040.519 I print_info: n_swa            = 0
0.00.040.519 I print_info: n_embd_head_k    = 128
0.00.040.519 I print_info: n_embd_head_v    = 128
0.00.040.519 I print_info: n_gqa            = 1
0.00.040.520 I print_info: n_embd_k_gqa     = 2048
0.00.040.521 I print_info: n_embd_v_gqa     = 2048
0.00.040.521 I print_info: f_norm_eps       = 1.0e-05
0.00.040.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.522 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.522 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.524 I print_info: f_logit_scale    = 0.0e+00
0.00.040.524 I print_info: n_ff             = 8192
0.00.040.524 I print_info: n_expert         = 0
0.00.040.525 I print_info: n_expert_used    = 0
0.00.040.525 I print_info: causal attn      = 1
0.00.040.525 I print_info: pooling type     = 0
0.00.040.525 I print_info: rope type        = 2
0.00.040.525 I print_info: rope scaling     = linear
0.00.040.525 I print_info: freq_base_train  = 10000.0
0.00.040.526 I print_info: freq_scale_train = 1
0.00.040.526 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.526 I print_info: rope_finetuned   = unknown
0.00.040.526 I print_info: ssm_d_conv       = 0
0.00.040.526 I print_info: ssm_d_inner      = 0
0.00.040.526 I print_info: ssm_d_state      = 0
0.00.040.527 I print_info: ssm_dt_rank      = 0
0.00.040.527 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.527 I print_info: model type       = 1.4B
0.00.040.527 I print_info: model params     = 1.41 B
0.00.040.527 I print_info: general.name     = 1.4B
0.00.040.528 I print_info: vocab type       = BPE
0.00.040.528 I print_info: n_vocab          = 50304
0.00.040.528 I print_info: n_merges         = 50009
0.00.040.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.529 I print_info: LF token         = 187 'Ċ'
0.00.040.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.529 I print_info: max token length = 1024
0.00.040.530 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.686.335 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.357 I load_tensors: offloading output layer to GPU
0.00.686.358 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.410 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.686.412 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.687.699 I llama_context: n_seq_max     = 1
0.00.687.703 I llama_context: n_ctx         = 128
0.00.687.704 I llama_context: n_ctx_per_seq = 128
0.00.687.704 I llama_context: n_batch       = 128
0.00.687.705 I llama_context: n_ubatch      = 128
0.00.687.705 I llama_context: flash_attn    = 0
0.00.687.707 I llama_context: freq_base     = 10000.0
0.00.687.708 I llama_context: freq_scale    = 1
0.00.687.709 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.687.718 I ggml_metal_init: allocating
0.00.687.800 I ggml_metal_init: found device: Apple M4
0.00.687.815 I ggml_metal_init: picking default device: Apple M4
0.00.689.770 I ggml_metal_init: using embedded metal library
0.00.696.398 I ggml_metal_init: GPU name:   Apple M4
0.00.696.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.409 I ggml_metal_init: simdgroup reduction   = true
0.00.696.409 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.409 I ggml_metal_init: has residency sets    = true
0.00.696.410 I ggml_metal_init: has bfloat            = true
0.00.696.411 I ggml_metal_init: use bfloat            = true
0.00.696.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.415 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.775 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.714.779 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.252 I init:      Metal KV buffer size =    24.00 MiB
0.00.718.255 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.721.432 I init:      Metal compute buffer size =    25.56 MiB
0.00.721.434 I init:        CPU compute buffer size =     1.06 MiB
0.00.721.435 I init: graph nodes  = 967
0.00.721.435 I init: graph splits = 2
0.00.721.441 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.721.442 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.711 I 
0.00.751.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.774 I perplexity: tokenizing the input ..
0.00.758.495 I perplexity: tokenization took 6.716 ms
0.00.758.501 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.904.150 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.905.485 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.905.513 I llama_perf_context_print:        load time =     742.39 ms
0.00.905.514 I llama_perf_context_print: prompt eval time =     144.64 ms /   128 tokens (    1.13 ms per token,   884.93 tokens per second)
0.00.905.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.905.515 I llama_perf_context_print:       total time =     153.81 ms /   129 tokens
0.00.906.171 I ggml_metal_free: deallocating

real	0m0.920s
user	0m0.080s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.402 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.483 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.492 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.493 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.493 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.494 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.495 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.495 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.496 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.496 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.498 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.500 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.500 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.500 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.541 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.543 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.543 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.543 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.544 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.544 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.545 I llama_model_loader: - type  f32:  194 tensors
0.00.026.545 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.545 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.545 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.546 I print_info: file format = GGUF V3 (latest)
0.00.026.547 I print_info: file type   = Q2_K - Medium
0.00.026.548 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.005 I load: special tokens cache size = 25
0.00.041.263 I load: token to piece cache size = 0.2984 MB
0.00.041.280 I print_info: arch             = gptneox
0.00.041.281 I print_info: vocab_only       = 0
0.00.041.282 I print_info: n_ctx_train      = 2048
0.00.041.282 I print_info: n_embd           = 2048
0.00.041.282 I print_info: n_layer          = 24
0.00.041.287 I print_info: n_head           = 16
0.00.041.287 I print_info: n_head_kv        = 16
0.00.041.287 I print_info: n_rot            = 32
0.00.041.288 I print_info: n_swa            = 0
0.00.041.288 I print_info: n_embd_head_k    = 128
0.00.041.288 I print_info: n_embd_head_v    = 128
0.00.041.289 I print_info: n_gqa            = 1
0.00.041.289 I print_info: n_embd_k_gqa     = 2048
0.00.041.290 I print_info: n_embd_v_gqa     = 2048
0.00.041.293 I print_info: f_norm_eps       = 1.0e-05
0.00.041.293 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.293 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.294 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.294 I print_info: f_logit_scale    = 0.0e+00
0.00.041.294 I print_info: n_ff             = 8192
0.00.041.294 I print_info: n_expert         = 0
0.00.041.294 I print_info: n_expert_used    = 0
0.00.041.295 I print_info: causal attn      = 1
0.00.041.295 I print_info: pooling type     = 0
0.00.041.295 I print_info: rope type        = 2
0.00.041.295 I print_info: rope scaling     = linear
0.00.041.296 I print_info: freq_base_train  = 10000.0
0.00.041.296 I print_info: freq_scale_train = 1
0.00.041.296 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.296 I print_info: rope_finetuned   = unknown
0.00.041.296 I print_info: ssm_d_conv       = 0
0.00.041.296 I print_info: ssm_d_inner      = 0
0.00.041.297 I print_info: ssm_d_state      = 0
0.00.041.297 I print_info: ssm_dt_rank      = 0
0.00.041.297 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.298 I print_info: model type       = 1.4B
0.00.041.298 I print_info: model params     = 1.41 B
0.00.041.298 I print_info: general.name     = 1.4B
0.00.041.298 I print_info: vocab type       = BPE
0.00.041.299 I print_info: n_vocab          = 50304
0.00.041.300 I print_info: n_merges         = 50009
0.00.041.300 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.300 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.300 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.300 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.301 I print_info: LF token         = 187 'Ċ'
0.00.041.301 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.301 I print_info: max token length = 1024
0.00.041.302 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.400.255 I load_tensors: offloading 24 repeating layers to GPU
0.00.400.276 I load_tensors: offloading output layer to GPU
0.00.400.277 I load_tensors: offloaded 25/25 layers to GPU
0.00.400.314 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.400.316 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.401.738 I llama_context: n_seq_max     = 1
0.00.401.742 I llama_context: n_ctx         = 128
0.00.401.742 I llama_context: n_ctx_per_seq = 128
0.00.401.743 I llama_context: n_batch       = 128
0.00.401.743 I llama_context: n_ubatch      = 128
0.00.401.743 I llama_context: flash_attn    = 0
0.00.401.745 I llama_context: freq_base     = 10000.0
0.00.401.745 I llama_context: freq_scale    = 1
0.00.401.746 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.401.748 I ggml_metal_init: allocating
0.00.401.805 I ggml_metal_init: found device: Apple M4
0.00.401.819 I ggml_metal_init: picking default device: Apple M4
0.00.403.699 I ggml_metal_init: using embedded metal library
0.00.409.474 I ggml_metal_init: GPU name:   Apple M4
0.00.409.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.409.497 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.409.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.409.498 I ggml_metal_init: simdgroup reduction   = true
0.00.409.499 I ggml_metal_init: simdgroup matrix mul. = true
0.00.409.499 I ggml_metal_init: has residency sets    = true
0.00.409.499 I ggml_metal_init: has bfloat            = true
0.00.409.499 I ggml_metal_init: use bfloat            = true
0.00.409.502 I ggml_metal_init: hasUnifiedMemory      = true
0.00.409.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.431.118 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.431.123 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.434.689 I init:      Metal KV buffer size =    24.00 MiB
0.00.434.694 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.437.967 I init:      Metal compute buffer size =    25.56 MiB
0.00.437.969 I init:        CPU compute buffer size =     1.06 MiB
0.00.437.969 I init: graph nodes  = 967
0.00.437.970 I init: graph splits = 2
0.00.437.979 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.437.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.466.552 I 
0.00.466.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.466.612 I perplexity: tokenizing the input ..
0.00.472.902 I perplexity: tokenization took 6.288 ms
0.00.472.907 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.605.309 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.606.653 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.606.674 I llama_perf_context_print:        load time =     456.14 ms
0.00.606.676 I llama_perf_context_print: prompt eval time =     131.78 ms /   128 tokens (    1.03 ms per token,   971.35 tokens per second)
0.00.606.677 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.606.677 I llama_perf_context_print:       total time =     140.12 ms /   129 tokens
0.00.607.256 I ggml_metal_free: deallocating

real	0m0.622s
user	0m0.082s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.318 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.592 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.606 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.608 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.609 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.609 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.610 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.411 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.411 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.411 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.412 I llama_model_loader: - type  f32:  194 tensors
0.00.025.412 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.413 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.413 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.413 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.414 I print_info: file format = GGUF V3 (latest)
0.00.025.418 I print_info: file type   = Q3_K - Medium
0.00.025.419 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.921 I load: special tokens cache size = 25
0.00.040.040 I load: token to piece cache size = 0.2984 MB
0.00.040.057 I print_info: arch             = gptneox
0.00.040.058 I print_info: vocab_only       = 0
0.00.040.058 I print_info: n_ctx_train      = 2048
0.00.040.058 I print_info: n_embd           = 2048
0.00.040.058 I print_info: n_layer          = 24
0.00.040.063 I print_info: n_head           = 16
0.00.040.063 I print_info: n_head_kv        = 16
0.00.040.063 I print_info: n_rot            = 32
0.00.040.064 I print_info: n_swa            = 0
0.00.040.064 I print_info: n_embd_head_k    = 128
0.00.040.064 I print_info: n_embd_head_v    = 128
0.00.040.064 I print_info: n_gqa            = 1
0.00.040.065 I print_info: n_embd_k_gqa     = 2048
0.00.040.066 I print_info: n_embd_v_gqa     = 2048
0.00.040.066 I print_info: f_norm_eps       = 1.0e-05
0.00.040.066 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.067 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.067 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.067 I print_info: f_logit_scale    = 0.0e+00
0.00.040.067 I print_info: n_ff             = 8192
0.00.040.068 I print_info: n_expert         = 0
0.00.040.068 I print_info: n_expert_used    = 0
0.00.040.068 I print_info: causal attn      = 1
0.00.040.068 I print_info: pooling type     = 0
0.00.040.068 I print_info: rope type        = 2
0.00.040.073 I print_info: rope scaling     = linear
0.00.040.074 I print_info: freq_base_train  = 10000.0
0.00.040.074 I print_info: freq_scale_train = 1
0.00.040.074 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.074 I print_info: rope_finetuned   = unknown
0.00.040.074 I print_info: ssm_d_conv       = 0
0.00.040.074 I print_info: ssm_d_inner      = 0
0.00.040.075 I print_info: ssm_d_state      = 0
0.00.040.076 I print_info: ssm_dt_rank      = 0
0.00.040.076 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.076 I print_info: model type       = 1.4B
0.00.040.077 I print_info: model params     = 1.41 B
0.00.040.077 I print_info: general.name     = 1.4B
0.00.040.077 I print_info: vocab type       = BPE
0.00.040.078 I print_info: n_vocab          = 50304
0.00.040.078 I print_info: n_merges         = 50009
0.00.040.078 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.079 I print_info: LF token         = 187 'Ċ'
0.00.040.079 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.079 I print_info: max token length = 1024
0.00.040.079 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.450.122 I load_tensors: offloading 24 repeating layers to GPU
0.00.450.141 I load_tensors: offloading output layer to GPU
0.00.450.142 I load_tensors: offloaded 25/25 layers to GPU
0.00.450.181 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.450.182 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.451.599 I llama_context: n_seq_max     = 1
0.00.451.602 I llama_context: n_ctx         = 128
0.00.451.602 I llama_context: n_ctx_per_seq = 128
0.00.451.603 I llama_context: n_batch       = 128
0.00.451.604 I llama_context: n_ubatch      = 128
0.00.451.604 I llama_context: flash_attn    = 0
0.00.451.607 I llama_context: freq_base     = 10000.0
0.00.451.607 I llama_context: freq_scale    = 1
0.00.451.608 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.451.610 I ggml_metal_init: allocating
0.00.451.699 I ggml_metal_init: found device: Apple M4
0.00.451.715 I ggml_metal_init: picking default device: Apple M4
0.00.453.621 I ggml_metal_init: using embedded metal library
0.00.459.293 I ggml_metal_init: GPU name:   Apple M4
0.00.459.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.459.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.459.316 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.459.316 I ggml_metal_init: simdgroup reduction   = true
0.00.459.317 I ggml_metal_init: simdgroup matrix mul. = true
0.00.459.317 I ggml_metal_init: has residency sets    = true
0.00.459.317 I ggml_metal_init: has bfloat            = true
0.00.459.318 I ggml_metal_init: use bfloat            = true
0.00.459.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.459.325 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.480.458 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.480.463 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.484.013 I init:      Metal KV buffer size =    24.00 MiB
0.00.484.019 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.487.245 I init:      Metal compute buffer size =    25.56 MiB
0.00.487.247 I init:        CPU compute buffer size =     1.06 MiB
0.00.487.247 I init: graph nodes  = 967
0.00.487.248 I init: graph splits = 2
0.00.487.252 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.487.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.512.149 I 
0.00.512.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.512.206 I perplexity: tokenizing the input ..
0.00.519.069 I perplexity: tokenization took 6.86 ms
0.00.519.074 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.652.308 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.653.662 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.653.688 I llama_perf_context_print:        load time =     502.82 ms
0.00.653.689 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.30 tokens per second)
0.00.653.689 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.653.690 I llama_perf_context_print:       total time =     141.54 ms /   129 tokens
0.00.654.289 I ggml_metal_free: deallocating

real	0m0.668s
user	0m0.083s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.391 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.599 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.606 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.608 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.610 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.611 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.611 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.611 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.612 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.612 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.613 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.506 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.256 I llama_model_loader: - type  f32:  194 tensors
0.00.026.257 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.257 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.257 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.258 I print_info: file format = GGUF V3 (latest)
0.00.026.258 I print_info: file type   = Q4_K - Medium
0.00.026.260 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.469 I load: special tokens cache size = 25
0.00.040.669 I load: token to piece cache size = 0.2984 MB
0.00.040.688 I print_info: arch             = gptneox
0.00.040.689 I print_info: vocab_only       = 0
0.00.040.689 I print_info: n_ctx_train      = 2048
0.00.040.689 I print_info: n_embd           = 2048
0.00.040.689 I print_info: n_layer          = 24
0.00.040.694 I print_info: n_head           = 16
0.00.040.695 I print_info: n_head_kv        = 16
0.00.040.695 I print_info: n_rot            = 32
0.00.040.695 I print_info: n_swa            = 0
0.00.040.695 I print_info: n_embd_head_k    = 128
0.00.040.695 I print_info: n_embd_head_v    = 128
0.00.040.696 I print_info: n_gqa            = 1
0.00.040.696 I print_info: n_embd_k_gqa     = 2048
0.00.040.697 I print_info: n_embd_v_gqa     = 2048
0.00.040.698 I print_info: f_norm_eps       = 1.0e-05
0.00.040.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.699 I print_info: f_logit_scale    = 0.0e+00
0.00.040.699 I print_info: n_ff             = 8192
0.00.040.699 I print_info: n_expert         = 0
0.00.040.700 I print_info: n_expert_used    = 0
0.00.040.700 I print_info: causal attn      = 1
0.00.040.700 I print_info: pooling type     = 0
0.00.040.700 I print_info: rope type        = 2
0.00.040.700 I print_info: rope scaling     = linear
0.00.040.704 I print_info: freq_base_train  = 10000.0
0.00.040.704 I print_info: freq_scale_train = 1
0.00.040.704 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.704 I print_info: rope_finetuned   = unknown
0.00.040.705 I print_info: ssm_d_conv       = 0
0.00.040.705 I print_info: ssm_d_inner      = 0
0.00.040.705 I print_info: ssm_d_state      = 0
0.00.040.705 I print_info: ssm_dt_rank      = 0
0.00.040.705 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.705 I print_info: model type       = 1.4B
0.00.040.706 I print_info: model params     = 1.41 B
0.00.040.706 I print_info: general.name     = 1.4B
0.00.040.706 I print_info: vocab type       = BPE
0.00.040.707 I print_info: n_vocab          = 50304
0.00.040.707 I print_info: n_merges         = 50009
0.00.040.707 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.709 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.709 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.709 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.709 I print_info: LF token         = 187 'Ċ'
0.00.040.710 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.710 I print_info: max token length = 1024
0.00.040.710 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.544.283 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.303 I load_tensors: offloading output layer to GPU
0.00.544.303 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.341 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.544.342 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.545.763 I llama_context: n_seq_max     = 1
0.00.545.768 I llama_context: n_ctx         = 128
0.00.545.769 I llama_context: n_ctx_per_seq = 128
0.00.545.769 I llama_context: n_batch       = 128
0.00.545.769 I llama_context: n_ubatch      = 128
0.00.545.770 I llama_context: flash_attn    = 0
0.00.545.772 I llama_context: freq_base     = 10000.0
0.00.545.773 I llama_context: freq_scale    = 1
0.00.545.773 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.545.783 I ggml_metal_init: allocating
0.00.545.867 I ggml_metal_init: found device: Apple M4
0.00.545.882 I ggml_metal_init: picking default device: Apple M4
0.00.547.829 I ggml_metal_init: using embedded metal library
0.00.554.685 I ggml_metal_init: GPU name:   Apple M4
0.00.554.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.554.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.554.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.554.700 I ggml_metal_init: simdgroup reduction   = true
0.00.554.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.554.701 I ggml_metal_init: has residency sets    = true
0.00.554.701 I ggml_metal_init: has bfloat            = true
0.00.554.701 I ggml_metal_init: use bfloat            = true
0.00.554.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.554.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.573.958 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.573.963 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.577.545 I init:      Metal KV buffer size =    24.00 MiB
0.00.577.551 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.580.706 I init:      Metal compute buffer size =    25.56 MiB
0.00.580.709 I init:        CPU compute buffer size =     1.06 MiB
0.00.580.709 I init: graph nodes  = 967
0.00.580.710 I init: graph splits = 2
0.00.580.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.580.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.612 I 
0.00.606.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.663 I perplexity: tokenizing the input ..
0.00.613.471 I perplexity: tokenization took 6.806 ms
0.00.613.478 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.886 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.748.223 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.748.244 I llama_perf_context_print:        load time =     596.21 ms
0.00.748.245 I llama_perf_context_print: prompt eval time =     132.39 ms /   128 tokens (    1.03 ms per token,   966.82 tokens per second)
0.00.748.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.246 I llama_perf_context_print:       total time =     141.64 ms /   129 tokens
0.00.748.836 I ggml_metal_free: deallocating

real	0m0.764s
user	0m0.081s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.349 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.382 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.382 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.382 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.385 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.385 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.388 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.390 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.391 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.354 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.391 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.391 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.392 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.392 I llama_model_loader: - type  f32:  194 tensors
0.00.025.393 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.393 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.394 I print_info: file format = GGUF V3 (latest)
0.00.025.394 I print_info: file type   = Q5_K - Medium
0.00.025.396 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.849 I load: special tokens cache size = 25
0.00.040.029 I load: token to piece cache size = 0.2984 MB
0.00.040.046 I print_info: arch             = gptneox
0.00.040.047 I print_info: vocab_only       = 0
0.00.040.047 I print_info: n_ctx_train      = 2048
0.00.040.047 I print_info: n_embd           = 2048
0.00.040.047 I print_info: n_layer          = 24
0.00.040.053 I print_info: n_head           = 16
0.00.040.054 I print_info: n_head_kv        = 16
0.00.040.054 I print_info: n_rot            = 32
0.00.040.054 I print_info: n_swa            = 0
0.00.040.054 I print_info: n_embd_head_k    = 128
0.00.040.055 I print_info: n_embd_head_v    = 128
0.00.040.055 I print_info: n_gqa            = 1
0.00.040.056 I print_info: n_embd_k_gqa     = 2048
0.00.040.056 I print_info: n_embd_v_gqa     = 2048
0.00.040.057 I print_info: f_norm_eps       = 1.0e-05
0.00.040.057 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.058 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.058 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.058 I print_info: f_logit_scale    = 0.0e+00
0.00.040.059 I print_info: n_ff             = 8192
0.00.040.059 I print_info: n_expert         = 0
0.00.040.059 I print_info: n_expert_used    = 0
0.00.040.059 I print_info: causal attn      = 1
0.00.040.059 I print_info: pooling type     = 0
0.00.040.059 I print_info: rope type        = 2
0.00.040.060 I print_info: rope scaling     = linear
0.00.040.060 I print_info: freq_base_train  = 10000.0
0.00.040.060 I print_info: freq_scale_train = 1
0.00.040.060 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.061 I print_info: rope_finetuned   = unknown
0.00.040.061 I print_info: ssm_d_conv       = 0
0.00.040.061 I print_info: ssm_d_inner      = 0
0.00.040.062 I print_info: ssm_d_state      = 0
0.00.040.062 I print_info: ssm_dt_rank      = 0
0.00.040.063 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.064 I print_info: model type       = 1.4B
0.00.040.064 I print_info: model params     = 1.41 B
0.00.040.064 I print_info: general.name     = 1.4B
0.00.040.065 I print_info: vocab type       = BPE
0.00.040.065 I print_info: n_vocab          = 50304
0.00.040.065 I print_info: n_merges         = 50009
0.00.040.065 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.065 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.066 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.066 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.066 I print_info: LF token         = 187 'Ċ'
0.00.040.066 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.066 I print_info: max token length = 1024
0.00.040.067 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.944 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.967 I load_tensors: offloading output layer to GPU
0.00.599.968 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.007 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.600.008 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.601.408 I llama_context: n_seq_max     = 1
0.00.601.413 I llama_context: n_ctx         = 128
0.00.601.414 I llama_context: n_ctx_per_seq = 128
0.00.601.414 I llama_context: n_batch       = 128
0.00.601.415 I llama_context: n_ubatch      = 128
0.00.601.415 I llama_context: flash_attn    = 0
0.00.601.417 I llama_context: freq_base     = 10000.0
0.00.601.418 I llama_context: freq_scale    = 1
0.00.601.418 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.420 I ggml_metal_init: allocating
0.00.601.510 I ggml_metal_init: found device: Apple M4
0.00.601.525 I ggml_metal_init: picking default device: Apple M4
0.00.603.430 I ggml_metal_init: using embedded metal library
0.00.610.272 I ggml_metal_init: GPU name:   Apple M4
0.00.610.282 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.283 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.283 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.284 I ggml_metal_init: simdgroup reduction   = true
0.00.610.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.285 I ggml_metal_init: has residency sets    = true
0.00.610.285 I ggml_metal_init: has bfloat            = true
0.00.610.285 I ggml_metal_init: use bfloat            = true
0.00.610.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.108 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.629.113 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.679 I init:      Metal KV buffer size =    24.00 MiB
0.00.632.682 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.635.942 I init:      Metal compute buffer size =    25.56 MiB
0.00.635.944 I init:        CPU compute buffer size =     1.06 MiB
0.00.635.945 I init: graph nodes  = 967
0.00.635.946 I init: graph splits = 2
0.00.635.949 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.950 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.753 I 
0.00.666.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.804 I perplexity: tokenizing the input ..
0.00.673.242 I perplexity: tokenization took 6.435 ms
0.00.673.247 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.683 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.812.026 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.812.050 I llama_perf_context_print:        load time =     657.40 ms
0.00.812.051 I llama_perf_context_print: prompt eval time =     136.80 ms /   128 tokens (    1.07 ms per token,   935.65 tokens per second)
0.00.812.051 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.052 I llama_perf_context_print:       total time =     145.30 ms /   129 tokens
0.00.812.608 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.082s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.396 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.527 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.540 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.541 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.541 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.542 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.545 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.547 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.547 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.547 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.447 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.449 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.449 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.450 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.451 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.451 I llama_model_loader: - type  f32:  194 tensors
0.00.026.452 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.452 I print_info: file format = GGUF V3 (latest)
0.00.026.453 I print_info: file type   = Q6_K
0.00.026.454 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.994 I load: special tokens cache size = 25
0.00.041.143 I load: token to piece cache size = 0.2984 MB
0.00.041.160 I print_info: arch             = gptneox
0.00.041.161 I print_info: vocab_only       = 0
0.00.041.161 I print_info: n_ctx_train      = 2048
0.00.041.161 I print_info: n_embd           = 2048
0.00.041.161 I print_info: n_layer          = 24
0.00.041.165 I print_info: n_head           = 16
0.00.041.166 I print_info: n_head_kv        = 16
0.00.041.166 I print_info: n_rot            = 32
0.00.041.166 I print_info: n_swa            = 0
0.00.041.167 I print_info: n_embd_head_k    = 128
0.00.041.167 I print_info: n_embd_head_v    = 128
0.00.041.167 I print_info: n_gqa            = 1
0.00.041.168 I print_info: n_embd_k_gqa     = 2048
0.00.041.169 I print_info: n_embd_v_gqa     = 2048
0.00.041.169 I print_info: f_norm_eps       = 1.0e-05
0.00.041.170 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.170 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.170 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.170 I print_info: f_logit_scale    = 0.0e+00
0.00.041.171 I print_info: n_ff             = 8192
0.00.041.172 I print_info: n_expert         = 0
0.00.041.172 I print_info: n_expert_used    = 0
0.00.041.172 I print_info: causal attn      = 1
0.00.041.173 I print_info: pooling type     = 0
0.00.041.173 I print_info: rope type        = 2
0.00.041.173 I print_info: rope scaling     = linear
0.00.041.173 I print_info: freq_base_train  = 10000.0
0.00.041.174 I print_info: freq_scale_train = 1
0.00.041.174 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.174 I print_info: rope_finetuned   = unknown
0.00.041.174 I print_info: ssm_d_conv       = 0
0.00.041.174 I print_info: ssm_d_inner      = 0
0.00.041.174 I print_info: ssm_d_state      = 0
0.00.041.174 I print_info: ssm_dt_rank      = 0
0.00.041.174 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.175 I print_info: model type       = 1.4B
0.00.041.175 I print_info: model params     = 1.41 B
0.00.041.175 I print_info: general.name     = 1.4B
0.00.041.176 I print_info: vocab type       = BPE
0.00.041.176 I print_info: n_vocab          = 50304
0.00.041.176 I print_info: n_merges         = 50009
0.00.041.176 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.176 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.179 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.179 I print_info: LF token         = 187 'Ċ'
0.00.041.179 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.179 I print_info: max token length = 1024
0.00.041.180 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.233.207 I load_tensors: offloading 24 repeating layers to GPU
0.00.233.214 I load_tensors: offloading output layer to GPU
0.00.233.215 I load_tensors: offloaded 25/25 layers to GPU
0.00.233.245 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.233.248 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.234.473 I llama_context: n_seq_max     = 1
0.00.234.476 I llama_context: n_ctx         = 128
0.00.234.476 I llama_context: n_ctx_per_seq = 128
0.00.234.477 I llama_context: n_batch       = 128
0.00.234.477 I llama_context: n_ubatch      = 128
0.00.234.477 I llama_context: flash_attn    = 0
0.00.234.479 I llama_context: freq_base     = 10000.0
0.00.234.480 I llama_context: freq_scale    = 1
0.00.234.480 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.234.482 I ggml_metal_init: allocating
0.00.234.559 I ggml_metal_init: found device: Apple M4
0.00.234.573 I ggml_metal_init: picking default device: Apple M4
0.00.236.163 I ggml_metal_init: using embedded metal library
0.00.242.168 I ggml_metal_init: GPU name:   Apple M4
0.00.242.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.242.175 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.242.176 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.242.177 I ggml_metal_init: simdgroup reduction   = true
0.00.242.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.242.177 I ggml_metal_init: has residency sets    = true
0.00.242.177 I ggml_metal_init: has bfloat            = true
0.00.242.178 I ggml_metal_init: use bfloat            = true
0.00.242.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.242.181 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.259.776 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.259.781 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.263.130 I init:      Metal KV buffer size =    24.00 MiB
0.00.263.135 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.266.261 I init:      Metal compute buffer size =    25.56 MiB
0.00.266.263 I init:        CPU compute buffer size =     1.06 MiB
0.00.266.263 I init: graph nodes  = 967
0.00.266.264 I init: graph splits = 2
0.00.266.267 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.266.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.300.325 I 
0.00.300.362 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.300.366 I perplexity: tokenizing the input ..
0.00.306.109 I perplexity: tokenization took 5.739 ms
0.00.306.122 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.437.753 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.439.109 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.439.133 I llama_perf_context_print:        load time =     289.92 ms
0.00.439.134 I llama_perf_context_print: prompt eval time =     130.65 ms /   128 tokens (    1.02 ms per token,   979.72 tokens per second)
0.00.439.134 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.439.135 I llama_perf_context_print:       total time =     138.81 ms /   129 tokens
0.00.439.717 I ggml_metal_free: deallocating

real	0m0.455s
user	0m0.079s
sys	0m0.088s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.381 I build: 4795 (9e50456e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.302 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.731 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.739 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.740 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.744 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.745 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.746 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.748 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.738 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.934 I llama_model_loader: - type  f32:  194 tensors
0.00.056.934 I llama_model_loader: - type  f16:   98 tensors
0.00.056.935 I print_info: file format = GGUF V3 (latest)
0.00.056.935 I print_info: file type   = all F32 (guessed)
0.00.056.937 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.952 I load: special tokens cache size = 25
0.00.075.580 I load: token to piece cache size = 0.2984 MB
0.00.075.594 I print_info: arch             = gptneox
0.00.075.595 I print_info: vocab_only       = 0
0.00.075.596 I print_info: n_ctx_train      = 2048
0.00.075.596 I print_info: n_embd           = 2048
0.00.075.596 I print_info: n_layer          = 24
0.00.075.600 I print_info: n_head           = 16
0.00.075.601 I print_info: n_head_kv        = 16
0.00.075.601 I print_info: n_rot            = 32
0.00.075.601 I print_info: n_swa            = 0
0.00.075.601 I print_info: n_embd_head_k    = 128
0.00.075.601 I print_info: n_embd_head_v    = 128
0.00.075.603 I print_info: n_gqa            = 1
0.00.075.603 I print_info: n_embd_k_gqa     = 2048
0.00.075.604 I print_info: n_embd_v_gqa     = 2048
0.00.075.605 I print_info: f_norm_eps       = 1.0e-05
0.00.075.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.606 I print_info: f_logit_scale    = 0.0e+00
0.00.075.606 I print_info: n_ff             = 8192
0.00.075.606 I print_info: n_expert         = 0
0.00.075.607 I print_info: n_expert_used    = 0
0.00.075.607 I print_info: causal attn      = 1
0.00.075.607 I print_info: pooling type     = 0
0.00.075.607 I print_info: rope type        = 2
0.00.075.607 I print_info: rope scaling     = linear
0.00.075.607 I print_info: freq_base_train  = 10000.0
0.00.075.608 I print_info: freq_scale_train = 1
0.00.075.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.608 I print_info: rope_finetuned   = unknown
0.00.075.608 I print_info: ssm_d_conv       = 0
0.00.075.608 I print_info: ssm_d_inner      = 0
0.00.075.610 I print_info: ssm_d_state      = 0
0.00.075.610 I print_info: ssm_dt_rank      = 0
0.00.075.612 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.612 I print_info: model type       = 1.4B
0.00.075.612 I print_info: model params     = 1.41 B
0.00.075.612 I print_info: general.name     = 1.4B
0.00.075.614 I print_info: vocab type       = BPE
0.00.075.614 I print_info: n_vocab          = 50304
0.00.075.614 I print_info: n_merges         = 50009
0.00.075.615 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.615 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.615 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.615 I print_info: LF token         = 187 'Ċ'
0.00.075.616 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.616 I print_info: max token length = 1024
0.00.075.616 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.373.012 I load_tensors: offloading 24 repeating layers to GPU
0.01.373.018 I load_tensors: offloading output layer to GPU
0.01.373.019 I load_tensors: offloaded 25/25 layers to GPU
0.01.373.043 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.373.045 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.373.852 I llama_context: n_seq_max     = 1
0.01.373.853 I llama_context: n_ctx         = 128
0.01.373.853 I llama_context: n_ctx_per_seq = 128
0.01.373.854 I llama_context: n_batch       = 128
0.01.373.854 I llama_context: n_ubatch      = 128
0.01.373.854 I llama_context: flash_attn    = 0
0.01.373.854 I llama_context: freq_base     = 10000.0
0.01.373.855 I llama_context: freq_scale    = 1
0.01.373.855 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.373.856 I ggml_metal_init: allocating
0.01.373.909 I ggml_metal_init: found device: Apple M4
0.01.373.916 I ggml_metal_init: picking default device: Apple M4
0.01.375.094 I ggml_metal_init: using embedded metal library
0.01.379.513 I ggml_metal_init: GPU name:   Apple M4
0.01.379.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.379.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.379.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.379.517 I ggml_metal_init: simdgroup reduction   = true
0.01.379.517 I ggml_metal_init: simdgroup matrix mul. = true
0.01.379.517 I ggml_metal_init: has residency sets    = true
0.01.379.518 I ggml_metal_init: has bfloat            = true
0.01.379.518 I ggml_metal_init: use bfloat            = true
0.01.379.519 I ggml_metal_init: hasUnifiedMemory      = true
0.01.379.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.392.133 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.392.136 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.394.042 I init:      Metal KV buffer size =    24.00 MiB
0.01.394.044 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.395.796 I init:      Metal compute buffer size =    25.56 MiB
0.01.395.797 I init:        CPU compute buffer size =     1.06 MiB
0.01.395.798 I init: graph nodes  = 967
0.01.395.798 I init: graph splits = 2
0.01.395.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.395.800 I 
0.01.395.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.395.828 I compute_imatrix: tokenizing the input ..
0.01.400.120 I compute_imatrix: tokenization took 4.291 ms
0.01.400.122 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.664.594 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.667.491 I llama_perf_context_print:        load time =    1642.29 ms
0.01.667.492 I llama_perf_context_print: prompt eval time =     263.20 ms /   128 tokens (    2.06 ms per token,   486.31 tokens per second)
0.01.667.492 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.667.493 I llama_perf_context_print:       total time =    1645.18 ms /   129 tokens
0.01.668.312 I ggml_metal_free: deallocating

real	0m1.855s
user	0m0.133s
sys	0m0.270s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4795 (9e50456e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14da05740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14da05e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14da06430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14da069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14da06f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14da07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14da07af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14da080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14da08650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14da08b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14da09050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14da09550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14da0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14da0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14da0b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14da0b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14da0be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14da0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14da0ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14da0d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14da0dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14da0e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14da0e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14da0f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14da0f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14da0fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14da10270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14da10ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14da11420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14da116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14da11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14da11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14da126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14da12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14da12ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14da13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14da13810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14da13cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14da14150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14da145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14da14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14da14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14da153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14da15870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14da15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14da16140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14da16750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14da17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14da17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14da17c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14da182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14da188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14da18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14da194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14da19cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14da1a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14da1a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14da1a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14da1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14da1b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14da1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14da1be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14da1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14da1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14da1cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14da1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14da1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14da1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14da1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14da1e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14da1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14da1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14da1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14da1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14da1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14da200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14da20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14da20b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14da210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14da21630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14da21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14da220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14da22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14da22b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14da230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14da23610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14da23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14da240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14da24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14da24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14da250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14da255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14da25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14da26090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14da265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14da26b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14da27080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14da16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14da274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14da27ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14da281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14da28740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14da28c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14da291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14da29730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14da29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14da2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14da2a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14da2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14da2b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14da2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14da2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14da2c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14da2c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14da2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14da2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14da2d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14da2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14da2dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14da2e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14da2e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14da2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14da2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14da2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14da2f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14da2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14da30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14da30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14da30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14da31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14da314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14da31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14da31e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14da322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14da32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14da32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14da330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14da33550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14da339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14da33e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14da34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14da347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14da34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14da35110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14da355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14da35a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14da35ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14da36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14da36830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14da36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14da37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14da37610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14da37ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14da37f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14da383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14da38890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14da38d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14da391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14da39670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14da39b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14da39fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14da3a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14da3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14da3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14da3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14da3b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14da3bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14da3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14da3c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14da3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14da3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14da3d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14da3d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14da3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14da3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14da3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14da3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14da3ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14da3f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14da3f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14da3fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14da400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14da40570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14da40a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14da40eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14da41350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14da417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14da41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14da42130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14da425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14da42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14da42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14da433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14da43900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14da43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14da443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14da448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14da44bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14da451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14da457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14da45de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14da465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14da46a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14da46d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14da47340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14da47950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14da48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14da485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14da48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14da48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14da496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14da49c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14da4a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14da4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14da4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14da4b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14da4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14da4bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14da4c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14da4c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14da4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14da4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14da4d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14da4dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14da4e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14da4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14da4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14da4f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14da4f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14da4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14da50110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14da50660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14da50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14da51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14da51650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14da51ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14da520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14da52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14da52b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14da530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14da53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14da53b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14da540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14da54620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14da54b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14da550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14da55610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14da55b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14da560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14da56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14da56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14da570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14da575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14da57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14da58090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14da585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14da58b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14da59080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14da595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14da59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14da5a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14da5a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14da5ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14da5b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14da5b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14da5bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14da5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14da5c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14da5c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14da5ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14da5d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14da5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14da5dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14da5e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14da5e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14da5e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14da5ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14da5f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14da5f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14da5fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14da60110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14da605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14da60b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14da61220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14da61940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14da62060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14da62780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14da62a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14da63230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14da634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14da63b00 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
0.00.695.654 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.695.658 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d9072e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d907750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d907bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d90a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d90a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d90a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d90b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d90b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d90bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d90c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d90c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d90c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d90d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d90d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d90e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d90e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d90ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d90f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d90fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d910760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d910e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d9115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d911cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d9123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d912b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d912dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d9133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d9139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d913ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d9147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d914c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d914f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d9157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d915d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d915fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d916470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d916910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d916db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d917250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d9176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d917b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d918030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d9184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d918970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d918c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d919240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d919850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d919e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d91a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d91aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d91b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d91b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d91bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d91c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d91cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d91cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d91d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d91d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d91dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d91e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d91e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d91edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d91f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d91f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d91fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d920070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d920510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d9209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d920e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d9212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d921790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d921c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d9220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d922620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d922b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d9230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d923610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d923b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d9240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d924600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d924b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d9250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d9255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d925b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d926090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d9265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d926b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d927080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d9275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d927b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d928070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d9285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d928b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d929060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d9295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d929b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d92a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d92a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d92aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d92b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d92b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d92bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d92c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d92c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d92cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d92d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d92d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d92dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d92e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d92e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d92eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d92f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d92f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d92f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d92fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d930330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d9307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d930c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d931110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d9315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d931a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d931ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d932390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d932830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d932cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d933170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d933610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d933ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d933f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d9343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d934890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d934d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d9351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d935670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d935b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d935fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d936450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d9368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d936d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d937230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d9376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d937b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d938010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d9384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d938950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d938df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d939290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d939730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d939bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d93a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d93a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d93a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d93ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d93b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d93b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d93bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d93c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d93c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d93ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d93ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d93d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d93d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d93dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d93e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d93e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d93ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d93ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d93f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d93f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d93fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d940190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d940630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d940ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d940f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d941410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d9418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d941d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d9421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d942690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d942b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d942fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d943470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d943910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d943db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d944250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d9446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d944b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d945030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d9454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d945970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d945e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d9462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d946750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d946ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d9471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d947740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d947c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d947f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d948560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d948b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d949180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d949970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d949e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d94a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d94a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d94acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d94b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d94b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d94be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d94c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d94ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d94cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d94d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d94da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d94dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d94e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d94ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d94efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d94f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d94fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d94ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d9504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d950a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d950f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d9514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d951a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d951f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d9524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d952a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d952f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d9534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d953a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d953f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d9544a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d9549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d954f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d955490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d9559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d955f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d956480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d9569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d956f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d957470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d9579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d957f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d958460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d9589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d958f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d959450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d9599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d959ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d95a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d95a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d95aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d95b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d95b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d95bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d95c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d95c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d95cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d95d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d95d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d95deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d95e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d95e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d95eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d95f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d95f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d95fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d9601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d960670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d960b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d960fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d961450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d9618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d961d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d962230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d9626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d962b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d963010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d9634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d963950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d963ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d9645c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d964ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d965400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d965b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d965de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d9665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d966890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d966ea0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14db04e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14db052f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14db05760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14db05bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14db06040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14db064b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14db06920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14db06d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14db07200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14db07670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14db07ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14db08200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14db08d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14db094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14db09ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14db0a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14db0ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14db0b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14db0b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14db0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14db0c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14db0ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14db0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14db0dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14db0e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14db0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14db0e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14db0ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14db0f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14db0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14db0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14db100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14db10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14db107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14db10c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14db110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14db11520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14db11990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14db11e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14db12270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14db126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14db12b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14db12fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14db13430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14db138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14db13d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14db14180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14db145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14db14a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14db14ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14db15340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14db157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14db15c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14db16090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14db16500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14db16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14db16ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14db173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14db17850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14db17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14db18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14db185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14db18a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14db18e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14db192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14db19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14db19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14db1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14db1a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14db1a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14db1ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14db1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14db1b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14db1bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14db1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14db1c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14db1c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14db1cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14db1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14db1d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14db1d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14db1de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14db1e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14db1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14db1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14db1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14db1f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14db1f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14db1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14db201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14db20650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14db20ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14db20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14db213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14db21810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14db21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14db220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14db22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14db229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14db22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14db232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14db23720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14db23b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14db24000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14db24970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14db24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14db250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14db25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14db25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14db25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14db26260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14db266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14db26b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14db26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14db27420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14db27890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14db27d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14db28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14db285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14db28a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14db28ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14db29330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14db297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14db29c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14db2a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14db2a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14db2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14db2add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14db2b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14db2b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14db2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14db2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14db2c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14db2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14db2cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14db2d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14db2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14db2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14db2dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14db2e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14db2e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14db2ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14db2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14db2f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14db2f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14db2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14db30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14db30690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14db30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14db30f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14db313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14db31850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14db31cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14db32130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14db325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14db32a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14db32e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14db332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14db33760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14db33bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14db34040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14db344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14db34920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14db34d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14db35200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14db35670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14db35ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14db35f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14db363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14db36830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14db36ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14db37110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14db37580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14db379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14db37e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14db382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14db38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14db38bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14db39020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14db39490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14db39900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14db39d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14db3a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14db3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14db3aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14db3af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14db3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14db3b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14db3bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14db3c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14db3c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14db3c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14db3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14db3d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14db3d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14db3db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14db3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14db3e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14db3e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14db3ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14db3f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14db3f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14db3faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14db3ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14db40380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14db407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14db40c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14db411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14db41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14db41ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14db42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14db428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14db42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14db43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14db43480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14db438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14db43d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14db441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14db44640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14db44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14db44f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14db45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14db45800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14db45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14db460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14db46550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14db469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14db46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14db472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14db47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14db47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14db47ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14db48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14db488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14db48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14db491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14db49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14db49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14db49f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14db4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14db4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14db4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14db4b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14db4b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14db4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14db4be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14db4c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14db4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14db4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14db4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14db4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14db4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14db4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14db4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14db4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14db4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14db4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14db4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14db4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14db4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14db500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14db50510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14db50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14db50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14db51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14db516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14db51b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14db51fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14db52420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14db52890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14db52d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14db53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14db535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14db53a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14db53ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14db54330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14db547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14db54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14db55080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14db554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14db55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14db55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14db56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14db56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14db573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14db57af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14db58210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14db584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14db58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14db58f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14db59550 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.780s
user	0m0.278s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4795 (9e50456e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159f0b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159f0b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159f0bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159f0c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159f0c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159f0ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159f0d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159f0da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159f0dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159f0e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159f0e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159f0eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x159f0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159f10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159f109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x159f110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x159f117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x159f11f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x159f12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159f12df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159f13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159f14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159f14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159f15310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159f155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159f15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159f16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159f16d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159f174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159f177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159f18040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159f18580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159f18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159f18ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159f19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159f19620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159f19ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159f19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159f1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159f1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159f1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159f1b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159f1b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159f1bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159f1c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159f1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159f1cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159f1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159f1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159f1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159f1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159f1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159f1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159f1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159f1ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159f20230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159f20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159f21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159f212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159f21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159f21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159f220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159f22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159f22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159f22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159f23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159f237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159f23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159f24130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159f245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159f24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159f24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159f25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x159f25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159f25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159f26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159f26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159f26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159f274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159f27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159f27f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159f284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x159f28a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159f28f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159f294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159f29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159f29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159f2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159f2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159f2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159f2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159f2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159f2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159f2c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159f2c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159f1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159f2ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159f2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159f2db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159f2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159f2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159f2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159f2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159f2f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159f30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159f305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159f30b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x159f31080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159f315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159f31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159f31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159f32460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159f32900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159f32da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159f33240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159f336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159f33b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159f34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159f344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159f34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159f34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159f352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159f35740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159f35be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159f36080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159f36520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159f369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159f36e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159f377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x159f37c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159f380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159f38580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159f38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159f38ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159f39360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159f39800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159f39ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159f3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159f3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159f3aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159f3af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159f3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159f3b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159f3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159f3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159f3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159f3cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159f3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159f3d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159f3d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159f3dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159f3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159f3e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159f3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159f3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159f3f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159f3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159f3fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159f40260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159f40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159f41040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159f414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159f41980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159f41e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159f422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159f42760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159f42c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159f430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159f43540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159f439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159f43e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159f44320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159f447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159f44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159f45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159f455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x159f45a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159f45ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159f46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159f46820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159f46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159f47160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159f47600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159f47aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159f47f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159f483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159f48880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159f48d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159f49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159f497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159f49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159f4a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159f4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x159f4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159f4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159f4b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159f4bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159f4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159f4c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159f4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159f4d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159f4dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159f4df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159f4e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159f4e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159f4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159f4f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159f4fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159f50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159f50ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159f51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159f51570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159f51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159f52010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159f52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159f52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159f53000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159f53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159f53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159f53ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159f54540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159f54a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159f54fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159f55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159f55a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159f55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159f56520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159f56a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159f56fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159f57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159f57a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159f57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159f58500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x159f58a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159f58fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159f594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x159f59a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159f59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159f5a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159f5aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159f5af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159f5b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159f5ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159f5bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159f5c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159f5ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159f5cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159f5da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159f5df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159f5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159f5e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159f5ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159f5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159f5f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159f5ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159f60480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159f609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159f60f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159f61470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159f619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159f61e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159f62300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159f627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159f62c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159f630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159f63580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159f63a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159f63ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159f64360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159f64800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159f64ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159f65140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159f655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159f65a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159f65f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159f66470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159f66b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159f672b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159f679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159f680f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159f683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159f68ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159f68e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159f69470 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
0.00.102.020 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.025 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159e08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159e091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159e0bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159e0c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159e0c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159e0c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159e0cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159e0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159e0d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159e0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159e0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159e0e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x159e0efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159e0f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159e0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x159e106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x159e10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x159e11500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x159e11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159e123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159e12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159e13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159e13950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159e14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159e14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159e14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159e14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159e15180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159e155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159e15a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159e15ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159e16400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159e16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159e16b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159e16fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159e17410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159e17880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159e17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159e18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159e185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159e18a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159e18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159e19320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159e19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159e19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159e1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159e1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159e1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159e1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159e1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159e1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159e1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159e1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159e1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159e1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159e1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159e1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159e1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159e1dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159e1e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159e1e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159e1e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159e1ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159e1f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159e1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159e1fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159e1ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159e203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159e20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159e20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159e210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159e21560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159e219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159e21e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159e222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x159e22720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159e22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159e23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159e23470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159e238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159e23d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159e241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159e24630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159e24aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x159e24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159e25380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159e257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159e25c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159e260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159e26540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159e269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159e26e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159e27290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159e27700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159e27b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159e27fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159e28450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159e288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159e28d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159e291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159e29610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159e29a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159e29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159e2a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159e2a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159e2ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159e2b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159e2b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159e2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159e2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x159e2c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159e2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159e2cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159e2cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159e2d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159e2d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159e2dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159e2e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159e2e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159e2ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159e2eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159e2f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159e2f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159e2fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159e30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159e30500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159e30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159e30de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159e31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159e316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159e31b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159e31fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159e32410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x159e32880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159e32cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159e33160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159e335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159e33a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159e33eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159e34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159e34790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159e34c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159e35070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159e354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159e35950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159e35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159e36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159e366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159e36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159e36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159e373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159e37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159e37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159e38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159e385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159e38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159e38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159e39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159e39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159e39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159e3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159e3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159e3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159e3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159e3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159e3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159e3c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159e3c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159e3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159e3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159e3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159e3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159e3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159e3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159e3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159e3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159e3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159e3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159e3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159e3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159e3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x159e401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159e40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159e40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159e40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159e413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159e41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159e41c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159e420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159e42560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159e429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159e42e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159e432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159e43720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159e43b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159e44000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159e44470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159e448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x159e44d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159e451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159e45630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159e45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159e46000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159e46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159e46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159e46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159e47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159e476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159e47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159e48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159e48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159e48f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159e494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159e49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159e4a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159e4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159e4abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159e4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159e4b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159e4bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159e4c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159e4c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159e4ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159e4d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159e4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159e4dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159e4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159e4eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159e4f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159e4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159e4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159e50230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159e507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159e50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159e51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159e51930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159e51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159e524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159e52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x159e53030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159e535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159e53bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x159e54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159e54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159e54cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159e552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159e55870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159e55e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159e563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159e569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159e56f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159e57530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159e57af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159e580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159e58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159e58c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159e591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159e597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159e59d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159e5a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159e5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159e5aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159e5b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159e5ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159e5bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159e5c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159e5cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159e5d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159e5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159e5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159e5e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159e5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159e5ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159e5ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159e5f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159e5f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159e5fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159e60330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159e60830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159e60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159e61230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159e61730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159e61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159e62640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159e62d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159e63480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159e63ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159e63e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159e64650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159e64910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159e64f20 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159f4adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159f4c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159f69120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159f4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159f4b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159f1e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159f1ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159f204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159f4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159f1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159f1cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x159f1bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159f1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159f1d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x159f14890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x159f2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x159f68670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x159f17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159f17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159f4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159f4ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159f15ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159f16160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159f16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159f698d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159f69b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159f69e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159f6a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159f6a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159f6a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159f6a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159f6ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159f6aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159f6b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159f6b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159f6b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159f6b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159f6bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159f6bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159f6c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159f6c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159f6c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159f6ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159f6cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159f6cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159f6d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159f6d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159f6d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159f6dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159f6dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159f6e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159f6e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159f6e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159f6e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159f6eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159f6ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159f6f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159f6f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159f6f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159f6f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159f6fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159f6fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159f70150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159f70410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159f706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159f70990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159f70c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159f70f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159f711d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159f71490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159f71750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159f71a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159f71cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159f71f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x159f72250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159f72510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159f727d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159f72a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159f72d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159f73010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159f732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159f73590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159f73850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x159f73b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159f73dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159f74090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159f74350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159f74610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159f748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159f74b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159f74e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159f75110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159f753d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159f75690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159f75950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159f75c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159f75ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159f76190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159f76450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159f76710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159f769d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159f76c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159f76f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159f77210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159f774d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159f77790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159f77a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159f77d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159f77fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x159f78290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159f78550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159f78810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159f78ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159f78d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159f79050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159f79310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159f795d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159f79890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159f79b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159f79e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159f7a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159f7a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159f7a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159f7a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159f7abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159f7ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159f7b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159f7b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159f7b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159f7b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159f7bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159f7bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x159f7c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159f7c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159f7c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159f7ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159f7ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159f7cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159f7d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159f7d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159f7d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159f7da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159f7dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159f7e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159f7e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159f7e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159f7e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159f7eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159f7edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159f7f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159f7f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159f7f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159f7f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159f7fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159f7fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159f80110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159f803d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159f80690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159f80950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159f80c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159f80ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159f81190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159f81450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159f81710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159f819d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159f81c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159f81f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159f82210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159f824d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159f82790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159f82a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159f82d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159f82fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159f83290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159f83550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159f83810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159f83ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159f83d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159f84050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159f84310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x159f845d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159f84890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159f84b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159f84e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159f850d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159f85390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159f85650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159f85910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159f85bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159f85e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159f86150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159f86410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159f866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159f86990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159f86c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159f86f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159f871d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x159f87490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159f87750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159f87a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159f87cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159f87f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159f88250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159f88510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159f887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159f88a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159f88d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159f89010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159f89410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159f89bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159f89e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159f8a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159f8a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159f8aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159f8ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159f8b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159f8b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159f8bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159f8c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159f8c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159f8c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159f8cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159f8d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159f8d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159f8daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159f8df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159f8e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159f8e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159f8ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159f8f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159f8f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159f8fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159f8fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159f902e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159f90750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159f90bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159f91030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159f914a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x159f91910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159f91d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159f921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x159f92660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159f92ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159f92f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159f933b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159f93820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159f93c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159f94100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159f94570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159f949e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159f94e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159f952c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159f95730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159f95ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159f96010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159f96480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159f968f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159f96d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159f971d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159f97640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159f97ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159f97f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159f98390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159f98800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159f98c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159f990e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159f99550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159f999c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159f99e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159f9a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159f9a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159f9ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159f9aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159f9b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159f9b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159f9bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159f9c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159f9c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159f9ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159f9cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159f9d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159f9d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159f9e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159f9e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159f9f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159f9f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159f9fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159fa0260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159fa0520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159fa0b30 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.953s
user	0m0.231s
sys	0m0.175s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
