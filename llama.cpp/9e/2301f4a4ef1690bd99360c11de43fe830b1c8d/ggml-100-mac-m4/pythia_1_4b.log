Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:42 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:104 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.583s
user	0m0.736s
sys	0m0.963s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target sha1
[  6%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 11%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Linking CXX shared library libllama.dylib
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Built target llama-gguf-hash
[ 21%] Built target llama
[ 21%] Built target llama-gguf
[ 21%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 21%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 21%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 21%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 22%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Linking C executable ../bin/test-c
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Linking CXX executable ../../bin/llama-run
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Built target llava
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target test-c
[ 31%] Built target llama-simple
[ 31%] Built target llama-simple-chat
[ 31%] Built target llama-quantize-stats
[ 31%] Built target llama-run
[ 31%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target common
[ 32%] Built target llava_static
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 40%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-grammar-parser
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-chat-template
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-log
[ 48%] Built target test-sampling
[ 48%] Built target test-arg-parser
[ 48%] Built target test-chat-template
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 51%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-model-load-cancel
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Built target test-llama-grammar
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-quantize-fns
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 61%] Linking CXX executable ../bin/test-rope
[ 62%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-backend-ops
[ 62%] Built target test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 62%] Linking CXX executable ../../bin/llama-batched
[ 62%] Built target test-barrier
[ 62%] Built target test-autorelease
[ 62%] Built target test-quantize-perf
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Built target test-rope
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Built target llama-batched-bench
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Built target llama-batched
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target test-json-schema-to-grammar
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-imatrix
[ 69%] Linking CXX executable ../../bin/llama-infill
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 70%] Built target llama-embedding
[ 70%] Built target llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-bench
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Linking CXX executable ../../bin/llama-lookup
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-infill
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target llama-imatrix
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Built target llama-lookahead
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Built target llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Generating loading.html.hpp
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating completion.js.hpp
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-cli
[ 82%] Built target llama-lookup-stats
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Generating deps_daisyui.min.css.hpp
[ 84%] Built target llama-passkey
[ 84%] Built target llama-parallel
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Built target llama-perplexity
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 85%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 87%] Built target llama-retrieval
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Linking CXX executable ../../bin/llama-cvector-generator
[ 92%] Linking CXX executable ../../bin/llama-export-lora
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-speculative
[ 95%] Built target llama-speculative-simple
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-tokenize
[ 95%] Built target llama-save-load-state
[ 95%] Built target llama-cvector-generator
[ 95%] Generating deps_markdown-it.js.hpp
[ 97%] Generating deps_tailwindcss.js.hpp
[ 97%] Generating deps_vue.esm-browser.js.hpp
[ 97%] Generating index.html.hpp
[ 97%] Built target llama-export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.554s
user	0m5.980s
sys	0m9.630s

main: quantize time =  4501.93 ms
main:    total time =  4501.93 ms

main: quantize time =  2077.71 ms
main:    total time =  2077.71 ms

main: quantize time =  1901.69 ms
main:    total time =  1901.69 ms

main: quantize time =  2064.71 ms
main:    total time =  2064.71 ms

main: quantize time =  2193.46 ms
main:    total time =  2193.46 ms

main: quantize time =  4723.42 ms
main:    total time =  4723.42 ms

main: quantize time =  5266.60 ms
main:    total time =  5266.60 ms

main: quantize time =  6330.80 ms
main:    total time =  6330.80 ms

main: quantize time =  5451.79 ms
main:    total time =  5451.79 ms

main: quantize time =  4240.56 ms
main:    total time =  4240.56 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.148 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.272 I main: llama backend init
0.00.000.291 I main: load the model and apply lora adapter, if any
0.00.046.042 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.060.621 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.635 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.642 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.643 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.646 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.656 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.657 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.664 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.069.247 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.071.540 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.894 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.078.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.898 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.899 I llama_model_loader: - type  f32:  194 tensors
0.00.078.899 I llama_model_loader: - type  f16:   98 tensors
0.00.110.728 I llm_load_vocab: special tokens cache size = 25
0.00.117.798 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.117.801 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.117.801 I llm_load_print_meta: arch             = gptneox
0.00.117.802 I llm_load_print_meta: vocab type       = BPE
0.00.117.802 I llm_load_print_meta: n_vocab          = 50304
0.00.117.802 I llm_load_print_meta: n_merges         = 50009
0.00.117.802 I llm_load_print_meta: vocab_only       = 0
0.00.117.802 I llm_load_print_meta: n_ctx_train      = 2048
0.00.117.803 I llm_load_print_meta: n_embd           = 2048
0.00.117.803 I llm_load_print_meta: n_layer          = 24
0.00.117.805 I llm_load_print_meta: n_head           = 16
0.00.117.806 I llm_load_print_meta: n_head_kv        = 16
0.00.117.806 I llm_load_print_meta: n_rot            = 32
0.00.117.807 I llm_load_print_meta: n_swa            = 0
0.00.117.807 I llm_load_print_meta: n_embd_head_k    = 128
0.00.117.807 I llm_load_print_meta: n_embd_head_v    = 128
0.00.117.807 I llm_load_print_meta: n_gqa            = 1
0.00.117.808 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.117.809 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.117.809 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.117.810 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.117.810 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.117.810 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.117.810 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.117.811 I llm_load_print_meta: n_ff             = 8192
0.00.117.811 I llm_load_print_meta: n_expert         = 0
0.00.117.811 I llm_load_print_meta: n_expert_used    = 0
0.00.117.811 I llm_load_print_meta: causal attn      = 1
0.00.117.813 I llm_load_print_meta: pooling type     = 0
0.00.117.813 I llm_load_print_meta: rope type        = 2
0.00.117.813 I llm_load_print_meta: rope scaling     = linear
0.00.117.813 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.117.814 I llm_load_print_meta: freq_scale_train = 1
0.00.117.814 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.117.814 I llm_load_print_meta: rope_finetuned   = unknown
0.00.117.814 I llm_load_print_meta: ssm_d_conv       = 0
0.00.117.814 I llm_load_print_meta: ssm_d_inner      = 0
0.00.117.814 I llm_load_print_meta: ssm_d_state      = 0
0.00.117.816 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.117.816 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.117.828 I llm_load_print_meta: model type       = 1.4B
0.00.117.829 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.117.829 I llm_load_print_meta: model params     = 1.41 B
0.00.117.829 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.117.830 I llm_load_print_meta: general.name     = 1.4B
0.00.117.830 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.117.830 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.117.830 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.117.831 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.117.831 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.117.831 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.117.831 I llm_load_print_meta: max token length = 1024
0.00.119.891 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.119.891 I llm_load_tensors: offloading output layer to GPU
0.00.119.891 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.119.909 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.119.910 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.120.815 I llama_new_context_with_model: n_seq_max     = 1
0.00.120.816 I llama_new_context_with_model: n_ctx         = 2048
0.00.120.816 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.120.816 I llama_new_context_with_model: n_batch       = 2048
0.00.120.816 I llama_new_context_with_model: n_ubatch      = 512
0.00.120.817 I llama_new_context_with_model: flash_attn    = 0
0.00.120.817 I llama_new_context_with_model: freq_base     = 10000.0
0.00.120.817 I llama_new_context_with_model: freq_scale    = 1
0.00.120.818 I ggml_metal_init: allocating
0.00.120.825 I ggml_metal_init: found device: Apple M4
0.00.120.827 I ggml_metal_init: picking default device: Apple M4
0.00.121.482 I ggml_metal_init: using embedded metal library
0.00.130.165 I ggml_metal_init: GPU name:   Apple M4
0.00.130.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.130.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.130.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.130.168 I ggml_metal_init: simdgroup reduction   = true
0.00.130.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.130.168 I ggml_metal_init: has bfloat            = true
0.00.130.168 I ggml_metal_init: use bfloat            = true
0.00.130.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.130.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.167.747 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.167.753 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.167.773 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.168.709 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.168.710 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.168.710 I llama_new_context_with_model: graph nodes  = 967
0.00.168.711 I llama_new_context_with_model: graph splits = 2
0.00.168.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.238.230 I main: llama threadpool init, n_threads = 4
0.00.238.263 I 
0.00.238.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.238.279 I 
0.00.238.354 I sampler seed: 1234
0.00.238.358 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.238.381 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.238.383 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.238.383 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.030.330 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.02.030.331 I llama_perf_context_print:        load time =     192.18 ms
0.02.030.332 I llama_perf_context_print: prompt eval time =      37.59 ms /     7 tokens (    5.37 ms per token,   186.20 tokens per second)
0.02.030.333 I llama_perf_context_print:        eval time =    1751.38 ms /    63 runs   (   27.80 ms per token,    35.97 tokens per second)
0.02.030.335 I llama_perf_context_print:       total time =    1792.10 ms /    70 tokens
0.02.030.507 I ggml_metal_free: deallocating

real	0m2.364s
user	0m0.153s
sys	0m0.094s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.271 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.429 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.429 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.429 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.437 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.437 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.268 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.248 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.250 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.251 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.251 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.251 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.252 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.252 I llama_model_loader: - type  f32:  194 tensors
0.00.034.252 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.870 I llm_load_vocab: special tokens cache size = 25
0.00.064.191 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.196 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.196 I llm_load_print_meta: arch             = gptneox
0.00.064.196 I llm_load_print_meta: vocab type       = BPE
0.00.064.197 I llm_load_print_meta: n_vocab          = 50304
0.00.064.197 I llm_load_print_meta: n_merges         = 50009
0.00.064.197 I llm_load_print_meta: vocab_only       = 0
0.00.064.197 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.197 I llm_load_print_meta: n_embd           = 2048
0.00.064.197 I llm_load_print_meta: n_layer          = 24
0.00.064.202 I llm_load_print_meta: n_head           = 16
0.00.064.203 I llm_load_print_meta: n_head_kv        = 16
0.00.064.203 I llm_load_print_meta: n_rot            = 32
0.00.064.203 I llm_load_print_meta: n_swa            = 0
0.00.064.203 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.203 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.206 I llm_load_print_meta: n_gqa            = 1
0.00.064.207 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.207 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.208 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.209 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.209 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.209 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.209 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.210 I llm_load_print_meta: n_ff             = 8192
0.00.064.210 I llm_load_print_meta: n_expert         = 0
0.00.064.213 I llm_load_print_meta: n_expert_used    = 0
0.00.064.213 I llm_load_print_meta: causal attn      = 1
0.00.064.213 I llm_load_print_meta: pooling type     = 0
0.00.064.213 I llm_load_print_meta: rope type        = 2
0.00.064.213 I llm_load_print_meta: rope scaling     = linear
0.00.064.214 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.214 I llm_load_print_meta: freq_scale_train = 1
0.00.064.214 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.214 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.214 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.215 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.215 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.215 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.215 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.228 I llm_load_print_meta: model type       = 1.4B
0.00.064.228 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.228 I llm_load_print_meta: model params     = 1.41 B
0.00.064.229 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.229 I llm_load_print_meta: general.name     = 1.4B
0.00.064.229 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.229 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.230 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.230 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.230 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.230 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.231 I llm_load_print_meta: max token length = 1024
0.00.066.392 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.392 I llm_load_tensors: offloading output layer to GPU
0.00.066.392 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.402 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.403 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.336 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.337 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.337 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.338 I llama_new_context_with_model: n_batch       = 2048
0.00.067.338 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.338 I llama_new_context_with_model: flash_attn    = 0
0.00.067.338 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.339 I llama_new_context_with_model: freq_scale    = 1
0.00.067.339 I ggml_metal_init: allocating
0.00.067.347 I ggml_metal_init: found device: Apple M4
0.00.067.349 I ggml_metal_init: picking default device: Apple M4
0.00.068.151 I ggml_metal_init: using embedded metal library
0.00.070.531 I ggml_metal_init: GPU name:   Apple M4
0.00.070.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.534 I ggml_metal_init: simdgroup reduction   = true
0.00.070.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.535 I ggml_metal_init: has bfloat            = true
0.00.070.535 I ggml_metal_init: use bfloat            = true
0.00.070.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.597 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.613 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.658 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.752 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.753 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.754 I llama_new_context_with_model: graph nodes  = 967
0.00.105.754 I llama_new_context_with_model: graph splits = 2
0.00.105.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.310.579 I main: llama threadpool init, n_threads = 4
0.01.310.644 I 
0.01.310.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.310.681 I 
0.01.311.019 I sampler seed: 1234
0.01.311.026 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.311.053 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.311.053 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.311.053 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.391.800 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.02.391.800 I llama_perf_context_print:        load time =    1301.30 ms
0.02.391.801 I llama_perf_context_print: prompt eval time =      34.48 ms /     7 tokens (    4.93 ms per token,   203.03 tokens per second)
0.02.391.802 I llama_perf_context_print:        eval time =    1043.31 ms /    63 runs   (   16.56 ms per token,    60.38 tokens per second)
0.02.391.802 I llama_perf_context_print:       total time =    1081.23 ms /    70 tokens
0.02.391.969 I ggml_metal_free: deallocating

real	0m2.409s
user	0m0.125s
sys	0m0.288s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.012.330 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.162 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.170 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.172 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.178 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.935 I llama_model_loader: - type  f32:  194 tensors
0.00.028.935 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.936 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.258 I llm_load_vocab: special tokens cache size = 25
0.00.055.360 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.363 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.363 I llm_load_print_meta: arch             = gptneox
0.00.055.364 I llm_load_print_meta: vocab type       = BPE
0.00.055.364 I llm_load_print_meta: n_vocab          = 50304
0.00.055.364 I llm_load_print_meta: n_merges         = 50009
0.00.055.365 I llm_load_print_meta: vocab_only       = 0
0.00.055.365 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.365 I llm_load_print_meta: n_embd           = 2048
0.00.055.365 I llm_load_print_meta: n_layer          = 24
0.00.055.370 I llm_load_print_meta: n_head           = 16
0.00.055.370 I llm_load_print_meta: n_head_kv        = 16
0.00.055.371 I llm_load_print_meta: n_rot            = 32
0.00.055.371 I llm_load_print_meta: n_swa            = 0
0.00.055.371 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.371 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.372 I llm_load_print_meta: n_gqa            = 1
0.00.055.373 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.373 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.374 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.375 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.375 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.375 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.375 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.376 I llm_load_print_meta: n_ff             = 8192
0.00.055.376 I llm_load_print_meta: n_expert         = 0
0.00.055.376 I llm_load_print_meta: n_expert_used    = 0
0.00.055.377 I llm_load_print_meta: causal attn      = 1
0.00.055.377 I llm_load_print_meta: pooling type     = 0
0.00.055.377 I llm_load_print_meta: rope type        = 2
0.00.055.377 I llm_load_print_meta: rope scaling     = linear
0.00.055.378 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.378 I llm_load_print_meta: freq_scale_train = 1
0.00.055.378 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.379 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.379 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.379 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.379 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.379 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.379 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.392 I llm_load_print_meta: model type       = 1.4B
0.00.055.392 I llm_load_print_meta: model ftype      = Q4_0
0.00.055.393 I llm_load_print_meta: model params     = 1.41 B
0.00.055.393 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.055.393 I llm_load_print_meta: general.name     = 1.4B
0.00.055.393 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.394 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.394 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.394 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.394 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.394 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.395 I llm_load_print_meta: max token length = 1024
0.00.057.439 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.439 I llm_load_tensors: offloading output layer to GPU
0.00.057.439 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.449 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.450 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.058.332 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.333 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.333 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.333 I llama_new_context_with_model: n_batch       = 2048
0.00.058.334 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.334 I llama_new_context_with_model: flash_attn    = 0
0.00.058.334 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.334 I llama_new_context_with_model: freq_scale    = 1
0.00.058.335 I ggml_metal_init: allocating
0.00.058.341 I ggml_metal_init: found device: Apple M4
0.00.058.343 I ggml_metal_init: picking default device: Apple M4
0.00.059.024 I ggml_metal_init: using embedded metal library
0.00.061.187 I ggml_metal_init: GPU name:   Apple M4
0.00.061.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.190 I ggml_metal_init: simdgroup reduction   = true
0.00.061.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.190 I ggml_metal_init: has bfloat            = true
0.00.061.190 I ggml_metal_init: use bfloat            = true
0.00.061.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.191 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.372 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.385 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.418 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.557 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.560 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.560 I llama_new_context_with_model: graph nodes  = 967
0.00.094.560 I llama_new_context_with_model: graph splits = 2
0.00.094.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.821 I main: llama threadpool init, n_threads = 4
0.00.692.858 I 
0.00.692.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.692.882 I 
0.00.693.039 I sampler seed: 1234
0.00.693.044 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.054 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.054 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.054 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.372.882 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.372.883 I llama_perf_context_print:        load time =     680.49 ms
0.01.372.884 I llama_perf_context_print: prompt eval time =      39.09 ms /     7 tokens (    5.58 ms per token,   179.08 tokens per second)
0.01.372.885 I llama_perf_context_print:        eval time =     637.76 ms /    63 runs   (   10.12 ms per token,    98.78 tokens per second)
0.01.372.885 I llama_perf_context_print:       total time =     680.07 ms /    70 tokens
0.01.373.050 I ggml_metal_free: deallocating

real	0m1.393s
user	0m0.109s
sys	0m0.184s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.290 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.162 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.166 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.274 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.275 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.276 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.276 I llama_model_loader: - type  f32:  194 tensors
0.00.037.276 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.277 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.829 I llm_load_vocab: special tokens cache size = 25
0.00.068.818 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.822 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.822 I llm_load_print_meta: arch             = gptneox
0.00.068.822 I llm_load_print_meta: vocab type       = BPE
0.00.068.823 I llm_load_print_meta: n_vocab          = 50304
0.00.068.823 I llm_load_print_meta: n_merges         = 50009
0.00.068.823 I llm_load_print_meta: vocab_only       = 0
0.00.068.823 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.823 I llm_load_print_meta: n_embd           = 2048
0.00.068.823 I llm_load_print_meta: n_layer          = 24
0.00.068.827 I llm_load_print_meta: n_head           = 16
0.00.068.828 I llm_load_print_meta: n_head_kv        = 16
0.00.068.828 I llm_load_print_meta: n_rot            = 32
0.00.068.828 I llm_load_print_meta: n_swa            = 0
0.00.068.828 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.828 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.829 I llm_load_print_meta: n_gqa            = 1
0.00.068.830 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.831 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.831 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.832 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.832 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.832 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.832 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.833 I llm_load_print_meta: n_ff             = 8192
0.00.068.833 I llm_load_print_meta: n_expert         = 0
0.00.068.834 I llm_load_print_meta: n_expert_used    = 0
0.00.068.834 I llm_load_print_meta: causal attn      = 1
0.00.068.834 I llm_load_print_meta: pooling type     = 0
0.00.068.834 I llm_load_print_meta: rope type        = 2
0.00.068.834 I llm_load_print_meta: rope scaling     = linear
0.00.068.835 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.835 I llm_load_print_meta: freq_scale_train = 1
0.00.068.835 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.835 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.836 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.836 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.836 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.836 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.836 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.848 I llm_load_print_meta: model type       = 1.4B
0.00.068.848 I llm_load_print_meta: model ftype      = Q4_1
0.00.068.849 I llm_load_print_meta: model params     = 1.41 B
0.00.068.849 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.068.850 I llm_load_print_meta: general.name     = 1.4B
0.00.068.851 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.851 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.851 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.852 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.852 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.852 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.853 I llm_load_print_meta: max token length = 1024
0.00.071.034 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.034 I llm_load_tensors: offloading output layer to GPU
0.00.071.035 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.044 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.071.046 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.072.117 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.118 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.118 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.118 I llama_new_context_with_model: n_batch       = 2048
0.00.072.118 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.119 I llama_new_context_with_model: flash_attn    = 0
0.00.072.119 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.119 I llama_new_context_with_model: freq_scale    = 1
0.00.072.120 I ggml_metal_init: allocating
0.00.072.124 I ggml_metal_init: found device: Apple M4
0.00.072.126 I ggml_metal_init: picking default device: Apple M4
0.00.072.777 I ggml_metal_init: using embedded metal library
0.00.075.125 I ggml_metal_init: GPU name:   Apple M4
0.00.075.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.127 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.128 I ggml_metal_init: simdgroup reduction   = true
0.00.075.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.128 I ggml_metal_init: has bfloat            = true
0.00.075.128 I ggml_metal_init: use bfloat            = true
0.00.075.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.837 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.844 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.863 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.928 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.930 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.930 I llama_new_context_with_model: graph nodes  = 967
0.00.107.930 I llama_new_context_with_model: graph splits = 2
0.00.107.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.835.212 I main: llama threadpool init, n_threads = 4
0.00.835.251 I 
0.00.835.284 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.835.285 I 
0.00.835.442 I sampler seed: 1234
0.00.835.447 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.835.465 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.835.465 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.835.465 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.555.400 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66541.71 tokens per second)
0.01.555.400 I llama_perf_context_print:        load time =     825.92 ms
0.01.555.401 I llama_perf_context_print: prompt eval time =      33.13 ms /     7 tokens (    4.73 ms per token,   211.30 tokens per second)
0.01.555.402 I llama_perf_context_print:        eval time =     684.00 ms /    63 runs   (   10.86 ms per token,    92.10 tokens per second)
0.01.555.402 I llama_perf_context_print:       total time =     720.19 ms /    70 tokens
0.01.555.582 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.116s
sys	0m0.192s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.013.871 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.790 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.022.795 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.797 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.797 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.798 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.805 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.805 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.798 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.207 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.209 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.209 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.210 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.210 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.211 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.034.211 I llama_model_loader: - type  f32:  194 tensors
0.00.034.211 I llama_model_loader: - type q5_0:   97 tensors
0.00.034.212 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.246 I llm_load_vocab: special tokens cache size = 25
0.00.078.346 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.350 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.350 I llm_load_print_meta: arch             = gptneox
0.00.078.351 I llm_load_print_meta: vocab type       = BPE
0.00.078.351 I llm_load_print_meta: n_vocab          = 50304
0.00.078.351 I llm_load_print_meta: n_merges         = 50009
0.00.078.351 I llm_load_print_meta: vocab_only       = 0
0.00.078.351 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.352 I llm_load_print_meta: n_embd           = 2048
0.00.078.352 I llm_load_print_meta: n_layer          = 24
0.00.078.355 I llm_load_print_meta: n_head           = 16
0.00.078.356 I llm_load_print_meta: n_head_kv        = 16
0.00.078.356 I llm_load_print_meta: n_rot            = 32
0.00.078.356 I llm_load_print_meta: n_swa            = 0
0.00.078.356 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.356 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.357 I llm_load_print_meta: n_gqa            = 1
0.00.078.358 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.360 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.361 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.361 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.361 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.364 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.364 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.364 I llm_load_print_meta: n_ff             = 8192
0.00.078.365 I llm_load_print_meta: n_expert         = 0
0.00.078.365 I llm_load_print_meta: n_expert_used    = 0
0.00.078.366 I llm_load_print_meta: causal attn      = 1
0.00.078.367 I llm_load_print_meta: pooling type     = 0
0.00.078.367 I llm_load_print_meta: rope type        = 2
0.00.078.368 I llm_load_print_meta: rope scaling     = linear
0.00.078.368 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.368 I llm_load_print_meta: freq_scale_train = 1
0.00.078.369 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.369 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.369 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.369 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.369 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.370 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.370 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.381 I llm_load_print_meta: model type       = 1.4B
0.00.078.382 I llm_load_print_meta: model ftype      = Q5_0
0.00.078.382 I llm_load_print_meta: model params     = 1.41 B
0.00.078.383 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.078.383 I llm_load_print_meta: general.name     = 1.4B
0.00.078.383 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.384 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.384 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.384 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.385 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.078.385 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.385 I llm_load_print_meta: max token length = 1024
0.00.080.754 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.754 I llm_load_tensors: offloading output layer to GPU
0.00.080.754 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.764 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.080.765 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.081.930 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.931 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.932 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.932 I llama_new_context_with_model: n_batch       = 2048
0.00.081.932 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.932 I llama_new_context_with_model: flash_attn    = 0
0.00.081.933 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.933 I llama_new_context_with_model: freq_scale    = 1
0.00.081.934 I ggml_metal_init: allocating
0.00.081.937 I ggml_metal_init: found device: Apple M4
0.00.081.940 I ggml_metal_init: picking default device: Apple M4
0.00.082.722 I ggml_metal_init: using embedded metal library
0.00.085.560 I ggml_metal_init: GPU name:   Apple M4
0.00.085.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.563 I ggml_metal_init: simdgroup reduction   = true
0.00.085.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.564 I ggml_metal_init: has bfloat            = true
0.00.085.564 I ggml_metal_init: use bfloat            = true
0.00.085.564 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.566 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.116.648 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.655 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.676 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.605 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.606 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.606 I llama_new_context_with_model: graph nodes  = 967
0.00.117.606 I llama_new_context_with_model: graph splits = 2
0.00.117.629 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.896.304 I main: llama threadpool init, n_threads = 4
0.00.896.390 I 
0.00.896.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.896.429 I 
0.00.896.704 I sampler seed: 1234
0.00.896.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.896.758 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.896.763 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.896.764 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.677.765 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.677.765 I llama_perf_context_print:        load time =     882.42 ms
0.01.677.766 I llama_perf_context_print: prompt eval time =      37.67 ms /     7 tokens (    5.38 ms per token,   185.84 tokens per second)
0.01.677.767 I llama_perf_context_print:        eval time =     740.32 ms /    63 runs   (   11.75 ms per token,    85.10 tokens per second)
0.01.677.767 I llama_perf_context_print:       total time =     781.47 ms /    70 tokens
0.01.677.935 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.136s
sys	0m0.202s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.105 I main: load the model and apply lora adapter, if any
0.00.010.784 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.634 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.638 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.644 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.647 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.652 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.653 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.654 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.655 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.655 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.511 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.035 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.478 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.479 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.030.479 I llama_model_loader: - type  f32:  194 tensors
0.00.030.480 I llama_model_loader: - type q5_1:   97 tensors
0.00.030.480 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.083 I llm_load_vocab: special tokens cache size = 25
0.00.073.922 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.926 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.926 I llm_load_print_meta: arch             = gptneox
0.00.073.926 I llm_load_print_meta: vocab type       = BPE
0.00.073.927 I llm_load_print_meta: n_vocab          = 50304
0.00.073.927 I llm_load_print_meta: n_merges         = 50009
0.00.073.927 I llm_load_print_meta: vocab_only       = 0
0.00.073.927 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.928 I llm_load_print_meta: n_embd           = 2048
0.00.073.928 I llm_load_print_meta: n_layer          = 24
0.00.073.930 I llm_load_print_meta: n_head           = 16
0.00.073.931 I llm_load_print_meta: n_head_kv        = 16
0.00.073.931 I llm_load_print_meta: n_rot            = 32
0.00.073.931 I llm_load_print_meta: n_swa            = 0
0.00.073.932 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.932 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.933 I llm_load_print_meta: n_gqa            = 1
0.00.073.934 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.934 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.935 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.935 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.936 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.938 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.938 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.939 I llm_load_print_meta: n_ff             = 8192
0.00.073.939 I llm_load_print_meta: n_expert         = 0
0.00.073.939 I llm_load_print_meta: n_expert_used    = 0
0.00.073.939 I llm_load_print_meta: causal attn      = 1
0.00.073.939 I llm_load_print_meta: pooling type     = 0
0.00.073.939 I llm_load_print_meta: rope type        = 2
0.00.073.940 I llm_load_print_meta: rope scaling     = linear
0.00.073.941 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.941 I llm_load_print_meta: freq_scale_train = 1
0.00.073.942 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.942 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.942 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.942 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.942 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.943 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.943 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.954 I llm_load_print_meta: model type       = 1.4B
0.00.073.955 I llm_load_print_meta: model ftype      = Q5_1
0.00.073.955 I llm_load_print_meta: model params     = 1.41 B
0.00.073.956 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.073.956 I llm_load_print_meta: general.name     = 1.4B
0.00.073.957 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.957 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.957 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.957 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.958 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.073.958 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.958 I llm_load_print_meta: max token length = 1024
0.00.076.286 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.286 I llm_load_tensors: offloading output layer to GPU
0.00.076.286 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.296 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.076.297 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.077.494 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.495 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.495 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.495 I llama_new_context_with_model: n_batch       = 2048
0.00.077.496 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.496 I llama_new_context_with_model: flash_attn    = 0
0.00.077.496 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.497 I llama_new_context_with_model: freq_scale    = 1
0.00.077.497 I ggml_metal_init: allocating
0.00.077.505 I ggml_metal_init: found device: Apple M4
0.00.077.507 I ggml_metal_init: picking default device: Apple M4
0.00.078.200 I ggml_metal_init: using embedded metal library
0.00.080.876 I ggml_metal_init: GPU name:   Apple M4
0.00.080.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.879 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.879 I ggml_metal_init: simdgroup reduction   = true
0.00.080.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.880 I ggml_metal_init: has bfloat            = true
0.00.080.880 I ggml_metal_init: use bfloat            = true
0.00.080.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.174 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.182 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.205 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.164 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.165 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.165 I llama_new_context_with_model: graph nodes  = 967
0.00.111.165 I llama_new_context_with_model: graph splits = 2
0.00.111.187 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.977.941 I main: llama threadpool init, n_threads = 4
0.00.978.007 I 
0.00.978.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.978.042 I 
0.00.978.338 I sampler seed: 1234
0.00.978.345 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.978.391 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.978.396 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.978.397 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.809.906 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.809.907 I llama_perf_context_print:        load time =     967.15 ms
0.01.809.908 I llama_perf_context_print: prompt eval time =      37.57 ms /     7 tokens (    5.37 ms per token,   186.31 tokens per second)
0.01.809.908 I llama_perf_context_print:        eval time =     791.04 ms /    63 runs   (   12.56 ms per token,    79.64 tokens per second)
0.01.809.909 I llama_perf_context_print:       total time =     831.97 ms /    70 tokens
0.01.810.066 I ggml_metal_free: deallocating

real	0m1.851s
user	0m0.139s
sys	0m0.214s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.019.595 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.026.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.811 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.812 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.813 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.813 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.270 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.576 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.577 I llama_model_loader: - type  f32:  194 tensors
0.00.036.577 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.578 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.578 I llama_model_loader: - type q6_K:    1 tensors
0.00.062.471 I llm_load_vocab: special tokens cache size = 25
0.00.072.050 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.055 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.056 I llm_load_print_meta: arch             = gptneox
0.00.072.056 I llm_load_print_meta: vocab type       = BPE
0.00.072.056 I llm_load_print_meta: n_vocab          = 50304
0.00.072.056 I llm_load_print_meta: n_merges         = 50009
0.00.072.057 I llm_load_print_meta: vocab_only       = 0
0.00.072.057 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.057 I llm_load_print_meta: n_embd           = 2048
0.00.072.057 I llm_load_print_meta: n_layer          = 24
0.00.072.061 I llm_load_print_meta: n_head           = 16
0.00.072.062 I llm_load_print_meta: n_head_kv        = 16
0.00.072.062 I llm_load_print_meta: n_rot            = 32
0.00.072.062 I llm_load_print_meta: n_swa            = 0
0.00.072.062 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.063 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.064 I llm_load_print_meta: n_gqa            = 1
0.00.072.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.065 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.066 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.066 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.067 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.067 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.068 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.069 I llm_load_print_meta: n_ff             = 8192
0.00.072.069 I llm_load_print_meta: n_expert         = 0
0.00.072.069 I llm_load_print_meta: n_expert_used    = 0
0.00.072.070 I llm_load_print_meta: causal attn      = 1
0.00.072.070 I llm_load_print_meta: pooling type     = 0
0.00.072.070 I llm_load_print_meta: rope type        = 2
0.00.072.070 I llm_load_print_meta: rope scaling     = linear
0.00.072.071 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.071 I llm_load_print_meta: freq_scale_train = 1
0.00.072.072 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.072 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.072 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.072 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.072 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.073 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.073 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.085 I llm_load_print_meta: model type       = 1.4B
0.00.072.085 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.072.085 I llm_load_print_meta: model params     = 1.41 B
0.00.072.086 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.072.086 I llm_load_print_meta: general.name     = 1.4B
0.00.072.087 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.087 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.087 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.087 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.088 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.088 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.088 I llm_load_print_meta: max token length = 1024
0.00.074.279 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.279 I llm_load_tensors: offloading output layer to GPU
0.00.074.279 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.288 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.074.290 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.075.369 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.371 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.371 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.371 I llama_new_context_with_model: n_batch       = 2048
0.00.075.371 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.372 I llama_new_context_with_model: flash_attn    = 0
0.00.075.372 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.372 I llama_new_context_with_model: freq_scale    = 1
0.00.075.373 I ggml_metal_init: allocating
0.00.075.379 I ggml_metal_init: found device: Apple M4
0.00.075.381 I ggml_metal_init: picking default device: Apple M4
0.00.076.062 I ggml_metal_init: using embedded metal library
0.00.078.655 I ggml_metal_init: GPU name:   Apple M4
0.00.078.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.658 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.658 I ggml_metal_init: simdgroup reduction   = true
0.00.078.659 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.659 I ggml_metal_init: has bfloat            = true
0.00.078.659 I ggml_metal_init: use bfloat            = true
0.00.078.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.390 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.396 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.415 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.461 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.463 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.463 I llama_new_context_with_model: graph nodes  = 967
0.00.113.463 I llama_new_context_with_model: graph splits = 2
0.00.113.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.916 I main: llama threadpool init, n_threads = 4
0.00.606.971 I 
0.00.607.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.607.028 I 
0.00.607.185 I sampler seed: 1234
0.00.607.190 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.607.220 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.607.224 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.607.224 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.306.858 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62887.51 tokens per second)
0.01.306.858 I llama_perf_context_print:        load time =     587.32 ms
0.01.306.859 I llama_perf_context_print: prompt eval time =      42.16 ms /     7 tokens (    6.02 ms per token,   166.03 tokens per second)
0.01.306.860 I llama_perf_context_print:        eval time =     654.50 ms /    63 runs   (   10.39 ms per token,    96.26 tokens per second)
0.01.306.860 I llama_perf_context_print:       total time =     699.95 ms /    70 tokens
0.01.307.049 I ggml_metal_free: deallocating

real	0m1.326s
user	0m0.125s
sys	0m0.138s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.930 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.322 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.026.327 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.328 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.329 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.329 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.329 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.330 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.331 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.331 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.331 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.332 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.332 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.332 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.333 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.335 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.335 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.335 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.242 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.114 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.116 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.116 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.116 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.117 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.117 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.035.117 I llama_model_loader: - type  f32:  194 tensors
0.00.035.118 I llama_model_loader: - type q3_K:   25 tensors
0.00.035.118 I llama_model_loader: - type q4_K:   71 tensors
0.00.035.118 I llama_model_loader: - type q5_K:    1 tensors
0.00.035.118 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.192 I llm_load_vocab: special tokens cache size = 25
0.00.067.696 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.699 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.700 I llm_load_print_meta: arch             = gptneox
0.00.067.700 I llm_load_print_meta: vocab type       = BPE
0.00.067.700 I llm_load_print_meta: n_vocab          = 50304
0.00.067.700 I llm_load_print_meta: n_merges         = 50009
0.00.067.701 I llm_load_print_meta: vocab_only       = 0
0.00.067.701 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.701 I llm_load_print_meta: n_embd           = 2048
0.00.067.701 I llm_load_print_meta: n_layer          = 24
0.00.067.703 I llm_load_print_meta: n_head           = 16
0.00.067.704 I llm_load_print_meta: n_head_kv        = 16
0.00.067.704 I llm_load_print_meta: n_rot            = 32
0.00.067.704 I llm_load_print_meta: n_swa            = 0
0.00.067.705 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.705 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.705 I llm_load_print_meta: n_gqa            = 1
0.00.067.706 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.707 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.707 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.708 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.708 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.711 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.711 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.712 I llm_load_print_meta: n_ff             = 8192
0.00.067.713 I llm_load_print_meta: n_expert         = 0
0.00.067.714 I llm_load_print_meta: n_expert_used    = 0
0.00.067.714 I llm_load_print_meta: causal attn      = 1
0.00.067.715 I llm_load_print_meta: pooling type     = 0
0.00.067.715 I llm_load_print_meta: rope type        = 2
0.00.067.715 I llm_load_print_meta: rope scaling     = linear
0.00.067.715 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.716 I llm_load_print_meta: freq_scale_train = 1
0.00.067.716 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.716 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.716 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.716 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.716 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.717 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.717 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.728 I llm_load_print_meta: model type       = 1.4B
0.00.067.729 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.067.729 I llm_load_print_meta: model params     = 1.41 B
0.00.067.729 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.067.730 I llm_load_print_meta: general.name     = 1.4B
0.00.067.730 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.730 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.730 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.730 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.731 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.067.731 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.731 I llm_load_print_meta: max token length = 1024
0.00.069.696 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.696 I llm_load_tensors: offloading output layer to GPU
0.00.069.696 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.706 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.069.707 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.070.637 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.638 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.638 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.639 I llama_new_context_with_model: n_batch       = 2048
0.00.070.639 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.639 I llama_new_context_with_model: flash_attn    = 0
0.00.070.640 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.640 I llama_new_context_with_model: freq_scale    = 1
0.00.070.640 I ggml_metal_init: allocating
0.00.070.647 I ggml_metal_init: found device: Apple M4
0.00.070.649 I ggml_metal_init: picking default device: Apple M4
0.00.071.259 I ggml_metal_init: using embedded metal library
0.00.073.558 I ggml_metal_init: GPU name:   Apple M4
0.00.073.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.562 I ggml_metal_init: simdgroup reduction   = true
0.00.073.562 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.562 I ggml_metal_init: has bfloat            = true
0.00.073.562 I ggml_metal_init: use bfloat            = true
0.00.073.563 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.650 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.657 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.674 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.701 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.702 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.702 I llama_new_context_with_model: graph nodes  = 967
0.00.103.702 I llama_new_context_with_model: graph splits = 2
0.00.103.723 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.133 I main: llama threadpool init, n_threads = 4
0.00.663.182 I 
0.00.663.204 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.663.204 I 
0.00.663.382 I sampler seed: 1234
0.00.663.386 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.417 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.428 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.428 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.409.184 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61900.61 tokens per second)
0.01.409.185 I llama_perf_context_print:        load time =     654.20 ms
0.01.409.186 I llama_perf_context_print: prompt eval time =      36.01 ms /     7 tokens (    5.14 ms per token,   194.41 tokens per second)
0.01.409.186 I llama_perf_context_print:        eval time =     706.81 ms /    63 runs   (   11.22 ms per token,    89.13 tokens per second)
0.01.409.187 I llama_perf_context_print:       total time =     746.05 ms /    70 tokens
0.01.409.359 I ggml_metal_free: deallocating

real	0m1.424s
user	0m0.116s
sys	0m0.153s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.699 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.545 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.549 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.555 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.556 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.556 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.562 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.563 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.481 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.280 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.281 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.281 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.282 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.282 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.282 I llama_model_loader: - type  f32:  194 tensors
0.00.024.283 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.283 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.283 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.072 I llm_load_vocab: special tokens cache size = 25
0.00.051.155 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.158 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.158 I llm_load_print_meta: arch             = gptneox
0.00.051.158 I llm_load_print_meta: vocab type       = BPE
0.00.051.159 I llm_load_print_meta: n_vocab          = 50304
0.00.051.159 I llm_load_print_meta: n_merges         = 50009
0.00.051.159 I llm_load_print_meta: vocab_only       = 0
0.00.051.159 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.159 I llm_load_print_meta: n_embd           = 2048
0.00.051.160 I llm_load_print_meta: n_layer          = 24
0.00.051.162 I llm_load_print_meta: n_head           = 16
0.00.051.163 I llm_load_print_meta: n_head_kv        = 16
0.00.051.163 I llm_load_print_meta: n_rot            = 32
0.00.051.163 I llm_load_print_meta: n_swa            = 0
0.00.051.163 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.164 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.164 I llm_load_print_meta: n_gqa            = 1
0.00.051.165 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.166 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.167 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.167 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.167 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.167 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.167 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.168 I llm_load_print_meta: n_ff             = 8192
0.00.051.169 I llm_load_print_meta: n_expert         = 0
0.00.051.171 I llm_load_print_meta: n_expert_used    = 0
0.00.051.171 I llm_load_print_meta: causal attn      = 1
0.00.051.171 I llm_load_print_meta: pooling type     = 0
0.00.051.171 I llm_load_print_meta: rope type        = 2
0.00.051.172 I llm_load_print_meta: rope scaling     = linear
0.00.051.172 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.172 I llm_load_print_meta: freq_scale_train = 1
0.00.051.172 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.173 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.173 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.173 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.173 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.173 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.173 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.191 I llm_load_print_meta: model type       = 1.4B
0.00.051.192 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.192 I llm_load_print_meta: model params     = 1.41 B
0.00.051.192 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.193 I llm_load_print_meta: general.name     = 1.4B
0.00.051.194 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.194 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.194 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.195 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.195 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.195 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.195 I llm_load_print_meta: max token length = 1024
0.00.053.002 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.002 I llm_load_tensors: offloading output layer to GPU
0.00.053.003 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.012 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.013 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.870 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.872 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.872 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.872 I llama_new_context_with_model: n_batch       = 2048
0.00.053.872 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.873 I llama_new_context_with_model: flash_attn    = 0
0.00.053.873 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.873 I llama_new_context_with_model: freq_scale    = 1
0.00.053.874 I ggml_metal_init: allocating
0.00.053.877 I ggml_metal_init: found device: Apple M4
0.00.053.879 I ggml_metal_init: picking default device: Apple M4
0.00.054.463 I ggml_metal_init: using embedded metal library
0.00.056.397 I ggml_metal_init: GPU name:   Apple M4
0.00.056.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.399 I ggml_metal_init: simdgroup reduction   = true
0.00.056.400 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.400 I ggml_metal_init: has bfloat            = true
0.00.056.400 I ggml_metal_init: use bfloat            = true
0.00.056.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.674 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.682 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.700 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.679 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.680 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.680 I llama_new_context_with_model: graph nodes  = 967
0.00.084.681 I llama_new_context_with_model: graph splits = 2
0.00.084.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.294 I main: llama threadpool init, n_threads = 4
0.00.675.328 I 
0.00.675.348 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.675.348 I 
0.00.675.508 I sampler seed: 1234
0.00.675.511 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.675.520 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.675.522 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.675.522 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.419.931 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56349.21 tokens per second)
0.01.419.932 I llama_perf_context_print:        load time =     666.59 ms
0.01.419.933 I llama_perf_context_print: prompt eval time =      36.80 ms /     7 tokens (    5.26 ms per token,   190.19 tokens per second)
0.01.419.934 I llama_perf_context_print:        eval time =     704.55 ms /    63 runs   (   11.18 ms per token,    89.42 tokens per second)
0.01.419.934 I llama_perf_context_print:       total time =     744.64 ms /    70 tokens
0.01.420.124 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.109s
sys	0m0.174s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.388 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.550 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.560 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.562 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.517 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.386 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.387 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.388 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.388 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.388 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.389 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.389 I llama_model_loader: - type  f32:  194 tensors
0.00.023.389 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.390 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.295 I llm_load_vocab: special tokens cache size = 25
0.00.049.399 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.404 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.404 I llm_load_print_meta: arch             = gptneox
0.00.049.404 I llm_load_print_meta: vocab type       = BPE
0.00.049.404 I llm_load_print_meta: n_vocab          = 50304
0.00.049.405 I llm_load_print_meta: n_merges         = 50009
0.00.049.405 I llm_load_print_meta: vocab_only       = 0
0.00.049.405 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.405 I llm_load_print_meta: n_embd           = 2048
0.00.049.405 I llm_load_print_meta: n_layer          = 24
0.00.049.408 I llm_load_print_meta: n_head           = 16
0.00.049.409 I llm_load_print_meta: n_head_kv        = 16
0.00.049.409 I llm_load_print_meta: n_rot            = 32
0.00.049.410 I llm_load_print_meta: n_swa            = 0
0.00.049.410 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.410 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.411 I llm_load_print_meta: n_gqa            = 1
0.00.049.416 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.418 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.419 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.419 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.419 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.420 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.420 I llm_load_print_meta: n_ff             = 8192
0.00.049.421 I llm_load_print_meta: n_expert         = 0
0.00.049.421 I llm_load_print_meta: n_expert_used    = 0
0.00.049.421 I llm_load_print_meta: causal attn      = 1
0.00.049.421 I llm_load_print_meta: pooling type     = 0
0.00.049.421 I llm_load_print_meta: rope type        = 2
0.00.049.422 I llm_load_print_meta: rope scaling     = linear
0.00.049.422 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.425 I llm_load_print_meta: freq_scale_train = 1
0.00.049.425 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.425 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.425 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.425 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.426 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.426 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.426 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.436 I llm_load_print_meta: model type       = 1.4B
0.00.049.436 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.436 I llm_load_print_meta: model params     = 1.41 B
0.00.049.437 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.437 I llm_load_print_meta: general.name     = 1.4B
0.00.049.437 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.437 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.437 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.437 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.438 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.438 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.438 I llm_load_print_meta: max token length = 1024
0.00.051.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.189 I llm_load_tensors: offloading output layer to GPU
0.00.051.189 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.198 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.199 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.034 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.035 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.035 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.035 I llama_new_context_with_model: n_batch       = 2048
0.00.052.036 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.036 I llama_new_context_with_model: flash_attn    = 0
0.00.052.036 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.037 I llama_new_context_with_model: freq_scale    = 1
0.00.052.037 I ggml_metal_init: allocating
0.00.052.044 I ggml_metal_init: found device: Apple M4
0.00.052.046 I ggml_metal_init: picking default device: Apple M4
0.00.052.604 I ggml_metal_init: using embedded metal library
0.00.054.540 I ggml_metal_init: GPU name:   Apple M4
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.542 I ggml_metal_init: simdgroup reduction   = true
0.00.054.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.543 I ggml_metal_init: has bfloat            = true
0.00.054.543 I ggml_metal_init: use bfloat            = true
0.00.054.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.308 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.318 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.338 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.292 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.294 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.294 I llama_new_context_with_model: graph nodes  = 967
0.00.082.294 I llama_new_context_with_model: graph splits = 2
0.00.082.316 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.280 I main: llama threadpool init, n_threads = 4
0.00.758.320 I 
0.00.758.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.758.369 I 
0.00.758.511 I sampler seed: 1234
0.00.758.516 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.526 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.527 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.527 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.590.662 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.590.662 I llama_perf_context_print:        load time =     749.89 ms
0.01.590.663 I llama_perf_context_print: prompt eval time =      39.00 ms /     7 tokens (    5.57 ms per token,   179.49 tokens per second)
0.01.590.664 I llama_perf_context_print:        eval time =     790.14 ms /    63 runs   (   12.54 ms per token,    79.73 tokens per second)
0.01.590.664 I llama_perf_context_print:       total time =     832.38 ms /    70 tokens
0.01.590.837 I ggml_metal_free: deallocating

real	0m1.606s
user	0m0.107s
sys	0m0.195s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.573 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.915 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.926 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.927 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.928 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.928 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.930 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.930 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.931 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.931 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.840 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.841 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.842 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.842 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.842 I llama_model_loader: - type  f32:  194 tensors
0.00.024.842 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.800 I llm_load_vocab: special tokens cache size = 25
0.00.050.830 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.833 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.833 I llm_load_print_meta: arch             = gptneox
0.00.050.834 I llm_load_print_meta: vocab type       = BPE
0.00.050.834 I llm_load_print_meta: n_vocab          = 50304
0.00.050.834 I llm_load_print_meta: n_merges         = 50009
0.00.050.834 I llm_load_print_meta: vocab_only       = 0
0.00.050.834 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.835 I llm_load_print_meta: n_embd           = 2048
0.00.050.835 I llm_load_print_meta: n_layer          = 24
0.00.050.837 I llm_load_print_meta: n_head           = 16
0.00.050.837 I llm_load_print_meta: n_head_kv        = 16
0.00.050.838 I llm_load_print_meta: n_rot            = 32
0.00.050.839 I llm_load_print_meta: n_swa            = 0
0.00.050.839 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.839 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.840 I llm_load_print_meta: n_gqa            = 1
0.00.050.840 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.841 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.842 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.842 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.842 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.842 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.842 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.843 I llm_load_print_meta: n_ff             = 8192
0.00.050.843 I llm_load_print_meta: n_expert         = 0
0.00.050.843 I llm_load_print_meta: n_expert_used    = 0
0.00.050.843 I llm_load_print_meta: causal attn      = 1
0.00.050.843 I llm_load_print_meta: pooling type     = 0
0.00.050.845 I llm_load_print_meta: rope type        = 2
0.00.050.845 I llm_load_print_meta: rope scaling     = linear
0.00.050.846 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.846 I llm_load_print_meta: freq_scale_train = 1
0.00.050.846 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.846 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.847 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.847 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.847 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.847 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.847 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.858 I llm_load_print_meta: model type       = 1.4B
0.00.050.859 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.859 I llm_load_print_meta: model params     = 1.41 B
0.00.050.860 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.860 I llm_load_print_meta: general.name     = 1.4B
0.00.050.860 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.860 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.862 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.862 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.862 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.863 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.863 I llm_load_print_meta: max token length = 1024
0.00.052.527 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.527 I llm_load_tensors: offloading output layer to GPU
0.00.052.528 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.537 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.538 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.370 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.370 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.371 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.371 I llama_new_context_with_model: n_batch       = 2048
0.00.053.371 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.371 I llama_new_context_with_model: flash_attn    = 0
0.00.053.372 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.372 I llama_new_context_with_model: freq_scale    = 1
0.00.053.372 I ggml_metal_init: allocating
0.00.053.378 I ggml_metal_init: found device: Apple M4
0.00.053.380 I ggml_metal_init: picking default device: Apple M4
0.00.053.938 I ggml_metal_init: using embedded metal library
0.00.055.850 I ggml_metal_init: GPU name:   Apple M4
0.00.055.851 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.852 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.852 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.852 I ggml_metal_init: simdgroup reduction   = true
0.00.055.853 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.853 I ggml_metal_init: has bfloat            = true
0.00.055.853 I ggml_metal_init: use bfloat            = true
0.00.055.853 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.854 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.693 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.699 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.720 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.676 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.678 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.678 I llama_new_context_with_model: graph nodes  = 967
0.00.083.679 I llama_new_context_with_model: graph splits = 2
0.00.083.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.839 I main: llama threadpool init, n_threads = 4
0.00.820.878 I 
0.00.820.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.820.897 I 
0.00.821.058 I sampler seed: 1234
0.00.821.063 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.073 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.073 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.075 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.671.900 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.671.901 I llama_perf_context_print:        load time =     811.26 ms
0.01.671.901 I llama_perf_context_print: prompt eval time =      38.85 ms /     7 tokens (    5.55 ms per token,   180.16 tokens per second)
0.01.671.902 I llama_perf_context_print:        eval time =     809.06 ms /    63 runs   (   12.84 ms per token,    77.87 tokens per second)
0.01.671.904 I llama_perf_context_print:       total time =     851.06 ms /    70 tokens
0.01.672.065 I ggml_metal_free: deallocating

real	0m1.690s
user	0m0.107s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.559 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.585 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.371 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.377 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.379 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.379 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.380 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.387 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.390 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.920 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.920 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.921 I llama_model_loader: - type  f32:  194 tensors
0.00.049.921 I llama_model_loader: - type  f16:   98 tensors
0.00.078.664 I llm_load_vocab: special tokens cache size = 25
0.00.085.289 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.292 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.292 I llm_load_print_meta: arch             = gptneox
0.00.085.292 I llm_load_print_meta: vocab type       = BPE
0.00.085.292 I llm_load_print_meta: n_vocab          = 50304
0.00.085.292 I llm_load_print_meta: n_merges         = 50009
0.00.085.293 I llm_load_print_meta: vocab_only       = 0
0.00.085.293 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.293 I llm_load_print_meta: n_embd           = 2048
0.00.085.293 I llm_load_print_meta: n_layer          = 24
0.00.085.295 I llm_load_print_meta: n_head           = 16
0.00.085.296 I llm_load_print_meta: n_head_kv        = 16
0.00.085.296 I llm_load_print_meta: n_rot            = 32
0.00.085.297 I llm_load_print_meta: n_swa            = 0
0.00.085.297 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.297 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.298 I llm_load_print_meta: n_gqa            = 1
0.00.085.298 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.299 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.301 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.301 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.301 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.301 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.301 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.302 I llm_load_print_meta: n_ff             = 8192
0.00.085.302 I llm_load_print_meta: n_expert         = 0
0.00.085.302 I llm_load_print_meta: n_expert_used    = 0
0.00.085.302 I llm_load_print_meta: causal attn      = 1
0.00.085.302 I llm_load_print_meta: pooling type     = 0
0.00.085.304 I llm_load_print_meta: rope type        = 2
0.00.085.304 I llm_load_print_meta: rope scaling     = linear
0.00.085.304 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.305 I llm_load_print_meta: freq_scale_train = 1
0.00.085.305 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.305 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.305 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.305 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.305 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.306 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.306 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.325 I llm_load_print_meta: model type       = 1.4B
0.00.085.326 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.326 I llm_load_print_meta: model params     = 1.41 B
0.00.085.326 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.327 I llm_load_print_meta: general.name     = 1.4B
0.00.085.327 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.328 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.328 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.329 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.330 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.330 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.330 I llm_load_print_meta: max token length = 1024
0.00.087.323 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.323 I llm_load_tensors: offloading output layer to GPU
0.00.087.324 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.341 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.342 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.252 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.253 I llama_new_context_with_model: n_ctx         = 128
0.00.088.253 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.253 I llama_new_context_with_model: n_batch       = 128
0.00.088.253 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.254 I llama_new_context_with_model: flash_attn    = 0
0.00.088.254 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.254 I llama_new_context_with_model: freq_scale    = 1
0.00.088.255 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.255 I ggml_metal_init: allocating
0.00.088.262 I ggml_metal_init: found device: Apple M4
0.00.088.265 I ggml_metal_init: picking default device: Apple M4
0.00.088.868 I ggml_metal_init: using embedded metal library
0.00.091.017 I ggml_metal_init: GPU name:   Apple M4
0.00.091.019 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.019 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.020 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.020 I ggml_metal_init: simdgroup reduction   = true
0.00.091.020 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.020 I ggml_metal_init: has bfloat            = true
0.00.091.020 I ggml_metal_init: use bfloat            = true
0.00.091.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.022 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.681 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.099.683 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.099.697 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.614 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.100.615 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.100.615 I llama_new_context_with_model: graph nodes  = 967
0.00.100.615 I llama_new_context_with_model: graph splits = 2
0.00.100.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.269.758 I 
0.01.269.781 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.269.786 I perplexity: tokenizing the input ..
0.01.280.372 I perplexity: tokenization took 10.585 ms
0.01.280.400 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.400.856 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.402.609 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.402.630 I llama_perf_context_print:        load time =    1249.17 ms
0.01.402.632 I llama_perf_context_print: prompt eval time =     120.20 ms /   128 tokens (    0.94 ms per token,  1064.88 tokens per second)
0.01.402.633 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.402.634 I llama_perf_context_print:       total time =     132.87 ms /   129 tokens
0.01.403.278 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.125s
sys	0m0.278s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.357 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.536 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.450 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.461 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.462 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.466 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.466 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.467 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.749 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.727 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.728 I llama_model_loader: - type  f32:  194 tensors
0.00.041.729 I llama_model_loader: - type q8_0:   98 tensors
0.00.068.106 I llm_load_vocab: special tokens cache size = 25
0.00.074.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.604 I llm_load_print_meta: arch             = gptneox
0.00.074.604 I llm_load_print_meta: vocab type       = BPE
0.00.074.605 I llm_load_print_meta: n_vocab          = 50304
0.00.074.605 I llm_load_print_meta: n_merges         = 50009
0.00.074.605 I llm_load_print_meta: vocab_only       = 0
0.00.074.605 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.605 I llm_load_print_meta: n_embd           = 2048
0.00.074.605 I llm_load_print_meta: n_layer          = 24
0.00.074.608 I llm_load_print_meta: n_head           = 16
0.00.074.609 I llm_load_print_meta: n_head_kv        = 16
0.00.074.609 I llm_load_print_meta: n_rot            = 32
0.00.074.609 I llm_load_print_meta: n_swa            = 0
0.00.074.610 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.611 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.612 I llm_load_print_meta: n_gqa            = 1
0.00.074.612 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.615 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.615 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.616 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.616 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.616 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.616 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.617 I llm_load_print_meta: n_ff             = 8192
0.00.074.617 I llm_load_print_meta: n_expert         = 0
0.00.074.617 I llm_load_print_meta: n_expert_used    = 0
0.00.074.618 I llm_load_print_meta: causal attn      = 1
0.00.074.618 I llm_load_print_meta: pooling type     = 0
0.00.074.618 I llm_load_print_meta: rope type        = 2
0.00.074.618 I llm_load_print_meta: rope scaling     = linear
0.00.074.618 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.619 I llm_load_print_meta: freq_scale_train = 1
0.00.074.619 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.619 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.619 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.619 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.620 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.620 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.620 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.632 I llm_load_print_meta: model type       = 1.4B
0.00.074.632 I llm_load_print_meta: model ftype      = Q8_0
0.00.074.632 I llm_load_print_meta: model params     = 1.41 B
0.00.074.633 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.074.633 I llm_load_print_meta: general.name     = 1.4B
0.00.074.633 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.633 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.634 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.634 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.634 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.074.634 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.634 I llm_load_print_meta: max token length = 1024
0.00.076.455 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.455 I llm_load_tensors: offloading output layer to GPU
0.00.076.456 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.465 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.076.466 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.077.320 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.322 I llama_new_context_with_model: n_ctx         = 128
0.00.077.322 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.077.322 I llama_new_context_with_model: n_batch       = 128
0.00.077.322 I llama_new_context_with_model: n_ubatch      = 128
0.00.077.322 I llama_new_context_with_model: flash_attn    = 0
0.00.077.323 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.323 I llama_new_context_with_model: freq_scale    = 1
0.00.077.324 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.077.324 I ggml_metal_init: allocating
0.00.077.331 I ggml_metal_init: found device: Apple M4
0.00.077.334 I ggml_metal_init: picking default device: Apple M4
0.00.077.898 I ggml_metal_init: using embedded metal library
0.00.080.038 I ggml_metal_init: GPU name:   Apple M4
0.00.080.040 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.040 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.040 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.041 I ggml_metal_init: simdgroup reduction   = true
0.00.080.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.041 I ggml_metal_init: has bfloat            = true
0.00.080.041 I ggml_metal_init: use bfloat            = true
0.00.080.042 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.043 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.865 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.089.868 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.089.883 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.783 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.090.785 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.090.785 I llama_new_context_with_model: graph nodes  = 967
0.00.090.785 I llama_new_context_with_model: graph splits = 2
0.00.090.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.997.325 I 
0.00.997.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.997.358 I perplexity: tokenizing the input ..
0.01.005.955 I perplexity: tokenization took 8.595 ms
0.01.005.968 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.130.811 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.131.917 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.131.929 I llama_perf_context_print:        load time =     979.77 ms
0.01.131.930 I llama_perf_context_print: prompt eval time =     124.63 ms /   128 tokens (    0.97 ms per token,  1027.06 tokens per second)
0.01.131.931 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.131.931 I llama_perf_context_print:       total time =     134.61 ms /   129 tokens
0.01.132.387 I ggml_metal_free: deallocating

real	0m1.158s
user	0m0.102s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.265 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.299 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.041 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.045 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.047 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.047 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.048 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.049 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.049 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.050 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.051 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.053 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.054 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.054 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.720 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.721 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.721 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.722 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.722 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.722 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.723 I llama_model_loader: - type  f32:  194 tensors
0.00.023.723 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.724 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.345 I llm_load_vocab: special tokens cache size = 25
0.00.050.561 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.563 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.564 I llm_load_print_meta: arch             = gptneox
0.00.050.564 I llm_load_print_meta: vocab type       = BPE
0.00.050.564 I llm_load_print_meta: n_vocab          = 50304
0.00.050.565 I llm_load_print_meta: n_merges         = 50009
0.00.050.565 I llm_load_print_meta: vocab_only       = 0
0.00.050.565 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.565 I llm_load_print_meta: n_embd           = 2048
0.00.050.565 I llm_load_print_meta: n_layer          = 24
0.00.050.568 I llm_load_print_meta: n_head           = 16
0.00.050.568 I llm_load_print_meta: n_head_kv        = 16
0.00.050.569 I llm_load_print_meta: n_rot            = 32
0.00.050.569 I llm_load_print_meta: n_swa            = 0
0.00.050.569 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.569 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.570 I llm_load_print_meta: n_gqa            = 1
0.00.050.570 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.571 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.573 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.574 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.574 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.574 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.574 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.575 I llm_load_print_meta: n_ff             = 8192
0.00.050.575 I llm_load_print_meta: n_expert         = 0
0.00.050.575 I llm_load_print_meta: n_expert_used    = 0
0.00.050.576 I llm_load_print_meta: causal attn      = 1
0.00.050.576 I llm_load_print_meta: pooling type     = 0
0.00.050.576 I llm_load_print_meta: rope type        = 2
0.00.050.576 I llm_load_print_meta: rope scaling     = linear
0.00.050.576 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.577 I llm_load_print_meta: freq_scale_train = 1
0.00.050.577 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.577 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.577 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.577 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.578 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.578 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.578 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.589 I llm_load_print_meta: model type       = 1.4B
0.00.050.589 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.590 I llm_load_print_meta: model params     = 1.41 B
0.00.050.590 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.590 I llm_load_print_meta: general.name     = 1.4B
0.00.050.590 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.591 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.591 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.591 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.591 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.591 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.591 I llm_load_print_meta: max token length = 1024
0.00.052.255 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.255 I llm_load_tensors: offloading output layer to GPU
0.00.052.256 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.265 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.266 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.076 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.076 I llama_new_context_with_model: n_ctx         = 128
0.00.053.076 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.077 I llama_new_context_with_model: n_batch       = 128
0.00.053.077 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.077 I llama_new_context_with_model: flash_attn    = 0
0.00.053.077 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.078 I llama_new_context_with_model: freq_scale    = 1
0.00.053.078 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.078 I ggml_metal_init: allocating
0.00.053.082 I ggml_metal_init: found device: Apple M4
0.00.053.084 I ggml_metal_init: picking default device: Apple M4
0.00.053.609 I ggml_metal_init: using embedded metal library
0.00.055.550 I ggml_metal_init: GPU name:   Apple M4
0.00.055.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.551 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.552 I ggml_metal_init: simdgroup reduction   = true
0.00.055.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.552 I ggml_metal_init: has bfloat            = true
0.00.055.553 I ggml_metal_init: use bfloat            = true
0.00.055.553 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.624 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.627 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.641 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.493 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.494 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.494 I llama_new_context_with_model: graph nodes  = 967
0.00.065.494 I llama_new_context_with_model: graph splits = 2
0.00.065.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.880 I 
0.00.649.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.649.913 I perplexity: tokenizing the input ..
0.00.657.800 I perplexity: tokenization took 7.885 ms
0.00.657.816 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.730 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.781.829 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.781.839 I llama_perf_context_print:        load time =     640.58 ms
0.00.781.840 I llama_perf_context_print: prompt eval time =     122.69 ms /   128 tokens (    0.96 ms per token,  1043.25 tokens per second)
0.00.781.841 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.842 I llama_perf_context_print:       total time =     131.96 ms /   129 tokens
0.00.782.221 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.078s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.340 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.074 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.078 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.079 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.080 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.080 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.799 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.864 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.588 I llama_model_loader: - type  f32:  194 tensors
0.00.023.588 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.589 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.239 I llm_load_vocab: special tokens cache size = 25
0.00.049.324 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.326 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.327 I llm_load_print_meta: arch             = gptneox
0.00.049.327 I llm_load_print_meta: vocab type       = BPE
0.00.049.327 I llm_load_print_meta: n_vocab          = 50304
0.00.049.328 I llm_load_print_meta: n_merges         = 50009
0.00.049.328 I llm_load_print_meta: vocab_only       = 0
0.00.049.328 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.328 I llm_load_print_meta: n_embd           = 2048
0.00.049.328 I llm_load_print_meta: n_layer          = 24
0.00.049.331 I llm_load_print_meta: n_head           = 16
0.00.049.332 I llm_load_print_meta: n_head_kv        = 16
0.00.049.332 I llm_load_print_meta: n_rot            = 32
0.00.049.332 I llm_load_print_meta: n_swa            = 0
0.00.049.332 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.332 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.333 I llm_load_print_meta: n_gqa            = 1
0.00.049.334 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.336 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.336 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.337 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.337 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.337 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.337 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.338 I llm_load_print_meta: n_ff             = 8192
0.00.049.338 I llm_load_print_meta: n_expert         = 0
0.00.049.338 I llm_load_print_meta: n_expert_used    = 0
0.00.049.339 I llm_load_print_meta: causal attn      = 1
0.00.049.339 I llm_load_print_meta: pooling type     = 0
0.00.049.339 I llm_load_print_meta: rope type        = 2
0.00.049.339 I llm_load_print_meta: rope scaling     = linear
0.00.049.339 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.340 I llm_load_print_meta: freq_scale_train = 1
0.00.049.340 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.340 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.341 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.341 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.341 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.341 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.341 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.352 I llm_load_print_meta: model type       = 1.4B
0.00.049.353 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.353 I llm_load_print_meta: model params     = 1.41 B
0.00.049.354 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.354 I llm_load_print_meta: general.name     = 1.4B
0.00.049.355 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.355 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.355 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.355 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.355 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.356 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.356 I llm_load_print_meta: max token length = 1024
0.00.051.051 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.052 I llm_load_tensors: offloading output layer to GPU
0.00.051.052 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.061 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.062 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.881 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.881 I llama_new_context_with_model: n_ctx         = 128
0.00.051.881 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.882 I llama_new_context_with_model: n_batch       = 128
0.00.051.882 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.882 I llama_new_context_with_model: flash_attn    = 0
0.00.051.882 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.883 I llama_new_context_with_model: freq_scale    = 1
0.00.051.883 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.883 I ggml_metal_init: allocating
0.00.051.886 I ggml_metal_init: found device: Apple M4
0.00.051.890 I ggml_metal_init: picking default device: Apple M4
0.00.052.411 I ggml_metal_init: using embedded metal library
0.00.054.382 I ggml_metal_init: GPU name:   Apple M4
0.00.054.384 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.384 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.385 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.385 I ggml_metal_init: simdgroup reduction   = true
0.00.054.385 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.385 I ggml_metal_init: has bfloat            = true
0.00.054.385 I ggml_metal_init: use bfloat            = true
0.00.054.386 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.388 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.412 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.415 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.429 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.297 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.298 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.299 I llama_new_context_with_model: graph nodes  = 967
0.00.064.299 I llama_new_context_with_model: graph splits = 2
0.00.064.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.032 I 
0.00.679.053 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.679.057 I perplexity: tokenizing the input ..
0.00.686.848 I perplexity: tokenization took 7.789 ms
0.00.686.862 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.892 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.810.985 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.810.999 I llama_perf_context_print:        load time =     669.69 ms
0.00.811.000 I llama_perf_context_print: prompt eval time =     122.81 ms /   128 tokens (    0.96 ms per token,  1042.24 tokens per second)
0.00.811.001 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.001 I llama_perf_context_print:       total time =     131.97 ms /   129 tokens
0.00.811.388 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.076s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.887 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.648 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.653 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.656 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.660 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.308 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.309 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.310 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.310 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.310 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.311 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.311 I llama_model_loader: - type  f32:  194 tensors
0.00.024.312 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.312 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.792 I llm_load_vocab: special tokens cache size = 25
0.00.050.854 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.857 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.858 I llm_load_print_meta: arch             = gptneox
0.00.050.858 I llm_load_print_meta: vocab type       = BPE
0.00.050.858 I llm_load_print_meta: n_vocab          = 50304
0.00.050.858 I llm_load_print_meta: n_merges         = 50009
0.00.050.859 I llm_load_print_meta: vocab_only       = 0
0.00.050.859 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.859 I llm_load_print_meta: n_embd           = 2048
0.00.050.859 I llm_load_print_meta: n_layer          = 24
0.00.050.861 I llm_load_print_meta: n_head           = 16
0.00.050.862 I llm_load_print_meta: n_head_kv        = 16
0.00.050.867 I llm_load_print_meta: n_rot            = 32
0.00.050.867 I llm_load_print_meta: n_swa            = 0
0.00.050.867 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.868 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.869 I llm_load_print_meta: n_gqa            = 1
0.00.050.869 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.870 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.871 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.871 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.871 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.871 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.873 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.874 I llm_load_print_meta: n_ff             = 8192
0.00.050.874 I llm_load_print_meta: n_expert         = 0
0.00.050.874 I llm_load_print_meta: n_expert_used    = 0
0.00.050.874 I llm_load_print_meta: causal attn      = 1
0.00.050.875 I llm_load_print_meta: pooling type     = 0
0.00.050.875 I llm_load_print_meta: rope type        = 2
0.00.050.875 I llm_load_print_meta: rope scaling     = linear
0.00.050.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.876 I llm_load_print_meta: freq_scale_train = 1
0.00.050.876 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.876 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.876 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.876 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.876 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.877 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.888 I llm_load_print_meta: model type       = 1.4B
0.00.050.889 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.889 I llm_load_print_meta: model params     = 1.41 B
0.00.050.889 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.890 I llm_load_print_meta: general.name     = 1.4B
0.00.050.890 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.890 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.890 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.890 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.891 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.891 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.891 I llm_load_print_meta: max token length = 1024
0.00.052.649 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.649 I llm_load_tensors: offloading output layer to GPU
0.00.052.650 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.659 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.660 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.523 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.523 I llama_new_context_with_model: n_ctx         = 128
0.00.053.524 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.524 I llama_new_context_with_model: n_batch       = 128
0.00.053.524 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.524 I llama_new_context_with_model: flash_attn    = 0
0.00.053.525 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.525 I llama_new_context_with_model: freq_scale    = 1
0.00.053.525 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.526 I ggml_metal_init: allocating
0.00.053.529 I ggml_metal_init: found device: Apple M4
0.00.053.531 I ggml_metal_init: picking default device: Apple M4
0.00.054.058 I ggml_metal_init: using embedded metal library
0.00.055.960 I ggml_metal_init: GPU name:   Apple M4
0.00.055.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.963 I ggml_metal_init: simdgroup reduction   = true
0.00.055.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.963 I ggml_metal_init: has bfloat            = true
0.00.055.963 I ggml_metal_init: use bfloat            = true
0.00.055.964 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.110 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.112 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.126 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.020 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.021 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.022 I llama_new_context_with_model: graph nodes  = 967
0.00.066.022 I llama_new_context_with_model: graph splits = 2
0.00.066.034 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.708 I 
0.00.787.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.787.736 I perplexity: tokenizing the input ..
0.00.795.251 I perplexity: tokenization took 7.514 ms
0.00.795.266 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.930.925 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.932.026 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.932.041 I llama_perf_context_print:        load time =     777.82 ms
0.00.932.042 I llama_perf_context_print: prompt eval time =     135.44 ms /   128 tokens (    1.06 ms per token,   945.08 tokens per second)
0.00.932.043 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.932.043 I llama_perf_context_print:       total time =     144.33 ms /   129 tokens
0.00.932.483 I ggml_metal_free: deallocating

real	0m0.949s
user	0m0.077s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.514 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.273 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.275 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.275 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.276 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.276 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.281 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.022 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.775 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.776 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.776 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.777 I llama_model_loader: - type  f32:  194 tensors
0.00.023.777 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.344 I llm_load_vocab: special tokens cache size = 25
0.00.049.257 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.260 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.261 I llm_load_print_meta: arch             = gptneox
0.00.049.261 I llm_load_print_meta: vocab type       = BPE
0.00.049.261 I llm_load_print_meta: n_vocab          = 50304
0.00.049.261 I llm_load_print_meta: n_merges         = 50009
0.00.049.262 I llm_load_print_meta: vocab_only       = 0
0.00.049.262 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.262 I llm_load_print_meta: n_embd           = 2048
0.00.049.262 I llm_load_print_meta: n_layer          = 24
0.00.049.265 I llm_load_print_meta: n_head           = 16
0.00.049.266 I llm_load_print_meta: n_head_kv        = 16
0.00.049.266 I llm_load_print_meta: n_rot            = 32
0.00.049.266 I llm_load_print_meta: n_swa            = 0
0.00.049.266 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.267 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.267 I llm_load_print_meta: n_gqa            = 1
0.00.049.268 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.269 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.269 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.270 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.270 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.270 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.270 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.271 I llm_load_print_meta: n_ff             = 8192
0.00.049.271 I llm_load_print_meta: n_expert         = 0
0.00.049.271 I llm_load_print_meta: n_expert_used    = 0
0.00.049.274 I llm_load_print_meta: causal attn      = 1
0.00.049.274 I llm_load_print_meta: pooling type     = 0
0.00.049.274 I llm_load_print_meta: rope type        = 2
0.00.049.274 I llm_load_print_meta: rope scaling     = linear
0.00.049.275 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.275 I llm_load_print_meta: freq_scale_train = 1
0.00.049.275 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.275 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.275 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.276 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.276 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.276 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.276 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.287 I llm_load_print_meta: model type       = 1.4B
0.00.049.288 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.288 I llm_load_print_meta: model params     = 1.41 B
0.00.049.289 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.289 I llm_load_print_meta: general.name     = 1.4B
0.00.049.289 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.289 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.289 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.289 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.290 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.290 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.290 I llm_load_print_meta: max token length = 1024
0.00.051.001 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.001 I llm_load_tensors: offloading output layer to GPU
0.00.051.002 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.011 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.012 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.870 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.870 I llama_new_context_with_model: n_ctx         = 128
0.00.051.871 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.871 I llama_new_context_with_model: n_batch       = 128
0.00.051.871 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.871 I llama_new_context_with_model: flash_attn    = 0
0.00.051.872 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.872 I llama_new_context_with_model: freq_scale    = 1
0.00.051.872 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.873 I ggml_metal_init: allocating
0.00.051.878 I ggml_metal_init: found device: Apple M4
0.00.051.880 I ggml_metal_init: picking default device: Apple M4
0.00.052.427 I ggml_metal_init: using embedded metal library
0.00.054.394 I ggml_metal_init: GPU name:   Apple M4
0.00.054.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.397 I ggml_metal_init: simdgroup reduction   = true
0.00.054.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.397 I ggml_metal_init: has bfloat            = true
0.00.054.397 I ggml_metal_init: use bfloat            = true
0.00.054.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.284 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.287 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.301 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.159 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.160 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.160 I llama_new_context_with_model: graph nodes  = 967
0.00.064.160 I llama_new_context_with_model: graph splits = 2
0.00.064.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.463 I 
0.00.793.481 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.793.485 I perplexity: tokenizing the input ..
0.00.801.089 I perplexity: tokenization took 7.602 ms
0.00.801.103 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.936.283 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.937.403 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.937.415 I llama_perf_context_print:        load time =     783.95 ms
0.00.937.416 I llama_perf_context_print: prompt eval time =     134.96 ms /   128 tokens (    1.05 ms per token,   948.42 tokens per second)
0.00.937.417 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.937.418 I llama_perf_context_print:       total time =     143.95 ms /   129 tokens
0.00.937.816 I ggml_metal_free: deallocating

real	0m0.951s
user	0m0.076s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.500 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.089 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.089 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.090 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.090 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.090 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.091 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.091 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.092 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.092 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.092 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.093 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.093 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.096 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.096 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.098 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.826 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.643 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.643 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.643 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.644 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.644 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.645 I llama_model_loader: - type  f32:  194 tensors
0.00.023.645 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.645 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.645 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.212 I llm_load_vocab: special tokens cache size = 25
0.00.049.366 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.369 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.369 I llm_load_print_meta: arch             = gptneox
0.00.049.369 I llm_load_print_meta: vocab type       = BPE
0.00.049.369 I llm_load_print_meta: n_vocab          = 50304
0.00.049.370 I llm_load_print_meta: n_merges         = 50009
0.00.049.370 I llm_load_print_meta: vocab_only       = 0
0.00.049.370 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.370 I llm_load_print_meta: n_embd           = 2048
0.00.049.370 I llm_load_print_meta: n_layer          = 24
0.00.049.373 I llm_load_print_meta: n_head           = 16
0.00.049.373 I llm_load_print_meta: n_head_kv        = 16
0.00.049.374 I llm_load_print_meta: n_rot            = 32
0.00.049.374 I llm_load_print_meta: n_swa            = 0
0.00.049.375 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.375 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.377 I llm_load_print_meta: n_gqa            = 1
0.00.049.377 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.378 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.379 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.379 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.379 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.379 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.379 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.380 I llm_load_print_meta: n_ff             = 8192
0.00.049.380 I llm_load_print_meta: n_expert         = 0
0.00.049.380 I llm_load_print_meta: n_expert_used    = 0
0.00.049.380 I llm_load_print_meta: causal attn      = 1
0.00.049.380 I llm_load_print_meta: pooling type     = 0
0.00.049.380 I llm_load_print_meta: rope type        = 2
0.00.049.381 I llm_load_print_meta: rope scaling     = linear
0.00.049.381 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.381 I llm_load_print_meta: freq_scale_train = 1
0.00.049.381 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.382 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.382 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.383 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.383 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.387 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.387 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.399 I llm_load_print_meta: model type       = 1.4B
0.00.049.399 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.399 I llm_load_print_meta: model params     = 1.41 B
0.00.049.400 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.400 I llm_load_print_meta: general.name     = 1.4B
0.00.049.401 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.401 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.401 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.401 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.402 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.402 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.402 I llm_load_print_meta: max token length = 1024
0.00.051.091 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.091 I llm_load_tensors: offloading output layer to GPU
0.00.051.091 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.101 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.102 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.946 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.947 I llama_new_context_with_model: n_ctx         = 128
0.00.051.947 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.947 I llama_new_context_with_model: n_batch       = 128
0.00.051.948 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.948 I llama_new_context_with_model: flash_attn    = 0
0.00.051.948 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.948 I llama_new_context_with_model: freq_scale    = 1
0.00.051.949 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.949 I ggml_metal_init: allocating
0.00.051.952 I ggml_metal_init: found device: Apple M4
0.00.051.954 I ggml_metal_init: picking default device: Apple M4
0.00.052.485 I ggml_metal_init: using embedded metal library
0.00.054.698 I ggml_metal_init: GPU name:   Apple M4
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.700 I ggml_metal_init: simdgroup reduction   = true
0.00.054.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.700 I ggml_metal_init: has bfloat            = true
0.00.054.700 I ggml_metal_init: use bfloat            = true
0.00.054.701 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.598 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.600 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.613 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.469 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.470 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.470 I llama_new_context_with_model: graph nodes  = 967
0.00.064.470 I llama_new_context_with_model: graph splits = 2
0.00.064.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.422.803 I 
0.00.422.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.422.836 I perplexity: tokenizing the input ..
0.00.431.148 I perplexity: tokenization took 8.311 ms
0.00.431.163 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.816 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.563.917 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.563.927 I llama_perf_context_print:        load time =     413.30 ms
0.00.563.929 I llama_perf_context_print: prompt eval time =     131.41 ms /   128 tokens (    1.03 ms per token,   974.09 tokens per second)
0.00.563.930 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.930 I llama_perf_context_print:       total time =     141.13 ms /   129 tokens
0.00.564.262 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.077s
sys	0m0.083s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.689 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.530 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.530 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.531 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.531 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.531 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.532 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.533 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.533 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.533 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.534 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.534 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.536 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.291 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.103 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.103 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.104 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.104 I llama_model_loader: - type  f32:  194 tensors
0.00.024.105 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.105 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.105 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.105 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.573 I llm_load_vocab: special tokens cache size = 25
0.00.050.581 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.583 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.584 I llm_load_print_meta: arch             = gptneox
0.00.050.584 I llm_load_print_meta: vocab type       = BPE
0.00.050.584 I llm_load_print_meta: n_vocab          = 50304
0.00.050.584 I llm_load_print_meta: n_merges         = 50009
0.00.050.585 I llm_load_print_meta: vocab_only       = 0
0.00.050.585 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.585 I llm_load_print_meta: n_embd           = 2048
0.00.050.585 I llm_load_print_meta: n_layer          = 24
0.00.050.588 I llm_load_print_meta: n_head           = 16
0.00.050.588 I llm_load_print_meta: n_head_kv        = 16
0.00.050.588 I llm_load_print_meta: n_rot            = 32
0.00.050.590 I llm_load_print_meta: n_swa            = 0
0.00.050.590 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.590 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.591 I llm_load_print_meta: n_gqa            = 1
0.00.050.594 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.594 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.595 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.595 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.595 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.596 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.596 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.596 I llm_load_print_meta: n_ff             = 8192
0.00.050.597 I llm_load_print_meta: n_expert         = 0
0.00.050.597 I llm_load_print_meta: n_expert_used    = 0
0.00.050.597 I llm_load_print_meta: causal attn      = 1
0.00.050.597 I llm_load_print_meta: pooling type     = 0
0.00.050.597 I llm_load_print_meta: rope type        = 2
0.00.050.597 I llm_load_print_meta: rope scaling     = linear
0.00.050.598 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.598 I llm_load_print_meta: freq_scale_train = 1
0.00.050.598 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.599 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.599 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.599 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.599 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.599 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.599 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.611 I llm_load_print_meta: model type       = 1.4B
0.00.050.611 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.612 I llm_load_print_meta: model params     = 1.41 B
0.00.050.612 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.612 I llm_load_print_meta: general.name     = 1.4B
0.00.050.613 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.613 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.613 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.613 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.613 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.614 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: max token length = 1024
0.00.052.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.269 I llm_load_tensors: offloading output layer to GPU
0.00.052.269 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.278 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.279 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.111 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.112 I llama_new_context_with_model: n_ctx         = 128
0.00.053.113 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.113 I llama_new_context_with_model: n_batch       = 128
0.00.053.113 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.113 I llama_new_context_with_model: flash_attn    = 0
0.00.053.113 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.114 I llama_new_context_with_model: freq_scale    = 1
0.00.053.114 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.114 I ggml_metal_init: allocating
0.00.053.117 I ggml_metal_init: found device: Apple M4
0.00.053.119 I ggml_metal_init: picking default device: Apple M4
0.00.053.653 I ggml_metal_init: using embedded metal library
0.00.055.593 I ggml_metal_init: GPU name:   Apple M4
0.00.055.595 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.596 I ggml_metal_init: simdgroup reduction   = true
0.00.055.596 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.596 I ggml_metal_init: has bfloat            = true
0.00.055.596 I ggml_metal_init: use bfloat            = true
0.00.055.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.720 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.724 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.738 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.602 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.603 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.603 I llama_new_context_with_model: graph nodes  = 967
0.00.065.604 I llama_new_context_with_model: graph splits = 2
0.00.065.615 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.542.643 I 
0.00.542.670 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.542.681 I perplexity: tokenizing the input ..
0.00.550.426 I perplexity: tokenization took 7.744 ms
0.00.550.437 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.682.953 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.684.074 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.684.085 I llama_perf_context_print:        load time =     532.95 ms
0.00.684.088 I llama_perf_context_print: prompt eval time =     132.30 ms /   128 tokens (    1.03 ms per token,   967.53 tokens per second)
0.00.684.089 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.684.089 I llama_perf_context_print:       total time =     141.44 ms /   129 tokens
0.00.684.476 I ggml_metal_free: deallocating

real	0m0.697s
user	0m0.077s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.172 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.746 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.747 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.748 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.748 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.748 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.749 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.751 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.751 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.751 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.528 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.281 I llama_model_loader: - type  f32:  194 tensors
0.00.025.282 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.282 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.282 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.930 I llm_load_vocab: special tokens cache size = 25
0.00.050.950 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.955 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.955 I llm_load_print_meta: arch             = gptneox
0.00.050.955 I llm_load_print_meta: vocab type       = BPE
0.00.050.956 I llm_load_print_meta: n_vocab          = 50304
0.00.050.956 I llm_load_print_meta: n_merges         = 50009
0.00.050.956 I llm_load_print_meta: vocab_only       = 0
0.00.050.956 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.956 I llm_load_print_meta: n_embd           = 2048
0.00.050.957 I llm_load_print_meta: n_layer          = 24
0.00.050.959 I llm_load_print_meta: n_head           = 16
0.00.050.959 I llm_load_print_meta: n_head_kv        = 16
0.00.050.959 I llm_load_print_meta: n_rot            = 32
0.00.050.961 I llm_load_print_meta: n_swa            = 0
0.00.050.961 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.961 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.962 I llm_load_print_meta: n_gqa            = 1
0.00.050.963 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.964 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.964 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.965 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.965 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.965 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.965 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.966 I llm_load_print_meta: n_ff             = 8192
0.00.050.966 I llm_load_print_meta: n_expert         = 0
0.00.050.968 I llm_load_print_meta: n_expert_used    = 0
0.00.050.968 I llm_load_print_meta: causal attn      = 1
0.00.050.968 I llm_load_print_meta: pooling type     = 0
0.00.050.968 I llm_load_print_meta: rope type        = 2
0.00.050.968 I llm_load_print_meta: rope scaling     = linear
0.00.050.969 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.969 I llm_load_print_meta: freq_scale_train = 1
0.00.050.969 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.970 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.975 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.975 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.976 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.976 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.976 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.987 I llm_load_print_meta: model type       = 1.4B
0.00.050.988 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.988 I llm_load_print_meta: model params     = 1.41 B
0.00.050.989 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.989 I llm_load_print_meta: general.name     = 1.4B
0.00.050.989 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.990 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.990 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: max token length = 1024
0.00.052.703 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.703 I llm_load_tensors: offloading output layer to GPU
0.00.052.704 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.713 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.714 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.560 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.561 I llama_new_context_with_model: n_ctx         = 128
0.00.053.561 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.561 I llama_new_context_with_model: n_batch       = 128
0.00.053.562 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.562 I llama_new_context_with_model: flash_attn    = 0
0.00.053.562 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.562 I llama_new_context_with_model: freq_scale    = 1
0.00.053.563 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.563 I ggml_metal_init: allocating
0.00.053.566 I ggml_metal_init: found device: Apple M4
0.00.053.568 I ggml_metal_init: picking default device: Apple M4
0.00.054.100 I ggml_metal_init: using embedded metal library
0.00.055.996 I ggml_metal_init: GPU name:   Apple M4
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.998 I ggml_metal_init: simdgroup reduction   = true
0.00.055.998 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.998 I ggml_metal_init: has bfloat            = true
0.00.055.998 I ggml_metal_init: use bfloat            = true
0.00.055.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.493 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.496 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.509 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.358 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.359 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.359 I llama_new_context_with_model: graph nodes  = 967
0.00.065.359 I llama_new_context_with_model: graph splits = 2
0.00.065.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.052 I 
0.00.636.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.636.081 I perplexity: tokenizing the input ..
0.00.643.544 I perplexity: tokenization took 7.462 ms
0.00.643.560 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.813 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.778.909 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.778.925 I llama_perf_context_print:        load time =     624.88 ms
0.00.778.925 I llama_perf_context_print: prompt eval time =     134.03 ms /   128 tokens (    1.05 ms per token,   954.99 tokens per second)
0.00.778.926 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.927 I llama_perf_context_print:       total time =     142.87 ms /   129 tokens
0.00.779.341 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.075s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.664 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.253 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.254 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.256 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.259 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.260 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.668 I llama_model_loader: - type  f32:  194 tensors
0.00.024.668 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.669 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.151 I llm_load_vocab: special tokens cache size = 25
0.00.051.094 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.096 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.097 I llm_load_print_meta: arch             = gptneox
0.00.051.097 I llm_load_print_meta: vocab type       = BPE
0.00.051.097 I llm_load_print_meta: n_vocab          = 50304
0.00.051.098 I llm_load_print_meta: n_merges         = 50009
0.00.051.098 I llm_load_print_meta: vocab_only       = 0
0.00.051.098 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.098 I llm_load_print_meta: n_embd           = 2048
0.00.051.098 I llm_load_print_meta: n_layer          = 24
0.00.051.101 I llm_load_print_meta: n_head           = 16
0.00.051.101 I llm_load_print_meta: n_head_kv        = 16
0.00.051.102 I llm_load_print_meta: n_rot            = 32
0.00.051.102 I llm_load_print_meta: n_swa            = 0
0.00.051.102 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.102 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.103 I llm_load_print_meta: n_gqa            = 1
0.00.051.104 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.104 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.105 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.105 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.105 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.106 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.106 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.107 I llm_load_print_meta: n_ff             = 8192
0.00.051.107 I llm_load_print_meta: n_expert         = 0
0.00.051.107 I llm_load_print_meta: n_expert_used    = 0
0.00.051.107 I llm_load_print_meta: causal attn      = 1
0.00.051.107 I llm_load_print_meta: pooling type     = 0
0.00.051.107 I llm_load_print_meta: rope type        = 2
0.00.051.107 I llm_load_print_meta: rope scaling     = linear
0.00.051.110 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.110 I llm_load_print_meta: freq_scale_train = 1
0.00.051.110 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.111 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.111 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.111 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.111 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.111 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.111 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.122 I llm_load_print_meta: model type       = 1.4B
0.00.051.123 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.123 I llm_load_print_meta: model params     = 1.41 B
0.00.051.124 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.124 I llm_load_print_meta: general.name     = 1.4B
0.00.051.124 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.124 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.124 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.125 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.125 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.126 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.126 I llm_load_print_meta: max token length = 1024
0.00.052.888 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.888 I llm_load_tensors: offloading output layer to GPU
0.00.052.889 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.898 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.899 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.735 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.735 I llama_new_context_with_model: n_ctx         = 128
0.00.053.735 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.736 I llama_new_context_with_model: n_batch       = 128
0.00.053.736 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.736 I llama_new_context_with_model: flash_attn    = 0
0.00.053.736 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.737 I llama_new_context_with_model: freq_scale    = 1
0.00.053.737 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.737 I ggml_metal_init: allocating
0.00.053.743 I ggml_metal_init: found device: Apple M4
0.00.053.745 I ggml_metal_init: picking default device: Apple M4
0.00.054.281 I ggml_metal_init: using embedded metal library
0.00.056.536 I ggml_metal_init: GPU name:   Apple M4
0.00.056.537 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.538 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.538 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.538 I ggml_metal_init: simdgroup reduction   = true
0.00.056.539 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.539 I ggml_metal_init: has bfloat            = true
0.00.056.539 I ggml_metal_init: use bfloat            = true
0.00.056.539 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.540 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.553 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.557 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.571 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.401 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.402 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.403 I llama_new_context_with_model: graph nodes  = 967
0.00.066.403 I llama_new_context_with_model: graph splits = 2
0.00.066.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.003 I 
0.00.722.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.722.031 I perplexity: tokenizing the input ..
0.00.729.279 I perplexity: tokenization took 7.244 ms
0.00.729.293 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.870.084 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.871.224 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.871.242 I llama_perf_context_print:        load time =     711.34 ms
0.00.871.243 I llama_perf_context_print: prompt eval time =     140.57 ms /   128 tokens (    1.10 ms per token,   910.57 tokens per second)
0.00.871.244 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.871.245 I llama_perf_context_print:       total time =     149.24 ms /   129 tokens
0.00.871.662 I ggml_metal_free: deallocating

real	0m0.886s
user	0m0.076s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.672 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.338 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.349 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.152 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.245 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.991 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.992 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.992 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.993 I llama_model_loader: - type  f32:  194 tensors
0.00.022.993 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.574 I llm_load_vocab: special tokens cache size = 25
0.00.049.639 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.642 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.643 I llm_load_print_meta: arch             = gptneox
0.00.049.643 I llm_load_print_meta: vocab type       = BPE
0.00.049.643 I llm_load_print_meta: n_vocab          = 50304
0.00.049.643 I llm_load_print_meta: n_merges         = 50009
0.00.049.644 I llm_load_print_meta: vocab_only       = 0
0.00.049.644 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.644 I llm_load_print_meta: n_embd           = 2048
0.00.049.644 I llm_load_print_meta: n_layer          = 24
0.00.049.647 I llm_load_print_meta: n_head           = 16
0.00.049.647 I llm_load_print_meta: n_head_kv        = 16
0.00.049.647 I llm_load_print_meta: n_rot            = 32
0.00.049.648 I llm_load_print_meta: n_swa            = 0
0.00.049.648 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.648 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.649 I llm_load_print_meta: n_gqa            = 1
0.00.049.649 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.650 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.651 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.651 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.651 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.651 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.651 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.652 I llm_load_print_meta: n_ff             = 8192
0.00.049.652 I llm_load_print_meta: n_expert         = 0
0.00.049.652 I llm_load_print_meta: n_expert_used    = 0
0.00.049.653 I llm_load_print_meta: causal attn      = 1
0.00.049.653 I llm_load_print_meta: pooling type     = 0
0.00.049.653 I llm_load_print_meta: rope type        = 2
0.00.049.653 I llm_load_print_meta: rope scaling     = linear
0.00.049.653 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.654 I llm_load_print_meta: freq_scale_train = 1
0.00.049.654 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.654 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.654 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.654 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.654 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.655 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.655 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.666 I llm_load_print_meta: model type       = 1.4B
0.00.049.666 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.667 I llm_load_print_meta: model params     = 1.41 B
0.00.049.668 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.668 I llm_load_print_meta: general.name     = 1.4B
0.00.049.668 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.668 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.669 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.669 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.669 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.669 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.669 I llm_load_print_meta: max token length = 1024
0.00.051.421 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.421 I llm_load_tensors: offloading output layer to GPU
0.00.051.421 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.430 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.431 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.260 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.261 I llama_new_context_with_model: n_ctx         = 128
0.00.052.261 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.261 I llama_new_context_with_model: n_batch       = 128
0.00.052.262 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.262 I llama_new_context_with_model: flash_attn    = 0
0.00.052.262 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.262 I llama_new_context_with_model: freq_scale    = 1
0.00.052.263 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.263 I ggml_metal_init: allocating
0.00.052.266 I ggml_metal_init: found device: Apple M4
0.00.052.268 I ggml_metal_init: picking default device: Apple M4
0.00.052.810 I ggml_metal_init: using embedded metal library
0.00.054.707 I ggml_metal_init: GPU name:   Apple M4
0.00.054.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.709 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.710 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.710 I ggml_metal_init: simdgroup reduction   = true
0.00.054.710 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.710 I ggml_metal_init: has bfloat            = true
0.00.054.710 I ggml_metal_init: use bfloat            = true
0.00.054.711 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.805 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.810 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.823 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.694 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.695 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.696 I llama_new_context_with_model: graph nodes  = 967
0.00.064.696 I llama_new_context_with_model: graph splits = 2
0.00.064.708 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.483.341 I 
0.00.483.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.483.370 I perplexity: tokenizing the input ..
0.00.490.946 I perplexity: tokenization took 7.575 ms
0.00.490.963 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.631.528 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.632.633 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.632.647 I llama_perf_context_print:        load time =     474.66 ms
0.00.632.648 I llama_perf_context_print: prompt eval time =     140.34 ms /   128 tokens (    1.10 ms per token,   912.06 tokens per second)
0.00.632.650 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.632.651 I llama_perf_context_print:       total time =     149.31 ms /   129 tokens
0.00.633.010 I ggml_metal_free: deallocating

real	0m0.646s
user	0m0.077s
sys	0m0.120s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.258 I build: 4199 (9e2301f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.836 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.325 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.332 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.334 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.334 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.335 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.335 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.340 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.341 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.342 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.344 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.346 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.925 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.984 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.985 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.985 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.986 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.986 I llama_model_loader: - type  f32:  194 tensors
0.00.047.987 I llama_model_loader: - type  f16:   98 tensors
0.00.075.000 I llm_load_vocab: special tokens cache size = 25
0.00.081.341 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.081.344 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.081.345 I llm_load_print_meta: arch             = gptneox
0.00.081.345 I llm_load_print_meta: vocab type       = BPE
0.00.081.345 I llm_load_print_meta: n_vocab          = 50304
0.00.081.345 I llm_load_print_meta: n_merges         = 50009
0.00.081.345 I llm_load_print_meta: vocab_only       = 0
0.00.081.345 I llm_load_print_meta: n_ctx_train      = 2048
0.00.081.346 I llm_load_print_meta: n_embd           = 2048
0.00.081.346 I llm_load_print_meta: n_layer          = 24
0.00.081.348 I llm_load_print_meta: n_head           = 16
0.00.081.349 I llm_load_print_meta: n_head_kv        = 16
0.00.081.349 I llm_load_print_meta: n_rot            = 32
0.00.081.350 I llm_load_print_meta: n_swa            = 0
0.00.081.350 I llm_load_print_meta: n_embd_head_k    = 128
0.00.081.350 I llm_load_print_meta: n_embd_head_v    = 128
0.00.081.351 I llm_load_print_meta: n_gqa            = 1
0.00.081.351 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.081.352 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.081.352 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.081.353 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.081.353 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.081.353 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.081.353 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.081.354 I llm_load_print_meta: n_ff             = 8192
0.00.081.354 I llm_load_print_meta: n_expert         = 0
0.00.081.354 I llm_load_print_meta: n_expert_used    = 0
0.00.081.354 I llm_load_print_meta: causal attn      = 1
0.00.081.355 I llm_load_print_meta: pooling type     = 0
0.00.081.356 I llm_load_print_meta: rope type        = 2
0.00.081.356 I llm_load_print_meta: rope scaling     = linear
0.00.081.356 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.081.356 I llm_load_print_meta: freq_scale_train = 1
0.00.081.356 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.081.357 I llm_load_print_meta: rope_finetuned   = unknown
0.00.081.357 I llm_load_print_meta: ssm_d_conv       = 0
0.00.081.357 I llm_load_print_meta: ssm_d_inner      = 0
0.00.081.357 I llm_load_print_meta: ssm_d_state      = 0
0.00.081.357 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.081.357 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.081.368 I llm_load_print_meta: model type       = 1.4B
0.00.081.369 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.081.369 I llm_load_print_meta: model params     = 1.41 B
0.00.081.370 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.081.370 I llm_load_print_meta: general.name     = 1.4B
0.00.081.370 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.081.370 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.081.370 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.081.371 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.081.371 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.081.371 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.081.371 I llm_load_print_meta: max token length = 1024
0.00.083.236 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.236 I llm_load_tensors: offloading output layer to GPU
0.00.083.236 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.246 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.083.247 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.084.110 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.111 I llama_new_context_with_model: n_ctx         = 128
0.00.084.111 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.084.112 I llama_new_context_with_model: n_batch       = 128
0.00.084.112 I llama_new_context_with_model: n_ubatch      = 128
0.00.084.112 I llama_new_context_with_model: flash_attn    = 0
0.00.084.112 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.113 I llama_new_context_with_model: freq_scale    = 1
0.00.084.113 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.084.114 I ggml_metal_init: allocating
0.00.084.120 I ggml_metal_init: found device: Apple M4
0.00.084.122 I ggml_metal_init: picking default device: Apple M4
0.00.084.672 I ggml_metal_init: using embedded metal library
0.00.086.752 I ggml_metal_init: GPU name:   Apple M4
0.00.086.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.754 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.755 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.755 I ggml_metal_init: simdgroup reduction   = true
0.00.086.755 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.755 I ggml_metal_init: has bfloat            = true
0.00.086.755 I ggml_metal_init: use bfloat            = true
0.00.086.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.488 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.095.490 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.095.504 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.401 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.096.402 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.096.402 I llama_new_context_with_model: graph nodes  = 967
0.00.096.403 I llama_new_context_with_model: graph splits = 2
0.00.096.423 I 
0.00.096.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.096.469 I compute_imatrix: tokenizing the input ..
0.00.102.984 I compute_imatrix: tokenization took 6.515 ms
0.00.102.986 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.791.777 I compute_imatrix: 1.69 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.794.634 I llama_perf_context_print:        load time =    1772.94 ms
0.01.794.635 I llama_perf_context_print: prompt eval time =    1688.26 ms /   128 tokens (   13.19 ms per token,    75.82 tokens per second)
0.01.794.636 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.794.637 I llama_perf_context_print:       total time =    1775.79 ms /   129 tokens
0.01.795.556 I ggml_metal_free: deallocating

real	0m1.997s
user	0m0.161s
sys	0m0.337s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4199 (9e2301f4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d30a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d30a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d30af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d30b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d30ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d30c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d30c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d30cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d30d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d30d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d30db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d30e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d30eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d30f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d30fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d310230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d310950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d311070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d311790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d311f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d312680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d312da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d3134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d313d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d314480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d314740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d314d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d3159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d315f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d3161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d316660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d316920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d3171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d3176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d3179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d317e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d3182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d318790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d318c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d3190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d319570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d319a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d319eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d31a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d31a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d31ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d31b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d31bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d31c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d31c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d31cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d31d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d31d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d31dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d31e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d31ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d31f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d31f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d31f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d3201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d320460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d320900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d320da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d321240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d3216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d321b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d322020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d3224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d322960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d322e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d3232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d323740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d323be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d324080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d324520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d3249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d324e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d325300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d3257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d325c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d3260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d326580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d326a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d326ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d327360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d327800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d327ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d328140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d3285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d328a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d328f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d3293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d329860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d329d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d32a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d32a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d32aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d31b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d32b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d32b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d32ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d32bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d32c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d32c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d32ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d32d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d32d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d32dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d32df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d32e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d32e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d32ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d32f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d32f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d32fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d32ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d330470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d330910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d330db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d331250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d3316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d331b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d332030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d3324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d332970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d332e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d3332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d333750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d333bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d334090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d334530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d3349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d334e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d335310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d3357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d335c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d3360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d336590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d336a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d336ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d337370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d337810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d337cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d338150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d3385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d338a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d338f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d3393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d339870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d339d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d33a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d33a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d33aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d33b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d33b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d33bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d33c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d33c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d33c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d33cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d33d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d33db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d33e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d33e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d33edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d33f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d33f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d33fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d340410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d340960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d340eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d341400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d341950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d341ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d3423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d342940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d342e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d3433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d343930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d343e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d3443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d344920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d344e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d3453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d345910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d345e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d3463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d346900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d346e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d3473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d3478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d347e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d348390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d3488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d348e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d349380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d3498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d349e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d34a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d34a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d34ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d34b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d34b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d34be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d34c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d34c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d34cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d34d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d34d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d34dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d34e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d34e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d34edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d34f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d34f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d34fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d350310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d350860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d350db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d351300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d351850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d351da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d3522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d352840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d352ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d353180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d353620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d353ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d353f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d354400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d3548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d354d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d3551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d355680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d355b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d355fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d356460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d3569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d3570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d3577f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d357f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d358630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d3588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d358f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d359510 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.178.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d106d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d107430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d1078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d107d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d108180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d1085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d108a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d108ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d105630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d105aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d109190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d109770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d10a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d10aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d10b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d10b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d10c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d10c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d10cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d10d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d10ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d10e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d10ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d10f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d10fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d10fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d110180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d1105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d110a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d110ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d111400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d111910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d111d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d1121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d1124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d1129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d112e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d113360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d113830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d113d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d1141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d1146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d114b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d115040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d115510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d115980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d115df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d116260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d1166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d116b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d116fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d117420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d117890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d117d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d118370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d118810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d118cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d118f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d1193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d119850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d119da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d11a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d11a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d11acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d11b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d11b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d11bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d11c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d11c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d11cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d11d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d11d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d11da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d11df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d11e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d11e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d11ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d11f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d11f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d11fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d1202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d1207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d120ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d1211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d121700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d121c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d122120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d122630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d122b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d123050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d123560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d123a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d123f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d124490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d1249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d124eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d1253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d1258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d125de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d1262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d126800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d126d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d127220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d127730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d127c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d128150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d128660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d128b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d129080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d129590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d129a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d129f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d12a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d12a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d12aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d12b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d12b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d12bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d12c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d12c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d12cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d12d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d12d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d12dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d12e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d12e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d12eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d12f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d12f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d12fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d12ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d1304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d1309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d130ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d131400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d131910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d131e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d132330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d132840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d132d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d133260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d133770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d133c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d134190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d1346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d134bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d1350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d1355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d135ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d135ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d136500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d136a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d136f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d137430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d1379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d137f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d138540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d138af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d139100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d139710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d139d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d13a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d13a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d13b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d13b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d13ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d13bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d13c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d13cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d13d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d13d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d13dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d13e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d13e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d13ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d13f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d13f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d13fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d140130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d140680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d140bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d141120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d141670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d141bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d142110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d142660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d142bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d143100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d143650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d143ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d1440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d144640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d144b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d1450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d145630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d145b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d1460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d146620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d146b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d1470c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d147610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d147b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d1480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d148600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d148b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d1490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d1495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d149b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d14a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d14a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d14ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d14b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d14b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d14bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d14c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d14c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d14cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d14d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d14d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d14db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d14e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d14e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d14eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d14f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d14f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d14f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d14fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d1502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d150760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d150c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d1510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d151540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d1519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d151e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d152320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d1527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d152c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d1531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d1538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d153ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d154710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d154e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d1550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d155700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d155d10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d1076e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d107b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d107fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d108430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d1088a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d108d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d109180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d1095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d109a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d109ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d10a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d10a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d10b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d10b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d10c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d10c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d10cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d10d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d10dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d10e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d10eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d10f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d10fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d110270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d110960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d110dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d111240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d1116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d111b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d111f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d112400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d112870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d112ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d112fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d113410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d113880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d113cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d114160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d1145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d114a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d114eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d115320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d115790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d115c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d116070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d1164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d116950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d116dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d117230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d1176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d117b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d117f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d1183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d118860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d118cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d119140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d1195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d119a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d119e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d11a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d11a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d11abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d11b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d11b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d11b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d11bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d11c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d11c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d11caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d11cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d11d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d11d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d11dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d11e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d11e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d11ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d11ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d11f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d11f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d11fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d120030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d1204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d120910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d120d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d1211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d121660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d121ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d121f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d1223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d122820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d122c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d123100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d123570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d1239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d123e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d1242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d124730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d124ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d125010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d125480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d1258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d125d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d1261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d126640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d126ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d126f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d127390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d127800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d127c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d1280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d128550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d1289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d128e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d1292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d129710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d129b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d129ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d12a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d12a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d12ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d12b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d12b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d12ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d12bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d12c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d12c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d12cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d12d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d12d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d12d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d12de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d12e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d12e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d12eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d12efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d12f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d12f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d12fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d130190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d130600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d130a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d130ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d131350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d1317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d131c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d1320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d132510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d132980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d132df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d133260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d1336d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d133b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d133fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d134420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d134890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d134d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d135170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d1355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d135a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d135ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d136330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d1367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d136c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d137080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d1374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d137960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d137dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d138550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d1389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d138e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d1392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d139710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d139b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d139ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d13a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d13a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d13ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d13b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d13b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d13ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d13bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d13c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d13c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d13cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d13d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d13d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d13d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d13de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d13e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d13e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d13eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d13efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d13f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d13f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d13fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d140190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d140600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d140a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d140ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d141350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d1417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d141c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d1420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d142510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d142980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d142df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d143260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d1436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d143b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d143fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d144420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d144890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d144d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d145170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d1455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d145a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d145ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d146330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d1467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d146c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d147080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d1474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d147960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d147dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d148240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d1486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d148b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d148f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d149400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d149870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d149ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d14a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d14a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d14aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d14aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d14b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d14b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d14bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d14c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d14c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d14d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d14d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d14dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d14e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d14e500 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.875s
user	0m0.309s
sys	0m0.323s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4199 (9e2301f4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e00ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e0106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e010c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e011220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e0117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e011d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e012330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e0128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e012e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e013390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e013890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e013d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e0148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e015060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e015870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e015f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e0166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e016dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e0174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e017cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e0183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e018b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e019220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e019ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e01a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e01a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e01aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e01b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e01bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e01bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e01c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e01c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e01cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e01d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e01d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e01dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e01e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e01e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e01e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e01ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e01f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e01f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e01fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e0200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e020370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e020980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e020f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e0218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e021ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e0224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e022ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e0230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e023700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e023d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e024500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e0249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e024e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e025100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e025710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e025f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e0261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e026660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e026b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e026fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e027440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e0278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e027d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e028220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e0286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e028b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e029000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e0294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e029940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e029de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e02a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e02a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e02abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e02b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e02b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e02b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e02be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e02c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e02c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e02cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e02d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e02d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e02da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e02dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e02e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e02e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e02ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e02f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e02f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e02fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e02ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e0303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e030840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e0215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e030e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e031330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e0317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e031c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e032110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e0325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e032a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e032ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e033390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e033830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e033cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e034170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e034610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e034ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e034f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e0353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e035890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e035d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e0361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e036670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e036b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e036fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e037450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e0378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e037d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e038230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e0386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e038b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e039010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e0394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e039950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e039df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e03a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e03a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e03abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e03b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e03b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e03b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e03be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e03c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e03c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e03cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e03d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e03d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e03da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e03deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e03e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e03e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e03ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e03f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e03f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e03fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e03ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e0403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e040850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e040da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e0412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e041840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e041d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e042050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e042660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e042c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e043280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e043890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e043ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e044690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e044b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e044fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e045470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e045c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e046170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e0466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e046c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e047160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e0476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e047c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e048150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e0486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e048bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e049140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e049690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e049be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e04a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e04a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e04abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e04b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e04b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e04bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e04c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e04c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e04cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e04d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e04d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e04dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e04e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e04e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e04eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e04f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e04f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e04fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e0500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e050620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e050b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e0510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e051610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e051b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e0520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e052600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e0530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e0535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e053b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e054090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e0545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e054b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e055080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e0555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e055b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e056070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e0565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e056b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e057060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e0575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e057b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e058050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e0585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e058a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e058ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e059380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e059820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e059cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e05a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e05a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e05aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e05af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e05b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e05b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e05bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e05c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e05c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e05ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e05d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e05dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e05e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e05e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e05ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e05f270 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.093.690 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d605310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d605780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d605bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d606060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d6064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d606940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d606db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d607220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d607690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d607b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d607f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d608600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d609120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d6098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d60a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d60a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d60af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d60b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d60bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d60c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d60cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d60d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d60da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d60e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d60e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d60eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d60ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d60f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d60f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d60fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d610010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d610540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d6109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d610c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d6110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d611550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d6119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d611e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d6122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d612710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d612b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d612ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d613460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d6138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d613d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d6141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d614620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d614a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d614f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d615370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d6157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d615c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d6160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d6169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d616e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d617380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d617880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d617cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d618160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d6185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d618a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d618eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d619320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d619790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d619c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d61a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d61a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d61a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d61adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d61b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d61b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d61bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d61bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d61c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d61c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d61ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d61d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d61d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d61da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d61de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d61e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d61e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d61ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d61f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d61f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d61f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d61fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d620210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d620680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d620af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d620f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d6213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d621840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d621cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d622120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d622590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d622a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d622e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d6232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d623750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d624030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d6244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d624910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d624d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d6251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d625660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d625ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d625f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d6263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d626820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d626c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d627100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d627570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d6279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d627e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d6282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d628730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d628ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d629010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d629480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d6298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d629d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d62a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d62a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d62aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d62af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d62b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d62b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d62bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d62c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d62c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d62c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d62ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d62d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d62d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d62db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d62dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d62e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d62e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d62ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d62f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d62f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d62fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d62ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d630370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d6307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d6310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d631530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d6319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d631e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d632280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d6326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d632b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d632fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d633440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d6338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d633d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d634190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d634600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d634a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d635350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d6357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d635c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d6367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d636a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d636d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d6371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d637620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d637a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d637f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d638370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d6387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d638c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d6390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d639530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d6399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d639e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d63a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d63a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d63ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d63afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d63b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d63b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d63bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d63c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d63c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d63ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d63cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d63d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d63d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d63dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d63e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d63e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d63e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d63edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d63f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d63f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d63fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d63ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d640420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d640890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d640d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d641170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d6415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d641a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d641ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d642330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d6427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d642c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d643080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d6434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d643960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d643dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d644240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d6446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d644b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d644f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d645400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d645870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d645ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d646150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d6465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d646a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d646ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d647310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d647780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d647bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d648060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d6484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d648940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d648db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d649220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d649690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d649b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d64a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d64ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d64b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d64bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d64be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d64c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d64c590 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d605310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d605780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d605bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d606060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d6064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d606940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d606db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d607220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d607690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d607b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d607f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d608550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d608e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d6095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d609da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d60a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d60ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d60b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d60b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d60c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d60c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d60d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d60d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d60dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d60e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d60ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d60ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d60f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d60f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d60fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d610030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d6104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d610bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d611040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d6114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d611920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d611d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d612200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d612670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d612ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d612f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d6133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d613830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d613ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d614110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d614580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d6149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d614e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d6152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d615740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d615bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d616020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d616490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d616900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d616d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d6171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d617650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d617ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d6183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d618810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d618c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d6190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d619560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d6199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d619e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d61a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d61a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d61ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d61b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d61b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d61b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d61bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d61c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d61c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d61caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d61cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d61d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d61d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d61dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d61e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d61e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d61e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d61ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d61f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d61f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d61fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d61ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d620450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d6208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d620d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d6211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d621610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d621a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d621ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d622360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d6227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d622c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d6230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d623520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d623990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d623e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d624270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d6246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d624b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d624fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d625430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d6258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d625d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d626180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d6265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d626a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d626ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d627340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d6277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d627c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d628090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d628500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d628970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d628de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d629250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d6296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d629b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d629fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d62a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d62a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d62acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d62b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d62b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d62ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d62beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d62c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d62c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d62cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d62d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d62d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d62d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d62ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d62e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d62e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d62eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d62ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d62f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d62f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d62fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d630140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d6305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d630e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d631300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d631770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d631be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d632050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d6324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d632da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d633210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d633680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d633af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d633f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d6343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d634840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d634cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d635120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d635590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d635a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d636180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d6365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d636a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d636ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d637340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d6377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d637c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d638090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d638500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d638970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d638de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d639250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d6396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d639b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d639fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d63a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d63a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d63acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d63b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d63b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d63ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d63beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d63c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d63c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d63cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d63d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d63d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d63d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d63ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d63e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d63e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d63eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d63ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d63f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d63f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d63fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d640140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d6405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d640a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d640e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d641300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d641770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d641be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d642050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d6424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d642930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d642da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d643210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d643680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d643af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d643f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d6443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d644840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d644cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d645120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d645590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d645a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d645e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d6462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d646bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d647030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d6474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d647910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d647d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d6481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d648660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d648ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d648f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d6493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d649820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d64a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d64acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d64b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d64b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d64bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d64c130 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.894s
user	0m0.239s
sys	0m0.119s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
