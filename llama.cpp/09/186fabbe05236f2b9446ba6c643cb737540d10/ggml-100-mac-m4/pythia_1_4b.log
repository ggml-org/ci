Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.528s
user	0m0.877s
sys	0m1.203s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target xxhash
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking C executable ../bin/test-c
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Built target test-sampling
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Linking CXX executable ../bin/test-gguf
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Built target test-quantize-fns
[ 66%] Built target test-barrier
[ 66%] Built target test-autorelease
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Built target test-quantize-perf
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Built target test-rope
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target llama-batched-bench
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-batched
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gbnf-validator
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-gguf-split
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-imatrix
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Built target llama-infill
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookahead
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 78%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-stats
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-passkey
[ 84%] Built target llama-quantize
[ 84%] Built target llama-perplexity
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Built target llama-parallel
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-retrieval
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-tts
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Built target llama-speculative-simple
[ 95%] Built target llama-speculative
[ 95%] Built target llama-tokenize
[ 95%] Built target llama-tts
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.891s
user	0m6.161s
sys	0m9.537s

main: quantize time =  4900.53 ms
main:    total time =  4900.53 ms

main: quantize time =  1565.68 ms
main:    total time =  1565.68 ms

main: quantize time =  1329.21 ms
main:    total time =  1329.21 ms

main: quantize time =  1373.98 ms
main:    total time =  1373.98 ms

main: quantize time =  3041.91 ms
main:    total time =  3041.91 ms

main: quantize time =  4934.71 ms
main:    total time =  4934.71 ms

main: quantize time =  5888.62 ms
main:    total time =  5888.62 ms

main: quantize time =  6891.88 ms
main:    total time =  6891.88 ms

main: quantize time =  5832.59 ms
main:    total time =  5832.59 ms

main: quantize time =  4514.23 ms
main:    total time =  4514.23 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.120 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.206 I main: llama backend init
0.00.000.212 I main: load the model and apply lora adapter, if any
0.00.046.532 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.057.115 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.131 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.133 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.134 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.135 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.142 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.148 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.064.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.066.346 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.073.318 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.073.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.073.323 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.073.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.073.324 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.073.325 I llama_model_loader: - type  f32:  194 tensors
0.00.073.325 I llama_model_loader: - type  f16:   98 tensors
0.00.102.140 I llm_load_vocab: special tokens cache size = 25
0.00.108.336 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.108.341 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.108.342 I llm_load_print_meta: arch             = gptneox
0.00.108.342 I llm_load_print_meta: vocab type       = BPE
0.00.108.342 I llm_load_print_meta: n_vocab          = 50304
0.00.108.342 I llm_load_print_meta: n_merges         = 50009
0.00.108.342 I llm_load_print_meta: vocab_only       = 0
0.00.108.343 I llm_load_print_meta: n_ctx_train      = 2048
0.00.108.343 I llm_load_print_meta: n_embd           = 2048
0.00.108.345 I llm_load_print_meta: n_layer          = 24
0.00.108.349 I llm_load_print_meta: n_head           = 16
0.00.108.350 I llm_load_print_meta: n_head_kv        = 16
0.00.108.350 I llm_load_print_meta: n_rot            = 32
0.00.108.350 I llm_load_print_meta: n_swa            = 0
0.00.108.351 I llm_load_print_meta: n_embd_head_k    = 128
0.00.108.351 I llm_load_print_meta: n_embd_head_v    = 128
0.00.108.351 I llm_load_print_meta: n_gqa            = 1
0.00.108.352 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.108.353 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.108.354 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.108.354 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.108.354 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.108.354 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.108.355 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.108.355 I llm_load_print_meta: n_ff             = 8192
0.00.108.355 I llm_load_print_meta: n_expert         = 0
0.00.108.355 I llm_load_print_meta: n_expert_used    = 0
0.00.108.355 I llm_load_print_meta: causal attn      = 1
0.00.108.356 I llm_load_print_meta: pooling type     = 0
0.00.108.357 I llm_load_print_meta: rope type        = 2
0.00.108.357 I llm_load_print_meta: rope scaling     = linear
0.00.108.358 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.108.358 I llm_load_print_meta: freq_scale_train = 1
0.00.108.358 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.108.358 I llm_load_print_meta: rope_finetuned   = unknown
0.00.108.358 I llm_load_print_meta: ssm_d_conv       = 0
0.00.108.358 I llm_load_print_meta: ssm_d_inner      = 0
0.00.108.359 I llm_load_print_meta: ssm_d_state      = 0
0.00.108.359 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.108.359 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.108.359 I llm_load_print_meta: model type       = 1.4B
0.00.108.360 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.108.360 I llm_load_print_meta: model params     = 1.41 B
0.00.108.361 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.108.361 I llm_load_print_meta: general.name     = 1.4B
0.00.108.361 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.108.361 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.108.361 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.108.361 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.108.362 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.108.362 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.108.362 I llm_load_print_meta: max token length = 1024
0.00.110.780 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.110.781 I llm_load_tensors: offloading output layer to GPU
0.00.110.781 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.110.799 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.110.800 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.111.730 I llama_new_context_with_model: n_seq_max     = 1
0.00.111.732 I llama_new_context_with_model: n_ctx         = 2048
0.00.111.732 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.111.732 I llama_new_context_with_model: n_batch       = 2048
0.00.111.732 I llama_new_context_with_model: n_ubatch      = 512
0.00.111.732 I llama_new_context_with_model: flash_attn    = 0
0.00.111.733 I llama_new_context_with_model: freq_base     = 10000.0
0.00.111.733 I llama_new_context_with_model: freq_scale    = 1
0.00.111.734 I ggml_metal_init: allocating
0.00.111.744 I ggml_metal_init: found device: Apple M4
0.00.111.746 I ggml_metal_init: picking default device: Apple M4
0.00.112.536 I ggml_metal_init: using embedded metal library
0.00.172.566 I ggml_metal_init: GPU name:   Apple M4
0.00.172.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.172.570 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.172.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.172.571 I ggml_metal_init: simdgroup reduction   = true
0.00.172.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.172.571 I ggml_metal_init: has bfloat            = true
0.00.172.571 I ggml_metal_init: use bfloat            = true
0.00.172.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.172.573 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.226.979 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.249.507 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.249.513 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.249.533 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.250.536 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.250.537 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.250.538 I llama_new_context_with_model: graph nodes  = 967
0.00.250.538 I llama_new_context_with_model: graph splits = 2
0.00.250.541 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.250.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.250.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.324.010 I main: llama threadpool init, n_threads = 4
0.00.324.045 I 
0.00.324.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.324.067 I 
0.00.324.134 I sampler seed: 1234
0.00.324.139 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.324.175 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.324.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.324.177 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.163.233 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.02.163.234 I llama_perf_context_print:        load time =     277.47 ms
0.02.163.234 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.18 tokens per second)
0.02.163.236 I llama_perf_context_print:        eval time =    1792.45 ms /    63 runs   (   28.45 ms per token,    35.15 tokens per second)
0.02.163.236 I llama_perf_context_print:       total time =    1839.22 ms /    70 tokens
0.02.163.456 I ggml_metal_free: deallocating

real	0m2.508s
user	0m0.139s
sys	0m0.090s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.798 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.171 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.176 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.178 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.179 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.186 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.189 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.190 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.773 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.773 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.774 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.774 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.774 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.775 I llama_model_loader: - type  f32:  194 tensors
0.00.032.775 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.742 I llm_load_vocab: special tokens cache size = 25
0.00.059.746 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.751 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.751 I llm_load_print_meta: arch             = gptneox
0.00.059.752 I llm_load_print_meta: vocab type       = BPE
0.00.059.752 I llm_load_print_meta: n_vocab          = 50304
0.00.059.752 I llm_load_print_meta: n_merges         = 50009
0.00.059.752 I llm_load_print_meta: vocab_only       = 0
0.00.059.753 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.753 I llm_load_print_meta: n_embd           = 2048
0.00.059.753 I llm_load_print_meta: n_layer          = 24
0.00.059.758 I llm_load_print_meta: n_head           = 16
0.00.059.759 I llm_load_print_meta: n_head_kv        = 16
0.00.059.760 I llm_load_print_meta: n_rot            = 32
0.00.059.760 I llm_load_print_meta: n_swa            = 0
0.00.059.760 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.760 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.761 I llm_load_print_meta: n_gqa            = 1
0.00.059.762 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.762 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.764 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.764 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.765 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.765 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.765 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.767 I llm_load_print_meta: n_ff             = 8192
0.00.059.768 I llm_load_print_meta: n_expert         = 0
0.00.059.768 I llm_load_print_meta: n_expert_used    = 0
0.00.059.768 I llm_load_print_meta: causal attn      = 1
0.00.059.768 I llm_load_print_meta: pooling type     = 0
0.00.059.770 I llm_load_print_meta: rope type        = 2
0.00.059.770 I llm_load_print_meta: rope scaling     = linear
0.00.059.771 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.771 I llm_load_print_meta: freq_scale_train = 1
0.00.059.771 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.772 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.772 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.772 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.773 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.773 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.773 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.774 I llm_load_print_meta: model type       = 1.4B
0.00.059.774 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.774 I llm_load_print_meta: model params     = 1.41 B
0.00.059.775 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.775 I llm_load_print_meta: general.name     = 1.4B
0.00.059.775 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.775 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.775 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.775 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.776 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.776 I llm_load_print_meta: max token length = 1024
0.00.062.167 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.167 I llm_load_tensors: offloading output layer to GPU
0.00.062.167 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.179 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.179 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.126 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.127 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.127 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.127 I llama_new_context_with_model: n_batch       = 2048
0.00.063.128 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.128 I llama_new_context_with_model: flash_attn    = 0
0.00.063.128 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.129 I llama_new_context_with_model: freq_scale    = 1
0.00.063.129 I ggml_metal_init: allocating
0.00.063.135 I ggml_metal_init: found device: Apple M4
0.00.063.137 I ggml_metal_init: picking default device: Apple M4
0.00.063.865 I ggml_metal_init: using embedded metal library
0.00.066.359 I ggml_metal_init: GPU name:   Apple M4
0.00.066.361 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.361 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.362 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.362 I ggml_metal_init: simdgroup reduction   = true
0.00.066.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.362 I ggml_metal_init: has bfloat            = true
0.00.066.363 I ggml_metal_init: use bfloat            = true
0.00.066.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.812 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.949 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.962 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.992 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.144 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.147 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.147 I llama_new_context_with_model: graph nodes  = 967
0.00.102.148 I llama_new_context_with_model: graph splits = 2
0.00.102.151 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.255.077 I main: llama threadpool init, n_threads = 4
0.01.255.111 I 
0.01.255.134 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.255.134 I 
0.01.255.371 I sampler seed: 1234
0.01.255.375 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.255.390 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.255.392 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.255.392 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.346.345 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.02.346.346 I llama_perf_context_print:        load time =    1245.28 ms
0.02.346.347 I llama_perf_context_print: prompt eval time =      46.18 ms /     7 tokens (    6.60 ms per token,   151.58 tokens per second)
0.02.346.348 I llama_perf_context_print:        eval time =    1041.72 ms /    63 runs   (   16.54 ms per token,    60.48 tokens per second)
0.02.346.348 I llama_perf_context_print:       total time =    1091.27 ms /    70 tokens
0.02.346.560 I ggml_metal_free: deallocating

real	0m2.366s
user	0m0.113s
sys	0m0.220s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.012.567 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.983 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.991 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.997 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.997 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.998 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.999 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.000 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.002 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.002 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.898 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.768 I llama_model_loader: - type  f32:  194 tensors
0.00.029.769 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.769 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.114 I llm_load_vocab: special tokens cache size = 25
0.00.056.098 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.102 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.102 I llm_load_print_meta: arch             = gptneox
0.00.056.103 I llm_load_print_meta: vocab type       = BPE
0.00.056.103 I llm_load_print_meta: n_vocab          = 50304
0.00.056.103 I llm_load_print_meta: n_merges         = 50009
0.00.056.103 I llm_load_print_meta: vocab_only       = 0
0.00.056.104 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.104 I llm_load_print_meta: n_embd           = 2048
0.00.056.104 I llm_load_print_meta: n_layer          = 24
0.00.056.110 I llm_load_print_meta: n_head           = 16
0.00.056.111 I llm_load_print_meta: n_head_kv        = 16
0.00.056.113 I llm_load_print_meta: n_rot            = 32
0.00.056.113 I llm_load_print_meta: n_swa            = 0
0.00.056.114 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.114 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.115 I llm_load_print_meta: n_gqa            = 1
0.00.056.116 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.123 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.126 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.127 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.128 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.128 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.128 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.131 I llm_load_print_meta: n_ff             = 8192
0.00.056.131 I llm_load_print_meta: n_expert         = 0
0.00.056.131 I llm_load_print_meta: n_expert_used    = 0
0.00.056.132 I llm_load_print_meta: causal attn      = 1
0.00.056.132 I llm_load_print_meta: pooling type     = 0
0.00.056.132 I llm_load_print_meta: rope type        = 2
0.00.056.132 I llm_load_print_meta: rope scaling     = linear
0.00.056.133 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.133 I llm_load_print_meta: freq_scale_train = 1
0.00.056.133 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.133 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.134 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.134 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.134 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.134 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.134 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.135 I llm_load_print_meta: model type       = 1.4B
0.00.056.135 I llm_load_print_meta: model ftype      = Q4_0
0.00.056.135 I llm_load_print_meta: model params     = 1.41 B
0.00.056.137 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.056.137 I llm_load_print_meta: general.name     = 1.4B
0.00.056.138 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.138 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.138 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.138 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.139 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.139 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.139 I llm_load_print_meta: max token length = 1024
0.00.058.343 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.344 I llm_load_tensors: offloading output layer to GPU
0.00.058.344 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.356 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.058.357 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.059.317 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.317 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.318 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.318 I llama_new_context_with_model: n_batch       = 2048
0.00.059.318 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.318 I llama_new_context_with_model: flash_attn    = 0
0.00.059.319 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.319 I llama_new_context_with_model: freq_scale    = 1
0.00.059.320 I ggml_metal_init: allocating
0.00.059.327 I ggml_metal_init: found device: Apple M4
0.00.059.330 I ggml_metal_init: picking default device: Apple M4
0.00.060.111 I ggml_metal_init: using embedded metal library
0.00.062.679 I ggml_metal_init: GPU name:   Apple M4
0.00.062.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.682 I ggml_metal_init: simdgroup reduction   = true
0.00.062.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.682 I ggml_metal_init: has bfloat            = true
0.00.062.682 I ggml_metal_init: use bfloat            = true
0.00.062.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.683 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.107 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.970 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.981 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.014 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.049 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.052 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.052 I llama_new_context_with_model: graph nodes  = 967
0.00.097.052 I llama_new_context_with_model: graph splits = 2
0.00.097.056 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.199 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.726 I main: llama threadpool init, n_threads = 4
0.00.766.771 I 
0.00.766.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.795 I 
0.00.767.041 I sampler seed: 1234
0.00.767.056 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.072 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.074 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.454.178 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.454.178 I llama_perf_context_print:        load time =     754.15 ms
0.01.454.180 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.27 tokens per second)
0.01.454.180 I llama_perf_context_print:        eval time =     640.35 ms /    63 runs   (   10.16 ms per token,    98.38 tokens per second)
0.01.454.181 I llama_perf_context_print:       total time =     687.46 ms /    70 tokens
0.01.454.376 I ggml_metal_free: deallocating

real	0m1.475s
user	0m0.111s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.301 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.004 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.006 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.007 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.008 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.008 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.008 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.009 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.009 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.009 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.014 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.007 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.027 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.029 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.029 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.030 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.030 I llama_model_loader: - type  f32:  194 tensors
0.00.028.030 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.030 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.705 I llm_load_vocab: special tokens cache size = 25
0.00.056.969 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.972 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.972 I llm_load_print_meta: arch             = gptneox
0.00.056.972 I llm_load_print_meta: vocab type       = BPE
0.00.056.973 I llm_load_print_meta: n_vocab          = 50304
0.00.056.973 I llm_load_print_meta: n_merges         = 50009
0.00.056.973 I llm_load_print_meta: vocab_only       = 0
0.00.056.973 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.973 I llm_load_print_meta: n_embd           = 2048
0.00.056.974 I llm_load_print_meta: n_layer          = 24
0.00.056.976 I llm_load_print_meta: n_head           = 16
0.00.056.977 I llm_load_print_meta: n_head_kv        = 16
0.00.056.977 I llm_load_print_meta: n_rot            = 32
0.00.056.978 I llm_load_print_meta: n_swa            = 0
0.00.056.978 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.978 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.979 I llm_load_print_meta: n_gqa            = 1
0.00.056.980 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.981 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.984 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.984 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.984 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.984 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.985 I llm_load_print_meta: n_ff             = 8192
0.00.056.985 I llm_load_print_meta: n_expert         = 0
0.00.056.985 I llm_load_print_meta: n_expert_used    = 0
0.00.056.987 I llm_load_print_meta: causal attn      = 1
0.00.056.988 I llm_load_print_meta: pooling type     = 0
0.00.056.988 I llm_load_print_meta: rope type        = 2
0.00.056.988 I llm_load_print_meta: rope scaling     = linear
0.00.056.988 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.989 I llm_load_print_meta: freq_scale_train = 1
0.00.056.989 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.989 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.989 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.989 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.990 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.990 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.990 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.990 I llm_load_print_meta: model type       = 1.4B
0.00.056.995 I llm_load_print_meta: model ftype      = Q4_1
0.00.056.995 I llm_load_print_meta: model params     = 1.41 B
0.00.056.996 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.056.996 I llm_load_print_meta: general.name     = 1.4B
0.00.056.996 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.997 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.997 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.997 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.997 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.997 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.997 I llm_load_print_meta: max token length = 1024
0.00.058.962 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.962 I llm_load_tensors: offloading output layer to GPU
0.00.058.962 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.973 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.058.974 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.059.864 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.865 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.865 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.865 I llama_new_context_with_model: n_batch       = 2048
0.00.059.865 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.866 I llama_new_context_with_model: flash_attn    = 0
0.00.059.866 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.866 I llama_new_context_with_model: freq_scale    = 1
0.00.059.867 I ggml_metal_init: allocating
0.00.059.871 I ggml_metal_init: found device: Apple M4
0.00.059.874 I ggml_metal_init: picking default device: Apple M4
0.00.060.479 I ggml_metal_init: using embedded metal library
0.00.062.796 I ggml_metal_init: GPU name:   Apple M4
0.00.062.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.798 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.798 I ggml_metal_init: simdgroup reduction   = true
0.00.062.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.798 I ggml_metal_init: has bfloat            = true
0.00.062.799 I ggml_metal_init: use bfloat            = true
0.00.062.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.781 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.535 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.543 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.562 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.610 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.612 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.612 I llama_new_context_with_model: graph nodes  = 967
0.00.097.612 I llama_new_context_with_model: graph splits = 2
0.00.097.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.775 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.775 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.064 I main: llama threadpool init, n_threads = 4
0.00.718.101 I 
0.00.718.125 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.125 I 
0.00.718.363 I sampler seed: 1234
0.00.718.367 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.718.383 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.718.384 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.718.384 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.445.946 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.445.947 I llama_perf_context_print:        load time =     706.76 ms
0.01.445.948 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.55 tokens per second)
0.01.445.948 I llama_perf_context_print:        eval time =     681.00 ms /    63 runs   (   10.81 ms per token,    92.51 tokens per second)
0.01.445.949 I llama_perf_context_print:       total time =     727.88 ms /    70 tokens
0.01.446.134 I ggml_metal_free: deallocating

real	0m1.472s
user	0m0.113s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.790 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.975 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.980 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.986 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.987 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.987 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.988 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.989 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.990 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.990 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.992 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.993 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.995 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.957 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.744 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.745 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.746 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.746 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.746 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.747 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.747 I llama_model_loader: - type  f32:  194 tensors
0.00.023.747 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.748 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.497 I llm_load_vocab: special tokens cache size = 25
0.00.050.679 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.681 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.682 I llm_load_print_meta: arch             = gptneox
0.00.050.682 I llm_load_print_meta: vocab type       = BPE
0.00.050.683 I llm_load_print_meta: n_vocab          = 50304
0.00.050.683 I llm_load_print_meta: n_merges         = 50009
0.00.050.683 I llm_load_print_meta: vocab_only       = 0
0.00.050.683 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.683 I llm_load_print_meta: n_embd           = 2048
0.00.050.684 I llm_load_print_meta: n_layer          = 24
0.00.050.686 I llm_load_print_meta: n_head           = 16
0.00.050.687 I llm_load_print_meta: n_head_kv        = 16
0.00.050.687 I llm_load_print_meta: n_rot            = 32
0.00.050.687 I llm_load_print_meta: n_swa            = 0
0.00.050.688 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.690 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.691 I llm_load_print_meta: n_gqa            = 1
0.00.050.692 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.692 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.693 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.695 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.695 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.695 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.695 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.696 I llm_load_print_meta: n_ff             = 8192
0.00.050.696 I llm_load_print_meta: n_expert         = 0
0.00.050.696 I llm_load_print_meta: n_expert_used    = 0
0.00.050.697 I llm_load_print_meta: causal attn      = 1
0.00.050.697 I llm_load_print_meta: pooling type     = 0
0.00.050.697 I llm_load_print_meta: rope type        = 2
0.00.050.697 I llm_load_print_meta: rope scaling     = linear
0.00.050.699 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.699 I llm_load_print_meta: freq_scale_train = 1
0.00.050.699 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.699 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.700 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.700 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.700 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.700 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.700 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.701 I llm_load_print_meta: model type       = 1.4B
0.00.050.701 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.705 I llm_load_print_meta: model params     = 1.41 B
0.00.050.705 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.706 I llm_load_print_meta: general.name     = 1.4B
0.00.050.706 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.706 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.706 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.706 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.707 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.707 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.707 I llm_load_print_meta: max token length = 1024
0.00.052.714 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.714 I llm_load_tensors: offloading output layer to GPU
0.00.052.714 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.725 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.727 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.641 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.642 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.642 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.642 I llama_new_context_with_model: n_batch       = 2048
0.00.053.642 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.643 I llama_new_context_with_model: flash_attn    = 0
0.00.053.643 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.643 I llama_new_context_with_model: freq_scale    = 1
0.00.053.644 I ggml_metal_init: allocating
0.00.053.650 I ggml_metal_init: found device: Apple M4
0.00.053.652 I ggml_metal_init: picking default device: Apple M4
0.00.054.249 I ggml_metal_init: using embedded metal library
0.00.056.574 I ggml_metal_init: GPU name:   Apple M4
0.00.056.576 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.577 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.577 I ggml_metal_init: simdgroup reduction   = true
0.00.056.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.577 I ggml_metal_init: has bfloat            = true
0.00.056.577 I ggml_metal_init: use bfloat            = true
0.00.056.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.022 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.699 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.706 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.726 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.732 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.733 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.733 I llama_new_context_with_model: graph nodes  = 967
0.00.085.733 I llama_new_context_with_model: graph splits = 2
0.00.085.736 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.879 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.077 I main: llama threadpool init, n_threads = 4
0.00.778.120 I 
0.00.778.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.152 I 
0.00.778.381 I sampler seed: 1234
0.00.778.385 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.422 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.424 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.424 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.557.957 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.557.958 I llama_perf_context_print:        load time =     769.28 ms
0.01.557.959 I llama_perf_context_print: prompt eval time =      43.18 ms /     7 tokens (    6.17 ms per token,   162.13 tokens per second)
0.01.557.959 I llama_perf_context_print:        eval time =     733.32 ms /    63 runs   (   11.64 ms per token,    85.91 tokens per second)
0.01.557.960 I llama_perf_context_print:       total time =     779.88 ms /    70 tokens
0.01.558.193 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.109s
sys	0m0.168s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.657 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.758 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.762 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.769 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.769 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.770 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.770 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.770 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.771 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.773 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.774 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.774 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.774 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.775 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.775 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.777 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.777 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.669 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.712 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.569 I llama_model_loader: - type  f32:  194 tensors
0.00.025.569 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.569 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.401 I llm_load_vocab: special tokens cache size = 25
0.00.052.604 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.607 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.607 I llm_load_print_meta: arch             = gptneox
0.00.052.607 I llm_load_print_meta: vocab type       = BPE
0.00.052.608 I llm_load_print_meta: n_vocab          = 50304
0.00.052.608 I llm_load_print_meta: n_merges         = 50009
0.00.052.608 I llm_load_print_meta: vocab_only       = 0
0.00.052.608 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.609 I llm_load_print_meta: n_embd           = 2048
0.00.052.609 I llm_load_print_meta: n_layer          = 24
0.00.052.612 I llm_load_print_meta: n_head           = 16
0.00.052.612 I llm_load_print_meta: n_head_kv        = 16
0.00.052.613 I llm_load_print_meta: n_rot            = 32
0.00.052.613 I llm_load_print_meta: n_swa            = 0
0.00.052.613 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.613 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.614 I llm_load_print_meta: n_gqa            = 1
0.00.052.615 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.615 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.616 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.616 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.616 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.616 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.617 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.617 I llm_load_print_meta: n_ff             = 8192
0.00.052.617 I llm_load_print_meta: n_expert         = 0
0.00.052.620 I llm_load_print_meta: n_expert_used    = 0
0.00.052.620 I llm_load_print_meta: causal attn      = 1
0.00.052.621 I llm_load_print_meta: pooling type     = 0
0.00.052.621 I llm_load_print_meta: rope type        = 2
0.00.052.621 I llm_load_print_meta: rope scaling     = linear
0.00.052.621 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.622 I llm_load_print_meta: freq_scale_train = 1
0.00.052.622 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.622 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.622 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.622 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.622 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.623 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.623 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.623 I llm_load_print_meta: model type       = 1.4B
0.00.052.624 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.624 I llm_load_print_meta: model params     = 1.41 B
0.00.052.624 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.625 I llm_load_print_meta: general.name     = 1.4B
0.00.052.625 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.625 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.625 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.625 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.626 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.626 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.626 I llm_load_print_meta: max token length = 1024
0.00.054.720 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.720 I llm_load_tensors: offloading output layer to GPU
0.00.054.721 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.731 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.732 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.654 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.655 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.655 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.655 I llama_new_context_with_model: n_batch       = 2048
0.00.055.655 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.655 I llama_new_context_with_model: flash_attn    = 0
0.00.055.656 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.656 I llama_new_context_with_model: freq_scale    = 1
0.00.055.656 I ggml_metal_init: allocating
0.00.055.659 I ggml_metal_init: found device: Apple M4
0.00.055.661 I ggml_metal_init: picking default device: Apple M4
0.00.056.263 I ggml_metal_init: using embedded metal library
0.00.058.576 I ggml_metal_init: GPU name:   Apple M4
0.00.058.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.577 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.578 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.578 I ggml_metal_init: simdgroup reduction   = true
0.00.058.578 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.578 I ggml_metal_init: has bfloat            = true
0.00.058.579 I ggml_metal_init: use bfloat            = true
0.00.058.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.407 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.934 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.943 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.965 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.067 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.068 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.068 I llama_new_context_with_model: graph nodes  = 967
0.00.089.069 I llama_new_context_with_model: graph splits = 2
0.00.089.071 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.219 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.395 I main: llama threadpool init, n_threads = 4
0.00.724.432 I 
0.00.724.456 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.456 I 
0.00.724.672 I sampler seed: 1234
0.00.724.677 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.724.720 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.724.725 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.724.725 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.565.198 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.565.199 I llama_perf_context_print:        load time =     714.73 ms
0.01.565.199 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.67 tokens per second)
0.01.565.200 I llama_perf_context_print:        eval time =     795.12 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.565.200 I llama_perf_context_print:       total time =     840.81 ms /    70 tokens
0.01.565.433 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.110s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.339 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.746 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.751 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.447 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.200 I llama_model_loader: - type  f32:  194 tensors
0.00.024.200 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.200 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.200 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.130 I llm_load_vocab: special tokens cache size = 25
0.00.050.227 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.229 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.230 I llm_load_print_meta: arch             = gptneox
0.00.050.230 I llm_load_print_meta: vocab type       = BPE
0.00.050.230 I llm_load_print_meta: n_vocab          = 50304
0.00.050.230 I llm_load_print_meta: n_merges         = 50009
0.00.050.231 I llm_load_print_meta: vocab_only       = 0
0.00.050.231 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.231 I llm_load_print_meta: n_embd           = 2048
0.00.050.231 I llm_load_print_meta: n_layer          = 24
0.00.050.234 I llm_load_print_meta: n_head           = 16
0.00.050.235 I llm_load_print_meta: n_head_kv        = 16
0.00.050.235 I llm_load_print_meta: n_rot            = 32
0.00.050.235 I llm_load_print_meta: n_swa            = 0
0.00.050.235 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.236 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.236 I llm_load_print_meta: n_gqa            = 1
0.00.050.237 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.238 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.238 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.238 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.239 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.239 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.239 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.240 I llm_load_print_meta: n_ff             = 8192
0.00.050.240 I llm_load_print_meta: n_expert         = 0
0.00.050.240 I llm_load_print_meta: n_expert_used    = 0
0.00.050.240 I llm_load_print_meta: causal attn      = 1
0.00.050.240 I llm_load_print_meta: pooling type     = 0
0.00.050.240 I llm_load_print_meta: rope type        = 2
0.00.050.243 I llm_load_print_meta: rope scaling     = linear
0.00.050.243 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.244 I llm_load_print_meta: freq_scale_train = 1
0.00.050.244 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.244 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.244 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.245 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.245 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.245 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.245 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.245 I llm_load_print_meta: model type       = 1.4B
0.00.050.246 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.246 I llm_load_print_meta: model params     = 1.41 B
0.00.050.247 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.247 I llm_load_print_meta: general.name     = 1.4B
0.00.050.247 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.249 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.249 I llm_load_print_meta: max token length = 1024
0.00.052.084 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.085 I llm_load_tensors: offloading output layer to GPU
0.00.052.085 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.096 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.097 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.979 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.980 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.980 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.980 I llama_new_context_with_model: n_batch       = 2048
0.00.052.980 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.980 I llama_new_context_with_model: flash_attn    = 0
0.00.052.981 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.981 I llama_new_context_with_model: freq_scale    = 1
0.00.052.982 I ggml_metal_init: allocating
0.00.052.988 I ggml_metal_init: found device: Apple M4
0.00.052.991 I ggml_metal_init: picking default device: Apple M4
0.00.053.562 I ggml_metal_init: using embedded metal library
0.00.055.868 I ggml_metal_init: GPU name:   Apple M4
0.00.055.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.870 I ggml_metal_init: simdgroup reduction   = true
0.00.055.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.870 I ggml_metal_init: has bfloat            = true
0.00.055.871 I ggml_metal_init: use bfloat            = true
0.00.055.871 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.872 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.266 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.261 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.271 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.290 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.295 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.296 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.296 I llama_new_context_with_model: graph nodes  = 967
0.00.085.297 I llama_new_context_with_model: graph splits = 2
0.00.085.299 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.447 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.440.081 I main: llama threadpool init, n_threads = 4
0.00.440.123 I 
0.00.440.146 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.440.146 I 
0.00.440.387 I sampler seed: 1234
0.00.440.391 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.440.407 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.440.408 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.440.409 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.123.150 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63848.92 tokens per second)
0.01.123.151 I llama_perf_context_print:        load time =     429.74 ms
0.01.123.152 I llama_perf_context_print: prompt eval time =      39.38 ms /     7 tokens (    5.63 ms per token,   177.76 tokens per second)
0.01.123.152 I llama_perf_context_print:        eval time =     640.49 ms /    63 runs   (   10.17 ms per token,    98.36 tokens per second)
0.01.123.153 I llama_perf_context_print:       total time =     683.07 ms /    70 tokens
0.01.123.396 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.109s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.965 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.422 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.427 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.432 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.433 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.433 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.434 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.435 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.435 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.437 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.251 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.262 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.994 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.994 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.995 I llama_model_loader: - type  f32:  194 tensors
0.00.023.995 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.995 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.996 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.996 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.037 I llm_load_vocab: special tokens cache size = 25
0.00.049.813 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.815 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.816 I llm_load_print_meta: arch             = gptneox
0.00.049.816 I llm_load_print_meta: vocab type       = BPE
0.00.049.816 I llm_load_print_meta: n_vocab          = 50304
0.00.049.816 I llm_load_print_meta: n_merges         = 50009
0.00.049.817 I llm_load_print_meta: vocab_only       = 0
0.00.049.817 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.817 I llm_load_print_meta: n_embd           = 2048
0.00.049.817 I llm_load_print_meta: n_layer          = 24
0.00.049.821 I llm_load_print_meta: n_head           = 16
0.00.049.821 I llm_load_print_meta: n_head_kv        = 16
0.00.049.821 I llm_load_print_meta: n_rot            = 32
0.00.049.822 I llm_load_print_meta: n_swa            = 0
0.00.049.822 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.822 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.823 I llm_load_print_meta: n_gqa            = 1
0.00.049.823 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.824 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.825 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.826 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.827 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.827 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.827 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.828 I llm_load_print_meta: n_ff             = 8192
0.00.049.828 I llm_load_print_meta: n_expert         = 0
0.00.049.828 I llm_load_print_meta: n_expert_used    = 0
0.00.049.828 I llm_load_print_meta: causal attn      = 1
0.00.049.828 I llm_load_print_meta: pooling type     = 0
0.00.049.828 I llm_load_print_meta: rope type        = 2
0.00.049.829 I llm_load_print_meta: rope scaling     = linear
0.00.049.829 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.830 I llm_load_print_meta: freq_scale_train = 1
0.00.049.830 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.830 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.832 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.832 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.832 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.832 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.832 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.833 I llm_load_print_meta: model type       = 1.4B
0.00.049.833 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.834 I llm_load_print_meta: model params     = 1.41 B
0.00.049.834 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.835 I llm_load_print_meta: general.name     = 1.4B
0.00.049.835 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.839 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.839 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.839 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.839 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.839 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.840 I llm_load_print_meta: max token length = 1024
0.00.051.783 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.783 I llm_load_tensors: offloading output layer to GPU
0.00.051.783 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.794 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.795 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.672 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.673 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.673 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.673 I llama_new_context_with_model: n_batch       = 2048
0.00.052.673 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.673 I llama_new_context_with_model: flash_attn    = 0
0.00.052.674 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.674 I llama_new_context_with_model: freq_scale    = 1
0.00.052.675 I ggml_metal_init: allocating
0.00.052.680 I ggml_metal_init: found device: Apple M4
0.00.052.682 I ggml_metal_init: picking default device: Apple M4
0.00.053.296 I ggml_metal_init: using embedded metal library
0.00.055.613 I ggml_metal_init: GPU name:   Apple M4
0.00.055.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.615 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.616 I ggml_metal_init: simdgroup reduction   = true
0.00.055.616 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.616 I ggml_metal_init: has bfloat            = true
0.00.055.616 I ggml_metal_init: use bfloat            = true
0.00.055.617 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.617 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.072 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.993 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.003 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.027 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.971 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.972 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.973 I llama_new_context_with_model: graph nodes  = 967
0.00.084.973 I llama_new_context_with_model: graph splits = 2
0.00.084.976 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.105 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.106 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.790 I main: llama threadpool init, n_threads = 4
0.00.541.830 I 
0.00.541.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.877 I 
0.00.542.097 I sampler seed: 1234
0.00.542.101 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.542.135 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.542.136 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.542.136 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.288.084 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.288.085 I llama_perf_context_print:        load time =     532.82 ms
0.01.288.086 I llama_perf_context_print: prompt eval time =      40.46 ms /     7 tokens (    5.78 ms per token,   172.99 tokens per second)
0.01.288.086 I llama_perf_context_print:        eval time =     702.44 ms /    63 runs   (   11.15 ms per token,    89.69 tokens per second)
0.01.288.091 I llama_perf_context_print:       total time =     746.30 ms /    70 tokens
0.01.288.299 I ggml_metal_free: deallocating

real	0m1.306s
user	0m0.109s
sys	0m0.122s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.228 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.597 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.445 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.263 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.263 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.263 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.264 I llama_model_loader: - type  f32:  194 tensors
0.00.024.264 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.265 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.265 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.204 I llm_load_vocab: special tokens cache size = 25
0.00.050.173 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.176 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.176 I llm_load_print_meta: arch             = gptneox
0.00.050.177 I llm_load_print_meta: vocab type       = BPE
0.00.050.177 I llm_load_print_meta: n_vocab          = 50304
0.00.050.177 I llm_load_print_meta: n_merges         = 50009
0.00.050.177 I llm_load_print_meta: vocab_only       = 0
0.00.050.177 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.178 I llm_load_print_meta: n_embd           = 2048
0.00.050.178 I llm_load_print_meta: n_layer          = 24
0.00.050.180 I llm_load_print_meta: n_head           = 16
0.00.050.181 I llm_load_print_meta: n_head_kv        = 16
0.00.050.181 I llm_load_print_meta: n_rot            = 32
0.00.050.182 I llm_load_print_meta: n_swa            = 0
0.00.050.182 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.182 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.183 I llm_load_print_meta: n_gqa            = 1
0.00.050.184 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.184 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.185 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.185 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.185 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.185 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.186 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.186 I llm_load_print_meta: n_ff             = 8192
0.00.050.186 I llm_load_print_meta: n_expert         = 0
0.00.050.187 I llm_load_print_meta: n_expert_used    = 0
0.00.050.187 I llm_load_print_meta: causal attn      = 1
0.00.050.187 I llm_load_print_meta: pooling type     = 0
0.00.050.187 I llm_load_print_meta: rope type        = 2
0.00.050.187 I llm_load_print_meta: rope scaling     = linear
0.00.050.188 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.188 I llm_load_print_meta: freq_scale_train = 1
0.00.050.188 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.189 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.189 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.189 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.189 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.189 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.189 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.190 I llm_load_print_meta: model type       = 1.4B
0.00.050.190 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.190 I llm_load_print_meta: model params     = 1.41 B
0.00.050.191 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.191 I llm_load_print_meta: general.name     = 1.4B
0.00.050.194 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.194 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.194 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.194 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.195 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: max token length = 1024
0.00.052.140 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.140 I llm_load_tensors: offloading output layer to GPU
0.00.052.140 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.150 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.151 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.026 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.027 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.027 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.027 I llama_new_context_with_model: n_batch       = 2048
0.00.053.027 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.027 I llama_new_context_with_model: flash_attn    = 0
0.00.053.028 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.028 I llama_new_context_with_model: freq_scale    = 1
0.00.053.029 I ggml_metal_init: allocating
0.00.053.035 I ggml_metal_init: found device: Apple M4
0.00.053.038 I ggml_metal_init: picking default device: Apple M4
0.00.053.624 I ggml_metal_init: using embedded metal library
0.00.055.945 I ggml_metal_init: GPU name:   Apple M4
0.00.055.947 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.947 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.948 I ggml_metal_init: simdgroup reduction   = true
0.00.055.948 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.948 I ggml_metal_init: has bfloat            = true
0.00.055.948 I ggml_metal_init: use bfloat            = true
0.00.055.948 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.548 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.715 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.723 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.745 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.744 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.745 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.745 I llama_new_context_with_model: graph nodes  = 967
0.00.085.746 I llama_new_context_with_model: graph splits = 2
0.00.085.749 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.883 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.884 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.121 I main: llama threadpool init, n_threads = 4
0.00.613.172 I 
0.00.613.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.202 I 
0.00.613.459 I sampler seed: 1234
0.00.613.476 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.492 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.492 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.492 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.366.285 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46618.52 tokens per second)
0.01.366.286 I llama_perf_context_print:        load time =     603.89 ms
0.01.366.287 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.75 tokens per second)
0.01.366.288 I llama_perf_context_print:        eval time =     703.01 ms /    63 runs   (   11.16 ms per token,    89.61 tokens per second)
0.01.366.289 I llama_perf_context_print:       total time =     753.16 ms /    70 tokens
0.01.366.516 I ggml_metal_free: deallocating

real	0m1.385s
user	0m0.109s
sys	0m0.137s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.693 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.983 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.989 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.990 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.990 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.993 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.994 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.996 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.999 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.999 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.999 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.851 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.624 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.625 I llama_model_loader: - type  f32:  194 tensors
0.00.023.625 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.625 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.589 I llm_load_vocab: special tokens cache size = 25
0.00.049.653 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.656 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.656 I llm_load_print_meta: arch             = gptneox
0.00.049.657 I llm_load_print_meta: vocab type       = BPE
0.00.049.657 I llm_load_print_meta: n_vocab          = 50304
0.00.049.657 I llm_load_print_meta: n_merges         = 50009
0.00.049.657 I llm_load_print_meta: vocab_only       = 0
0.00.049.657 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.658 I llm_load_print_meta: n_embd           = 2048
0.00.049.658 I llm_load_print_meta: n_layer          = 24
0.00.049.661 I llm_load_print_meta: n_head           = 16
0.00.049.662 I llm_load_print_meta: n_head_kv        = 16
0.00.049.662 I llm_load_print_meta: n_rot            = 32
0.00.049.662 I llm_load_print_meta: n_swa            = 0
0.00.049.662 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.662 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.665 I llm_load_print_meta: n_gqa            = 1
0.00.049.666 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.667 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.667 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.668 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.668 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.668 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.668 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.670 I llm_load_print_meta: n_ff             = 8192
0.00.049.670 I llm_load_print_meta: n_expert         = 0
0.00.049.670 I llm_load_print_meta: n_expert_used    = 0
0.00.049.672 I llm_load_print_meta: causal attn      = 1
0.00.049.673 I llm_load_print_meta: pooling type     = 0
0.00.049.674 I llm_load_print_meta: rope type        = 2
0.00.049.674 I llm_load_print_meta: rope scaling     = linear
0.00.049.674 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.675 I llm_load_print_meta: freq_scale_train = 1
0.00.049.675 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.675 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.675 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.675 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.676 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.676 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.676 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.676 I llm_load_print_meta: model type       = 1.4B
0.00.049.677 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.677 I llm_load_print_meta: model params     = 1.41 B
0.00.049.678 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.678 I llm_load_print_meta: general.name     = 1.4B
0.00.049.678 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.678 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.679 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.679 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.679 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.679 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.679 I llm_load_print_meta: max token length = 1024
0.00.051.455 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.456 I llm_load_tensors: offloading output layer to GPU
0.00.051.456 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.466 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.467 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.292 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.293 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.294 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.294 I llama_new_context_with_model: n_batch       = 2048
0.00.052.294 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.294 I llama_new_context_with_model: flash_attn    = 0
0.00.052.295 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.295 I llama_new_context_with_model: freq_scale    = 1
0.00.052.295 I ggml_metal_init: allocating
0.00.052.302 I ggml_metal_init: found device: Apple M4
0.00.052.304 I ggml_metal_init: picking default device: Apple M4
0.00.052.899 I ggml_metal_init: using embedded metal library
0.00.055.215 I ggml_metal_init: GPU name:   Apple M4
0.00.055.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.219 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.220 I ggml_metal_init: simdgroup reduction   = true
0.00.055.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.220 I ggml_metal_init: has bfloat            = true
0.00.055.220 I ggml_metal_init: use bfloat            = true
0.00.055.220 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.789 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.812 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.820 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.845 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.815 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.817 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.817 I llama_new_context_with_model: graph nodes  = 967
0.00.084.817 I llama_new_context_with_model: graph splits = 2
0.00.084.820 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.488 I main: llama threadpool init, n_threads = 4
0.00.698.531 I 
0.00.698.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.577 I 
0.00.698.823 I sampler seed: 1234
0.00.698.828 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.698.861 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.698.863 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.698.863 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.544.648 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.01.544.648 I llama_perf_context_print:        load time =     689.79 ms
0.01.544.649 I llama_perf_context_print: prompt eval time =      51.68 ms /     7 tokens (    7.38 ms per token,   135.44 tokens per second)
0.01.544.650 I llama_perf_context_print:        eval time =     791.14 ms /    63 runs   (   12.56 ms per token,    79.63 tokens per second)
0.01.544.653 I llama_perf_context_print:       total time =     846.16 ms /    70 tokens
0.01.544.840 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.109s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.023 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.720 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.724 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.725 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.726 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.726 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.727 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.728 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.728 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.729 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.730 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.734 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.693 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.517 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.518 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.519 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.519 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.519 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.520 I llama_model_loader: - type  f32:  194 tensors
0.00.025.521 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.576 I llm_load_vocab: special tokens cache size = 25
0.00.051.627 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.630 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.630 I llm_load_print_meta: arch             = gptneox
0.00.051.630 I llm_load_print_meta: vocab type       = BPE
0.00.051.631 I llm_load_print_meta: n_vocab          = 50304
0.00.051.631 I llm_load_print_meta: n_merges         = 50009
0.00.051.631 I llm_load_print_meta: vocab_only       = 0
0.00.051.631 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.632 I llm_load_print_meta: n_embd           = 2048
0.00.051.632 I llm_load_print_meta: n_layer          = 24
0.00.051.634 I llm_load_print_meta: n_head           = 16
0.00.051.635 I llm_load_print_meta: n_head_kv        = 16
0.00.051.635 I llm_load_print_meta: n_rot            = 32
0.00.051.636 I llm_load_print_meta: n_swa            = 0
0.00.051.636 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.636 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.637 I llm_load_print_meta: n_gqa            = 1
0.00.051.637 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.638 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.639 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.639 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.639 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.639 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.639 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.640 I llm_load_print_meta: n_ff             = 8192
0.00.051.640 I llm_load_print_meta: n_expert         = 0
0.00.051.640 I llm_load_print_meta: n_expert_used    = 0
0.00.051.641 I llm_load_print_meta: causal attn      = 1
0.00.051.642 I llm_load_print_meta: pooling type     = 0
0.00.051.644 I llm_load_print_meta: rope type        = 2
0.00.051.644 I llm_load_print_meta: rope scaling     = linear
0.00.051.645 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.645 I llm_load_print_meta: freq_scale_train = 1
0.00.051.645 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.647 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.647 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.647 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.647 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.648 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.648 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.648 I llm_load_print_meta: model type       = 1.4B
0.00.051.649 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.649 I llm_load_print_meta: model params     = 1.41 B
0.00.051.649 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.650 I llm_load_print_meta: general.name     = 1.4B
0.00.051.653 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.653 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.654 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.654 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.655 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.655 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.655 I llm_load_print_meta: max token length = 1024
0.00.053.697 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.697 I llm_load_tensors: offloading output layer to GPU
0.00.053.697 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.708 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.709 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.617 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.618 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.618 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.618 I llama_new_context_with_model: n_batch       = 2048
0.00.054.618 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.618 I llama_new_context_with_model: flash_attn    = 0
0.00.054.619 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.619 I llama_new_context_with_model: freq_scale    = 1
0.00.054.619 I ggml_metal_init: allocating
0.00.054.622 I ggml_metal_init: found device: Apple M4
0.00.054.624 I ggml_metal_init: picking default device: Apple M4
0.00.055.226 I ggml_metal_init: using embedded metal library
0.00.057.518 I ggml_metal_init: GPU name:   Apple M4
0.00.057.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.520 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.520 I ggml_metal_init: simdgroup reduction   = true
0.00.057.522 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.522 I ggml_metal_init: has bfloat            = true
0.00.057.522 I ggml_metal_init: use bfloat            = true
0.00.057.523 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.523 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.152 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.331 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.337 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.356 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.369 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.370 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.370 I llama_new_context_with_model: graph nodes  = 967
0.00.087.371 I llama_new_context_with_model: graph splits = 2
0.00.087.374 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.892 I main: llama threadpool init, n_threads = 4
0.00.739.934 I 
0.00.739.969 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.971 I 
0.00.740.206 I sampler seed: 1234
0.00.740.210 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.245 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.246 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.246 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.624.679 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.624.680 I llama_perf_context_print:        load time =     729.86 ms
0.01.624.681 I llama_perf_context_print: prompt eval time =      54.50 ms /     7 tokens (    7.79 ms per token,   128.45 tokens per second)
0.01.624.682 I llama_perf_context_print:        eval time =     826.97 ms /    63 runs   (   13.13 ms per token,    76.18 tokens per second)
0.01.624.683 I llama_perf_context_print:       total time =     884.79 ms /    70 tokens
0.01.624.963 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.551 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.123 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.445 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.460 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.461 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.464 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.464 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.466 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.467 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.468 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.269 I llama_model_loader: - type  f32:  194 tensors
0.00.054.270 I llama_model_loader: - type  f16:   98 tensors
0.00.083.023 I llm_load_vocab: special tokens cache size = 25
0.00.089.769 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.772 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.772 I llm_load_print_meta: arch             = gptneox
0.00.089.773 I llm_load_print_meta: vocab type       = BPE
0.00.089.773 I llm_load_print_meta: n_vocab          = 50304
0.00.089.773 I llm_load_print_meta: n_merges         = 50009
0.00.089.773 I llm_load_print_meta: vocab_only       = 0
0.00.089.773 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.773 I llm_load_print_meta: n_embd           = 2048
0.00.089.774 I llm_load_print_meta: n_layer          = 24
0.00.089.776 I llm_load_print_meta: n_head           = 16
0.00.089.777 I llm_load_print_meta: n_head_kv        = 16
0.00.089.777 I llm_load_print_meta: n_rot            = 32
0.00.089.778 I llm_load_print_meta: n_swa            = 0
0.00.089.778 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.778 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.779 I llm_load_print_meta: n_gqa            = 1
0.00.089.779 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.780 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.780 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.780 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.781 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.782 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.783 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.783 I llm_load_print_meta: n_ff             = 8192
0.00.089.783 I llm_load_print_meta: n_expert         = 0
0.00.089.783 I llm_load_print_meta: n_expert_used    = 0
0.00.089.783 I llm_load_print_meta: causal attn      = 1
0.00.089.785 I llm_load_print_meta: pooling type     = 0
0.00.089.785 I llm_load_print_meta: rope type        = 2
0.00.089.785 I llm_load_print_meta: rope scaling     = linear
0.00.089.786 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.786 I llm_load_print_meta: freq_scale_train = 1
0.00.089.786 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.786 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.786 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.787 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.787 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.790 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.790 I llm_load_print_meta: model type       = 1.4B
0.00.089.791 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.792 I llm_load_print_meta: model params     = 1.41 B
0.00.089.792 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.793 I llm_load_print_meta: general.name     = 1.4B
0.00.089.794 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.794 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.794 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.794 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.794 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.795 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.795 I llm_load_print_meta: max token length = 1024
0.00.092.244 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.245 I llm_load_tensors: offloading output layer to GPU
0.00.092.245 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.255 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.256 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.153 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.154 I llama_new_context_with_model: n_ctx         = 128
0.00.093.154 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.154 I llama_new_context_with_model: n_batch       = 128
0.00.093.155 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.155 I llama_new_context_with_model: flash_attn    = 0
0.00.093.155 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.155 I llama_new_context_with_model: freq_scale    = 1
0.00.093.156 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.156 I ggml_metal_init: allocating
0.00.093.159 I ggml_metal_init: found device: Apple M4
0.00.093.161 I ggml_metal_init: picking default device: Apple M4
0.00.093.750 I ggml_metal_init: using embedded metal library
0.00.096.251 I ggml_metal_init: GPU name:   Apple M4
0.00.096.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.253 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.253 I ggml_metal_init: simdgroup reduction   = true
0.00.096.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.254 I ggml_metal_init: has bfloat            = true
0.00.096.254 I ggml_metal_init: use bfloat            = true
0.00.096.254 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.315 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.559 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.561 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.575 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.390 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.391 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.391 I llama_new_context_with_model: graph nodes  = 967
0.00.108.391 I llama_new_context_with_model: graph splits = 2
0.00.108.392 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.393 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.043.484 I 
0.01.043.566 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.043.631 I perplexity: tokenizing the input ..
0.01.057.050 I perplexity: tokenization took 13.413 ms
0.01.057.057 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.179.586 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.181.529 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.181.576 I llama_perf_context_print:        load time =    1019.34 ms
0.01.181.578 I llama_perf_context_print: prompt eval time =     121.60 ms /   128 tokens (    0.95 ms per token,  1052.64 tokens per second)
0.01.181.582 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.181.583 I llama_perf_context_print:       total time =     138.10 ms /   129 tokens
0.01.182.494 I ggml_metal_free: deallocating

real	0m1.378s
user	0m0.125s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.136 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.106 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.430 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.437 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.441 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.441 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.442 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.442 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.443 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.444 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.444 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.450 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.457 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.458 I llama_model_loader: - type  f32:  194 tensors
0.00.034.458 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.372 I llm_load_vocab: special tokens cache size = 25
0.00.065.416 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.419 I llm_load_print_meta: arch             = gptneox
0.00.065.420 I llm_load_print_meta: vocab type       = BPE
0.00.065.420 I llm_load_print_meta: n_vocab          = 50304
0.00.065.420 I llm_load_print_meta: n_merges         = 50009
0.00.065.420 I llm_load_print_meta: vocab_only       = 0
0.00.065.421 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.421 I llm_load_print_meta: n_embd           = 2048
0.00.065.421 I llm_load_print_meta: n_layer          = 24
0.00.065.425 I llm_load_print_meta: n_head           = 16
0.00.065.426 I llm_load_print_meta: n_head_kv        = 16
0.00.065.426 I llm_load_print_meta: n_rot            = 32
0.00.065.426 I llm_load_print_meta: n_swa            = 0
0.00.065.427 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.427 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.428 I llm_load_print_meta: n_gqa            = 1
0.00.065.428 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.429 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.429 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.430 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.430 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.430 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.430 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.431 I llm_load_print_meta: n_ff             = 8192
0.00.065.431 I llm_load_print_meta: n_expert         = 0
0.00.065.432 I llm_load_print_meta: n_expert_used    = 0
0.00.065.432 I llm_load_print_meta: causal attn      = 1
0.00.065.432 I llm_load_print_meta: pooling type     = 0
0.00.065.432 I llm_load_print_meta: rope type        = 2
0.00.065.432 I llm_load_print_meta: rope scaling     = linear
0.00.065.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.433 I llm_load_print_meta: freq_scale_train = 1
0.00.065.433 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.433 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.436 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.437 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.437 I llm_load_print_meta: model type       = 1.4B
0.00.065.437 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.438 I llm_load_print_meta: model params     = 1.41 B
0.00.065.438 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.438 I llm_load_print_meta: general.name     = 1.4B
0.00.065.439 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.439 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.439 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.439 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.440 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.440 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.441 I llm_load_print_meta: max token length = 1024
0.00.067.822 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.822 I llm_load_tensors: offloading output layer to GPU
0.00.067.822 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.834 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.835 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.759 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.760 I llama_new_context_with_model: n_ctx         = 128
0.00.068.760 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.761 I llama_new_context_with_model: n_batch       = 128
0.00.068.761 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.761 I llama_new_context_with_model: flash_attn    = 0
0.00.068.761 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.762 I llama_new_context_with_model: freq_scale    = 1
0.00.068.762 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.762 I ggml_metal_init: allocating
0.00.068.765 I ggml_metal_init: found device: Apple M4
0.00.068.767 I ggml_metal_init: picking default device: Apple M4
0.00.069.450 I ggml_metal_init: using embedded metal library
0.00.072.130 I ggml_metal_init: GPU name:   Apple M4
0.00.072.132 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.133 I ggml_metal_init: simdgroup reduction   = true
0.00.072.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.134 I ggml_metal_init: has bfloat            = true
0.00.072.134 I ggml_metal_init: use bfloat            = true
0.00.072.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.348 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.065 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.068 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.084 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.134 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.135 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.135 I llama_new_context_with_model: graph nodes  = 967
0.00.084.136 I llama_new_context_with_model: graph splits = 2
0.00.084.137 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.929.665 I 
0.00.929.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.929.726 I perplexity: tokenizing the input ..
0.00.937.487 I perplexity: tokenization took 7.76 ms
0.00.937.494 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.062.136 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.063.391 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.063.411 I llama_perf_context_print:        load time =     915.55 ms
0.01.063.411 I llama_perf_context_print: prompt eval time =     124.38 ms /   128 tokens (    0.97 ms per token,  1029.07 tokens per second)
0.01.063.412 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.063.412 I llama_perf_context_print:       total time =     133.75 ms /   129 tokens
0.01.063.932 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.091s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.478 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.351 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.355 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.147 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.940 I llama_model_loader: - type  f32:  194 tensors
0.00.024.940 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.871 I llm_load_vocab: special tokens cache size = 25
0.00.050.807 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.809 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.810 I llm_load_print_meta: arch             = gptneox
0.00.050.810 I llm_load_print_meta: vocab type       = BPE
0.00.050.810 I llm_load_print_meta: n_vocab          = 50304
0.00.050.810 I llm_load_print_meta: n_merges         = 50009
0.00.050.811 I llm_load_print_meta: vocab_only       = 0
0.00.050.811 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.811 I llm_load_print_meta: n_embd           = 2048
0.00.050.811 I llm_load_print_meta: n_layer          = 24
0.00.050.814 I llm_load_print_meta: n_head           = 16
0.00.050.815 I llm_load_print_meta: n_head_kv        = 16
0.00.050.815 I llm_load_print_meta: n_rot            = 32
0.00.050.816 I llm_load_print_meta: n_swa            = 0
0.00.050.816 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.816 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.817 I llm_load_print_meta: n_gqa            = 1
0.00.050.817 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.818 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.819 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.819 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.819 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.819 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.824 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.825 I llm_load_print_meta: n_ff             = 8192
0.00.050.825 I llm_load_print_meta: n_expert         = 0
0.00.050.825 I llm_load_print_meta: n_expert_used    = 0
0.00.050.826 I llm_load_print_meta: causal attn      = 1
0.00.050.826 I llm_load_print_meta: pooling type     = 0
0.00.050.826 I llm_load_print_meta: rope type        = 2
0.00.050.827 I llm_load_print_meta: rope scaling     = linear
0.00.050.827 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.827 I llm_load_print_meta: freq_scale_train = 1
0.00.050.828 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.828 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.829 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.830 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.830 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.830 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.830 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.830 I llm_load_print_meta: model type       = 1.4B
0.00.050.831 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.831 I llm_load_print_meta: model params     = 1.41 B
0.00.050.832 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.832 I llm_load_print_meta: general.name     = 1.4B
0.00.050.832 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.832 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.832 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.832 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.833 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.833 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.833 I llm_load_print_meta: max token length = 1024
0.00.052.806 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.806 I llm_load_tensors: offloading output layer to GPU
0.00.052.806 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.817 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.818 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.765 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.765 I llama_new_context_with_model: n_ctx         = 128
0.00.053.766 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.766 I llama_new_context_with_model: n_batch       = 128
0.00.053.766 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.766 I llama_new_context_with_model: flash_attn    = 0
0.00.053.767 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.767 I llama_new_context_with_model: freq_scale    = 1
0.00.053.767 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.768 I ggml_metal_init: allocating
0.00.053.771 I ggml_metal_init: found device: Apple M4
0.00.053.773 I ggml_metal_init: picking default device: Apple M4
0.00.054.303 I ggml_metal_init: using embedded metal library
0.00.056.625 I ggml_metal_init: GPU name:   Apple M4
0.00.056.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.627 I ggml_metal_init: simdgroup reduction   = true
0.00.056.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.627 I ggml_metal_init: has bfloat            = true
0.00.056.628 I ggml_metal_init: use bfloat            = true
0.00.056.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.301 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.596 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.599 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.616 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.480 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.481 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.482 I llama_new_context_with_model: graph nodes  = 967
0.00.068.482 I llama_new_context_with_model: graph splits = 2
0.00.068.483 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.483 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.354 I 
0.00.659.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.399 I perplexity: tokenizing the input ..
0.00.667.300 I perplexity: tokenization took 7.899 ms
0.00.667.304 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.031 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.791.177 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.791.196 I llama_perf_context_print:        load time =     648.87 ms
0.00.791.197 I llama_perf_context_print: prompt eval time =     122.50 ms /   128 tokens (    0.96 ms per token,  1044.87 tokens per second)
0.00.791.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.198 I llama_perf_context_print:       total time =     131.84 ms /   129 tokens
0.00.791.653 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.077s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.874 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.522 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.529 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.530 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.530 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.532 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.398 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.200 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.201 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.201 I llama_model_loader: - type  f32:  194 tensors
0.00.023.202 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.202 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.967 I llm_load_vocab: special tokens cache size = 25
0.00.049.939 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.942 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.942 I llm_load_print_meta: arch             = gptneox
0.00.049.942 I llm_load_print_meta: vocab type       = BPE
0.00.049.943 I llm_load_print_meta: n_vocab          = 50304
0.00.049.943 I llm_load_print_meta: n_merges         = 50009
0.00.049.943 I llm_load_print_meta: vocab_only       = 0
0.00.049.943 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.943 I llm_load_print_meta: n_embd           = 2048
0.00.049.943 I llm_load_print_meta: n_layer          = 24
0.00.049.946 I llm_load_print_meta: n_head           = 16
0.00.049.947 I llm_load_print_meta: n_head_kv        = 16
0.00.049.947 I llm_load_print_meta: n_rot            = 32
0.00.049.947 I llm_load_print_meta: n_swa            = 0
0.00.049.947 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.948 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.948 I llm_load_print_meta: n_gqa            = 1
0.00.049.949 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.951 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.951 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.951 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.952 I llm_load_print_meta: n_ff             = 8192
0.00.049.952 I llm_load_print_meta: n_expert         = 0
0.00.049.952 I llm_load_print_meta: n_expert_used    = 0
0.00.049.952 I llm_load_print_meta: causal attn      = 1
0.00.049.953 I llm_load_print_meta: pooling type     = 0
0.00.049.953 I llm_load_print_meta: rope type        = 2
0.00.049.953 I llm_load_print_meta: rope scaling     = linear
0.00.049.953 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.954 I llm_load_print_meta: freq_scale_train = 1
0.00.049.954 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.954 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.954 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.955 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.955 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.955 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.955 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.955 I llm_load_print_meta: model type       = 1.4B
0.00.049.956 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.956 I llm_load_print_meta: model params     = 1.41 B
0.00.049.957 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.957 I llm_load_print_meta: general.name     = 1.4B
0.00.049.957 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.957 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.957 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.958 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.958 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.958 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.960 I llm_load_print_meta: max token length = 1024
0.00.051.953 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.953 I llm_load_tensors: offloading output layer to GPU
0.00.051.953 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.964 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.965 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.865 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.866 I llama_new_context_with_model: n_ctx         = 128
0.00.052.866 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.866 I llama_new_context_with_model: n_batch       = 128
0.00.052.866 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.867 I llama_new_context_with_model: flash_attn    = 0
0.00.052.867 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.868 I llama_new_context_with_model: freq_scale    = 1
0.00.052.868 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.868 I ggml_metal_init: allocating
0.00.052.875 I ggml_metal_init: found device: Apple M4
0.00.052.877 I ggml_metal_init: picking default device: Apple M4
0.00.053.456 I ggml_metal_init: using embedded metal library
0.00.055.769 I ggml_metal_init: GPU name:   Apple M4
0.00.055.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.771 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.771 I ggml_metal_init: simdgroup reduction   = true
0.00.055.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.771 I ggml_metal_init: has bfloat            = true
0.00.055.772 I ggml_metal_init: use bfloat            = true
0.00.055.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.096 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.389 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.393 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.408 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.254 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.255 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.255 I llama_new_context_with_model: graph nodes  = 967
0.00.067.256 I llama_new_context_with_model: graph splits = 2
0.00.067.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.282 I 
0.00.653.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.328 I perplexity: tokenizing the input ..
0.00.661.219 I perplexity: tokenization took 7.89 ms
0.00.661.223 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.124 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.785.350 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.785.369 I llama_perf_context_print:        load time =     644.40 ms
0.00.785.370 I llama_perf_context_print: prompt eval time =     122.66 ms /   128 tokens (    0.96 ms per token,  1043.55 tokens per second)
0.00.785.372 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.372 I llama_perf_context_print:       total time =     132.09 ms /   129 tokens
0.00.785.830 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.078s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.935 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.486 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.496 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.496 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.497 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.497 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.497 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.499 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.499 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.500 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.417 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.202 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.203 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.204 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.204 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.205 I llama_model_loader: - type  f32:  194 tensors
0.00.024.206 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.206 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.034 I llm_load_vocab: special tokens cache size = 25
0.00.051.029 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.032 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.032 I llm_load_print_meta: arch             = gptneox
0.00.051.032 I llm_load_print_meta: vocab type       = BPE
0.00.051.033 I llm_load_print_meta: n_vocab          = 50304
0.00.051.033 I llm_load_print_meta: n_merges         = 50009
0.00.051.033 I llm_load_print_meta: vocab_only       = 0
0.00.051.033 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.033 I llm_load_print_meta: n_embd           = 2048
0.00.051.034 I llm_load_print_meta: n_layer          = 24
0.00.051.036 I llm_load_print_meta: n_head           = 16
0.00.051.037 I llm_load_print_meta: n_head_kv        = 16
0.00.051.037 I llm_load_print_meta: n_rot            = 32
0.00.051.038 I llm_load_print_meta: n_swa            = 0
0.00.051.038 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.038 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.039 I llm_load_print_meta: n_gqa            = 1
0.00.051.039 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.040 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.041 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.041 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.041 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.041 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.042 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.042 I llm_load_print_meta: n_ff             = 8192
0.00.051.042 I llm_load_print_meta: n_expert         = 0
0.00.051.043 I llm_load_print_meta: n_expert_used    = 0
0.00.051.043 I llm_load_print_meta: causal attn      = 1
0.00.051.043 I llm_load_print_meta: pooling type     = 0
0.00.051.043 I llm_load_print_meta: rope type        = 2
0.00.051.043 I llm_load_print_meta: rope scaling     = linear
0.00.051.045 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.047 I llm_load_print_meta: freq_scale_train = 1
0.00.051.047 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.048 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.048 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.048 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.048 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.048 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.050 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.050 I llm_load_print_meta: model type       = 1.4B
0.00.051.050 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.051 I llm_load_print_meta: model params     = 1.41 B
0.00.051.052 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.052 I llm_load_print_meta: general.name     = 1.4B
0.00.051.052 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.054 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.054 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.054 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.054 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.054 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.054 I llm_load_print_meta: max token length = 1024
0.00.052.883 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.883 I llm_load_tensors: offloading output layer to GPU
0.00.052.883 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.889 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.889 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.887 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.888 I llama_new_context_with_model: n_ctx         = 128
0.00.053.888 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.888 I llama_new_context_with_model: n_batch       = 128
0.00.053.888 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.888 I llama_new_context_with_model: flash_attn    = 0
0.00.053.889 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.889 I llama_new_context_with_model: freq_scale    = 1
0.00.053.889 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.890 I ggml_metal_init: allocating
0.00.053.895 I ggml_metal_init: found device: Apple M4
0.00.053.897 I ggml_metal_init: picking default device: Apple M4
0.00.054.526 I ggml_metal_init: using embedded metal library
0.00.057.134 I ggml_metal_init: GPU name:   Apple M4
0.00.057.135 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.137 I ggml_metal_init: simdgroup reduction   = true
0.00.057.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.137 I ggml_metal_init: has bfloat            = true
0.00.057.137 I ggml_metal_init: use bfloat            = true
0.00.057.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.869 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.183 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.187 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.203 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.074 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.075 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.076 I llama_new_context_with_model: graph nodes  = 967
0.00.069.076 I llama_new_context_with_model: graph splits = 2
0.00.069.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.895 I 
0.00.686.922 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.935 I perplexity: tokenizing the input ..
0.00.693.818 I perplexity: tokenization took 6.882 ms
0.00.693.821 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.016 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.829.499 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.829.516 I llama_perf_context_print:        load time =     676.96 ms
0.00.829.516 I llama_perf_context_print: prompt eval time =     133.97 ms /   128 tokens (    1.05 ms per token,   955.42 tokens per second)
0.00.829.517 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.517 I llama_perf_context_print:       total time =     142.62 ms /   129 tokens
0.00.829.889 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.078s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.457 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.557 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.559 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.562 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.565 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.435 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.492 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.431 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.432 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.433 I llama_model_loader: - type  f32:  194 tensors
0.00.024.433 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.434 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.415 I llm_load_vocab: special tokens cache size = 25
0.00.051.300 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.304 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.305 I llm_load_print_meta: arch             = gptneox
0.00.051.305 I llm_load_print_meta: vocab type       = BPE
0.00.051.305 I llm_load_print_meta: n_vocab          = 50304
0.00.051.305 I llm_load_print_meta: n_merges         = 50009
0.00.051.305 I llm_load_print_meta: vocab_only       = 0
0.00.051.311 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.311 I llm_load_print_meta: n_embd           = 2048
0.00.051.311 I llm_load_print_meta: n_layer          = 24
0.00.051.314 I llm_load_print_meta: n_head           = 16
0.00.051.315 I llm_load_print_meta: n_head_kv        = 16
0.00.051.315 I llm_load_print_meta: n_rot            = 32
0.00.051.316 I llm_load_print_meta: n_swa            = 0
0.00.051.316 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.316 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.317 I llm_load_print_meta: n_gqa            = 1
0.00.051.317 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.318 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.318 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.318 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.318 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.319 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.319 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.319 I llm_load_print_meta: n_ff             = 8192
0.00.051.319 I llm_load_print_meta: n_expert         = 0
0.00.051.319 I llm_load_print_meta: n_expert_used    = 0
0.00.051.320 I llm_load_print_meta: causal attn      = 1
0.00.051.320 I llm_load_print_meta: pooling type     = 0
0.00.051.320 I llm_load_print_meta: rope type        = 2
0.00.051.320 I llm_load_print_meta: rope scaling     = linear
0.00.051.320 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.321 I llm_load_print_meta: freq_scale_train = 1
0.00.051.321 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.321 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.321 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.321 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.322 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.322 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.322 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.322 I llm_load_print_meta: model type       = 1.4B
0.00.051.322 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.323 I llm_load_print_meta: model params     = 1.41 B
0.00.051.323 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.324 I llm_load_print_meta: general.name     = 1.4B
0.00.051.324 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.325 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.325 I llm_load_print_meta: max token length = 1024
0.00.053.292 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.293 I llm_load_tensors: offloading output layer to GPU
0.00.053.293 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.304 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.305 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.264 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.265 I llama_new_context_with_model: n_ctx         = 128
0.00.054.265 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.265 I llama_new_context_with_model: n_batch       = 128
0.00.054.265 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.265 I llama_new_context_with_model: flash_attn    = 0
0.00.054.266 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.266 I llama_new_context_with_model: freq_scale    = 1
0.00.054.267 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.267 I ggml_metal_init: allocating
0.00.054.274 I ggml_metal_init: found device: Apple M4
0.00.054.277 I ggml_metal_init: picking default device: Apple M4
0.00.054.892 I ggml_metal_init: using embedded metal library
0.00.057.337 I ggml_metal_init: GPU name:   Apple M4
0.00.057.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.340 I ggml_metal_init: simdgroup reduction   = true
0.00.057.340 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.340 I ggml_metal_init: has bfloat            = true
0.00.057.340 I ggml_metal_init: use bfloat            = true
0.00.057.341 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.583 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.912 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.915 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.934 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.797 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.798 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.798 I llama_new_context_with_model: graph nodes  = 967
0.00.068.799 I llama_new_context_with_model: graph splits = 2
0.00.068.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.999 I 
0.00.665.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.051 I perplexity: tokenizing the input ..
0.00.672.309 I perplexity: tokenization took 7.257 ms
0.00.672.314 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.268 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.807.688 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.807.703 I llama_perf_context_print:        load time =     655.53 ms
0.00.807.704 I llama_perf_context_print: prompt eval time =     133.73 ms /   128 tokens (    1.04 ms per token,   957.17 tokens per second)
0.00.807.704 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.705 I llama_perf_context_print:       total time =     142.70 ms /   129 tokens
0.00.808.019 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.079s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.969 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.582 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.583 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.583 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.586 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.304 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.197 I llama_model_loader: - type  f32:  194 tensors
0.00.024.198 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.198 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.097 I llm_load_vocab: special tokens cache size = 25
0.00.051.159 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.164 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.164 I llm_load_print_meta: arch             = gptneox
0.00.051.165 I llm_load_print_meta: vocab type       = BPE
0.00.051.167 I llm_load_print_meta: n_vocab          = 50304
0.00.051.167 I llm_load_print_meta: n_merges         = 50009
0.00.051.167 I llm_load_print_meta: vocab_only       = 0
0.00.051.167 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.168 I llm_load_print_meta: n_embd           = 2048
0.00.051.168 I llm_load_print_meta: n_layer          = 24
0.00.051.172 I llm_load_print_meta: n_head           = 16
0.00.051.173 I llm_load_print_meta: n_head_kv        = 16
0.00.051.174 I llm_load_print_meta: n_rot            = 32
0.00.051.175 I llm_load_print_meta: n_swa            = 0
0.00.051.175 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.175 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.175 I llm_load_print_meta: n_gqa            = 1
0.00.051.176 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.176 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.177 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.177 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.178 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.178 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.179 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.181 I llm_load_print_meta: n_ff             = 8192
0.00.051.181 I llm_load_print_meta: n_expert         = 0
0.00.051.181 I llm_load_print_meta: n_expert_used    = 0
0.00.051.181 I llm_load_print_meta: causal attn      = 1
0.00.051.181 I llm_load_print_meta: pooling type     = 0
0.00.051.181 I llm_load_print_meta: rope type        = 2
0.00.051.181 I llm_load_print_meta: rope scaling     = linear
0.00.051.182 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.182 I llm_load_print_meta: freq_scale_train = 1
0.00.051.182 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.183 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.183 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.183 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.183 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.183 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.183 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.183 I llm_load_print_meta: model type       = 1.4B
0.00.051.184 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.184 I llm_load_print_meta: model params     = 1.41 B
0.00.051.185 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.185 I llm_load_print_meta: general.name     = 1.4B
0.00.051.185 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.185 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.185 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.186 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: max token length = 1024
0.00.053.125 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.125 I llm_load_tensors: offloading output layer to GPU
0.00.053.125 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.136 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.137 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.157 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.158 I llama_new_context_with_model: n_ctx         = 128
0.00.054.158 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.158 I llama_new_context_with_model: n_batch       = 128
0.00.054.158 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.158 I llama_new_context_with_model: flash_attn    = 0
0.00.054.159 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.159 I llama_new_context_with_model: freq_scale    = 1
0.00.054.159 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.160 I ggml_metal_init: allocating
0.00.054.164 I ggml_metal_init: found device: Apple M4
0.00.054.166 I ggml_metal_init: picking default device: Apple M4
0.00.054.780 I ggml_metal_init: using embedded metal library
0.00.057.243 I ggml_metal_init: GPU name:   Apple M4
0.00.057.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.247 I ggml_metal_init: simdgroup reduction   = true
0.00.057.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.248 I ggml_metal_init: has bfloat            = true
0.00.057.248 I ggml_metal_init: use bfloat            = true
0.00.057.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.977 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.296 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.300 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.315 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.234 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.236 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.236 I llama_new_context_with_model: graph nodes  = 967
0.00.069.236 I llama_new_context_with_model: graph splits = 2
0.00.069.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.237 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.386.025 I 
0.00.386.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.386.065 I perplexity: tokenizing the input ..
0.00.393.740 I perplexity: tokenization took 7.674 ms
0.00.393.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.526.453 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.527.645 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.527.664 I llama_perf_context_print:        load time =     376.05 ms
0.00.527.665 I llama_perf_context_print: prompt eval time =     132.48 ms /   128 tokens (    1.04 ms per token,   966.16 tokens per second)
0.00.527.666 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.527.666 I llama_perf_context_print:       total time =     141.64 ms /   129 tokens
0.00.528.235 I ggml_metal_free: deallocating

real	0m0.544s
user	0m0.079s
sys	0m0.071s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.747 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.485 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.486 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.486 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.488 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.488 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.489 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.493 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.495 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.495 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.321 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.360 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.148 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.148 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.149 I llama_model_loader: - type  f32:  194 tensors
0.00.023.149 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.149 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.150 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.150 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.139 I llm_load_vocab: special tokens cache size = 25
0.00.048.956 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.959 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.959 I llm_load_print_meta: arch             = gptneox
0.00.048.959 I llm_load_print_meta: vocab type       = BPE
0.00.048.959 I llm_load_print_meta: n_vocab          = 50304
0.00.048.960 I llm_load_print_meta: n_merges         = 50009
0.00.048.960 I llm_load_print_meta: vocab_only       = 0
0.00.048.960 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.960 I llm_load_print_meta: n_embd           = 2048
0.00.048.960 I llm_load_print_meta: n_layer          = 24
0.00.048.963 I llm_load_print_meta: n_head           = 16
0.00.048.964 I llm_load_print_meta: n_head_kv        = 16
0.00.048.964 I llm_load_print_meta: n_rot            = 32
0.00.048.964 I llm_load_print_meta: n_swa            = 0
0.00.048.964 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.965 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.965 I llm_load_print_meta: n_gqa            = 1
0.00.048.966 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.967 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.967 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.968 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.968 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.968 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.968 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.969 I llm_load_print_meta: n_ff             = 8192
0.00.048.969 I llm_load_print_meta: n_expert         = 0
0.00.048.970 I llm_load_print_meta: n_expert_used    = 0
0.00.048.970 I llm_load_print_meta: causal attn      = 1
0.00.048.970 I llm_load_print_meta: pooling type     = 0
0.00.048.971 I llm_load_print_meta: rope type        = 2
0.00.048.971 I llm_load_print_meta: rope scaling     = linear
0.00.048.971 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.972 I llm_load_print_meta: freq_scale_train = 1
0.00.048.972 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.973 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.973 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.973 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.973 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.973 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.973 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.974 I llm_load_print_meta: model type       = 1.4B
0.00.048.974 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.975 I llm_load_print_meta: model params     = 1.41 B
0.00.048.975 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.975 I llm_load_print_meta: general.name     = 1.4B
0.00.048.976 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.976 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.976 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.976 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.977 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.977 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.978 I llm_load_print_meta: max token length = 1024
0.00.050.883 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.883 I llm_load_tensors: offloading output layer to GPU
0.00.050.883 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.894 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.895 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.758 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.759 I llama_new_context_with_model: n_ctx         = 128
0.00.051.759 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.759 I llama_new_context_with_model: n_batch       = 128
0.00.051.759 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.760 I llama_new_context_with_model: flash_attn    = 0
0.00.051.760 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.760 I llama_new_context_with_model: freq_scale    = 1
0.00.051.761 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.761 I ggml_metal_init: allocating
0.00.051.764 I ggml_metal_init: found device: Apple M4
0.00.051.766 I ggml_metal_init: picking default device: Apple M4
0.00.052.330 I ggml_metal_init: using embedded metal library
0.00.054.635 I ggml_metal_init: GPU name:   Apple M4
0.00.054.637 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.638 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.638 I ggml_metal_init: simdgroup reduction   = true
0.00.054.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.640 I ggml_metal_init: has bfloat            = true
0.00.054.640 I ggml_metal_init: use bfloat            = true
0.00.054.641 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.126 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.397 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.401 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.417 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.329 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.330 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.330 I llama_new_context_with_model: graph nodes  = 967
0.00.066.330 I llama_new_context_with_model: graph splits = 2
0.00.066.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.562 I 
0.00.474.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.607 I perplexity: tokenizing the input ..
0.00.482.705 I perplexity: tokenization took 8.097 ms
0.00.482.709 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.195 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.616.439 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.616.462 I llama_perf_context_print:        load time =     465.81 ms
0.00.616.463 I llama_perf_context_print: prompt eval time =     132.26 ms /   128 tokens (    1.03 ms per token,   967.81 tokens per second)
0.00.616.464 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.464 I llama_perf_context_print:       total time =     141.90 ms /   129 tokens
0.00.616.990 I ggml_metal_free: deallocating

real	0m0.630s
user	0m0.077s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.717 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.461 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.462 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.464 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.464 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.465 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.466 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.467 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.467 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.468 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.169 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.961 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.961 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.962 I llama_model_loader: - type  f32:  194 tensors
0.00.023.962 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.962 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.962 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.859 I llm_load_vocab: special tokens cache size = 25
0.00.049.907 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.912 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.912 I llm_load_print_meta: arch             = gptneox
0.00.049.912 I llm_load_print_meta: vocab type       = BPE
0.00.049.913 I llm_load_print_meta: n_vocab          = 50304
0.00.049.914 I llm_load_print_meta: n_merges         = 50009
0.00.049.916 I llm_load_print_meta: vocab_only       = 0
0.00.049.916 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.916 I llm_load_print_meta: n_embd           = 2048
0.00.049.916 I llm_load_print_meta: n_layer          = 24
0.00.049.918 I llm_load_print_meta: n_head           = 16
0.00.049.919 I llm_load_print_meta: n_head_kv        = 16
0.00.049.920 I llm_load_print_meta: n_rot            = 32
0.00.049.921 I llm_load_print_meta: n_swa            = 0
0.00.049.921 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.921 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.922 I llm_load_print_meta: n_gqa            = 1
0.00.049.922 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.923 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.924 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.924 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.924 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.924 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.924 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.925 I llm_load_print_meta: n_ff             = 8192
0.00.049.925 I llm_load_print_meta: n_expert         = 0
0.00.049.925 I llm_load_print_meta: n_expert_used    = 0
0.00.049.925 I llm_load_print_meta: causal attn      = 1
0.00.049.926 I llm_load_print_meta: pooling type     = 0
0.00.049.926 I llm_load_print_meta: rope type        = 2
0.00.049.926 I llm_load_print_meta: rope scaling     = linear
0.00.049.926 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.927 I llm_load_print_meta: freq_scale_train = 1
0.00.049.927 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.927 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.927 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.927 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.928 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.928 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.928 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.928 I llm_load_print_meta: model type       = 1.4B
0.00.049.929 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.929 I llm_load_print_meta: model params     = 1.41 B
0.00.049.930 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.930 I llm_load_print_meta: general.name     = 1.4B
0.00.049.930 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.930 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.930 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.931 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.935 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.935 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.935 I llm_load_print_meta: max token length = 1024
0.00.051.928 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.929 I llm_load_tensors: offloading output layer to GPU
0.00.051.929 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.940 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.941 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.923 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.924 I llama_new_context_with_model: n_ctx         = 128
0.00.052.924 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.924 I llama_new_context_with_model: n_batch       = 128
0.00.052.924 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.924 I llama_new_context_with_model: flash_attn    = 0
0.00.052.925 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.925 I llama_new_context_with_model: freq_scale    = 1
0.00.052.925 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.926 I ggml_metal_init: allocating
0.00.052.929 I ggml_metal_init: found device: Apple M4
0.00.052.930 I ggml_metal_init: picking default device: Apple M4
0.00.053.511 I ggml_metal_init: using embedded metal library
0.00.055.809 I ggml_metal_init: GPU name:   Apple M4
0.00.055.810 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.811 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.811 I ggml_metal_init: simdgroup reduction   = true
0.00.055.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.812 I ggml_metal_init: has bfloat            = true
0.00.055.812 I ggml_metal_init: use bfloat            = true
0.00.055.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.813 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.407 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.723 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.727 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.743 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.639 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.640 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.641 I llama_new_context_with_model: graph nodes  = 967
0.00.067.641 I llama_new_context_with_model: graph splits = 2
0.00.067.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.682 I 
0.00.558.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.558.766 I perplexity: tokenizing the input ..
0.00.566.851 I perplexity: tokenization took 8.084 ms
0.00.566.855 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.701.301 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.702.477 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.702.495 I llama_perf_context_print:        load time =     548.96 ms
0.00.702.497 I llama_perf_context_print: prompt eval time =     134.22 ms /   128 tokens (    1.05 ms per token,   953.69 tokens per second)
0.00.702.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.702.500 I llama_perf_context_print:       total time =     143.82 ms /   129 tokens
0.00.703.009 I ggml_metal_free: deallocating

real	0m0.718s
user	0m0.078s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.830 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.738 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.741 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.742 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.742 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.523 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.282 I llama_model_loader: - type  f32:  194 tensors
0.00.023.282 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.283 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.012 I llm_load_vocab: special tokens cache size = 25
0.00.049.888 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.890 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.891 I llm_load_print_meta: arch             = gptneox
0.00.049.891 I llm_load_print_meta: vocab type       = BPE
0.00.049.891 I llm_load_print_meta: n_vocab          = 50304
0.00.049.891 I llm_load_print_meta: n_merges         = 50009
0.00.049.892 I llm_load_print_meta: vocab_only       = 0
0.00.049.892 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.892 I llm_load_print_meta: n_embd           = 2048
0.00.049.892 I llm_load_print_meta: n_layer          = 24
0.00.049.895 I llm_load_print_meta: n_head           = 16
0.00.049.895 I llm_load_print_meta: n_head_kv        = 16
0.00.049.896 I llm_load_print_meta: n_rot            = 32
0.00.049.896 I llm_load_print_meta: n_swa            = 0
0.00.049.896 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.896 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.897 I llm_load_print_meta: n_gqa            = 1
0.00.049.898 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.898 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.899 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.899 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.899 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.899 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.900 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.900 I llm_load_print_meta: n_ff             = 8192
0.00.049.900 I llm_load_print_meta: n_expert         = 0
0.00.049.901 I llm_load_print_meta: n_expert_used    = 0
0.00.049.901 I llm_load_print_meta: causal attn      = 1
0.00.049.901 I llm_load_print_meta: pooling type     = 0
0.00.049.901 I llm_load_print_meta: rope type        = 2
0.00.049.901 I llm_load_print_meta: rope scaling     = linear
0.00.049.902 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.902 I llm_load_print_meta: freq_scale_train = 1
0.00.049.902 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.902 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.905 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.905 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.905 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.905 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.906 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.906 I llm_load_print_meta: model type       = 1.4B
0.00.049.906 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.907 I llm_load_print_meta: model params     = 1.41 B
0.00.049.912 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.912 I llm_load_print_meta: general.name     = 1.4B
0.00.049.912 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.912 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.912 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.912 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.913 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.913 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.913 I llm_load_print_meta: max token length = 1024
0.00.051.929 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.930 I llm_load_tensors: offloading output layer to GPU
0.00.051.930 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.940 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.941 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.804 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.804 I llama_new_context_with_model: n_ctx         = 128
0.00.052.804 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.805 I llama_new_context_with_model: n_batch       = 128
0.00.052.805 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.805 I llama_new_context_with_model: flash_attn    = 0
0.00.052.805 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.806 I llama_new_context_with_model: freq_scale    = 1
0.00.052.806 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.806 I ggml_metal_init: allocating
0.00.052.812 I ggml_metal_init: found device: Apple M4
0.00.052.814 I ggml_metal_init: picking default device: Apple M4
0.00.053.367 I ggml_metal_init: using embedded metal library
0.00.055.703 I ggml_metal_init: GPU name:   Apple M4
0.00.055.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.706 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.706 I ggml_metal_init: simdgroup reduction   = true
0.00.055.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.706 I ggml_metal_init: has bfloat            = true
0.00.055.706 I ggml_metal_init: use bfloat            = true
0.00.055.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.090 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.364 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.366 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.381 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.312 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.313 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.313 I llama_new_context_with_model: graph nodes  = 967
0.00.067.313 I llama_new_context_with_model: graph splits = 2
0.00.067.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.977 I 
0.00.627.009 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.026 I perplexity: tokenizing the input ..
0.00.635.215 I perplexity: tokenization took 8.187 ms
0.00.635.223 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.378 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.777.626 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.777.640 I llama_perf_context_print:        load time =     618.14 ms
0.00.777.641 I llama_perf_context_print: prompt eval time =     140.92 ms /   128 tokens (    1.10 ms per token,   908.33 tokens per second)
0.00.777.642 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.643 I llama_perf_context_print:       total time =     150.67 ms /   129 tokens
0.00.778.140 I ggml_metal_free: deallocating

real	0m0.792s
user	0m0.078s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.988 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.641 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.645 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.647 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.647 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.648 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.648 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.648 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.649 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.649 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.650 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.650 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.650 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.651 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.652 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.656 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.566 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.504 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.506 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.507 I llama_model_loader: - type  f32:  194 tensors
0.00.024.507 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.292 I llm_load_vocab: special tokens cache size = 25
0.00.051.372 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.374 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.375 I llm_load_print_meta: arch             = gptneox
0.00.051.375 I llm_load_print_meta: vocab type       = BPE
0.00.051.375 I llm_load_print_meta: n_vocab          = 50304
0.00.051.376 I llm_load_print_meta: n_merges         = 50009
0.00.051.376 I llm_load_print_meta: vocab_only       = 0
0.00.051.376 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.376 I llm_load_print_meta: n_embd           = 2048
0.00.051.376 I llm_load_print_meta: n_layer          = 24
0.00.051.379 I llm_load_print_meta: n_head           = 16
0.00.051.379 I llm_load_print_meta: n_head_kv        = 16
0.00.051.380 I llm_load_print_meta: n_rot            = 32
0.00.051.380 I llm_load_print_meta: n_swa            = 0
0.00.051.380 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.380 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.381 I llm_load_print_meta: n_gqa            = 1
0.00.051.382 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.382 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.387 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.388 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.388 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.388 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.388 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.389 I llm_load_print_meta: n_ff             = 8192
0.00.051.389 I llm_load_print_meta: n_expert         = 0
0.00.051.389 I llm_load_print_meta: n_expert_used    = 0
0.00.051.390 I llm_load_print_meta: causal attn      = 1
0.00.051.390 I llm_load_print_meta: pooling type     = 0
0.00.051.390 I llm_load_print_meta: rope type        = 2
0.00.051.390 I llm_load_print_meta: rope scaling     = linear
0.00.051.391 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.392 I llm_load_print_meta: freq_scale_train = 1
0.00.051.394 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.394 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.394 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.395 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.395 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.395 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.396 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.397 I llm_load_print_meta: model type       = 1.4B
0.00.051.397 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.397 I llm_load_print_meta: model params     = 1.41 B
0.00.051.398 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.398 I llm_load_print_meta: general.name     = 1.4B
0.00.051.398 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.398 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.398 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.398 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.399 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.400 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.400 I llm_load_print_meta: max token length = 1024
0.00.053.528 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.529 I llm_load_tensors: offloading output layer to GPU
0.00.053.529 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.539 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.540 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.491 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.492 I llama_new_context_with_model: n_ctx         = 128
0.00.054.492 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.492 I llama_new_context_with_model: n_batch       = 128
0.00.054.493 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.493 I llama_new_context_with_model: flash_attn    = 0
0.00.054.493 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.493 I llama_new_context_with_model: freq_scale    = 1
0.00.054.494 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.494 I ggml_metal_init: allocating
0.00.054.498 I ggml_metal_init: found device: Apple M4
0.00.054.500 I ggml_metal_init: picking default device: Apple M4
0.00.055.104 I ggml_metal_init: using embedded metal library
0.00.057.464 I ggml_metal_init: GPU name:   Apple M4
0.00.057.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.465 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.466 I ggml_metal_init: simdgroup reduction   = true
0.00.057.466 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.466 I ggml_metal_init: has bfloat            = true
0.00.057.466 I ggml_metal_init: use bfloat            = true
0.00.057.468 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.469 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.270 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.546 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.549 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.562 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.514 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.515 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.516 I llama_new_context_with_model: graph nodes  = 967
0.00.069.516 I llama_new_context_with_model: graph splits = 2
0.00.069.517 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.289.974 I 
0.00.290.000 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.290.013 I perplexity: tokenizing the input ..
0.00.297.389 I perplexity: tokenization took 7.375 ms
0.00.297.393 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.437.215 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.438.375 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.438.394 I llama_perf_context_print:        load time =     279.98 ms
0.00.438.395 I llama_perf_context_print: prompt eval time =     139.59 ms /   128 tokens (    1.09 ms per token,   917.00 tokens per second)
0.00.438.396 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.438.396 I llama_perf_context_print:       total time =     148.42 ms /   129 tokens
0.00.438.820 I ggml_metal_free: deallocating

real	0m0.454s
user	0m0.079s
sys	0m0.054s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.260 I build: 4427 (09186fab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.421 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.916 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.927 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.934 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.942 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.942 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.116 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.688 I llama_model_loader: - type  f32:  194 tensors
0.00.051.688 I llama_model_loader: - type  f16:   98 tensors
0.00.080.262 I llm_load_vocab: special tokens cache size = 25
0.00.086.836 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.839 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.839 I llm_load_print_meta: arch             = gptneox
0.00.086.840 I llm_load_print_meta: vocab type       = BPE
0.00.086.840 I llm_load_print_meta: n_vocab          = 50304
0.00.086.840 I llm_load_print_meta: n_merges         = 50009
0.00.086.840 I llm_load_print_meta: vocab_only       = 0
0.00.086.840 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.841 I llm_load_print_meta: n_embd           = 2048
0.00.086.841 I llm_load_print_meta: n_layer          = 24
0.00.086.844 I llm_load_print_meta: n_head           = 16
0.00.086.845 I llm_load_print_meta: n_head_kv        = 16
0.00.086.847 I llm_load_print_meta: n_rot            = 32
0.00.086.847 I llm_load_print_meta: n_swa            = 0
0.00.086.847 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.847 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.848 I llm_load_print_meta: n_gqa            = 1
0.00.086.850 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.850 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.851 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.851 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.852 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.852 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.852 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.853 I llm_load_print_meta: n_ff             = 8192
0.00.086.853 I llm_load_print_meta: n_expert         = 0
0.00.086.853 I llm_load_print_meta: n_expert_used    = 0
0.00.086.853 I llm_load_print_meta: causal attn      = 1
0.00.086.853 I llm_load_print_meta: pooling type     = 0
0.00.086.853 I llm_load_print_meta: rope type        = 2
0.00.086.854 I llm_load_print_meta: rope scaling     = linear
0.00.086.854 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.854 I llm_load_print_meta: freq_scale_train = 1
0.00.086.855 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.855 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.856 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.856 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.856 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.857 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.857 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.857 I llm_load_print_meta: model type       = 1.4B
0.00.086.858 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.858 I llm_load_print_meta: model params     = 1.41 B
0.00.086.859 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.859 I llm_load_print_meta: general.name     = 1.4B
0.00.086.859 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.859 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.860 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.860 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.860 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.860 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.861 I llm_load_print_meta: max token length = 1024
0.00.089.531 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.532 I llm_load_tensors: offloading output layer to GPU
0.00.089.532 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.543 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.544 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.465 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.466 I llama_new_context_with_model: n_ctx         = 128
0.00.090.466 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.466 I llama_new_context_with_model: n_batch       = 128
0.00.090.466 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.467 I llama_new_context_with_model: flash_attn    = 0
0.00.090.467 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.468 I llama_new_context_with_model: freq_scale    = 1
0.00.090.468 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.468 I ggml_metal_init: allocating
0.00.090.473 I ggml_metal_init: found device: Apple M4
0.00.090.474 I ggml_metal_init: picking default device: Apple M4
0.00.091.127 I ggml_metal_init: using embedded metal library
0.00.093.656 I ggml_metal_init: GPU name:   Apple M4
0.00.093.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.658 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.658 I ggml_metal_init: simdgroup reduction   = true
0.00.093.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.658 I ggml_metal_init: has bfloat            = true
0.00.093.659 I ggml_metal_init: use bfloat            = true
0.00.093.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.632 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.901 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.904 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.917 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.758 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.759 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.759 I llama_new_context_with_model: graph nodes  = 967
0.00.104.759 I llama_new_context_with_model: graph splits = 2
0.00.104.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.761 I 
0.00.104.786 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.788 I compute_imatrix: tokenizing the input ..
0.00.111.316 I compute_imatrix: tokenization took 6.528 ms
0.00.111.318 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.658.837 I compute_imatrix: 1.55 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.661.284 I llama_perf_context_print:        load time =    1636.41 ms
0.01.661.285 I llama_perf_context_print: prompt eval time =    1546.90 ms /   128 tokens (   12.09 ms per token,    82.75 tokens per second)
0.01.661.289 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.661.290 I llama_perf_context_print:       total time =    1638.86 ms /   129 tokens
0.01.661.949 I ggml_metal_free: deallocating

real	0m1.846s
user	0m0.167s
sys	0m0.260s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4427 (09186fab)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d20a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d20a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d20aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d20b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d20ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d20bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d20c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d20cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d20d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d20d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d20daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d20dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d20eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d20f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d20fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d2101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d210910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d211030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d211750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d211f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d212640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d212d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d213480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d213d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d214440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d214700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d214d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d215980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d215ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d216180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d216620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d2168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d217170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d2176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d217970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d217e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d2182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d218750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d218bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d219090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d219530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d2199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d219e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d21a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d21a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d21abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d21b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d21bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d21c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d21c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d21cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d21d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d21d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d21df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d21e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d21ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d21f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d21f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d21f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d220160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d220420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d2208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d220d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d221200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d2216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d221b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d221fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d222480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d222920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d222dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d223260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d223700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d223ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d2240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d224640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d224b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d2250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d225630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d225b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d2260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d226620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d226b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d2270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d227610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d227b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d2280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d228600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d228b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d2290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d2295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d229b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d22a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d22a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d22ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d22b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d22b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d22bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d21b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d22bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d22c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d22cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d22d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d22d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d22dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d22e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d22e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d22ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d22f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d22f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d22fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d2301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d230700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d230c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d2310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d231590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d231a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d231ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d232370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d232810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d232cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d233150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d2335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d233a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d233f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d2343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d234870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d234d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d2351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d235650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d235af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d235f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d236430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d2368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d236d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d237210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d2376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d237b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d237ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d238490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d238930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d238dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d239270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d239710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d239bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d23a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d23a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d23a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d23ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d23b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d23b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d23bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d23c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d23c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d23c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d23ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d23d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d23d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d23dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d23e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d23e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d23ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d23eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d23f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d23f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d23fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d240170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d240610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d240ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d240f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d2413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d241890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d241d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d2421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d242670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d242b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d242fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d243450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d2438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d243d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d244230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d2446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d244b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d245010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d2454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d245950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d245df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d246290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d246730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d246bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d247070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d247510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d2479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d247e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d2483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d2488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d248e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d249390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d249650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d249c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d24a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d24a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d24b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d24b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d24b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d24bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d24c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d24cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d24d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d24d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d24d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d24e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d24e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d24ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d24f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d24f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d24fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d250150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d2506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d250bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d251140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d251690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d251be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d252130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d252680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d252bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d253120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d253670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d253bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d254110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d254660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d254bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d255100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d255650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d255ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d2560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d256640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d256b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d2570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d257630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d257b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d2580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d258620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d258b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d2590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d259610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d259b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d25a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d25a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d25ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d25b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d25b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d25bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d25c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d25c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d25cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d25d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d25d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d25db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d25e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d25e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d25eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d25f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d25f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d25fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d260050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d2605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d260af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d260f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d261430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d2618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d261d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d262210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d2626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d262b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d262ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d263490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d263930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d263dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d264270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d264710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d264bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d265050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d2655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d265cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d2663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d266b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d267220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d2674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d267cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d267f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d2685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.144.475 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fb08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fb08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fb091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fb09740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fb09c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fb0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fb0a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fb0ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fb0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fb0b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fb0ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fb0bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fb0c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fb0cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fb0d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fb0ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fb0e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fb0ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fb0f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fb0fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fb10420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fb10b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fb11260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fb11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fb120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fb12360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fb12970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fb12f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fb13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fb13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fb14220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fb144e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fb14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fb152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fb15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fb15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fb15eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fb16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fb167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fb16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fb17130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fb175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fb17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fb17f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fb181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fb187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fb18df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fb19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fb19a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fb1a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fb1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fb1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fb1b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fb1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fb1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fb1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fb1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fb1cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fb1d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fb1da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fb1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fb1e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fb1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fb1ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fb1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fb1f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fb1fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fb1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fb203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fb20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fb20d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fb211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fb21670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fb21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fb22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fb22660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fb22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fb23100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fb23650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fb23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fb240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fb24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fb24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fb250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fb25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fb25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fb260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fb26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fb26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fb270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fb27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fb27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fb280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fb28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fb28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fb290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fb295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fb29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fb2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fb2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fb2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fb2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fb2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fb2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fb2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fb2c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fb2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fb2d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fb2d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fb2db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fb2e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fb2e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fb2eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fb2ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fb2f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fb2f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fb2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fb30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fb306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fb30b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fb30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fb31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fb31930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fb31dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fb32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fb32710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fb32bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fb33050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fb334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fb33990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fb33e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fb342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fb34770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fb34c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fb350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fb35550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fb359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fb35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fb36330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fb367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fb36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fb37110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fb375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fb37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fb37ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fb38390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fb38830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fb38cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fb39170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fb39610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fb39ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fb39f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fb3a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fb3a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fb3ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fb3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fb3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fb3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fb3bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fb3c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fb3c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fb3cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fb3d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fb3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fb3db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fb3e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fb3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fb3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fb3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fb3f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fb3f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fb3fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fb40070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fb40510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fb409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fb40e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fb412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fb41790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fb41c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fb420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fb42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fb42a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fb42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fb43350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fb437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fb43c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fb44130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fb445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fb44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fb44f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fb453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fb45850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fb45cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fb46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fb46790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fb46ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fb47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fb474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fb47b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fb48110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fb48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fb48f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fb493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fb49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fb49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fb4a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fb4aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fb4af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fb4b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fb4b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fb4c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fb4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fb4cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fb4d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fb4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fb4daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fb4dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fb4e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fb4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fb4efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fb4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fb4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fb4ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fb50520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fb50a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fb50fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fb51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fb51a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fb51fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fb52500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fb52a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fb52fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fb534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fb53a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fb53f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fb544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fb54a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fb54f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fb554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fb55a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fb55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fb564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fb56a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fb56f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fb574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fb57a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fb57f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fb584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fb589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fb58f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12fb59490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12fb599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12fb59f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12fb5a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12fb5a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12fb5af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12fb5b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12fb5b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12fb5bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12fb5c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12fb5c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12fb5cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12fb5d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12fb5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12fb5def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12fb5e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12fb5e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12fb5ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12fb5f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12fb5f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12fb5fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12fb600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12fb60550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12fb609f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12fb60e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12fb61330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12fb617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12fb61c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12fb62110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12fb625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12fb62a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12fb62ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12fb63440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12fb63b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12fb64280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12fb649a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12fb650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12fb65380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12fb65b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12fb65e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12fb66440 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d268250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d249f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d249910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d24a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d21d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d21d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d21f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d24c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d2149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d21b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d21bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d21c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d21a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d21c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d2139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d209840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d21e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d21fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d22c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d2677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d216ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d216e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d24c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d24ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d214fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d215290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d215550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d268a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d268cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d268f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d269240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d269500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d2697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d269a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d269d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d26a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d26a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d26a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d26a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d26ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d26adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d26b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d26b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d26b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d26b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d26bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d26be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d26c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d26c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d26c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d26c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d26cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d26cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d26d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d26d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d26d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d26d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d26dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d26df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d26e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d26e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d26e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d26ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d26ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d26efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d26f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d26f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d26f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d26fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d26fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d270040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d270300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d2705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d270880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d270b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d270e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d2710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d271380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d271640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d271900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d271bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d271e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d272140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d272400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d2726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d272980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d272c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d272f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d2731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d273480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d273740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d273a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d273cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d273f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d274240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d274500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d2747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d274a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d274d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d275000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d2752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d275580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d275840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d275b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d275dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d276080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d276340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d276600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d2768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d276b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d276e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d277100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d2773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d277680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d277940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d277c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d277ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d278180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d278440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d278700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d2789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d278c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d278f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d279200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d2794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d279780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d279a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d279d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d279fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d27a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d27a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d27a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d27aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d27ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d27b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d27b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d27b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d27b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d27bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d27be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d27c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d27c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d27c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d27c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d27cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d27ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d27d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d27d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d27d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d27d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d27dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d27df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d27e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d27e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d27e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d27ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d27ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d27ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d27f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d27f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d27f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d27fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d27fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d280000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d2802c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d280580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d280840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d280b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d280dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d281080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d281340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d281600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d2818c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d281b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d281e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d282100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d2823c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d282680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d282940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d282c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d282ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d283180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d283440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d283700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d2839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d283c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d283f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d284200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d2844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d284780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d284a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d284d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d284fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d285280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d285540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d285800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d285ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d285d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d286040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d286300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d2865c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d286880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d286b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d286e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d2870c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d287380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d287640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d287900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d287bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d287e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d288450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d288710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d2889d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d288c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d288f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d289210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d2894d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d289790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d289a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d289d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d289fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d28a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d28a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d28a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d28aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d28ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d28b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d28b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d28b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d28b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d28bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d28be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d28c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d28c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d28c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d28c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d28cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d28ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d28d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d28d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d28d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d28d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d28dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d28e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d28e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d28eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d28f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d28f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d28fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d290410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d290960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d290eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d291400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d291950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d291ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d2923f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d292940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d292e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d2933e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d293930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d293e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d2943d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d294920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d294e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d2953c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d295910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d295e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d296120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d2963e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d2968e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d296de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d2972e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d2977e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d297ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d2981e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d2986e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d298be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d2990e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d2995e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d299ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d299fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d29a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d29a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d29b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d29bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d29c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d29c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d29cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d29d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d29d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d29dcd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.835s
user	0m0.296s
sys	0m0.330s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4427 (09186fab)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144e0a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144e0afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144e0b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144e0bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144e0c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144e0c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144e0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144e0d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144e0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144e0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144e0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144e0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144e0f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144e0f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144e10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144e108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144e10fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144e116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144e11e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144e125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144e12cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144e13410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144e13b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144e143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144e14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144e14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144e153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144e16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144e16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144e16830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144e16cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144e16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144e17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144e17d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144e18020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144e184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144e18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144e18e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144e192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144e19740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144e19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144e1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144e1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144e1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144e1ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144e1b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144e1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144e1c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144e1c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144e1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144e1d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144e1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144e1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144e1e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144e1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144e1f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144e1f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144e1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144e20020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144e20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144e20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144e20f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144e21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144e218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144e21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144e221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144e22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144e22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144e22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144e23470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144e23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144e23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144e24250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144e247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144e24cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144e25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144e25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144e25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144e26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144e26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144e26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144e27220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144e27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144e27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144e28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144e28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144e28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144e29200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144e29750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144e29ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144e2a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144e2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144e2ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144e2b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144e2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144e2bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144e2c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144e1beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144e2c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144e2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144e2d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144e2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144e2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144e2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144e2e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144e2edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144e2f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144e2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144e2fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144e30310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144e30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144e30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144e31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144e317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144e31c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144e320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144e32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144e32a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144e32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144e33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144e33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144e33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144e34140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144e345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144e34a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144e34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144e353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144e35860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144e35d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144e361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144e36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144e36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144e36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144e37420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144e378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144e37d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144e38200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144e386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144e38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144e38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144e39480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144e39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144e39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144e3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144e3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144e3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144e3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144e3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144e3b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144e3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144e3c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144e3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144e3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144e3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144e3d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144e3d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144e3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144e3e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144e3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144e3ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144e3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144e3f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144e3fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144e3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144e40380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144e40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144e40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144e41160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144e41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144e41aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144e41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144e423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144e42880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144e42d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144e431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144e43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144e43b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144e43fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144e44440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144e448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144e44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144e45220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144e456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144e45b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144e46000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144e464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144e46940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144e46de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144e47280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144e47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144e47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144e48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144e48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144e48a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144e48fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144e494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144e49a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144e49d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144e4a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144e4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144e4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144e4b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144e4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144e4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144e4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144e4caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144e4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144e4d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144e4dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144e4e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144e4e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144e4ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144e4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144e4f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144e4fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144e502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144e50800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144e50d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144e512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144e517f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144e51d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144e52290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144e527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144e52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144e53280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144e537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144e53d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144e54270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144e547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144e54d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144e55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144e557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144e55d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144e56250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144e567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144e56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144e57240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144e57790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144e57ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144e58230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144e58780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144e58cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144e59220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144e59770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144e59cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144e5a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144e5a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144e5acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144e5b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144e5b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144e5bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144e5c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144e5c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144e5cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144e5d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144e5d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144e5dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144e5e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144e5e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144e5ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144e5f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144e5f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144e5fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144e601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144e60700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144e60c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144e611a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144e61640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144e61ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144e61f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144e62420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144e628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144e62d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144e63200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144e636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144e63b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144e63fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144e64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144e64920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144e64dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144e65260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144e65700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144e65c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144e66370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144e66a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144e671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144e678d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144e67b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144e68380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144e68640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144e68c50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.273 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.277 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134f04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134f05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134f056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134f05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134f05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134f06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134f06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134f06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134f07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134f075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134f07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134f08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134f08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134f093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134f09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134f0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134f0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134f0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134f0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134f0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134f0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134f0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134f0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134f0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134f0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134f0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134f0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134f0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134f0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134f0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134f0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134f10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134f106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134f10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134f10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134f11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134f118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134f11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134f12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134f12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134f12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134f12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134f13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134f137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134f13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134f140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134f14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134f14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134f14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134f15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134f156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134f15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134f15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134f16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134f16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134f16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134f17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134f17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134f17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134f18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134f184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134f18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134f18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134f19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134f19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134f19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134f19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134f1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134f1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134f1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134f1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134f1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134f1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134f1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134f1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134f1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134f1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134f1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134f1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134f1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134f1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134f1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134f1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134f1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134f1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134f1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134f1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134f1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134f20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134f20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134f209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134f20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134f21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134f21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134f22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134f22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134f228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134f22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134f231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134f23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134f23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134f23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134f24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134f24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134f24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134f250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134f25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134f259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134f25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134f262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134f26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134f26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134f26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134f27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134f278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134f27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134f28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134f28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134f28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134f29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134f297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134f29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134f2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134f2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134f2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134f2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134f2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134f2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134f2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134f2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134f2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134f2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134f2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134f2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134f2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134f2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134f2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134f2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134f2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134f2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134f2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134f2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134f2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134f30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134f306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134f30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134f30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134f31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134f31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134f31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134f32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134f325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134f32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134f32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134f33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134f337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134f33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134f34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134f34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134f34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134f35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134f35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134f36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134f363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134f36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134f36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134f37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134f375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134f37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134f37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134f38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134f38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134f39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134f394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134f39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134f39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134f3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134f3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134f3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134f3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134f3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134f3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134f3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134f3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134f3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134f3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134f3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134f3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134f3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134f3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134f3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134f3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134f3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134f3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134f3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134f3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134f400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134f40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134f409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134f40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134f41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134f417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134f41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134f42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134f42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134f430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134f43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134f43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134f441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134f447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134f45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134f458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134f45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134f46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134f46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134f46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134f475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134f47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134f48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134f486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134f48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134f49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134f49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134f49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134f4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134f4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134f4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134f4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134f4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134f4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134f4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134f4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134f4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134f4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134f4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134f4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134f4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134f4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134f4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134f4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134f50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134f50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134f510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134f516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134f51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134f52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134f527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134f52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134f53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134f53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134f53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134f544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134f54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134f55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134f555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134f55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134f56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134f56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134f56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134f571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134f576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134f57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134f580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134f585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134f58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134f58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134f594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134f599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134f59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134f5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134f5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134f5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134f5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134f5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134f5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134f5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134f5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134f5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134f5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134f5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134f5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134f5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134f5bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134f4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134f4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134f483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134f45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134f552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134f52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134f50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134f4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134f46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134f43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134f48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134f4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134f4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134f4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134f541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134f47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134f513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134f4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134f4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134f47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134f558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134f44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134f43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134f455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134f55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134f4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134f53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134f49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134f4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134f4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134f472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134f50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134f51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134f46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134f54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134f51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134f4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134f569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134f45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134f56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134f444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134f54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134f4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134f50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134f53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134f524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134f4a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134f41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134f04880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134f5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134f0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134f5ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134f5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134f5f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134f5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134f5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134f5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134f5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134f60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134f60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134f60800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134f60ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134f60d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134f61040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134f61300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134f615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134f61880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134f61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134f61e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134f620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134f62380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134f62640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134f62900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134f62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134f62e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134f63140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134f63400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134f636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134f63980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134f63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134f63f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134f641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134f64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134f64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134f64a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134f64cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134f64f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134f65240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134f65500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134f657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134f65a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134f65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134f66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134f662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134f66580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134f66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134f66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134f66dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134f67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134f67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134f67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134f678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134f67b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134f67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134f68100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134f683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134f68680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134f68940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134f68c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134f68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134f69180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134f69440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134f69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134f699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134f69c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134f69f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134f6a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134f6a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134f6a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134f6aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134f6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134f6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134f6b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134f6b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134f6b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134f6bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134f6bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134f6c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134f6c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134f6c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134f6c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134f6cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134f6ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134f6d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134f6d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134f6d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134f6d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134f6dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134f6de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134f6e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134f6e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134f6e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134f6e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134f6ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134f6ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134f6f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134f6f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134f6f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134f6fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134f6fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134f6ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134f70240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134f70500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134f707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134f70a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134f70d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134f71000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134f712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134f71580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134f71840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134f71b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134f71dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134f72080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134f72340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134f72600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134f728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134f72b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134f72e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134f73100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134f733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134f73680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134f73940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134f73c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134f73ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134f74180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134f74440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134f74700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134f749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134f74c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134f74f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134f75200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134f754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134f75780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134f75a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134f75d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134f75fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134f76280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134f76540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134f76800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134f76ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134f76d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134f77040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134f77300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134f775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134f77880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134f77b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134f77e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134f780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134f78380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134f78640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134f78900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134f78bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134f78e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134f79140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134f79400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134f796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134f79980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134f79c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134f79f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134f7a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134f7a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134f7aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134f7ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134f7afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134f7b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134f7b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134f7b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134f7bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134f7c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134f7c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134f7cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134f7d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134f7d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134f7dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134f7e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134f7e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134f7ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134f7f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134f7f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134f7fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134f80270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134f807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134f80d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134f81260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134f817b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134f81d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134f82250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134f827a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134f82cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134f83240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134f83790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134f83ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134f84230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134f84780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134f84cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134f85220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134f85770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134f85cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134f86210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134f86760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134f86cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134f87200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134f87750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134f87ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134f881f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134f88740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134f88c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134f891e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134f89730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134f89c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134f8a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134f8a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134f8ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134f8b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134f8b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134f8bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134f8bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134f8c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134f8c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134f8c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134f8cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134f8d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134f8d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134f8dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134f8df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134f8e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134f8e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134f8ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134f8f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134f8f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134f8f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134f8fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134f902c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134f90fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134f916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134f91df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134f920b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134f92520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134f92b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134f93130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.912s
user	0m0.241s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
