Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.552s
user	0m0.876s
sys	0m1.232s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha1
[  6%] Built target build_info
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target sha256
[  7%] Built target xxhash
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Built target llava
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 31%] Linking CXX static library libcommon.a
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target test-c
[ 32%] Built target llama-simple-chat
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 33%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-grammar-parser
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Built target test-log
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-arg-parser
[ 46%] Built target test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../../bin/llama-batched-bench
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Built target test-chat-template
[ 61%] Built target test-gguf
[ 61%] Built target llama-batched-bench
[ 61%] Built target test-backend-ops
[ 61%] Built target test-model-load-cancel
[ 61%] Built target test-autorelease
[ 61%] Built target test-barrier
[ 61%] Built target test-quantize-perf
[ 61%] Built target test-rope
[ 61%] Built target test-quantize-fns
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-infill
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-infill
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-bench
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 73%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 74%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 75%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 76%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup
[ 82%] Built target llama-perplexity
[ 82%] Built target llama-parallel
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-cli
[ 82%] Built target llama-passkey
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-quantize
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-retrieval
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-run
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Built target llama-cvector-generator
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.960s
user	0m5.813s
sys	0m9.119s

main: quantize time =  2239.33 ms
main:    total time =  2239.33 ms

main: quantize time =  1324.15 ms
main:    total time =  1324.15 ms

main: quantize time =  1284.19 ms
main:    total time =  1284.19 ms

main: quantize time =  1407.72 ms
main:    total time =  1407.72 ms

main: quantize time =  1896.35 ms
main:    total time =  1896.35 ms

main: quantize time =  4959.43 ms
main:    total time =  4959.43 ms

main: quantize time =  5653.00 ms
main:    total time =  5653.00 ms

main: quantize time =  6983.44 ms
main:    total time =  6983.44 ms

main: quantize time =  6097.26 ms
main:    total time =  6097.26 ms

main: quantize time =  4498.47 ms
main:    total time =  4498.47 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.111 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.222 I main: llama backend init
0.00.000.228 I main: load the model and apply lora adapter, if any
0.00.052.115 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.063.325 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.361 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.070.500 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.072.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.081.667 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.668 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.669 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.670 I llama_model_loader: - type  f32:  194 tensors
0.00.081.671 I llama_model_loader: - type  f16:   98 tensors
0.00.117.362 I llm_load_vocab: special tokens cache size = 25
0.00.124.644 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.124.647 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.124.647 I llm_load_print_meta: arch             = gptneox
0.00.124.648 I llm_load_print_meta: vocab type       = BPE
0.00.124.648 I llm_load_print_meta: n_vocab          = 50304
0.00.124.648 I llm_load_print_meta: n_merges         = 50009
0.00.124.648 I llm_load_print_meta: vocab_only       = 0
0.00.124.648 I llm_load_print_meta: n_ctx_train      = 2048
0.00.124.648 I llm_load_print_meta: n_embd           = 2048
0.00.124.649 I llm_load_print_meta: n_layer          = 24
0.00.124.652 I llm_load_print_meta: n_head           = 16
0.00.124.652 I llm_load_print_meta: n_head_kv        = 16
0.00.124.653 I llm_load_print_meta: n_rot            = 32
0.00.124.653 I llm_load_print_meta: n_swa            = 0
0.00.124.653 I llm_load_print_meta: n_embd_head_k    = 128
0.00.124.653 I llm_load_print_meta: n_embd_head_v    = 128
0.00.124.654 I llm_load_print_meta: n_gqa            = 1
0.00.124.655 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.124.655 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.124.656 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.124.656 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.124.657 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.124.657 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.124.657 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.124.660 I llm_load_print_meta: n_ff             = 8192
0.00.124.660 I llm_load_print_meta: n_expert         = 0
0.00.124.660 I llm_load_print_meta: n_expert_used    = 0
0.00.124.661 I llm_load_print_meta: causal attn      = 1
0.00.124.661 I llm_load_print_meta: pooling type     = 0
0.00.124.661 I llm_load_print_meta: rope type        = 2
0.00.124.661 I llm_load_print_meta: rope scaling     = linear
0.00.124.661 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.124.662 I llm_load_print_meta: freq_scale_train = 1
0.00.124.662 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.124.662 I llm_load_print_meta: rope_finetuned   = unknown
0.00.124.662 I llm_load_print_meta: ssm_d_conv       = 0
0.00.124.662 I llm_load_print_meta: ssm_d_inner      = 0
0.00.124.662 I llm_load_print_meta: ssm_d_state      = 0
0.00.124.663 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.124.663 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.124.663 I llm_load_print_meta: model type       = 1.4B
0.00.124.664 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.124.665 I llm_load_print_meta: model params     = 1.41 B
0.00.124.665 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.124.665 I llm_load_print_meta: general.name     = 1.4B
0.00.124.666 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.124.666 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.124.666 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.124.666 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.124.667 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.124.667 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.124.667 I llm_load_print_meta: max token length = 1024
0.00.127.370 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.127.370 I llm_load_tensors: offloading output layer to GPU
0.00.127.370 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.127.389 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.127.390 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.128.427 I llama_new_context_with_model: n_seq_max     = 1
0.00.128.428 I llama_new_context_with_model: n_ctx         = 2048
0.00.128.428 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.128.428 I llama_new_context_with_model: n_batch       = 2048
0.00.128.428 I llama_new_context_with_model: n_ubatch      = 512
0.00.128.428 I llama_new_context_with_model: flash_attn    = 0
0.00.128.429 I llama_new_context_with_model: freq_base     = 10000.0
0.00.128.429 I llama_new_context_with_model: freq_scale    = 1
0.00.128.430 I ggml_metal_init: allocating
0.00.128.433 I ggml_metal_init: found device: Apple M4
0.00.128.435 I ggml_metal_init: picking default device: Apple M4
0.00.129.141 I ggml_metal_init: using embedded metal library
0.00.147.619 I ggml_metal_init: GPU name:   Apple M4
0.00.147.621 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.147.621 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.147.622 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.147.622 I ggml_metal_init: simdgroup reduction   = true
0.00.147.622 I ggml_metal_init: simdgroup matrix mul. = true
0.00.147.622 I ggml_metal_init: has bfloat            = true
0.00.147.622 I ggml_metal_init: use bfloat            = true
0.00.147.623 I ggml_metal_init: hasUnifiedMemory      = true
0.00.147.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.204.988 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.230.592 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.230.599 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.230.622 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.231.653 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.231.655 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.231.655 I llama_new_context_with_model: graph nodes  = 967
0.00.231.655 I llama_new_context_with_model: graph splits = 2
0.00.231.683 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.231.825 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.231.825 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.320.092 I main: llama threadpool init, n_threads = 4
0.00.320.128 I 
0.00.320.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.320.166 I 
0.00.320.237 I sampler seed: 1234
0.00.320.242 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.320.270 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.320.271 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.320.272 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.160.272 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55468.75 tokens per second)
0.02.160.272 I llama_perf_context_print:        load time =     267.96 ms
0.02.160.273 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.02.160.274 I llama_perf_context_print:        eval time =    1793.28 ms /    63 runs   (   28.46 ms per token,    35.13 tokens per second)
0.02.160.275 I llama_perf_context_print:       total time =    1840.18 ms /    70 tokens
0.02.160.487 I ggml_metal_free: deallocating

real	0m2.485s
user	0m0.152s
sys	0m0.118s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.839 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.982 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.989 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.990 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.991 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.992 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.993 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.994 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.996 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.959 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.982 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.897 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.898 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.899 I llama_model_loader: - type  f32:  194 tensors
0.00.027.899 I llama_model_loader: - type q8_0:   98 tensors
0.00.050.152 I llm_load_vocab: special tokens cache size = 25
0.00.055.926 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.931 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.931 I llm_load_print_meta: arch             = gptneox
0.00.055.932 I llm_load_print_meta: vocab type       = BPE
0.00.055.934 I llm_load_print_meta: n_vocab          = 50304
0.00.055.934 I llm_load_print_meta: n_merges         = 50009
0.00.055.934 I llm_load_print_meta: vocab_only       = 0
0.00.055.936 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.936 I llm_load_print_meta: n_embd           = 2048
0.00.055.937 I llm_load_print_meta: n_layer          = 24
0.00.055.942 I llm_load_print_meta: n_head           = 16
0.00.055.943 I llm_load_print_meta: n_head_kv        = 16
0.00.055.943 I llm_load_print_meta: n_rot            = 32
0.00.055.943 I llm_load_print_meta: n_swa            = 0
0.00.055.943 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.943 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.944 I llm_load_print_meta: n_gqa            = 1
0.00.055.945 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.946 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.947 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.947 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.947 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.947 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.948 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.949 I llm_load_print_meta: n_ff             = 8192
0.00.055.949 I llm_load_print_meta: n_expert         = 0
0.00.055.950 I llm_load_print_meta: n_expert_used    = 0
0.00.055.950 I llm_load_print_meta: causal attn      = 1
0.00.055.950 I llm_load_print_meta: pooling type     = 0
0.00.055.951 I llm_load_print_meta: rope type        = 2
0.00.055.951 I llm_load_print_meta: rope scaling     = linear
0.00.055.952 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.952 I llm_load_print_meta: freq_scale_train = 1
0.00.055.952 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.953 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.953 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.953 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.953 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.953 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.953 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.954 I llm_load_print_meta: model type       = 1.4B
0.00.055.954 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.954 I llm_load_print_meta: model params     = 1.41 B
0.00.055.955 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.955 I llm_load_print_meta: general.name     = 1.4B
0.00.055.955 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.955 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.955 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.956 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.956 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.956 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.956 I llm_load_print_meta: max token length = 1024
0.00.058.382 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.383 I llm_load_tensors: offloading output layer to GPU
0.00.058.383 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.394 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.395 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.059.391 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.392 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.392 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.392 I llama_new_context_with_model: n_batch       = 2048
0.00.059.393 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.393 I llama_new_context_with_model: flash_attn    = 0
0.00.059.394 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.394 I llama_new_context_with_model: freq_scale    = 1
0.00.059.395 I ggml_metal_init: allocating
0.00.059.404 I ggml_metal_init: found device: Apple M4
0.00.059.407 I ggml_metal_init: picking default device: Apple M4
0.00.060.209 I ggml_metal_init: using embedded metal library
0.00.062.753 I ggml_metal_init: GPU name:   Apple M4
0.00.062.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.755 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.756 I ggml_metal_init: simdgroup reduction   = true
0.00.062.756 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.756 I ggml_metal_init: has bfloat            = true
0.00.062.756 I ggml_metal_init: use bfloat            = true
0.00.062.757 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.052 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.095.848 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.856 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.879 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.987 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.989 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.990 I llama_new_context_with_model: graph nodes  = 967
0.00.096.990 I llama_new_context_with_model: graph splits = 2
0.00.097.009 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.152 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.152 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.544.919 I main: llama threadpool init, n_threads = 4
0.01.544.963 I 
0.01.544.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.544.996 I 
0.01.545.219 I sampler seed: 1234
0.01.545.223 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.545.234 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.545.235 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.545.235 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.643.593 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.02.643.593 I llama_perf_context_print:        load time =    1535.07 ms
0.02.643.594 I llama_perf_context_print: prompt eval time =      49.29 ms /     7 tokens (    7.04 ms per token,   142.01 tokens per second)
0.02.643.595 I llama_perf_context_print:        eval time =    1045.97 ms /    63 runs   (   16.60 ms per token,    60.23 tokens per second)
0.02.643.595 I llama_perf_context_print:       total time =    1098.68 ms /    70 tokens
0.02.643.792 I ggml_metal_free: deallocating

real	0m2.663s
user	0m0.116s
sys	0m0.219s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.017.031 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.393 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.399 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.409 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.409 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.413 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.609 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.612 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.612 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.613 I llama_model_loader: - type  f32:  194 tensors
0.00.046.613 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.613 I llama_model_loader: - type q6_K:    1 tensors
0.00.073.569 I llm_load_vocab: special tokens cache size = 25
0.00.082.729 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.733 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.733 I llm_load_print_meta: arch             = gptneox
0.00.082.734 I llm_load_print_meta: vocab type       = BPE
0.00.082.734 I llm_load_print_meta: n_vocab          = 50304
0.00.082.734 I llm_load_print_meta: n_merges         = 50009
0.00.082.734 I llm_load_print_meta: vocab_only       = 0
0.00.082.734 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.735 I llm_load_print_meta: n_embd           = 2048
0.00.082.735 I llm_load_print_meta: n_layer          = 24
0.00.082.739 I llm_load_print_meta: n_head           = 16
0.00.082.740 I llm_load_print_meta: n_head_kv        = 16
0.00.082.740 I llm_load_print_meta: n_rot            = 32
0.00.082.740 I llm_load_print_meta: n_swa            = 0
0.00.082.744 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.744 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.745 I llm_load_print_meta: n_gqa            = 1
0.00.082.747 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.747 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.748 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.749 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.749 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.749 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.750 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.752 I llm_load_print_meta: n_ff             = 8192
0.00.082.752 I llm_load_print_meta: n_expert         = 0
0.00.082.753 I llm_load_print_meta: n_expert_used    = 0
0.00.082.753 I llm_load_print_meta: causal attn      = 1
0.00.082.753 I llm_load_print_meta: pooling type     = 0
0.00.082.753 I llm_load_print_meta: rope type        = 2
0.00.082.753 I llm_load_print_meta: rope scaling     = linear
0.00.082.754 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.755 I llm_load_print_meta: freq_scale_train = 1
0.00.082.755 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.755 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.755 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.756 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.756 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.756 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.756 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.756 I llm_load_print_meta: model type       = 1.4B
0.00.082.757 I llm_load_print_meta: model ftype      = Q4_0
0.00.082.757 I llm_load_print_meta: model params     = 1.41 B
0.00.082.763 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.082.763 I llm_load_print_meta: general.name     = 1.4B
0.00.082.764 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.764 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.764 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.765 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.765 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.082.765 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.766 I llm_load_print_meta: max token length = 1024
0.00.085.748 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.749 I llm_load_tensors: offloading output layer to GPU
0.00.085.749 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.762 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.085.763 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.087.301 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.302 I llama_new_context_with_model: n_ctx         = 2048
0.00.087.302 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.087.303 I llama_new_context_with_model: n_batch       = 2048
0.00.087.303 I llama_new_context_with_model: n_ubatch      = 512
0.00.087.303 I llama_new_context_with_model: flash_attn    = 0
0.00.087.304 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.304 I llama_new_context_with_model: freq_scale    = 1
0.00.087.305 I ggml_metal_init: allocating
0.00.087.309 I ggml_metal_init: found device: Apple M4
0.00.087.312 I ggml_metal_init: picking default device: Apple M4
0.00.088.307 I ggml_metal_init: using embedded metal library
0.00.092.322 I ggml_metal_init: GPU name:   Apple M4
0.00.092.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.325 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.326 I ggml_metal_init: simdgroup reduction   = true
0.00.092.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.326 I ggml_metal_init: has bfloat            = true
0.00.092.326 I ggml_metal_init: use bfloat            = true
0.00.092.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.222 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.131.080 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.131.088 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.131.111 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.132.282 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.132.284 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.132.285 I llama_new_context_with_model: graph nodes  = 967
0.00.132.285 I llama_new_context_with_model: graph splits = 2
0.00.132.304 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.132.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.132.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.042 I main: llama threadpool init, n_threads = 4
0.00.762.093 I 
0.00.762.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.146 I 
0.00.762.466 I sampler seed: 1234
0.00.762.471 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.497 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.500 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.500 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.439.515 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.439.516 I llama_perf_context_print:        load time =     745.00 ms
0.01.439.517 I llama_perf_context_print: prompt eval time =      40.27 ms /     7 tokens (    5.75 ms per token,   173.85 tokens per second)
0.01.439.517 I llama_perf_context_print:        eval time =     633.80 ms /    63 runs   (   10.06 ms per token,    99.40 tokens per second)
0.01.439.518 I llama_perf_context_print:       total time =     677.48 ms /    70 tokens
0.01.439.723 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.132s
sys	0m0.181s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.710 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.669 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.674 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.679 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.681 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.682 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.684 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.702 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.751 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.752 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.753 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.754 I llama_model_loader: - type  f32:  194 tensors
0.00.036.754 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.754 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.459 I llm_load_vocab: special tokens cache size = 25
0.00.069.007 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.010 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.010 I llm_load_print_meta: arch             = gptneox
0.00.069.011 I llm_load_print_meta: vocab type       = BPE
0.00.069.011 I llm_load_print_meta: n_vocab          = 50304
0.00.069.011 I llm_load_print_meta: n_merges         = 50009
0.00.069.011 I llm_load_print_meta: vocab_only       = 0
0.00.069.012 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.012 I llm_load_print_meta: n_embd           = 2048
0.00.069.012 I llm_load_print_meta: n_layer          = 24
0.00.069.015 I llm_load_print_meta: n_head           = 16
0.00.069.016 I llm_load_print_meta: n_head_kv        = 16
0.00.069.016 I llm_load_print_meta: n_rot            = 32
0.00.069.016 I llm_load_print_meta: n_swa            = 0
0.00.069.016 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.017 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.020 I llm_load_print_meta: n_gqa            = 1
0.00.069.021 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.021 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.022 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.022 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.023 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.023 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.023 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.024 I llm_load_print_meta: n_ff             = 8192
0.00.069.024 I llm_load_print_meta: n_expert         = 0
0.00.069.024 I llm_load_print_meta: n_expert_used    = 0
0.00.069.024 I llm_load_print_meta: causal attn      = 1
0.00.069.024 I llm_load_print_meta: pooling type     = 0
0.00.069.024 I llm_load_print_meta: rope type        = 2
0.00.069.025 I llm_load_print_meta: rope scaling     = linear
0.00.069.025 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.025 I llm_load_print_meta: freq_scale_train = 1
0.00.069.025 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.026 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.026 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.026 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.026 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.026 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.026 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.026 I llm_load_print_meta: model type       = 1.4B
0.00.069.027 I llm_load_print_meta: model ftype      = Q4_1
0.00.069.027 I llm_load_print_meta: model params     = 1.41 B
0.00.069.028 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.069.028 I llm_load_print_meta: general.name     = 1.4B
0.00.069.028 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.028 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.028 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.029 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.029 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.029 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.029 I llm_load_print_meta: max token length = 1024
0.00.071.289 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.289 I llm_load_tensors: offloading output layer to GPU
0.00.071.290 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.300 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.071.302 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.072.367 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.368 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.368 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.368 I llama_new_context_with_model: n_batch       = 2048
0.00.072.368 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.369 I llama_new_context_with_model: flash_attn    = 0
0.00.072.369 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.369 I llama_new_context_with_model: freq_scale    = 1
0.00.072.370 I ggml_metal_init: allocating
0.00.072.373 I ggml_metal_init: found device: Apple M4
0.00.072.375 I ggml_metal_init: picking default device: Apple M4
0.00.073.061 I ggml_metal_init: using embedded metal library
0.00.075.938 I ggml_metal_init: GPU name:   Apple M4
0.00.075.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.941 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.941 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.941 I ggml_metal_init: simdgroup reduction   = true
0.00.075.941 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.942 I ggml_metal_init: has bfloat            = true
0.00.075.942 I ggml_metal_init: use bfloat            = true
0.00.075.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.943 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.136 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.108.575 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.586 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.609 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.645 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.647 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.647 I llama_new_context_with_model: graph nodes  = 967
0.00.109.648 I llama_new_context_with_model: graph splits = 2
0.00.109.663 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.806 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.541 I main: llama threadpool init, n_threads = 4
0.00.812.582 I 
0.00.812.620 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.812.620 I 
0.00.812.842 I sampler seed: 1234
0.00.812.849 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.812.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.812.904 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.812.904 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.539.603 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62555.07 tokens per second)
0.01.539.604 I llama_perf_context_print:        load time =     803.83 ms
0.01.539.604 I llama_perf_context_print: prompt eval time =      39.56 ms /     7 tokens (    5.65 ms per token,   176.93 tokens per second)
0.01.539.605 I llama_perf_context_print:        eval time =     684.24 ms /    63 runs   (   10.86 ms per token,    92.07 tokens per second)
0.01.539.605 I llama_perf_context_print:       total time =     727.07 ms /    70 tokens
0.01.539.816 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.117s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.010.050 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.316 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.324 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.325 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.326 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.326 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.326 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.330 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.330 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.330 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.331 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.333 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.333 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.993 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.073 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.047 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.049 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.049 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.049 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.050 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.050 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.035.050 I llama_model_loader: - type  f32:  194 tensors
0.00.035.051 I llama_model_loader: - type q5_0:   97 tensors
0.00.035.051 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.880 I llm_load_vocab: special tokens cache size = 25
0.00.063.901 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.904 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.904 I llm_load_print_meta: arch             = gptneox
0.00.063.904 I llm_load_print_meta: vocab type       = BPE
0.00.063.904 I llm_load_print_meta: n_vocab          = 50304
0.00.063.905 I llm_load_print_meta: n_merges         = 50009
0.00.063.905 I llm_load_print_meta: vocab_only       = 0
0.00.063.905 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.905 I llm_load_print_meta: n_embd           = 2048
0.00.063.905 I llm_load_print_meta: n_layer          = 24
0.00.063.908 I llm_load_print_meta: n_head           = 16
0.00.063.908 I llm_load_print_meta: n_head_kv        = 16
0.00.063.909 I llm_load_print_meta: n_rot            = 32
0.00.063.909 I llm_load_print_meta: n_swa            = 0
0.00.063.909 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.909 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.910 I llm_load_print_meta: n_gqa            = 1
0.00.063.911 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.911 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.913 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.914 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.914 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.914 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.914 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.915 I llm_load_print_meta: n_ff             = 8192
0.00.063.915 I llm_load_print_meta: n_expert         = 0
0.00.063.915 I llm_load_print_meta: n_expert_used    = 0
0.00.063.915 I llm_load_print_meta: causal attn      = 1
0.00.063.915 I llm_load_print_meta: pooling type     = 0
0.00.063.915 I llm_load_print_meta: rope type        = 2
0.00.063.916 I llm_load_print_meta: rope scaling     = linear
0.00.063.916 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.916 I llm_load_print_meta: freq_scale_train = 1
0.00.063.916 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.916 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.916 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.917 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.917 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.917 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.918 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.918 I llm_load_print_meta: model type       = 1.4B
0.00.063.918 I llm_load_print_meta: model ftype      = Q5_0
0.00.063.918 I llm_load_print_meta: model params     = 1.41 B
0.00.063.919 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.063.919 I llm_load_print_meta: general.name     = 1.4B
0.00.063.919 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.920 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.921 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.921 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.921 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.921 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.921 I llm_load_print_meta: max token length = 1024
0.00.065.977 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.977 I llm_load_tensors: offloading output layer to GPU
0.00.065.977 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.988 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.065.989 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.066.904 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.905 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.905 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.906 I llama_new_context_with_model: n_batch       = 2048
0.00.066.906 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.906 I llama_new_context_with_model: flash_attn    = 0
0.00.066.906 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.907 I llama_new_context_with_model: freq_scale    = 1
0.00.066.907 I ggml_metal_init: allocating
0.00.066.910 I ggml_metal_init: found device: Apple M4
0.00.066.912 I ggml_metal_init: picking default device: Apple M4
0.00.067.512 I ggml_metal_init: using embedded metal library
0.00.070.039 I ggml_metal_init: GPU name:   Apple M4
0.00.070.041 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.041 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.041 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.041 I ggml_metal_init: simdgroup reduction   = true
0.00.070.042 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.042 I ggml_metal_init: has bfloat            = true
0.00.070.042 I ggml_metal_init: use bfloat            = true
0.00.070.042 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.043 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.786 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.102 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.111 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.135 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.280 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.282 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.282 I llama_new_context_with_model: graph nodes  = 967
0.00.103.283 I llama_new_context_with_model: graph splits = 2
0.00.103.299 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.454 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.286 I main: llama threadpool init, n_threads = 4
0.00.781.324 I 
0.00.781.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.361 I 
0.00.781.580 I sampler seed: 1234
0.00.781.586 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.597 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.598 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.598 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.562.628 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53625.38 tokens per second)
0.01.562.628 I llama_perf_context_print:        load time =     771.23 ms
0.01.562.629 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.35 tokens per second)
0.01.562.630 I llama_perf_context_print:        eval time =     734.90 ms /    63 runs   (   11.67 ms per token,    85.73 tokens per second)
0.01.562.630 I llama_perf_context_print:       total time =     781.34 ms /    70 tokens
0.01.562.802 I ggml_metal_free: deallocating

real	0m1.583s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.815 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.996 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.998 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.004 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.005 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.005 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.008 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.008 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.009 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.012 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.014 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.014 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.970 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.892 I llama_model_loader: - type  f32:  194 tensors
0.00.023.892 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.892 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.994 I llm_load_vocab: special tokens cache size = 25
0.00.050.837 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.839 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.840 I llm_load_print_meta: arch             = gptneox
0.00.050.840 I llm_load_print_meta: vocab type       = BPE
0.00.050.840 I llm_load_print_meta: n_vocab          = 50304
0.00.050.840 I llm_load_print_meta: n_merges         = 50009
0.00.050.841 I llm_load_print_meta: vocab_only       = 0
0.00.050.841 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.841 I llm_load_print_meta: n_embd           = 2048
0.00.050.841 I llm_load_print_meta: n_layer          = 24
0.00.050.844 I llm_load_print_meta: n_head           = 16
0.00.050.845 I llm_load_print_meta: n_head_kv        = 16
0.00.050.845 I llm_load_print_meta: n_rot            = 32
0.00.050.845 I llm_load_print_meta: n_swa            = 0
0.00.050.846 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.846 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.846 I llm_load_print_meta: n_gqa            = 1
0.00.050.847 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.848 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.848 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.849 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.849 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.849 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.849 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.850 I llm_load_print_meta: n_ff             = 8192
0.00.050.850 I llm_load_print_meta: n_expert         = 0
0.00.050.850 I llm_load_print_meta: n_expert_used    = 0
0.00.050.852 I llm_load_print_meta: causal attn      = 1
0.00.050.852 I llm_load_print_meta: pooling type     = 0
0.00.050.852 I llm_load_print_meta: rope type        = 2
0.00.050.853 I llm_load_print_meta: rope scaling     = linear
0.00.050.853 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.853 I llm_load_print_meta: freq_scale_train = 1
0.00.050.854 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.854 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.854 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.854 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.854 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.854 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.854 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.856 I llm_load_print_meta: model type       = 1.4B
0.00.050.856 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.857 I llm_load_print_meta: model params     = 1.41 B
0.00.050.857 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.857 I llm_load_print_meta: general.name     = 1.4B
0.00.050.858 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.858 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.858 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.858 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.858 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.864 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.867 I llm_load_print_meta: max token length = 1024
0.00.052.885 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.885 I llm_load_tensors: offloading output layer to GPU
0.00.052.886 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.896 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.897 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.870 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.871 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.872 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.872 I llama_new_context_with_model: n_batch       = 2048
0.00.053.872 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.872 I llama_new_context_with_model: flash_attn    = 0
0.00.053.873 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.873 I llama_new_context_with_model: freq_scale    = 1
0.00.053.874 I ggml_metal_init: allocating
0.00.053.877 I ggml_metal_init: found device: Apple M4
0.00.053.879 I ggml_metal_init: picking default device: Apple M4
0.00.054.468 I ggml_metal_init: using embedded metal library
0.00.056.862 I ggml_metal_init: GPU name:   Apple M4
0.00.056.864 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.864 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.864 I ggml_metal_init: simdgroup reduction   = true
0.00.056.865 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.865 I ggml_metal_init: has bfloat            = true
0.00.056.865 I ggml_metal_init: use bfloat            = true
0.00.056.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.866 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.673 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.924 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.930 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.948 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.015 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.016 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.016 I llama_new_context_with_model: graph nodes  = 967
0.00.087.017 I llama_new_context_with_model: graph splits = 2
0.00.087.034 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.626 I main: llama threadpool init, n_threads = 4
0.00.703.668 I 
0.00.703.724 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.726 I 
0.00.703.951 I sampler seed: 1234
0.00.703.957 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.989 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.992 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.992 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.550.259 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.01.550.260 I llama_perf_context_print:        load time =     694.81 ms
0.01.550.260 I llama_perf_context_print: prompt eval time =      46.22 ms /     7 tokens (    6.60 ms per token,   151.44 tokens per second)
0.01.550.261 I llama_perf_context_print:        eval time =     797.20 ms /    63 runs   (   12.65 ms per token,    79.03 tokens per second)
0.01.550.261 I llama_perf_context_print:       total time =     846.64 ms /    70 tokens
0.01.550.489 I ggml_metal_free: deallocating

real	0m1.568s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.753 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.339 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.345 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.351 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.352 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.354 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.865 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.865 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.866 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.866 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.867 I llama_model_loader: - type  f32:  194 tensors
0.00.023.867 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.867 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.868 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.812 I llm_load_vocab: special tokens cache size = 25
0.00.050.831 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.837 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.837 I llm_load_print_meta: arch             = gptneox
0.00.050.838 I llm_load_print_meta: vocab type       = BPE
0.00.050.838 I llm_load_print_meta: n_vocab          = 50304
0.00.050.838 I llm_load_print_meta: n_merges         = 50009
0.00.050.838 I llm_load_print_meta: vocab_only       = 0
0.00.050.838 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.838 I llm_load_print_meta: n_embd           = 2048
0.00.050.839 I llm_load_print_meta: n_layer          = 24
0.00.050.843 I llm_load_print_meta: n_head           = 16
0.00.050.844 I llm_load_print_meta: n_head_kv        = 16
0.00.050.844 I llm_load_print_meta: n_rot            = 32
0.00.050.844 I llm_load_print_meta: n_swa            = 0
0.00.050.844 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.844 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.845 I llm_load_print_meta: n_gqa            = 1
0.00.050.846 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.846 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.847 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.847 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.847 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.848 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.848 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.848 I llm_load_print_meta: n_ff             = 8192
0.00.050.848 I llm_load_print_meta: n_expert         = 0
0.00.050.849 I llm_load_print_meta: n_expert_used    = 0
0.00.050.849 I llm_load_print_meta: causal attn      = 1
0.00.050.849 I llm_load_print_meta: pooling type     = 0
0.00.050.849 I llm_load_print_meta: rope type        = 2
0.00.050.849 I llm_load_print_meta: rope scaling     = linear
0.00.050.849 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.850 I llm_load_print_meta: freq_scale_train = 1
0.00.050.850 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.850 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.850 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.850 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.850 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.850 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.850 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.851 I llm_load_print_meta: model type       = 1.4B
0.00.050.851 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.851 I llm_load_print_meta: model params     = 1.41 B
0.00.050.852 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.852 I llm_load_print_meta: general.name     = 1.4B
0.00.050.852 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.852 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.852 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.853 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.853 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.853 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.853 I llm_load_print_meta: max token length = 1024
0.00.052.766 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.766 I llm_load_tensors: offloading output layer to GPU
0.00.052.767 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.777 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.778 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.778 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.778 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.779 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.779 I llama_new_context_with_model: n_batch       = 2048
0.00.053.779 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.779 I llama_new_context_with_model: flash_attn    = 0
0.00.053.780 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.780 I llama_new_context_with_model: freq_scale    = 1
0.00.053.781 I ggml_metal_init: allocating
0.00.053.784 I ggml_metal_init: found device: Apple M4
0.00.053.785 I ggml_metal_init: picking default device: Apple M4
0.00.054.406 I ggml_metal_init: using embedded metal library
0.00.056.898 I ggml_metal_init: GPU name:   Apple M4
0.00.056.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.900 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.901 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.901 I ggml_metal_init: simdgroup reduction   = true
0.00.056.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.901 I ggml_metal_init: has bfloat            = true
0.00.056.901 I ggml_metal_init: use bfloat            = true
0.00.056.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.106 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.302 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.323 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.347 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.349 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.349 I llama_new_context_with_model: graph nodes  = 967
0.00.086.349 I llama_new_context_with_model: graph splits = 2
0.00.086.367 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.433.036 I main: llama threadpool init, n_threads = 4
0.00.433.083 I 
0.00.433.122 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.433.123 I 
0.00.433.365 I sampler seed: 1234
0.00.433.371 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.433.382 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.433.382 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.433.382 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.110.178 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.01.110.179 I llama_perf_context_print:        load time =     423.28 ms
0.01.110.180 I llama_perf_context_print: prompt eval time =      36.02 ms /     7 tokens (    5.15 ms per token,   194.34 tokens per second)
0.01.110.181 I llama_perf_context_print:        eval time =     637.91 ms /    63 runs   (   10.13 ms per token,    98.76 tokens per second)
0.01.110.181 I llama_perf_context_print:       total time =     677.15 ms /    70 tokens
0.01.110.425 I ggml_metal_free: deallocating

real	0m1.131s
user	0m0.109s
sys	0m0.093s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.906 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.569 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.575 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.582 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.585 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.354 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.354 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.355 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.355 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.356 I llama_model_loader: - type  f32:  194 tensors
0.00.024.356 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.356 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.356 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.356 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.593 I llm_load_vocab: special tokens cache size = 25
0.00.050.688 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.691 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.691 I llm_load_print_meta: arch             = gptneox
0.00.050.691 I llm_load_print_meta: vocab type       = BPE
0.00.050.692 I llm_load_print_meta: n_vocab          = 50304
0.00.050.692 I llm_load_print_meta: n_merges         = 50009
0.00.050.692 I llm_load_print_meta: vocab_only       = 0
0.00.050.692 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.692 I llm_load_print_meta: n_embd           = 2048
0.00.050.692 I llm_load_print_meta: n_layer          = 24
0.00.050.695 I llm_load_print_meta: n_head           = 16
0.00.050.696 I llm_load_print_meta: n_head_kv        = 16
0.00.050.696 I llm_load_print_meta: n_rot            = 32
0.00.050.696 I llm_load_print_meta: n_swa            = 0
0.00.050.696 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.696 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.697 I llm_load_print_meta: n_gqa            = 1
0.00.050.698 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.699 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.699 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.700 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.700 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.700 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.700 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.701 I llm_load_print_meta: n_ff             = 8192
0.00.050.702 I llm_load_print_meta: n_expert         = 0
0.00.050.704 I llm_load_print_meta: n_expert_used    = 0
0.00.050.704 I llm_load_print_meta: causal attn      = 1
0.00.050.704 I llm_load_print_meta: pooling type     = 0
0.00.050.704 I llm_load_print_meta: rope type        = 2
0.00.050.705 I llm_load_print_meta: rope scaling     = linear
0.00.050.705 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.705 I llm_load_print_meta: freq_scale_train = 1
0.00.050.706 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.706 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.706 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.706 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.706 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.707 I llm_load_print_meta: model type       = 1.4B
0.00.050.707 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.707 I llm_load_print_meta: model params     = 1.41 B
0.00.050.708 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.708 I llm_load_print_meta: general.name     = 1.4B
0.00.050.708 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.709 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.709 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.709 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.709 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.709 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.710 I llm_load_print_meta: max token length = 1024
0.00.052.289 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.289 I llm_load_tensors: offloading output layer to GPU
0.00.052.290 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.300 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.301 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.132 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.133 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.133 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.133 I llama_new_context_with_model: n_batch       = 2048
0.00.053.133 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.134 I llama_new_context_with_model: flash_attn    = 0
0.00.053.134 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.134 I llama_new_context_with_model: freq_scale    = 1
0.00.053.135 I ggml_metal_init: allocating
0.00.053.141 I ggml_metal_init: found device: Apple M4
0.00.053.145 I ggml_metal_init: picking default device: Apple M4
0.00.053.729 I ggml_metal_init: using embedded metal library
0.00.056.035 I ggml_metal_init: GPU name:   Apple M4
0.00.056.038 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.038 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.038 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.039 I ggml_metal_init: simdgroup reduction   = true
0.00.056.039 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.039 I ggml_metal_init: has bfloat            = true
0.00.056.039 I ggml_metal_init: use bfloat            = true
0.00.056.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.040 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.103 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.109 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.127 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.104 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.106 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.106 I llama_new_context_with_model: graph nodes  = 967
0.00.086.106 I llama_new_context_with_model: graph splits = 2
0.00.086.121 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.250 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.439 I main: llama threadpool init, n_threads = 4
0.00.539.477 I 
0.00.539.511 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.539.511 I 
0.00.539.736 I sampler seed: 1234
0.00.539.741 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.539.762 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.539.762 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.539.762 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.283.136 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.283.136 I llama_perf_context_print:        load time =     530.53 ms
0.01.283.137 I llama_perf_context_print: prompt eval time =      40.56 ms /     7 tokens (    5.79 ms per token,   172.58 tokens per second)
0.01.283.138 I llama_perf_context_print:        eval time =     699.76 ms /    63 runs   (   11.11 ms per token,    90.03 tokens per second)
0.01.283.138 I llama_perf_context_print:       total time =     743.70 ms /    70 tokens
0.01.283.292 I ggml_metal_free: deallocating

real	0m1.300s
user	0m0.109s
sys	0m0.129s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.711 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.432 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.437 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.438 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.439 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.439 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.440 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.441 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.441 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.442 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.442 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.442 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.443 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.446 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.446 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.446 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.200 I llama_model_loader: - type  f32:  194 tensors
0.00.024.200 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.200 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.200 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.399 I llm_load_vocab: special tokens cache size = 25
0.00.050.101 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.103 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.104 I llm_load_print_meta: arch             = gptneox
0.00.050.104 I llm_load_print_meta: vocab type       = BPE
0.00.050.104 I llm_load_print_meta: n_vocab          = 50304
0.00.050.105 I llm_load_print_meta: n_merges         = 50009
0.00.050.105 I llm_load_print_meta: vocab_only       = 0
0.00.050.105 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.105 I llm_load_print_meta: n_embd           = 2048
0.00.050.105 I llm_load_print_meta: n_layer          = 24
0.00.050.108 I llm_load_print_meta: n_head           = 16
0.00.050.109 I llm_load_print_meta: n_head_kv        = 16
0.00.050.109 I llm_load_print_meta: n_rot            = 32
0.00.050.109 I llm_load_print_meta: n_swa            = 0
0.00.050.109 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.109 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.110 I llm_load_print_meta: n_gqa            = 1
0.00.050.111 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.112 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.112 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.112 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.113 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.113 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.113 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.114 I llm_load_print_meta: n_ff             = 8192
0.00.050.114 I llm_load_print_meta: n_expert         = 0
0.00.050.116 I llm_load_print_meta: n_expert_used    = 0
0.00.050.117 I llm_load_print_meta: causal attn      = 1
0.00.050.117 I llm_load_print_meta: pooling type     = 0
0.00.050.117 I llm_load_print_meta: rope type        = 2
0.00.050.118 I llm_load_print_meta: rope scaling     = linear
0.00.050.118 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.118 I llm_load_print_meta: freq_scale_train = 1
0.00.050.119 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.119 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.119 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.119 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.119 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.121 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.121 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.121 I llm_load_print_meta: model type       = 1.4B
0.00.050.122 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.122 I llm_load_print_meta: model params     = 1.41 B
0.00.050.123 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.123 I llm_load_print_meta: general.name     = 1.4B
0.00.050.123 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.124 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.127 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.128 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.128 I llm_load_print_meta: max token length = 1024
0.00.052.132 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.132 I llm_load_tensors: offloading output layer to GPU
0.00.052.133 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.143 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.144 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.105 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.106 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.106 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.106 I llama_new_context_with_model: n_batch       = 2048
0.00.053.107 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.107 I llama_new_context_with_model: flash_attn    = 0
0.00.053.107 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.108 I llama_new_context_with_model: freq_scale    = 1
0.00.053.108 I ggml_metal_init: allocating
0.00.053.111 I ggml_metal_init: found device: Apple M4
0.00.053.113 I ggml_metal_init: picking default device: Apple M4
0.00.053.710 I ggml_metal_init: using embedded metal library
0.00.055.992 I ggml_metal_init: GPU name:   Apple M4
0.00.055.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.994 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.996 I ggml_metal_init: simdgroup reduction   = true
0.00.055.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.997 I ggml_metal_init: has bfloat            = true
0.00.055.997 I ggml_metal_init: use bfloat            = true
0.00.055.997 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.998 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.702 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.586 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.591 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.609 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.586 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.587 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.587 I llama_new_context_with_model: graph nodes  = 967
0.00.085.588 I llama_new_context_with_model: graph splits = 2
0.00.085.598 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.727 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.431 I main: llama threadpool init, n_threads = 4
0.00.608.473 I 
0.00.608.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.511 I 
0.00.608.745 I sampler seed: 1234
0.00.608.749 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.608.761 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.608.761 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.608.761 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.368.913 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.368.914 I llama_perf_context_print:        load time =     599.72 ms
0.01.368.915 I llama_perf_context_print: prompt eval time =      47.15 ms /     7 tokens (    6.74 ms per token,   148.46 tokens per second)
0.01.368.916 I llama_perf_context_print:        eval time =     709.87 ms /    63 runs   (   11.27 ms per token,    88.75 tokens per second)
0.01.368.916 I llama_perf_context_print:       total time =     760.49 ms /    70 tokens
0.01.369.109 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.109s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.011.080 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.738 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.745 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.381 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.384 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.384 I llama_model_loader: - type  f32:  194 tensors
0.00.026.385 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.385 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.701 I llm_load_vocab: special tokens cache size = 25
0.00.052.680 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.683 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.683 I llm_load_print_meta: arch             = gptneox
0.00.052.683 I llm_load_print_meta: vocab type       = BPE
0.00.052.684 I llm_load_print_meta: n_vocab          = 50304
0.00.052.684 I llm_load_print_meta: n_merges         = 50009
0.00.052.684 I llm_load_print_meta: vocab_only       = 0
0.00.052.684 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.684 I llm_load_print_meta: n_embd           = 2048
0.00.052.685 I llm_load_print_meta: n_layer          = 24
0.00.052.687 I llm_load_print_meta: n_head           = 16
0.00.052.688 I llm_load_print_meta: n_head_kv        = 16
0.00.052.688 I llm_load_print_meta: n_rot            = 32
0.00.052.688 I llm_load_print_meta: n_swa            = 0
0.00.052.688 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.688 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.689 I llm_load_print_meta: n_gqa            = 1
0.00.052.690 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.691 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.691 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.692 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.692 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.694 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.694 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.695 I llm_load_print_meta: n_ff             = 8192
0.00.052.695 I llm_load_print_meta: n_expert         = 0
0.00.052.695 I llm_load_print_meta: n_expert_used    = 0
0.00.052.695 I llm_load_print_meta: causal attn      = 1
0.00.052.697 I llm_load_print_meta: pooling type     = 0
0.00.052.697 I llm_load_print_meta: rope type        = 2
0.00.052.697 I llm_load_print_meta: rope scaling     = linear
0.00.052.697 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.699 I llm_load_print_meta: freq_scale_train = 1
0.00.052.699 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.699 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.699 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.700 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.700 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.700 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.700 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.700 I llm_load_print_meta: model type       = 1.4B
0.00.052.701 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.701 I llm_load_print_meta: model params     = 1.41 B
0.00.052.706 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.706 I llm_load_print_meta: general.name     = 1.4B
0.00.052.706 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.706 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.707 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.708 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.708 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.708 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.708 I llm_load_print_meta: max token length = 1024
0.00.054.695 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.696 I llm_load_tensors: offloading output layer to GPU
0.00.054.696 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.706 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.707 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.600 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.600 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.601 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.601 I llama_new_context_with_model: n_batch       = 2048
0.00.055.601 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.601 I llama_new_context_with_model: flash_attn    = 0
0.00.055.602 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.602 I llama_new_context_with_model: freq_scale    = 1
0.00.055.602 I ggml_metal_init: allocating
0.00.055.608 I ggml_metal_init: found device: Apple M4
0.00.055.611 I ggml_metal_init: picking default device: Apple M4
0.00.056.215 I ggml_metal_init: using embedded metal library
0.00.058.539 I ggml_metal_init: GPU name:   Apple M4
0.00.058.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.542 I ggml_metal_init: simdgroup reduction   = true
0.00.058.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.542 I ggml_metal_init: has bfloat            = true
0.00.058.542 I ggml_metal_init: use bfloat            = true
0.00.058.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.188 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.606 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.614 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.632 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.605 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.606 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.606 I llama_new_context_with_model: graph nodes  = 967
0.00.088.607 I llama_new_context_with_model: graph splits = 2
0.00.088.624 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.740 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.740 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.958 I main: llama threadpool init, n_threads = 4
0.00.709.997 I 
0.00.710.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.029 I 
0.00.710.251 I sampler seed: 1234
0.00.710.256 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.267 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.268 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.269 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.555.049 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.555.050 I llama_perf_context_print:        load time =     698.87 ms
0.01.555.051 I llama_perf_context_print: prompt eval time =      51.52 ms /     7 tokens (    7.36 ms per token,   135.86 tokens per second)
0.01.555.051 I llama_perf_context_print:        eval time =     790.23 ms /    63 runs   (   12.54 ms per token,    79.72 tokens per second)
0.01.555.052 I llama_perf_context_print:       total time =     845.09 ms /    70 tokens
0.01.555.244 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.672 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.038 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.041 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.042 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.042 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.044 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.044 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.947 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.749 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.749 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.750 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.750 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.750 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.751 I llama_model_loader: - type  f32:  194 tensors
0.00.024.751 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.788 I llm_load_vocab: special tokens cache size = 25
0.00.051.670 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.673 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.673 I llm_load_print_meta: arch             = gptneox
0.00.051.673 I llm_load_print_meta: vocab type       = BPE
0.00.051.674 I llm_load_print_meta: n_vocab          = 50304
0.00.051.674 I llm_load_print_meta: n_merges         = 50009
0.00.051.674 I llm_load_print_meta: vocab_only       = 0
0.00.051.674 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.675 I llm_load_print_meta: n_embd           = 2048
0.00.051.675 I llm_load_print_meta: n_layer          = 24
0.00.051.677 I llm_load_print_meta: n_head           = 16
0.00.051.678 I llm_load_print_meta: n_head_kv        = 16
0.00.051.678 I llm_load_print_meta: n_rot            = 32
0.00.051.679 I llm_load_print_meta: n_swa            = 0
0.00.051.679 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.679 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.680 I llm_load_print_meta: n_gqa            = 1
0.00.051.682 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.683 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.684 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.684 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.684 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.684 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.685 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.685 I llm_load_print_meta: n_ff             = 8192
0.00.051.685 I llm_load_print_meta: n_expert         = 0
0.00.051.686 I llm_load_print_meta: n_expert_used    = 0
0.00.051.686 I llm_load_print_meta: causal attn      = 1
0.00.051.686 I llm_load_print_meta: pooling type     = 0
0.00.051.686 I llm_load_print_meta: rope type        = 2
0.00.051.686 I llm_load_print_meta: rope scaling     = linear
0.00.051.687 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.687 I llm_load_print_meta: freq_scale_train = 1
0.00.051.688 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.688 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.688 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.688 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.688 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.688 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.689 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.689 I llm_load_print_meta: model type       = 1.4B
0.00.051.689 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.690 I llm_load_print_meta: model params     = 1.41 B
0.00.051.690 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.690 I llm_load_print_meta: general.name     = 1.4B
0.00.051.691 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.691 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.691 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.691 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.692 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.692 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.692 I llm_load_print_meta: max token length = 1024
0.00.053.776 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.776 I llm_load_tensors: offloading output layer to GPU
0.00.053.776 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.787 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.788 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.732 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.732 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.733 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.733 I llama_new_context_with_model: n_batch       = 2048
0.00.054.733 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.733 I llama_new_context_with_model: flash_attn    = 0
0.00.054.734 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.734 I llama_new_context_with_model: freq_scale    = 1
0.00.054.734 I ggml_metal_init: allocating
0.00.054.738 I ggml_metal_init: found device: Apple M4
0.00.054.740 I ggml_metal_init: picking default device: Apple M4
0.00.055.325 I ggml_metal_init: using embedded metal library
0.00.057.683 I ggml_metal_init: GPU name:   Apple M4
0.00.057.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.685 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.685 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.686 I ggml_metal_init: simdgroup reduction   = true
0.00.057.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.686 I ggml_metal_init: has bfloat            = true
0.00.057.686 I ggml_metal_init: use bfloat            = true
0.00.057.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.863 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.381 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.387 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.407 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.474 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.475 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.476 I llama_new_context_with_model: graph nodes  = 967
0.00.089.476 I llama_new_context_with_model: graph splits = 2
0.00.089.491 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.633 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.634 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.819 I main: llama threadpool init, n_threads = 4
0.00.786.865 I 
0.00.786.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.920 I 
0.00.787.155 I sampler seed: 1234
0.00.787.159 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.787.200 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.787.203 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.787.203 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.668.517 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.01.668.518 I llama_perf_context_print:        load time =     778.14 ms
0.01.668.518 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.51 tokens per second)
0.01.668.519 I llama_perf_context_print:        eval time =     823.93 ms /    63 runs   (   13.08 ms per token,    76.46 tokens per second)
0.01.668.520 I llama_perf_context_print:       total time =     881.70 ms /    70 tokens
0.01.668.695 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.110s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.793 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.372 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.987 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.993 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.995 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.997 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.999 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.999 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.002 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.004 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.006 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.285 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.286 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.287 I llama_model_loader: - type  f32:  194 tensors
0.00.054.287 I llama_model_loader: - type  f16:   98 tensors
0.00.083.793 I llm_load_vocab: special tokens cache size = 25
0.00.090.167 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.170 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.170 I llm_load_print_meta: arch             = gptneox
0.00.090.171 I llm_load_print_meta: vocab type       = BPE
0.00.090.171 I llm_load_print_meta: n_vocab          = 50304
0.00.090.171 I llm_load_print_meta: n_merges         = 50009
0.00.090.171 I llm_load_print_meta: vocab_only       = 0
0.00.090.171 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.171 I llm_load_print_meta: n_embd           = 2048
0.00.090.172 I llm_load_print_meta: n_layer          = 24
0.00.090.174 I llm_load_print_meta: n_head           = 16
0.00.090.175 I llm_load_print_meta: n_head_kv        = 16
0.00.090.175 I llm_load_print_meta: n_rot            = 32
0.00.090.175 I llm_load_print_meta: n_swa            = 0
0.00.090.175 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.175 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.176 I llm_load_print_meta: n_gqa            = 1
0.00.090.177 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.178 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.178 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.178 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.179 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.181 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.181 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.181 I llm_load_print_meta: n_ff             = 8192
0.00.090.181 I llm_load_print_meta: n_expert         = 0
0.00.090.182 I llm_load_print_meta: n_expert_used    = 0
0.00.090.182 I llm_load_print_meta: causal attn      = 1
0.00.090.182 I llm_load_print_meta: pooling type     = 0
0.00.090.183 I llm_load_print_meta: rope type        = 2
0.00.090.183 I llm_load_print_meta: rope scaling     = linear
0.00.090.184 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.184 I llm_load_print_meta: freq_scale_train = 1
0.00.090.184 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.184 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.184 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.185 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.185 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.185 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.185 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.185 I llm_load_print_meta: model type       = 1.4B
0.00.090.186 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.190 I llm_load_print_meta: model params     = 1.41 B
0.00.090.190 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.190 I llm_load_print_meta: general.name     = 1.4B
0.00.090.192 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.192 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.192 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.192 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.192 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.193 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.193 I llm_load_print_meta: max token length = 1024
0.00.092.802 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.802 I llm_load_tensors: offloading output layer to GPU
0.00.092.803 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.813 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.814 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.787 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.787 I llama_new_context_with_model: n_ctx         = 128
0.00.093.788 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.788 I llama_new_context_with_model: n_batch       = 128
0.00.093.788 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.788 I llama_new_context_with_model: flash_attn    = 0
0.00.093.789 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.789 I llama_new_context_with_model: freq_scale    = 1
0.00.093.789 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.790 I ggml_metal_init: allocating
0.00.093.798 I ggml_metal_init: found device: Apple M4
0.00.093.800 I ggml_metal_init: picking default device: Apple M4
0.00.094.411 I ggml_metal_init: using embedded metal library
0.00.096.996 I ggml_metal_init: GPU name:   Apple M4
0.00.096.998 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.999 I ggml_metal_init: simdgroup reduction   = true
0.00.096.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.999 I ggml_metal_init: has bfloat            = true
0.00.096.999 I ggml_metal_init: use bfloat            = true
0.00.096.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.315 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.107.609 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.612 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.625 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.491 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.492 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.492 I llama_new_context_with_model: graph nodes  = 967
0.00.108.493 I llama_new_context_with_model: graph splits = 2
0.00.108.505 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.403.830 I 
0.01.403.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.403.911 I perplexity: tokenizing the input ..
0.01.415.905 I perplexity: tokenization took 11.985 ms
0.01.415.915 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.535.430 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.537.050 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.537.062 I llama_perf_context_print:        load time =    1379.44 ms
0.01.537.064 I llama_perf_context_print: prompt eval time =     119.14 ms /   128 tokens (    0.93 ms per token,  1074.38 tokens per second)
0.01.537.066 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.537.066 I llama_perf_context_print:       total time =     133.24 ms /   129 tokens
0.01.537.610 I ggml_metal_free: deallocating

real	0m1.728s
user	0m0.121s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.132 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.084 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.136 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.830 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.887 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.888 I llama_model_loader: - type  f32:  194 tensors
0.00.034.889 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.827 I llm_load_vocab: special tokens cache size = 25
0.00.067.070 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.073 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.073 I llm_load_print_meta: arch             = gptneox
0.00.067.073 I llm_load_print_meta: vocab type       = BPE
0.00.067.074 I llm_load_print_meta: n_vocab          = 50304
0.00.067.074 I llm_load_print_meta: n_merges         = 50009
0.00.067.074 I llm_load_print_meta: vocab_only       = 0
0.00.067.074 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.074 I llm_load_print_meta: n_embd           = 2048
0.00.067.074 I llm_load_print_meta: n_layer          = 24
0.00.067.078 I llm_load_print_meta: n_head           = 16
0.00.067.079 I llm_load_print_meta: n_head_kv        = 16
0.00.067.079 I llm_load_print_meta: n_rot            = 32
0.00.067.079 I llm_load_print_meta: n_swa            = 0
0.00.067.079 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.080 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.080 I llm_load_print_meta: n_gqa            = 1
0.00.067.081 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.081 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.082 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.082 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.082 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.083 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.083 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.083 I llm_load_print_meta: n_ff             = 8192
0.00.067.084 I llm_load_print_meta: n_expert         = 0
0.00.067.084 I llm_load_print_meta: n_expert_used    = 0
0.00.067.084 I llm_load_print_meta: causal attn      = 1
0.00.067.084 I llm_load_print_meta: pooling type     = 0
0.00.067.084 I llm_load_print_meta: rope type        = 2
0.00.067.084 I llm_load_print_meta: rope scaling     = linear
0.00.067.084 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.085 I llm_load_print_meta: freq_scale_train = 1
0.00.067.085 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.085 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.087 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.088 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.088 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.088 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.088 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.088 I llm_load_print_meta: model type       = 1.4B
0.00.067.088 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.089 I llm_load_print_meta: model params     = 1.41 B
0.00.067.089 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.090 I llm_load_print_meta: general.name     = 1.4B
0.00.067.090 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.091 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.091 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.091 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.091 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.067.092 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.092 I llm_load_print_meta: max token length = 1024
0.00.069.401 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.401 I llm_load_tensors: offloading output layer to GPU
0.00.069.401 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.413 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.414 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.379 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.380 I llama_new_context_with_model: n_ctx         = 128
0.00.070.380 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.380 I llama_new_context_with_model: n_batch       = 128
0.00.070.380 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.381 I llama_new_context_with_model: flash_attn    = 0
0.00.070.381 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.381 I llama_new_context_with_model: freq_scale    = 1
0.00.070.382 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.382 I ggml_metal_init: allocating
0.00.070.385 I ggml_metal_init: found device: Apple M4
0.00.070.387 I ggml_metal_init: picking default device: Apple M4
0.00.071.020 I ggml_metal_init: using embedded metal library
0.00.073.482 I ggml_metal_init: GPU name:   Apple M4
0.00.073.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.484 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.484 I ggml_metal_init: simdgroup reduction   = true
0.00.073.484 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.484 I ggml_metal_init: has bfloat            = true
0.00.073.485 I ggml_metal_init: use bfloat            = true
0.00.073.485 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.741 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.097 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.109 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.132 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.051 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.052 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.053 I llama_new_context_with_model: graph nodes  = 967
0.00.085.053 I llama_new_context_with_model: graph splits = 2
0.00.085.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.965.593 I 
0.00.965.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.965.673 I perplexity: tokenizing the input ..
0.00.973.846 I perplexity: tokenization took 8.168 ms
0.00.973.849 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.097.878 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.099.058 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.099.074 I llama_perf_context_print:        load time =     953.50 ms
0.01.099.075 I llama_perf_context_print: prompt eval time =     123.81 ms /   128 tokens (    0.97 ms per token,  1033.87 tokens per second)
0.01.099.076 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.099.076 I llama_perf_context_print:       total time =     133.48 ms /   129 tokens
0.01.099.487 I ggml_metal_free: deallocating

real	0m1.118s
user	0m0.095s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.056 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.890 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.895 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.900 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.906 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.906 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.858 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.859 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.859 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.859 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.860 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.860 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.861 I llama_model_loader: - type  f32:  194 tensors
0.00.024.861 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.861 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.699 I llm_load_vocab: special tokens cache size = 25
0.00.051.562 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.565 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.565 I llm_load_print_meta: arch             = gptneox
0.00.051.566 I llm_load_print_meta: vocab type       = BPE
0.00.051.566 I llm_load_print_meta: n_vocab          = 50304
0.00.051.566 I llm_load_print_meta: n_merges         = 50009
0.00.051.566 I llm_load_print_meta: vocab_only       = 0
0.00.051.566 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.567 I llm_load_print_meta: n_embd           = 2048
0.00.051.567 I llm_load_print_meta: n_layer          = 24
0.00.051.570 I llm_load_print_meta: n_head           = 16
0.00.051.571 I llm_load_print_meta: n_head_kv        = 16
0.00.051.571 I llm_load_print_meta: n_rot            = 32
0.00.051.571 I llm_load_print_meta: n_swa            = 0
0.00.051.571 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.573 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.574 I llm_load_print_meta: n_gqa            = 1
0.00.051.575 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.575 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.576 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.576 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.576 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.576 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.577 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.577 I llm_load_print_meta: n_ff             = 8192
0.00.051.577 I llm_load_print_meta: n_expert         = 0
0.00.051.578 I llm_load_print_meta: n_expert_used    = 0
0.00.051.578 I llm_load_print_meta: causal attn      = 1
0.00.051.578 I llm_load_print_meta: pooling type     = 0
0.00.051.578 I llm_load_print_meta: rope type        = 2
0.00.051.583 I llm_load_print_meta: rope scaling     = linear
0.00.051.583 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.584 I llm_load_print_meta: freq_scale_train = 1
0.00.051.584 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.584 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.584 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.587 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.587 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.587 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.587 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.588 I llm_load_print_meta: model type       = 1.4B
0.00.051.588 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.588 I llm_load_print_meta: model params     = 1.41 B
0.00.051.589 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.589 I llm_load_print_meta: general.name     = 1.4B
0.00.051.589 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.590 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.590 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.590 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.590 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.590 I llm_load_print_meta: max token length = 1024
0.00.053.527 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.527 I llm_load_tensors: offloading output layer to GPU
0.00.053.527 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.538 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.539 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.473 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.473 I llama_new_context_with_model: n_ctx         = 128
0.00.054.473 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.474 I llama_new_context_with_model: n_batch       = 128
0.00.054.474 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.474 I llama_new_context_with_model: flash_attn    = 0
0.00.054.474 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.474 I llama_new_context_with_model: freq_scale    = 1
0.00.054.475 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.475 I ggml_metal_init: allocating
0.00.054.479 I ggml_metal_init: found device: Apple M4
0.00.054.480 I ggml_metal_init: picking default device: Apple M4
0.00.055.061 I ggml_metal_init: using embedded metal library
0.00.057.340 I ggml_metal_init: GPU name:   Apple M4
0.00.057.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.342 I ggml_metal_init: simdgroup reduction   = true
0.00.057.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.342 I ggml_metal_init: has bfloat            = true
0.00.057.342 I ggml_metal_init: use bfloat            = true
0.00.057.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.343 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.128 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.391 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.395 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.410 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.332 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.333 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.333 I llama_new_context_with_model: graph nodes  = 967
0.00.069.334 I llama_new_context_with_model: graph splits = 2
0.00.069.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.448 I 
0.00.650.494 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.506 I perplexity: tokenizing the input ..
0.00.658.483 I perplexity: tokenization took 7.972 ms
0.00.658.486 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.781.227 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.782.374 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.782.392 I llama_perf_context_print:        load time =     640.39 ms
0.00.782.393 I llama_perf_context_print: prompt eval time =     122.52 ms /   128 tokens (    0.96 ms per token,  1044.76 tokens per second)
0.00.782.393 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.394 I llama_perf_context_print:       total time =     131.95 ms /   129 tokens
0.00.782.906 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.079s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.783 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.461 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.467 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.468 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.468 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.470 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.473 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.473 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.473 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.474 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.482 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.440 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.441 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.441 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.441 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.442 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.442 I llama_model_loader: - type  f32:  194 tensors
0.00.023.443 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.443 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.373 I llm_load_vocab: special tokens cache size = 25
0.00.050.334 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.337 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.338 I llm_load_print_meta: arch             = gptneox
0.00.050.338 I llm_load_print_meta: vocab type       = BPE
0.00.050.338 I llm_load_print_meta: n_vocab          = 50304
0.00.050.338 I llm_load_print_meta: n_merges         = 50009
0.00.050.338 I llm_load_print_meta: vocab_only       = 0
0.00.050.339 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.339 I llm_load_print_meta: n_embd           = 2048
0.00.050.339 I llm_load_print_meta: n_layer          = 24
0.00.050.342 I llm_load_print_meta: n_head           = 16
0.00.050.343 I llm_load_print_meta: n_head_kv        = 16
0.00.050.343 I llm_load_print_meta: n_rot            = 32
0.00.050.343 I llm_load_print_meta: n_swa            = 0
0.00.050.343 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.343 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.344 I llm_load_print_meta: n_gqa            = 1
0.00.050.345 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.346 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.346 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.347 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.347 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.347 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.347 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.348 I llm_load_print_meta: n_ff             = 8192
0.00.050.350 I llm_load_print_meta: n_expert         = 0
0.00.050.350 I llm_load_print_meta: n_expert_used    = 0
0.00.050.350 I llm_load_print_meta: causal attn      = 1
0.00.050.351 I llm_load_print_meta: pooling type     = 0
0.00.050.351 I llm_load_print_meta: rope type        = 2
0.00.050.351 I llm_load_print_meta: rope scaling     = linear
0.00.050.351 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.352 I llm_load_print_meta: freq_scale_train = 1
0.00.050.352 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.352 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.352 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.352 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.353 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.353 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.353 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.353 I llm_load_print_meta: model type       = 1.4B
0.00.050.354 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.358 I llm_load_print_meta: model params     = 1.41 B
0.00.050.358 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.358 I llm_load_print_meta: general.name     = 1.4B
0.00.050.359 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.359 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.359 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.359 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.360 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.360 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.360 I llm_load_print_meta: max token length = 1024
0.00.052.332 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.332 I llm_load_tensors: offloading output layer to GPU
0.00.052.332 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.343 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.344 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.245 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.246 I llama_new_context_with_model: n_ctx         = 128
0.00.053.246 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.246 I llama_new_context_with_model: n_batch       = 128
0.00.053.246 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.246 I llama_new_context_with_model: flash_attn    = 0
0.00.053.247 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.247 I llama_new_context_with_model: freq_scale    = 1
0.00.053.247 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.248 I ggml_metal_init: allocating
0.00.053.251 I ggml_metal_init: found device: Apple M4
0.00.053.253 I ggml_metal_init: picking default device: Apple M4
0.00.053.822 I ggml_metal_init: using embedded metal library
0.00.056.140 I ggml_metal_init: GPU name:   Apple M4
0.00.056.142 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.142 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.143 I ggml_metal_init: simdgroup reduction   = true
0.00.056.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.143 I ggml_metal_init: has bfloat            = true
0.00.056.143 I ggml_metal_init: use bfloat            = true
0.00.056.143 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.144 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.942 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.238 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.241 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.256 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.152 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.153 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.153 I llama_new_context_with_model: graph nodes  = 967
0.00.068.154 I llama_new_context_with_model: graph splits = 2
0.00.068.166 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.093 I 
0.00.658.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.140 I perplexity: tokenizing the input ..
0.00.666.088 I perplexity: tokenization took 7.943 ms
0.00.666.092 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.944 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.790.198 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.790.221 I llama_perf_context_print:        load time =     649.30 ms
0.00.790.223 I llama_perf_context_print: prompt eval time =     122.62 ms /   128 tokens (    0.96 ms per token,  1043.86 tokens per second)
0.00.790.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.224 I llama_perf_context_print:       total time =     132.13 ms /   129 tokens
0.00.790.710 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.079s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.995 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.657 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.661 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.665 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.666 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.666 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.669 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.670 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.670 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.671 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.674 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.674 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.668 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.624 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.625 I llama_model_loader: - type  f32:  194 tensors
0.00.024.625 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.626 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.603 I llm_load_vocab: special tokens cache size = 25
0.00.050.595 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.598 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.599 I llm_load_print_meta: arch             = gptneox
0.00.050.599 I llm_load_print_meta: vocab type       = BPE
0.00.050.599 I llm_load_print_meta: n_vocab          = 50304
0.00.050.599 I llm_load_print_meta: n_merges         = 50009
0.00.050.600 I llm_load_print_meta: vocab_only       = 0
0.00.050.600 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.600 I llm_load_print_meta: n_embd           = 2048
0.00.050.600 I llm_load_print_meta: n_layer          = 24
0.00.050.604 I llm_load_print_meta: n_head           = 16
0.00.050.604 I llm_load_print_meta: n_head_kv        = 16
0.00.050.604 I llm_load_print_meta: n_rot            = 32
0.00.050.605 I llm_load_print_meta: n_swa            = 0
0.00.050.605 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.605 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.607 I llm_load_print_meta: n_gqa            = 1
0.00.050.607 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.608 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.609 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.609 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.609 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.610 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.612 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.612 I llm_load_print_meta: n_ff             = 8192
0.00.050.613 I llm_load_print_meta: n_expert         = 0
0.00.050.613 I llm_load_print_meta: n_expert_used    = 0
0.00.050.613 I llm_load_print_meta: causal attn      = 1
0.00.050.613 I llm_load_print_meta: pooling type     = 0
0.00.050.613 I llm_load_print_meta: rope type        = 2
0.00.050.614 I llm_load_print_meta: rope scaling     = linear
0.00.050.616 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.617 I llm_load_print_meta: freq_scale_train = 1
0.00.050.617 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.618 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.618 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.618 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.618 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.618 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.618 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.619 I llm_load_print_meta: model type       = 1.4B
0.00.050.619 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.619 I llm_load_print_meta: model params     = 1.41 B
0.00.050.620 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.620 I llm_load_print_meta: general.name     = 1.4B
0.00.050.624 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.626 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.626 I llm_load_print_meta: max token length = 1024
0.00.052.404 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.404 I llm_load_tensors: offloading output layer to GPU
0.00.052.404 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.409 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.410 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.440 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.441 I llama_new_context_with_model: n_ctx         = 128
0.00.053.441 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.441 I llama_new_context_with_model: n_batch       = 128
0.00.053.441 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.442 I llama_new_context_with_model: flash_attn    = 0
0.00.053.442 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.442 I llama_new_context_with_model: freq_scale    = 1
0.00.053.442 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.443 I ggml_metal_init: allocating
0.00.053.448 I ggml_metal_init: found device: Apple M4
0.00.053.451 I ggml_metal_init: picking default device: Apple M4
0.00.054.004 I ggml_metal_init: using embedded metal library
0.00.056.287 I ggml_metal_init: GPU name:   Apple M4
0.00.056.289 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.290 I ggml_metal_init: simdgroup reduction   = true
0.00.056.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.290 I ggml_metal_init: has bfloat            = true
0.00.056.290 I ggml_metal_init: use bfloat            = true
0.00.056.290 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.787 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.213 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.216 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.231 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.105 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.106 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.106 I llama_new_context_with_model: graph nodes  = 967
0.00.068.107 I llama_new_context_with_model: graph splits = 2
0.00.068.119 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.120 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.892 I 
0.00.768.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.936 I perplexity: tokenizing the input ..
0.00.776.661 I perplexity: tokenization took 7.721 ms
0.00.776.669 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.911.336 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.912.509 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.912.523 I llama_perf_context_print:        load time =     758.89 ms
0.00.912.524 I llama_perf_context_print: prompt eval time =     134.44 ms /   128 tokens (    1.05 ms per token,   952.09 tokens per second)
0.00.912.525 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.912.525 I llama_perf_context_print:       total time =     143.63 ms /   129 tokens
0.00.912.960 I ggml_metal_free: deallocating

real	0m0.928s
user	0m0.078s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.786 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.341 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.345 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.349 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.306 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.308 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.309 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.309 I llama_model_loader: - type  f32:  194 tensors
0.00.023.309 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.310 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.297 I llm_load_vocab: special tokens cache size = 25
0.00.049.307 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.309 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.309 I llm_load_print_meta: arch             = gptneox
0.00.049.310 I llm_load_print_meta: vocab type       = BPE
0.00.049.310 I llm_load_print_meta: n_vocab          = 50304
0.00.049.310 I llm_load_print_meta: n_merges         = 50009
0.00.049.310 I llm_load_print_meta: vocab_only       = 0
0.00.049.311 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.311 I llm_load_print_meta: n_embd           = 2048
0.00.049.311 I llm_load_print_meta: n_layer          = 24
0.00.049.314 I llm_load_print_meta: n_head           = 16
0.00.049.315 I llm_load_print_meta: n_head_kv        = 16
0.00.049.315 I llm_load_print_meta: n_rot            = 32
0.00.049.315 I llm_load_print_meta: n_swa            = 0
0.00.049.316 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.316 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.316 I llm_load_print_meta: n_gqa            = 1
0.00.049.317 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.318 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.319 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.319 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.319 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.319 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.319 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.320 I llm_load_print_meta: n_ff             = 8192
0.00.049.320 I llm_load_print_meta: n_expert         = 0
0.00.049.320 I llm_load_print_meta: n_expert_used    = 0
0.00.049.321 I llm_load_print_meta: causal attn      = 1
0.00.049.321 I llm_load_print_meta: pooling type     = 0
0.00.049.321 I llm_load_print_meta: rope type        = 2
0.00.049.321 I llm_load_print_meta: rope scaling     = linear
0.00.049.321 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.322 I llm_load_print_meta: freq_scale_train = 1
0.00.049.322 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.322 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.322 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.322 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.323 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.323 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.323 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.323 I llm_load_print_meta: model type       = 1.4B
0.00.049.325 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.326 I llm_load_print_meta: model params     = 1.41 B
0.00.049.326 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.327 I llm_load_print_meta: general.name     = 1.4B
0.00.049.327 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.327 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.327 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.327 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.328 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.328 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.332 I llm_load_print_meta: max token length = 1024
0.00.051.274 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.274 I llm_load_tensors: offloading output layer to GPU
0.00.051.274 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.284 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.286 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.164 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.164 I llama_new_context_with_model: n_ctx         = 128
0.00.052.165 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.165 I llama_new_context_with_model: n_batch       = 128
0.00.052.165 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.165 I llama_new_context_with_model: flash_attn    = 0
0.00.052.166 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.166 I llama_new_context_with_model: freq_scale    = 1
0.00.052.166 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.167 I ggml_metal_init: allocating
0.00.052.170 I ggml_metal_init: found device: Apple M4
0.00.052.172 I ggml_metal_init: picking default device: Apple M4
0.00.052.735 I ggml_metal_init: using embedded metal library
0.00.055.281 I ggml_metal_init: GPU name:   Apple M4
0.00.055.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.283 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.283 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.284 I ggml_metal_init: simdgroup reduction   = true
0.00.055.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.284 I ggml_metal_init: has bfloat            = true
0.00.055.284 I ggml_metal_init: use bfloat            = true
0.00.055.285 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.742 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.990 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.992 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.008 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.867 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.868 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.869 I llama_new_context_with_model: graph nodes  = 967
0.00.066.869 I llama_new_context_with_model: graph splits = 2
0.00.066.881 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.882 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.529 I 
0.00.723.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.578 I perplexity: tokenizing the input ..
0.00.731.694 I perplexity: tokenization took 8.111 ms
0.00.731.697 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.865.574 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.866.916 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.866.929 I llama_perf_context_print:        load time =     714.74 ms
0.00.866.930 I llama_perf_context_print: prompt eval time =     133.65 ms /   128 tokens (    1.04 ms per token,   957.75 tokens per second)
0.00.866.931 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.932 I llama_perf_context_print:       total time =     143.40 ms /   129 tokens
0.00.867.293 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.078s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.873 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.571 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.444 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.391 I llama_model_loader: - type  f32:  194 tensors
0.00.024.391 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.391 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.391 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.625 I llm_load_vocab: special tokens cache size = 25
0.00.051.817 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.823 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.824 I llm_load_print_meta: arch             = gptneox
0.00.051.824 I llm_load_print_meta: vocab type       = BPE
0.00.051.824 I llm_load_print_meta: n_vocab          = 50304
0.00.051.825 I llm_load_print_meta: n_merges         = 50009
0.00.051.825 I llm_load_print_meta: vocab_only       = 0
0.00.051.825 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.825 I llm_load_print_meta: n_embd           = 2048
0.00.051.825 I llm_load_print_meta: n_layer          = 24
0.00.051.829 I llm_load_print_meta: n_head           = 16
0.00.051.830 I llm_load_print_meta: n_head_kv        = 16
0.00.051.830 I llm_load_print_meta: n_rot            = 32
0.00.051.831 I llm_load_print_meta: n_swa            = 0
0.00.051.831 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.831 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.831 I llm_load_print_meta: n_gqa            = 1
0.00.051.832 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.833 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.833 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.834 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.835 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.835 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.835 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.838 I llm_load_print_meta: n_ff             = 8192
0.00.051.838 I llm_load_print_meta: n_expert         = 0
0.00.051.838 I llm_load_print_meta: n_expert_used    = 0
0.00.051.838 I llm_load_print_meta: causal attn      = 1
0.00.051.838 I llm_load_print_meta: pooling type     = 0
0.00.051.838 I llm_load_print_meta: rope type        = 2
0.00.051.839 I llm_load_print_meta: rope scaling     = linear
0.00.051.839 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.839 I llm_load_print_meta: freq_scale_train = 1
0.00.051.839 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.839 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.840 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.840 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.840 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.840 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.840 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.840 I llm_load_print_meta: model type       = 1.4B
0.00.051.844 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.845 I llm_load_print_meta: model params     = 1.41 B
0.00.051.846 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.846 I llm_load_print_meta: general.name     = 1.4B
0.00.051.846 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.846 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.847 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.847 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.847 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.847 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.847 I llm_load_print_meta: max token length = 1024
0.00.053.947 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.947 I llm_load_tensors: offloading output layer to GPU
0.00.053.947 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.958 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.959 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.972 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.973 I llama_new_context_with_model: n_ctx         = 128
0.00.054.973 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.973 I llama_new_context_with_model: n_batch       = 128
0.00.054.974 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.974 I llama_new_context_with_model: flash_attn    = 0
0.00.054.974 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.974 I llama_new_context_with_model: freq_scale    = 1
0.00.054.975 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.975 I ggml_metal_init: allocating
0.00.054.980 I ggml_metal_init: found device: Apple M4
0.00.054.982 I ggml_metal_init: picking default device: Apple M4
0.00.055.607 I ggml_metal_init: using embedded metal library
0.00.058.421 I ggml_metal_init: GPU name:   Apple M4
0.00.058.423 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.424 I ggml_metal_init: simdgroup reduction   = true
0.00.058.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.424 I ggml_metal_init: has bfloat            = true
0.00.058.427 I ggml_metal_init: use bfloat            = true
0.00.058.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.772 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.108 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.113 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.126 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.970 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.971 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.972 I llama_new_context_with_model: graph nodes  = 967
0.00.069.972 I llama_new_context_with_model: graph splits = 2
0.00.069.985 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.583 I 
0.00.387.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.636 I perplexity: tokenizing the input ..
0.00.396.320 I perplexity: tokenization took 8.679 ms
0.00.396.325 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.800 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.530.051 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.530.070 I llama_perf_context_print:        load time =     377.70 ms
0.00.530.071 I llama_perf_context_print: prompt eval time =     132.16 ms /   128 tokens (    1.03 ms per token,   968.53 tokens per second)
0.00.530.073 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.530.074 I llama_perf_context_print:       total time =     142.49 ms /   129 tokens
0.00.530.600 I ggml_metal_free: deallocating

real	0m0.548s
user	0m0.080s
sys	0m0.064s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.797 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.724 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.729 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.731 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.731 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.732 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.733 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.733 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.650 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.786 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.769 I llama_model_loader: - type  f32:  194 tensors
0.00.023.769 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.770 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.770 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.770 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.054 I llm_load_vocab: special tokens cache size = 25
0.00.050.064 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.067 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.067 I llm_load_print_meta: arch             = gptneox
0.00.050.068 I llm_load_print_meta: vocab type       = BPE
0.00.050.068 I llm_load_print_meta: n_vocab          = 50304
0.00.050.068 I llm_load_print_meta: n_merges         = 50009
0.00.050.068 I llm_load_print_meta: vocab_only       = 0
0.00.050.069 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.069 I llm_load_print_meta: n_embd           = 2048
0.00.050.069 I llm_load_print_meta: n_layer          = 24
0.00.050.072 I llm_load_print_meta: n_head           = 16
0.00.050.072 I llm_load_print_meta: n_head_kv        = 16
0.00.050.073 I llm_load_print_meta: n_rot            = 32
0.00.050.073 I llm_load_print_meta: n_swa            = 0
0.00.050.073 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.076 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.077 I llm_load_print_meta: n_gqa            = 1
0.00.050.077 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.078 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.079 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.079 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.079 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.080 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.080 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.080 I llm_load_print_meta: n_ff             = 8192
0.00.050.081 I llm_load_print_meta: n_expert         = 0
0.00.050.081 I llm_load_print_meta: n_expert_used    = 0
0.00.050.081 I llm_load_print_meta: causal attn      = 1
0.00.050.081 I llm_load_print_meta: pooling type     = 0
0.00.050.083 I llm_load_print_meta: rope type        = 2
0.00.050.084 I llm_load_print_meta: rope scaling     = linear
0.00.050.085 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.085 I llm_load_print_meta: freq_scale_train = 1
0.00.050.085 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.085 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.087 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.087 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.087 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.087 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.087 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.088 I llm_load_print_meta: model type       = 1.4B
0.00.050.088 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.088 I llm_load_print_meta: model params     = 1.41 B
0.00.050.089 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.089 I llm_load_print_meta: general.name     = 1.4B
0.00.050.089 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.090 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.090 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.090 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.091 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.091 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.091 I llm_load_print_meta: max token length = 1024
0.00.051.867 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.867 I llm_load_tensors: offloading output layer to GPU
0.00.051.868 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.873 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.873 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.822 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.823 I llama_new_context_with_model: n_ctx         = 128
0.00.052.823 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.823 I llama_new_context_with_model: n_batch       = 128
0.00.052.823 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.823 I llama_new_context_with_model: flash_attn    = 0
0.00.052.824 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.824 I llama_new_context_with_model: freq_scale    = 1
0.00.052.824 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.825 I ggml_metal_init: allocating
0.00.052.828 I ggml_metal_init: found device: Apple M4
0.00.052.830 I ggml_metal_init: picking default device: Apple M4
0.00.053.465 I ggml_metal_init: using embedded metal library
0.00.055.863 I ggml_metal_init: GPU name:   Apple M4
0.00.055.865 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.865 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.867 I ggml_metal_init: simdgroup reduction   = true
0.00.055.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.868 I ggml_metal_init: has bfloat            = true
0.00.055.868 I ggml_metal_init: use bfloat            = true
0.00.055.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.441 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.023 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.027 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.044 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.967 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.968 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.968 I llama_new_context_with_model: graph nodes  = 967
0.00.067.969 I llama_new_context_with_model: graph splits = 2
0.00.067.981 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.272 I 
0.00.476.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.316 I perplexity: tokenizing the input ..
0.00.484.042 I perplexity: tokenization took 7.722 ms
0.00.484.046 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.350 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.617.522 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.617.541 I llama_perf_context_print:        load time =     467.47 ms
0.00.617.542 I llama_perf_context_print: prompt eval time =     132.08 ms /   128 tokens (    1.03 ms per token,   969.12 tokens per second)
0.00.617.543 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.617.543 I llama_perf_context_print:       total time =     141.27 ms /   129 tokens
0.00.618.014 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.078s
sys	0m0.081s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.986 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.977 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.979 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.982 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.983 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.985 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.986 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.988 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.911 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.989 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.940 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.940 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.941 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.941 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.941 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.942 I llama_model_loader: - type  f32:  194 tensors
0.00.023.942 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.942 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.943 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.685 I llm_load_vocab: special tokens cache size = 25
0.00.050.631 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.634 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.634 I llm_load_print_meta: arch             = gptneox
0.00.050.634 I llm_load_print_meta: vocab type       = BPE
0.00.050.635 I llm_load_print_meta: n_vocab          = 50304
0.00.050.635 I llm_load_print_meta: n_merges         = 50009
0.00.050.635 I llm_load_print_meta: vocab_only       = 0
0.00.050.635 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.635 I llm_load_print_meta: n_embd           = 2048
0.00.050.636 I llm_load_print_meta: n_layer          = 24
0.00.050.638 I llm_load_print_meta: n_head           = 16
0.00.050.639 I llm_load_print_meta: n_head_kv        = 16
0.00.050.639 I llm_load_print_meta: n_rot            = 32
0.00.050.639 I llm_load_print_meta: n_swa            = 0
0.00.050.640 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.640 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.643 I llm_load_print_meta: n_gqa            = 1
0.00.050.644 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.644 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.645 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.646 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.646 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.646 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.646 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.647 I llm_load_print_meta: n_ff             = 8192
0.00.050.647 I llm_load_print_meta: n_expert         = 0
0.00.050.647 I llm_load_print_meta: n_expert_used    = 0
0.00.050.647 I llm_load_print_meta: causal attn      = 1
0.00.050.647 I llm_load_print_meta: pooling type     = 0
0.00.050.647 I llm_load_print_meta: rope type        = 2
0.00.050.658 I llm_load_print_meta: rope scaling     = linear
0.00.050.664 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.664 I llm_load_print_meta: freq_scale_train = 1
0.00.050.664 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.664 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.665 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.665 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.665 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.665 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.665 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.665 I llm_load_print_meta: model type       = 1.4B
0.00.050.666 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.666 I llm_load_print_meta: model params     = 1.41 B
0.00.050.666 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.667 I llm_load_print_meta: general.name     = 1.4B
0.00.050.667 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.667 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.667 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.667 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.667 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.668 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.668 I llm_load_print_meta: max token length = 1024
0.00.052.573 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.574 I llm_load_tensors: offloading output layer to GPU
0.00.052.574 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.584 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.585 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.431 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.431 I llama_new_context_with_model: n_ctx         = 128
0.00.053.431 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.432 I llama_new_context_with_model: n_batch       = 128
0.00.053.432 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.432 I llama_new_context_with_model: flash_attn    = 0
0.00.053.432 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.432 I llama_new_context_with_model: freq_scale    = 1
0.00.053.433 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.433 I ggml_metal_init: allocating
0.00.053.436 I ggml_metal_init: found device: Apple M4
0.00.053.439 I ggml_metal_init: picking default device: Apple M4
0.00.054.001 I ggml_metal_init: using embedded metal library
0.00.056.323 I ggml_metal_init: GPU name:   Apple M4
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.325 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.325 I ggml_metal_init: simdgroup reduction   = true
0.00.056.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.325 I ggml_metal_init: has bfloat            = true
0.00.056.326 I ggml_metal_init: use bfloat            = true
0.00.056.326 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.874 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.090 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.093 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.108 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.981 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.982 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.982 I llama_new_context_with_model: graph nodes  = 967
0.00.067.982 I llama_new_context_with_model: graph splits = 2
0.00.067.994 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.995 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.854 I 
0.00.557.922 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.953 I perplexity: tokenizing the input ..
0.00.565.676 I perplexity: tokenization took 7.719 ms
0.00.565.679 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.700.345 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.701.577 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.701.589 I llama_perf_context_print:        load time =     548.86 ms
0.00.701.589 I llama_perf_context_print: prompt eval time =     134.42 ms /   128 tokens (    1.05 ms per token,   952.22 tokens per second)
0.00.701.590 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.701.590 I llama_perf_context_print:       total time =     143.74 ms /   129 tokens
0.00.702.115 I ggml_metal_free: deallocating

real	0m0.716s
user	0m0.078s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.103 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.808 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.812 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.814 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.814 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.815 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.815 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.816 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.817 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.818 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.820 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.713 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.653 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.655 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.655 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.655 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.656 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.656 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.656 I llama_model_loader: - type  f32:  194 tensors
0.00.024.657 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.657 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.741 I llm_load_vocab: special tokens cache size = 25
0.00.050.666 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.669 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.669 I llm_load_print_meta: arch             = gptneox
0.00.050.670 I llm_load_print_meta: vocab type       = BPE
0.00.050.670 I llm_load_print_meta: n_vocab          = 50304
0.00.050.670 I llm_load_print_meta: n_merges         = 50009
0.00.050.670 I llm_load_print_meta: vocab_only       = 0
0.00.050.670 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.670 I llm_load_print_meta: n_embd           = 2048
0.00.050.671 I llm_load_print_meta: n_layer          = 24
0.00.050.673 I llm_load_print_meta: n_head           = 16
0.00.050.674 I llm_load_print_meta: n_head_kv        = 16
0.00.050.674 I llm_load_print_meta: n_rot            = 32
0.00.050.674 I llm_load_print_meta: n_swa            = 0
0.00.050.674 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.675 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.675 I llm_load_print_meta: n_gqa            = 1
0.00.050.676 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.678 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.680 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.680 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.680 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.680 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.681 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.681 I llm_load_print_meta: n_ff             = 8192
0.00.050.681 I llm_load_print_meta: n_expert         = 0
0.00.050.682 I llm_load_print_meta: n_expert_used    = 0
0.00.050.682 I llm_load_print_meta: causal attn      = 1
0.00.050.682 I llm_load_print_meta: pooling type     = 0
0.00.050.682 I llm_load_print_meta: rope type        = 2
0.00.050.682 I llm_load_print_meta: rope scaling     = linear
0.00.050.683 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.683 I llm_load_print_meta: freq_scale_train = 1
0.00.050.683 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.683 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.684 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.684 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.684 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.688 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.689 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.691 I llm_load_print_meta: model type       = 1.4B
0.00.050.693 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.694 I llm_load_print_meta: model params     = 1.41 B
0.00.050.694 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.695 I llm_load_print_meta: general.name     = 1.4B
0.00.050.695 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.695 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.696 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: max token length = 1024
0.00.052.670 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.670 I llm_load_tensors: offloading output layer to GPU
0.00.052.671 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.681 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.682 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.609 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.609 I llama_new_context_with_model: n_ctx         = 128
0.00.053.610 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.610 I llama_new_context_with_model: n_batch       = 128
0.00.053.610 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.610 I llama_new_context_with_model: flash_attn    = 0
0.00.053.611 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.611 I llama_new_context_with_model: freq_scale    = 1
0.00.053.611 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.612 I ggml_metal_init: allocating
0.00.053.615 I ggml_metal_init: found device: Apple M4
0.00.053.617 I ggml_metal_init: picking default device: Apple M4
0.00.054.182 I ggml_metal_init: using embedded metal library
0.00.056.478 I ggml_metal_init: GPU name:   Apple M4
0.00.056.479 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.480 I ggml_metal_init: simdgroup reduction   = true
0.00.056.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.481 I ggml_metal_init: has bfloat            = true
0.00.056.481 I ggml_metal_init: use bfloat            = true
0.00.056.481 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.482 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.961 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.378 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.380 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.393 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.261 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.262 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.262 I llama_new_context_with_model: graph nodes  = 967
0.00.068.263 I llama_new_context_with_model: graph splits = 2
0.00.068.275 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.276 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.382 I 
0.00.662.425 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.437 I perplexity: tokenizing the input ..
0.00.670.446 I perplexity: tokenization took 8.005 ms
0.00.670.449 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.122 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.812.289 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.812.307 I llama_perf_context_print:        load time =     652.27 ms
0.00.812.308 I llama_perf_context_print: prompt eval time =     140.45 ms /   128 tokens (    1.10 ms per token,   911.38 tokens per second)
0.00.812.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.311 I llama_perf_context_print:       total time =     149.93 ms /   129 tokens
0.00.812.796 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.078s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.798 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.542 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.495 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.495 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.496 I llama_model_loader: - type  f32:  194 tensors
0.00.023.496 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.340 I llm_load_vocab: special tokens cache size = 25
0.00.050.309 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.312 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.312 I llm_load_print_meta: arch             = gptneox
0.00.050.312 I llm_load_print_meta: vocab type       = BPE
0.00.050.313 I llm_load_print_meta: n_vocab          = 50304
0.00.050.313 I llm_load_print_meta: n_merges         = 50009
0.00.050.313 I llm_load_print_meta: vocab_only       = 0
0.00.050.313 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.313 I llm_load_print_meta: n_embd           = 2048
0.00.050.314 I llm_load_print_meta: n_layer          = 24
0.00.050.316 I llm_load_print_meta: n_head           = 16
0.00.050.319 I llm_load_print_meta: n_head_kv        = 16
0.00.050.319 I llm_load_print_meta: n_rot            = 32
0.00.050.319 I llm_load_print_meta: n_swa            = 0
0.00.050.319 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.319 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.320 I llm_load_print_meta: n_gqa            = 1
0.00.050.321 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.322 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.322 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.323 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.323 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.323 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.323 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.324 I llm_load_print_meta: n_ff             = 8192
0.00.050.324 I llm_load_print_meta: n_expert         = 0
0.00.050.324 I llm_load_print_meta: n_expert_used    = 0
0.00.050.324 I llm_load_print_meta: causal attn      = 1
0.00.050.325 I llm_load_print_meta: pooling type     = 0
0.00.050.325 I llm_load_print_meta: rope type        = 2
0.00.050.325 I llm_load_print_meta: rope scaling     = linear
0.00.050.325 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.326 I llm_load_print_meta: freq_scale_train = 1
0.00.050.326 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.326 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.326 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.327 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.327 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.327 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.327 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.327 I llm_load_print_meta: model type       = 1.4B
0.00.050.328 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.328 I llm_load_print_meta: model params     = 1.41 B
0.00.050.330 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.330 I llm_load_print_meta: general.name     = 1.4B
0.00.050.330 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.330 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.330 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.331 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.331 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.332 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.333 I llm_load_print_meta: max token length = 1024
0.00.052.392 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.392 I llm_load_tensors: offloading output layer to GPU
0.00.052.392 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.402 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.404 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.300 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.301 I llama_new_context_with_model: n_ctx         = 128
0.00.053.301 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.301 I llama_new_context_with_model: n_batch       = 128
0.00.053.301 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.301 I llama_new_context_with_model: flash_attn    = 0
0.00.053.302 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.302 I llama_new_context_with_model: freq_scale    = 1
0.00.053.303 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.303 I ggml_metal_init: allocating
0.00.053.310 I ggml_metal_init: found device: Apple M4
0.00.053.312 I ggml_metal_init: picking default device: Apple M4
0.00.053.899 I ggml_metal_init: using embedded metal library
0.00.056.256 I ggml_metal_init: GPU name:   Apple M4
0.00.056.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.258 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.258 I ggml_metal_init: simdgroup reduction   = true
0.00.056.258 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.259 I ggml_metal_init: has bfloat            = true
0.00.056.259 I ggml_metal_init: use bfloat            = true
0.00.056.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.832 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.073 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.076 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.090 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.991 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.992 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.992 I llama_new_context_with_model: graph nodes  = 967
0.00.067.993 I llama_new_context_with_model: graph splits = 2
0.00.068.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.374 I 
0.00.604.410 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.423 I perplexity: tokenizing the input ..
0.00.612.736 I perplexity: tokenization took 8.31 ms
0.00.612.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.080 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.754.237 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.754.253 I llama_perf_context_print:        load time =     595.57 ms
0.00.754.254 I llama_perf_context_print: prompt eval time =     140.11 ms /   128 tokens (    1.09 ms per token,   913.57 tokens per second)
0.00.754.254 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.255 I llama_perf_context_print:       total time =     149.88 ms /   129 tokens
0.00.754.724 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.079s
sys	0m0.126s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.237 I build: 4389 (09fe2e76) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.152 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.009 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.019 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.034 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.034 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.039 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.042 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.042 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.760 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.787 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.806 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.807 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.807 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.807 I llama_model_loader: - type  f32:  194 tensors
0.00.052.808 I llama_model_loader: - type  f16:   98 tensors
0.00.082.642 I llm_load_vocab: special tokens cache size = 25
0.00.089.399 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.402 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.402 I llm_load_print_meta: arch             = gptneox
0.00.089.402 I llm_load_print_meta: vocab type       = BPE
0.00.089.402 I llm_load_print_meta: n_vocab          = 50304
0.00.089.403 I llm_load_print_meta: n_merges         = 50009
0.00.089.403 I llm_load_print_meta: vocab_only       = 0
0.00.089.403 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.403 I llm_load_print_meta: n_embd           = 2048
0.00.089.403 I llm_load_print_meta: n_layer          = 24
0.00.089.406 I llm_load_print_meta: n_head           = 16
0.00.089.407 I llm_load_print_meta: n_head_kv        = 16
0.00.089.407 I llm_load_print_meta: n_rot            = 32
0.00.089.407 I llm_load_print_meta: n_swa            = 0
0.00.089.408 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.408 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.408 I llm_load_print_meta: n_gqa            = 1
0.00.089.409 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.410 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.410 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.411 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.411 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.411 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.411 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.412 I llm_load_print_meta: n_ff             = 8192
0.00.089.412 I llm_load_print_meta: n_expert         = 0
0.00.089.412 I llm_load_print_meta: n_expert_used    = 0
0.00.089.412 I llm_load_print_meta: causal attn      = 1
0.00.089.412 I llm_load_print_meta: pooling type     = 0
0.00.089.412 I llm_load_print_meta: rope type        = 2
0.00.089.413 I llm_load_print_meta: rope scaling     = linear
0.00.089.415 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.415 I llm_load_print_meta: freq_scale_train = 1
0.00.089.415 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.416 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.416 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.416 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.416 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.416 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.416 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.416 I llm_load_print_meta: model type       = 1.4B
0.00.089.417 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.417 I llm_load_print_meta: model params     = 1.41 B
0.00.089.418 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.418 I llm_load_print_meta: general.name     = 1.4B
0.00.089.418 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.418 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.420 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.420 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.420 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.420 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.420 I llm_load_print_meta: max token length = 1024
0.00.091.937 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.937 I llm_load_tensors: offloading output layer to GPU
0.00.091.937 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.948 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.949 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.861 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.862 I llama_new_context_with_model: n_ctx         = 128
0.00.092.862 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.862 I llama_new_context_with_model: n_batch       = 128
0.00.092.862 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.862 I llama_new_context_with_model: flash_attn    = 0
0.00.092.863 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.863 I llama_new_context_with_model: freq_scale    = 1
0.00.092.863 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.864 I ggml_metal_init: allocating
0.00.092.870 I ggml_metal_init: found device: Apple M4
0.00.092.872 I ggml_metal_init: picking default device: Apple M4
0.00.093.481 I ggml_metal_init: using embedded metal library
0.00.096.027 I ggml_metal_init: GPU name:   Apple M4
0.00.096.029 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.030 I ggml_metal_init: simdgroup reduction   = true
0.00.096.030 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.030 I ggml_metal_init: has bfloat            = true
0.00.096.031 I ggml_metal_init: use bfloat            = true
0.00.096.031 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.182 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.423 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.428 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.445 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.364 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.365 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.365 I llama_new_context_with_model: graph nodes  = 967
0.00.107.365 I llama_new_context_with_model: graph splits = 2
0.00.107.378 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.379 I 
0.00.107.414 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.107.416 I compute_imatrix: tokenizing the input ..
0.00.114.262 I compute_imatrix: tokenization took 6.844 ms
0.00.114.263 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.624.214 I compute_imatrix: 1.51 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.627.296 I llama_perf_context_print:        load time =    1602.06 ms
0.01.627.297 I llama_perf_context_print: prompt eval time =    1509.30 ms /   128 tokens (   11.79 ms per token,    84.81 tokens per second)
0.01.627.299 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.627.299 I llama_perf_context_print:       total time =    1605.13 ms /   129 tokens
0.01.628.246 I ggml_metal_free: deallocating

real	0m1.814s
user	0m0.176s
sys	0m0.248s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4389 (09fe2e76)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101e0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x101e0a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101e0af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101e0b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101e0baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101e0c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x101e0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x101e0cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x101e0d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x101e0d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x101e0db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x101e0e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x101e0eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x101e0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x101e0fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x101e10260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x101e10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x101e110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x101e117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x101e11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x101e126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101e12dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101e134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101e13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x101e144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x101e14770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101e14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101e159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101e15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101e161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x101e16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101e16950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x101e171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101e17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x101e179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101e17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101e18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101e187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101e18c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101e19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101e195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101e19a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101e19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101e1a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x101e1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101e1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x101e1b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101e1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x101e1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x101e1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101e1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x101e1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x101e1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x101e1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x101e1e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x101e1ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x101e1f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x101e1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x101e1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x101e201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x101e20490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x101e20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x101e20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x101e21270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x101e21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101e21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101e22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101e224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x101e22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101e22e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x101e232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x101e23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101e23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x101e24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x101e246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101e24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x101e25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x101e256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101e25bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101e26140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x101e26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101e26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101e27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101e27680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101e27bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101e28120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101e28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101e28bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101e29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101e29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101e29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x101e2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x101e2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101e2aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x101e2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x101e2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101e2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x101e1b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x101e2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x101e2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x101e2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x101e2d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x101e2d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x101e2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x101e2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x101e2e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x101e2ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x101e2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x101e2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x101e2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x101e30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x101e30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x101e30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x101e31160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x101e31600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x101e31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x101e31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101e323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101e32880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101e32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x101e331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x101e33660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101e33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101e33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x101e34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x101e348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101e34d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101e35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x101e356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101e35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101e36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101e364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101e36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101e36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101e37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101e37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101e37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101e38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101e38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101e389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101e38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101e392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101e39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101e39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101e3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x101e3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x101e3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101e3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x101e3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101e3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101e3bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x101e3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x101e3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x101e3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x101e3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x101e3d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x101e3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x101e3dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x101e3e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x101e3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x101e3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101e3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x101e3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x101e3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x101e3fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x101e401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x101e40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x101e40b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x101e40fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x101e41460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x101e41900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x101e41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x101e42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x101e426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101e42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101e43020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x101e434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x101e43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101e43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x101e442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x101e44740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101e44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101e45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101e45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101e459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101e45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101e46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101e467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101e46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x101e470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101e47580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101e47a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101e47ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101e48410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101e48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101e48eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101e49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101e496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101e49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101e4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x101e4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x101e4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x101e4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x101e4b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101e4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x101e4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x101e4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x101e4d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x101e4d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x101e4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x101e4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x101e4e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x101e4ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x101e4f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x101e4f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x101e4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x101e501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x101e50710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x101e50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x101e511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x101e51700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x101e51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x101e521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101e526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101e52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x101e53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x101e536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101e53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x101e54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x101e546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101e54c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x101e55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x101e556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101e55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101e56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101e566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101e56c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101e57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101e576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101e57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101e58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x101e58690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101e58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101e59130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101e59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101e59bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x101e5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x101e5a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101e5abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x101e5b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x101e5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101e5bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x101e5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x101e5c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x101e5cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x101e5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x101e5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x101e5db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x101e5e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x101e5e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x101e5eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x101e5f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x101e5f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x101e5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x101e600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x101e60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x101e60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x101e61000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x101e614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x101e61940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101e61de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x101e62280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101e62720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101e62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101e63060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101e63500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x101e639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101e63e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x101e642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x101e64780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101e64c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x101e650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101e65610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101e65d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x101e66450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101e66b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x101e67290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x101e67550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101e67d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101e68000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101e68610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.169.241 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.169.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101e253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x101e25840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101e25cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101e26120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101e26590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101e26a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x101e26e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x101e272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x101e27750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x101e27bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x101e28030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x101e28610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x101e28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x101e29680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x101e29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x101e2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x101e2ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x101e2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x101e2ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x101e2c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x101e2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101e2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101e2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101e2df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x101e2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x101e2eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101e2ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101e2f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101e2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101e2fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x101e300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101e30560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x101e309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101e30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x101e31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101e31570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101e319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101e31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101e322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101e32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101e32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101e33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101e33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101e338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x101e33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101e341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x101e34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101e34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x101e34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x101e35390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101e35800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x101e35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x101e360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x101e36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x101e369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x101e36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x101e372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x101e37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x101e37b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x101e37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x101e38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x101e388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x101e38d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x101e391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x101e39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101e39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101e39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101e3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x101e3a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101e3ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x101e3b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x101e3b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101e3b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x101e3be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x101e3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101e3c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x101e3cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x101e3cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101e3d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101e3d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x101e3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101e3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101e3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101e3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101e3eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101e3f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101e3f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101e3fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101e400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101e40510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101e40980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x101e40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x101e41260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101e416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x101e41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x101e41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101e42420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x101e42890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x101e42d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x101e43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x101e435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x101e43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x101e43ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x101e44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x101e447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x101e44c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x101e45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x101e454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x101e45960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x101e45dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x101e46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x101e466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x101e46b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x101e46f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x101e47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x101e47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x101e47ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101e48150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101e485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101e48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x101e48ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x101e49310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101e49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101e49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x101e4a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x101e4a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101e4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101e4adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x101e4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101e4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101e4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101e4bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101e4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101e4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101e4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101e4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101e4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101e4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101e4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101e4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101e4e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101e4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101e4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101e4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101e4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x101e4fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x101e50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101e50670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x101e50ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101e50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101e513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x101e51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x101e51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x101e52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x101e52580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x101e529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x101e52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x101e532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x101e53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x101e53bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x101e54020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101e54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x101e54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x101e54d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x101e551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x101e55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x101e55ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x101e55f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x101e563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x101e56810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x101e56c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x101e570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x101e57560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x101e579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101e57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101e582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x101e58720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x101e58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101e59000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x101e59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x101e598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101e59d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101e5a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101e5a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101e5aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101e5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101e5b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101e5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101e5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x101e5c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101e5c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101e5c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101e5ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101e5d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101e5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101e5db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101e5dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101e5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101e5e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101e5ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x101e5f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x101e5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x101e5fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x101e5fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101e60360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x101e607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x101e60c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x101e610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x101e61520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x101e61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x101e62110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x101e62580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x101e629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x101e62e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x101e632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x101e63740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x101e63bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x101e64020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x101e64490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x101e64900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x101e64d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x101e651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x101e65650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101e65ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101e65f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x101e663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x101e66810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101e66c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x101e670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x101e67560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101e679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x101e67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x101e682b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101e68720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101e0af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101e0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101e0baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101e0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101e0c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101e0c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101e0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x101e0d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101e0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101e17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101e179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101e17e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x101e182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x101e18720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101e18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x101e19000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x101e19470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101e198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x101e19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x101e1a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x101e1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x101e1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x101e1af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x101e1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x101e1b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x101e1bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x101e1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x101e1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x101e1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x101e1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x101e1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x101e1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x101e1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x101e1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x101e1e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x101e1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101e1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x101e1f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101e1f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101e1fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101e1fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101e20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x101e207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101e20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x101e210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x101e21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101e21990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x101e21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101e22270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101e22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x101e23050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101e23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x101e23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x101e242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101e15f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101e161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101e16650 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101e17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x101e17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101e18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101e188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101e18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101e191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x101e19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x101e19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x101e19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x101e1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x101e1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x101e1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x101e1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x101e1be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x101e1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x101e1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x101e1d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x101e1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x101e1e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x101e1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x101e1f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101e1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101e20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101e20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x101e20df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x101e21260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101e216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101e21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101e21fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101e22420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x101e22890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101e22d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x101e23170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101e23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x101e238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101e23d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101e24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101e245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101e0a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101e0baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101e0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101e0c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101e0c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101e0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x101e0d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101e0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x101e0af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101e25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x101e25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x101e25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101e25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x101e262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x101e26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x101e26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x101e27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x101e274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x101e27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x101e27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x101e281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x101e28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x101e28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x101e28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x101e293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x101e29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x101e29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101e2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101e2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101e2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x101e2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101e2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x101e2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x101e2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101e2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x101e2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x101e2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101e2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x101e2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x101e2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101e2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101e2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x101e2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101e2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101e2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101e2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101e2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101e2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101e2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101e302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101e30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101e30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x101e31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x101e318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101e31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x101e321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x101e32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101e32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x101e32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x101e33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x101e337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x101e33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x101e340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x101e34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x101e349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x101e34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x101e35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x101e356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x101e35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x101e35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x101e36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x101e368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x101e36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x101e37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x101e37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x101e37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x101e37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x101e38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101e387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101e38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101e390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x101e39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x101e39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101e39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101e3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x101e3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x101e3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101e3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101e3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x101e3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101e3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101e3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101e3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101e3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101e3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101e3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101e3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101e3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101e3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101e3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101e3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101e3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101e3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101e3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101e3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101e3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x101e40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x101e40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101e40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x101e41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101e415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101e41a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x101e41ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x101e42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x101e42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x101e42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x101e43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x101e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x101e43940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x101e43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x101e44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x101e44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101e44b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x101e44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x101e453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x101e45850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x101e45cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x101e46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x101e465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x101e46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x101e46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x101e472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x101e47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x101e47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x101e48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101e484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101e48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x101e48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x101e49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101e49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x101e49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x101e49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101e4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101e4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101e4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101e4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101e4b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101e4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101e4c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x101e4c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101e4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101e4d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101e4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101e4d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101e4dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101e4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101e4e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101e4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101e4ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101e4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x101e4f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x101e4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x101e500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x101e50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101e509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x101e50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x101e512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x101e51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x101e51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x101e52000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x101e52780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x101e52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x101e53060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x101e534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x101e53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x101e53db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x101e54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x101e54690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x101e54b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x101e54f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x101e553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x101e55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x101e55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101e56130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101e565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x101e56a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x101e56e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101e572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x101e57760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x101e57bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101e58040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x101e584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x101e58920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101e58d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101e59200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101e59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101e59ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101e59f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101e5a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101e5a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101e5aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x101e5b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101e5b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101e5b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101e5be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101e5c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x101e5c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x101e5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101e5d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x101e5d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x101e5d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101e5dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x101e5e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x101e5e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x101e5eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x101e5ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x101e5f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x101e5f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x101e5fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x101e600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x101e60560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x101e609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x101e60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x101e612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x101e61720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x101e61b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x101e62000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x101e62470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x101e628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x101e62d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101e631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x101e63630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101e63aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101e63f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101e64380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101e647f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x101e64c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101e650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x101e65540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x101e659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101e65e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x101e66290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101e66700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101e66f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x101e67650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101e67d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x101e68430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x101e098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101e15f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101e16390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101e16800 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.817s
user	0m0.294s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4389 (09fe2e76)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15660b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15660bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15660c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15660c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15660cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15660d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15660d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15660dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15660e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15660e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15660eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15660f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15660fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156610470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156610c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1566113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156611ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1566121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156612900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1566130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1566137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156613f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156614630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156614ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1566155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1566158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156615ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156616b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156617070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156617330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1566177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156618860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156618b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156619460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156619900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156619da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15661a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15661a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15661ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15661b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15661b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15661b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15661bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15661c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15661ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15661d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15661d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15661def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15661e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15661eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15661f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15661f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15661fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156620250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156620510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156620b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156621310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1566215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156621a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1566223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156622850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156622cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156623190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156623630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156623ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156623f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156624410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1566248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156624d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1566252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1566257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156625d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156626290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1566267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156626d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156627280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1566277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156627d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156628270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1566287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156628d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156629260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1566297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156629d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15662a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15662a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15662acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15662b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15662b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15662bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15662c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15662c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15662ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15661c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15662d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15662d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15662de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15662e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15662e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15662ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15662f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15662f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15662fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156630370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1566308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156630e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156631360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1566318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156631e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1566322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156632740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156632be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156633080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156633520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1566339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156633e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156634300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1566347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156634c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1566350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156635580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156635a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156635ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156636360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156636800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156636ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1566375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156637a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156637f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1566383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156638860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156638d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1566391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156639640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156639ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156639f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15663a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15663a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15663ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15663b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15663b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15663bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15663bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15663c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15663c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15663cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15663d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15663d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15663dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15663e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15663e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15663e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15663ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15663f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15663f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15663fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1566400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156640540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1566409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156640e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156641320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1566417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156641c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1566425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156642a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156642ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156643380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156643820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156643cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156644160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156644600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156644aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156644f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1566453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156645880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156645d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1566461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156646660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156646b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156646fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156647440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1566478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156647d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156648220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1566486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156648b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156649000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156649550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156649aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156649ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15664a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15664a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15664ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15664b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15664ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15664c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15664c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15664c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15664cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15664d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15664dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15664e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15664e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15664eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15664f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15664f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15664fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156650310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156650860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156650db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156651300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156651850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156651da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1566522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156652840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156652d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1566532e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156653830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156653d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1566542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156654820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156654d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1566552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156655810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156655d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1566562b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156656800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156656d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1566572a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1566577f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156657d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156658290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1566587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156658d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156659280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1566597d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156659d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15665a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15665a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15665ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15665b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15665b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15665bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15665c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15665c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15665ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15665d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15665d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15665dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15665e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15665e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15665ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15665f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15665f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15665fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156660210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156660760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156660cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156661200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156661750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156661ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156662140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1566625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156662a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156662f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1566633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156663860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156663d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1566641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156664640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156664ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156664f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156665420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1566658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156665d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156666200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156666750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156666e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156667590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156667cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1566683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156668690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156668e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156669140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156669750 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146606670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146606ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146606f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1466073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146607830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146607ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146608110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1466043f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1466046b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146604b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146604f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146608910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146609300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146609ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14660a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14660a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14660b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14660b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14660bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14660c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14660d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14660d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14660de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14660e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14660ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14660ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14660f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14660fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146610180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146610970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146610e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1466110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146611960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146611ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146612160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146612600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146612aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146612f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1466133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146613880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1466141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146614660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146614b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146614dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1466153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1466159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146615ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146616600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146616c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146617220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146617830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146617e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146618c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1466190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146619580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146619840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146619e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14661a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14661aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14661af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14661b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14661b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14661bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14661c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14661c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14661cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14661cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14661d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14661d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14661ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14661e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14661e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14661ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14661f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14661f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14661fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146620240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146620790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146620ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146621230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146621780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146621cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146622220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146622770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146622cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146623210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146623760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146623cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146624200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146624750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146624ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1466251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146625740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146625c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1466261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146626730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146626c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1466271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146627720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146627c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1466281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146628710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146628c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1466291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146629700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14662a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14662a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14662ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14662b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14662b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14662bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14662c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14662c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14662c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14662ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14662d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14662d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14662dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14662e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14662e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14662e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14662ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14662f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14662f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14662fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1466300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146630580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146630ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146631360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146631800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146631ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146632140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1466325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146632a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146632f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1466333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146633860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146633d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1466341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146634640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146634ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146634f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146635420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1466358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146635d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146636200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1466366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146636b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146636fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146637480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146637920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146637dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146638260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146638700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146638ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146639040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1466394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146639980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146639e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14663a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14663a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14663ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14663b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14663b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14663b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14663be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14663c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14663c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14663cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14663d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14663d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14663da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14663dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14663e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14663e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14663ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14663f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14663f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14663faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14663ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1466403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146640880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146640d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1466411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146641660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146641b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146641fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146642440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1466428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146642e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146643380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1466438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146643e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1466440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1466446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146644d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146645310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146645b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146645fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146646260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146646870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146646e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146647670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146647b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146647fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146648450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146648c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146649150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1466496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146649bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14664a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14664a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14664abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14664b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14664b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14664bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14664c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14664c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14664cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14664d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14664d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14664dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14664e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14664e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14664eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14664f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14664f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14664fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1466500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146650630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146650b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1466510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146651620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146651b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1466520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146652610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146652b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1466530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146653600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146653b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1466540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1466545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146654b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146655090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1466555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146655b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146656080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1466565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146656b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146657070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1466575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146657b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146658060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1466585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146658b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146659050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1466595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146659af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14665a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14665a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14665aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14665b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14665b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14665ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14665bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14665c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14665c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14665cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14665d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14665d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14665da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14665df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14665e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14665e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14665ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14665f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14665f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14665fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146660030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146660750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146660e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146661590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146661cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146661f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146662760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146662a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146663030 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158005c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1580060c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158006530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1580069a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158006e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158007280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1580076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158007b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158007fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158008440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1580088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158008f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158009a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15800a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15800a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15800b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15800b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15800bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15800c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15800ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15800d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15800dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15800e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15800eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15800f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15800f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15800f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15800fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158010040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1580104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158010920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158010e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1580112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158011580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1580119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158011e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1580122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158012740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158012bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158013020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158013490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158013900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158013d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1580141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158014650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158014ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158014f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1580153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158015810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158015c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1580160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158016560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1580169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158016e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1580172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158017720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158017c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158018190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158018600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158018a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158018ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158019350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1580197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158019c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15801a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15801a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15801a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15801adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15801b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15801b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15801bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15801bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15801c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15801c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15801cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15801d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15801d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15801da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15801dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15801e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15801e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15801ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15801f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15801f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15801f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15801fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158020240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1580206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158020b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158020f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158021400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158021870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158021ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158022150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1580225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158022a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158022ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158023310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158023780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158023bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158024060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1580244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158024940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158024db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158025220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158025690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158025b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158025f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1580263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158026850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158026cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158027130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1580275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158027a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158027e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1580282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158028760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158028bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158029040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1580294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158029920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158029d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15802a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15802a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15802aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15802af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15802b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15802b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15802bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15802c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15802c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15802c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15802ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15802d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15802d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15802dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15802e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15802e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15802e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15802ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15802f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15802f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15802fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15802ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1580303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158030810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158030c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1580310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158031560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1580319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158031e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1580322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158032720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158032b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158033000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158033470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1580338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158033d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1580341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158034630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158034aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158034f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158035380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1580357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158035c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1580360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158036540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1580369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158036e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158037290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158037700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158037b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158037fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158038450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1580388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158038d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1580391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158039610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158039a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158039ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15803a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15803a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15803ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15803b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15803b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15803b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15803be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15803c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15803c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15803cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15803cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15803d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15803d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15803dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15803e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15803e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15803ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15803eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15803f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15803f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15803fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158040090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158040500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158040970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158040de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158041250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1580416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158041c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1580420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158042530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158043080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158043340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158043600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158043a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158043ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158044350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1580447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158044c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1580450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158045510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158045980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158045df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158046260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1580466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158046b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158046fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158047420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158047890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158047d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158048170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1580485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158048ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158049330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1580497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158049c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15804a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15804a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15804a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15804add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15804b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15804b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15804bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15804bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15804c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15804cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15804d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15804d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15804d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15804dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15804e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15804e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15804ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15804ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15804f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15804f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15804fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158050100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158050570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1580509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158050e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1580512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158051730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158051ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158052010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158052480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1580528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158052d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1580531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158053640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158053ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158053f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158054390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158054800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158054c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1580550e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158055550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1580559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158055e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1580562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158056710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158056b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158056ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158057a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158058180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1580588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158058fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158059280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1580596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158059cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15805a300 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.929s
user	0m0.244s
sys	0m0.140s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
