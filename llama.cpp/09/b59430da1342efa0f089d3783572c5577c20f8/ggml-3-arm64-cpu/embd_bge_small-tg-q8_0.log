+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is'
main: build = 2137 (09b59430)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1707816307
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                        bert.context_length u32              = 512
llama_model_loader: - kv   3:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   4:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                      bert.attention.causal bool             = false
llama_model_loader: - kv   9:                          general.file_type u32              = 7
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  12:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q8_0:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 34.00 MiB (8.59 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =    34.00 MiB
................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.76 MiB
llama_new_context_with_model:        CPU compute buffer size =    13.50 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 

llama_print_timings:        load time =      24.11 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       9.09 ms /     9 tokens (    1.01 ms per token,   989.77 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =       8.69 ms /    10 tokens
-0.014024 -0.016028 0.012108 -0.016546 0.017559 0.011523 0.134053 0.044605 0.092373 -0.004432 -0.014648 -0.116163 0.006193 0.050288 0.047935 0.038291 0.017821 -0.003639 -0.071757 0.019125 0.078772 -0.058552 -0.044540 -0.012717 -0.001074 0.055904 0.008532 -0.019515 -0.018127 -0.093898 -0.000643 -0.021850 0.108203 -0.035487 -0.031325 0.000552 -0.054941 0.054907 -0.018553 0.025648 0.044663 -0.019874 -0.011381 0.034729 -0.020230 -0.037718 -0.011101 -0.004271 -0.011702 -0.024721 0.013417 -0.011062 -0.033550 0.074142 -0.083593 0.032080 0.018991 0.013966 0.069277 0.013114 0.055613 0.063568 -0.245268 0.097803 0.027302 -0.003513 -0.029280 0.015317 0.036771 0.007792 0.011839 -0.027380 0.005682 0.096366 0.040920 -0.057500 0.032224 -0.070953 -0.007083 0.017111 0.016250 -0.011751 -0.023209 -0.028958 -0.028116 -0.003972 -0.001216 -0.046333 0.062354 0.003649 -0.054252 -0.006774 -0.057691 0.061214 0.016854 0.011034 -0.050634 0.009341 0.002801 0.132391 -0.066186 0.009459 0.053962 -0.028973 -0.045831 -0.017944 -0.025191 -0.067961 -0.028856 -0.050001 -0.019918 -0.010567 -0.020635 0.025574 -0.004344 0.057837 0.059335 0.013269 0.057473 -0.060597 -0.052817 0.026057 0.046464 -0.013137 -0.063480 -0.080926 0.082703 0.051666 0.037209 0.017283 0.068387 -0.059643 -0.069921 -0.027368 -0.016865 0.016438 0.038364 0.019217 0.004526 -0.021969 -0.039284 -0.101874 -0.064133 -0.075201 -0.020903 0.038644 -0.036812 -0.026992 0.010497 0.035783 0.022307 0.032978 0.025279 0.067164 0.019312 -0.022041 -0.012274 -0.014353 -0.000738 0.058928 0.001737 0.002694 -0.049819 0.056552 -0.030466 0.036874 -0.060123 -0.004864 0.033149 0.007467 0.077970 0.028842 0.047676 0.063145 0.063323 0.028512 -0.005374 0.028364 0.052810 -0.010807 0.087010 -0.036199 -0.018091 0.060731 0.036452 0.010778 -0.042224 -0.010504 -0.016498 -0.004162 0.051862 0.103735 0.017761 0.033443 -0.028890 -0.052833 -0.004954 0.003019 0.000844 -0.056594 0.037634 -0.017570 -0.007361 0.035859 -0.006070 -0.037663 0.009593 0.045745 0.021969 -0.045470 -0.004599 -0.042860 0.089792 0.000235 0.001259 -0.024626 0.064016 0.046228 0.018536 0.018585 0.012891 -0.052359 -0.040896 -0.255266 0.004622 0.007733 -0.023208 -0.035050 -0.020054 0.002859 -0.005634 0.045762 0.033518 0.075331 -0.007337 0.000549 0.020447 -0.020742 -0.011915 -0.018146 -0.040606 -0.024769 0.071059 -0.031669 0.017445 -0.031921 -0.099708 0.016153 0.041654 0.194436 0.125868 -0.032740 0.059421 -0.042229 0.015351 -0.011487 -0.144826 0.035205 0.021806 -0.020491 -0.067527 -0.016051 -0.047799 0.009263 0.029318 -0.055743 -0.086632 -0.005727 -0.039219 -0.011734 0.042889 -0.004577 0.012790 0.068604 0.024685 0.026453 0.044349 0.011317 -0.027701 -0.102197 -0.019237 -0.054549 0.031440 0.022742 0.025021 0.017882 -0.009017 0.063677 0.001430 -0.050730 -0.040037 0.017100 -0.025188 -0.003337 0.000126 -0.005475 -0.080401 0.105761 -0.002301 -0.020777 -0.030119 0.030468 -0.003597 -0.032625 -0.004140 0.020449 -0.009559 0.025587 0.053093 0.009190 -0.001779 0.066037 -0.040632 -0.029672 0.001553 0.004525 -0.100785 -0.030565 -0.064200 -0.258443 0.059519 -0.028325 0.045765 -0.009394 0.065301 0.001050 0.068103 -0.085712 -0.044780 -0.007970 0.028898 0.036003 0.042189 -0.040479 0.025685 0.019486 -0.074039 0.017535 -0.037700 -0.018305 0.043594 0.239382 -0.032403 0.008623 0.000084 -0.036931 0.022579 -0.020227 -0.014797 -0.002224 -0.030398 0.032371 -0.010695 0.016028 -0.027797 -0.034246 -0.013953 0.012885 -0.040070 -0.018962 0.010447 -0.073382 0.009669 0.151155 -0.021900 -0.021540 -0.015673 -0.024195 -0.000880 -0.020912 0.033446 -0.033052 -0.005957 0.059157 0.027743 0.034567 -0.039818 0.008027 -0.034205 -0.033118 0.034659 0.027499 0.039721 -0.027642 

real	0m0.096s
user	0m0.099s
sys	0m0.032s
