### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.45 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.28 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.24 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.50 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.84 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.34 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.16 sec*proc (28 tests)

Total Test time (real) = 222.17 sec

real	3m42.254s
user	7m37.768s
sys	0m6.799s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.54 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.34 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.45 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.14 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.89 sec*proc (28 tests)

Total Test time (real) =  51.90 sec

real	0m51.930s
user	1m12.199s
sys	0m5.704s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.135 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.921 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.181 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.191 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.192 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.193 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.194 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.195 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.195 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.196 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.197 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.201 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.205 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.206 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.206 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.207 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.207 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.208 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.209 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.222 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.481 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.482 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.483 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.484 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.484 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.030.485 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.485 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.030.486 I llama_model_loader: - type  f32:  124 tensors
0.00.030.486 I llama_model_loader: - type  f16:   73 tensors
0.00.035.085 I llm_load_vocab: special tokens cache size = 5
0.00.037.534 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.037.538 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.037.539 I llm_load_print_meta: arch             = bert
0.00.037.539 I llm_load_print_meta: vocab type       = WPM
0.00.037.540 I llm_load_print_meta: n_vocab          = 30522
0.00.037.540 I llm_load_print_meta: n_merges         = 0
0.00.037.540 I llm_load_print_meta: vocab_only       = 0
0.00.037.541 I llm_load_print_meta: n_ctx_train      = 512
0.00.037.541 I llm_load_print_meta: n_embd           = 384
0.00.037.548 I llm_load_print_meta: n_layer          = 12
0.00.037.552 I llm_load_print_meta: n_head           = 12
0.00.037.553 I llm_load_print_meta: n_head_kv        = 12
0.00.037.553 I llm_load_print_meta: n_rot            = 32
0.00.037.553 I llm_load_print_meta: n_swa            = 0
0.00.037.553 I llm_load_print_meta: n_embd_head_k    = 32
0.00.037.553 I llm_load_print_meta: n_embd_head_v    = 32
0.00.037.554 I llm_load_print_meta: n_gqa            = 1
0.00.037.555 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.037.556 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.037.557 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.037.558 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.037.558 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.037.559 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.037.560 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.037.563 I llm_load_print_meta: n_ff             = 1536
0.00.037.563 I llm_load_print_meta: n_expert         = 0
0.00.037.564 I llm_load_print_meta: n_expert_used    = 0
0.00.037.564 I llm_load_print_meta: causal attn      = 0
0.00.037.564 I llm_load_print_meta: pooling type     = 2
0.00.037.564 I llm_load_print_meta: rope type        = 2
0.00.037.565 I llm_load_print_meta: rope scaling     = linear
0.00.037.565 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.037.566 I llm_load_print_meta: freq_scale_train = 1
0.00.037.566 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.037.566 I llm_load_print_meta: rope_finetuned   = unknown
0.00.037.567 I llm_load_print_meta: ssm_d_conv       = 0
0.00.037.568 I llm_load_print_meta: ssm_d_inner      = 0
0.00.037.568 I llm_load_print_meta: ssm_d_state      = 0
0.00.037.568 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.037.573 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.037.573 I llm_load_print_meta: model type       = 33M
0.00.037.574 I llm_load_print_meta: model ftype      = F16
0.00.037.574 I llm_load_print_meta: model params     = 33.21 M
0.00.037.575 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.037.576 I llm_load_print_meta: general.name     = Bge Small
0.00.037.576 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.037.576 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.037.577 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.037.577 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.037.578 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.037.578 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.037.578 I llm_load_print_meta: max token length = 21
0.00.039.667 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.670 I llm_load_tensors: offloading output layer to GPU
0.00.039.670 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.697 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.699 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.297 I llama_new_context_with_model: n_seq_max     = 1
0.00.040.299 I llama_new_context_with_model: n_ctx         = 512
0.00.040.299 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.040.299 I llama_new_context_with_model: n_batch       = 2048
0.00.040.300 I llama_new_context_with_model: n_ubatch      = 2048
0.00.040.300 I llama_new_context_with_model: flash_attn    = 0
0.00.040.301 I llama_new_context_with_model: freq_base     = 10000.0
0.00.040.301 I llama_new_context_with_model: freq_scale    = 1
0.00.040.302 I ggml_metal_init: allocating
0.00.040.313 I ggml_metal_init: found device: Apple M4
0.00.040.317 I ggml_metal_init: picking default device: Apple M4
0.00.041.237 I ggml_metal_init: using embedded metal library
0.00.045.402 I ggml_metal_init: GPU name:   Apple M4
0.00.045.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.405 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.406 I ggml_metal_init: simdgroup reduction   = true
0.00.045.406 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.407 I ggml_metal_init: has bfloat            = true
0.00.045.407 I ggml_metal_init: use bfloat            = true
0.00.045.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.092 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.058.752 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.754 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.756 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.059.653 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.059.654 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.059.655 I llama_new_context_with_model: graph nodes  = 429
0.00.059.655 I llama_new_context_with_model: graph splits = 2
0.00.059.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.059.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.148 I 
0.00.066.164 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.839 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.620 I llama_perf_context_print:        load time =      46.22 ms
0.00.071.622 I llama_perf_context_print: prompt eval time =       4.62 ms /     9 tokens (    0.51 ms per token,  1950.16 tokens per second)
0.00.071.622 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.623 I llama_perf_context_print:       total time =       5.47 ms /    10 tokens
0.00.071.787 I ggml_metal_free: deallocating

real	0m0.259s
user	0m0.050s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.041 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.454 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.610 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.613 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.615 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.616 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.616 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.616 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.617 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.618 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.618 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.618 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.619 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.621 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.621 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.622 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.622 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.622 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.623 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.623 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.860 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.861 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.862 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.862 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.862 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.863 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.863 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.863 I llama_model_loader: - type  f32:  124 tensors
0.00.014.864 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.542 I llm_load_vocab: special tokens cache size = 5
0.00.018.861 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.864 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.864 I llm_load_print_meta: arch             = bert
0.00.018.865 I llm_load_print_meta: vocab type       = WPM
0.00.018.865 I llm_load_print_meta: n_vocab          = 30522
0.00.018.865 I llm_load_print_meta: n_merges         = 0
0.00.018.865 I llm_load_print_meta: vocab_only       = 0
0.00.018.865 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.866 I llm_load_print_meta: n_embd           = 384
0.00.018.866 I llm_load_print_meta: n_layer          = 12
0.00.018.869 I llm_load_print_meta: n_head           = 12
0.00.018.869 I llm_load_print_meta: n_head_kv        = 12
0.00.018.870 I llm_load_print_meta: n_rot            = 32
0.00.018.870 I llm_load_print_meta: n_swa            = 0
0.00.018.870 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.870 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.871 I llm_load_print_meta: n_gqa            = 1
0.00.018.871 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.872 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.872 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.873 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.873 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.873 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.873 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.874 I llm_load_print_meta: n_ff             = 1536
0.00.018.874 I llm_load_print_meta: n_expert         = 0
0.00.018.874 I llm_load_print_meta: n_expert_used    = 0
0.00.018.874 I llm_load_print_meta: causal attn      = 0
0.00.018.874 I llm_load_print_meta: pooling type     = 2
0.00.018.874 I llm_load_print_meta: rope type        = 2
0.00.018.874 I llm_load_print_meta: rope scaling     = linear
0.00.018.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.875 I llm_load_print_meta: freq_scale_train = 1
0.00.018.875 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.876 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.876 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.876 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.876 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.876 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.876 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.877 I llm_load_print_meta: model type       = 33M
0.00.018.877 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.877 I llm_load_print_meta: model params     = 33.21 M
0.00.018.878 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.880 I llm_load_print_meta: general.name     = Bge Small
0.00.018.881 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.881 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.881 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.881 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.881 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.881 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.882 I llm_load_print_meta: max token length = 21
0.00.020.213 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.213 I llm_load_tensors: offloading output layer to GPU
0.00.020.213 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.220 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.221 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.570 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.571 I llama_new_context_with_model: n_ctx         = 512
0.00.020.571 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.571 I llama_new_context_with_model: n_batch       = 2048
0.00.020.572 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.572 I llama_new_context_with_model: flash_attn    = 0
0.00.020.572 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.572 I llama_new_context_with_model: freq_scale    = 1
0.00.020.573 I ggml_metal_init: allocating
0.00.020.576 I ggml_metal_init: found device: Apple M4
0.00.020.578 I ggml_metal_init: picking default device: Apple M4
0.00.021.189 I ggml_metal_init: using embedded metal library
0.00.023.806 I ggml_metal_init: GPU name:   Apple M4
0.00.023.807 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.808 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.809 I ggml_metal_init: simdgroup reduction   = true
0.00.023.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.809 I ggml_metal_init: has bfloat            = true
0.00.023.809 I ggml_metal_init: use bfloat            = true
0.00.023.809 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.216 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.718 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.721 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.722 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.357 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.358 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.358 I llama_new_context_with_model: graph nodes  = 429
0.00.035.358 I llama_new_context_with_model: graph splits = 2
0.00.035.360 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.537 I 
0.00.040.562 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.128 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.599 I llama_perf_context_print:        load time =      31.08 ms
0.00.045.600 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2077.08 tokens per second)
0.00.045.601 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.601 I llama_perf_context_print:       total time =       5.06 ms /    10 tokens
0.00.045.746 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.273 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.729 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.470 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.476 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.478 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.479 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.479 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.480 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.481 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.482 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.483 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.483 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.487 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.488 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.491 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.492 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.492 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.493 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.493 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.083 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.724 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.724 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.724 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.724 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.725 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.725 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.725 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.726 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.727 I llama_model_loader: - type  f32:   40 tensors
0.00.048.727 I llama_model_loader: - type  f16:   30 tensors
0.00.066.266 W llm_load_vocab: empty token at index 5
0.00.070.726 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.000 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.031 I llm_load_vocab: special tokens cache size = 5
0.00.335.041 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.335.050 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.335.050 I llm_load_print_meta: arch             = jina-bert-v2
0.00.335.051 I llm_load_print_meta: vocab type       = BPE
0.00.335.052 I llm_load_print_meta: n_vocab          = 61056
0.00.335.053 I llm_load_print_meta: n_merges         = 39382
0.00.335.055 I llm_load_print_meta: vocab_only       = 0
0.00.335.055 I llm_load_print_meta: n_ctx_train      = 8192
0.00.335.055 I llm_load_print_meta: n_embd           = 384
0.00.335.056 I llm_load_print_meta: n_layer          = 4
0.00.335.063 I llm_load_print_meta: n_head           = 12
0.00.335.064 I llm_load_print_meta: n_head_kv        = 12
0.00.335.064 I llm_load_print_meta: n_rot            = 32
0.00.335.064 I llm_load_print_meta: n_swa            = 0
0.00.335.065 I llm_load_print_meta: n_embd_head_k    = 32
0.00.335.065 I llm_load_print_meta: n_embd_head_v    = 32
0.00.335.066 I llm_load_print_meta: n_gqa            = 1
0.00.335.072 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.335.073 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.335.074 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.335.074 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.335.074 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.335.075 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.335.075 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.335.075 I llm_load_print_meta: n_ff             = 1536
0.00.335.076 I llm_load_print_meta: n_expert         = 0
0.00.335.076 I llm_load_print_meta: n_expert_used    = 0
0.00.335.076 I llm_load_print_meta: causal attn      = 0
0.00.335.076 I llm_load_print_meta: pooling type     = -1
0.00.335.076 I llm_load_print_meta: rope type        = -1
0.00.335.076 I llm_load_print_meta: rope scaling     = linear
0.00.335.076 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.335.077 I llm_load_print_meta: freq_scale_train = 1
0.00.335.077 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.335.077 I llm_load_print_meta: rope_finetuned   = unknown
0.00.335.077 I llm_load_print_meta: ssm_d_conv       = 0
0.00.335.077 I llm_load_print_meta: ssm_d_inner      = 0
0.00.335.078 I llm_load_print_meta: ssm_d_state      = 0
0.00.335.078 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.335.078 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.335.078 I llm_load_print_meta: model type       = 33M
0.00.335.079 I llm_load_print_meta: model ftype      = F16
0.00.335.079 I llm_load_print_meta: model params     = 32.90 M
0.00.335.080 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.335.080 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.335.080 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.335.080 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.335.083 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.335.083 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.335.085 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.335.085 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.335.085 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.335.085 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.335.085 I llm_load_print_meta: max token length = 45
0.00.336.400 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.336.400 I llm_load_tensors: offloading output layer to GPU
0.00.336.400 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.336.425 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.336.426 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.337.319 I llama_new_context_with_model: n_seq_max     = 1
0.00.337.320 I llama_new_context_with_model: n_ctx         = 8192
0.00.337.320 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.337.320 I llama_new_context_with_model: n_batch       = 2048
0.00.337.321 I llama_new_context_with_model: n_ubatch      = 2048
0.00.337.321 I llama_new_context_with_model: flash_attn    = 0
0.00.337.321 I llama_new_context_with_model: freq_base     = 10000.0
0.00.337.322 I llama_new_context_with_model: freq_scale    = 1
0.00.337.322 I ggml_metal_init: allocating
0.00.337.326 I ggml_metal_init: found device: Apple M4
0.00.337.327 I ggml_metal_init: picking default device: Apple M4
0.00.338.350 I ggml_metal_init: using embedded metal library
0.00.341.224 I ggml_metal_init: GPU name:   Apple M4
0.00.341.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.228 I ggml_metal_init: simdgroup reduction   = true
0.00.341.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.228 I ggml_metal_init: has bfloat            = true
0.00.341.229 I ggml_metal_init: use bfloat            = true
0.00.341.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.230 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.595 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.353.061 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.353.063 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.353.067 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.353.653 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.353.654 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.353.655 I llama_new_context_with_model: graph nodes  = 154
0.00.353.655 I llama_new_context_with_model: graph splits = 2
0.00.353.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.488 I 
0.00.366.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.366.776 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.366.777 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.366.786 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.366.786 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.366.791 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.366.791 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.367.291 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.370.925 I llama_perf_context_print:        load time =     341.75 ms
0.00.370.926 I llama_perf_context_print: prompt eval time =       3.63 ms /    62 tokens (    0.06 ms per token, 17098.73 tokens per second)
0.00.370.927 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.370.928 I llama_perf_context_print:       total time =       4.44 ms /    63 tokens
0.00.371.101 I ggml_metal_free: deallocating

real	0m1.088s
user	0m0.340s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.182 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.291 I main: llama backend init
0.00.000.298 I main: load the model and apply lora adapter, if any
0.00.029.999 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.227 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.248 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.253 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.254 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.256 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.257 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.257 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.258 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.260 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.715 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.699 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.703 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.704 I llama_model_loader: - type  f32:  194 tensors
0.00.059.705 I llama_model_loader: - type  f16:   98 tensors
0.00.091.278 I llm_load_vocab: special tokens cache size = 25
0.00.098.323 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.326 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.327 I llm_load_print_meta: arch             = gptneox
0.00.098.327 I llm_load_print_meta: vocab type       = BPE
0.00.098.327 I llm_load_print_meta: n_vocab          = 50304
0.00.098.327 I llm_load_print_meta: n_merges         = 50009
0.00.098.327 I llm_load_print_meta: vocab_only       = 0
0.00.098.328 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.328 I llm_load_print_meta: n_embd           = 2048
0.00.098.328 I llm_load_print_meta: n_layer          = 24
0.00.098.331 I llm_load_print_meta: n_head           = 16
0.00.098.332 I llm_load_print_meta: n_head_kv        = 16
0.00.098.332 I llm_load_print_meta: n_rot            = 32
0.00.098.332 I llm_load_print_meta: n_swa            = 0
0.00.098.332 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.332 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.333 I llm_load_print_meta: n_gqa            = 1
0.00.098.334 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.334 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.335 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.335 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.335 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.335 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.335 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.336 I llm_load_print_meta: n_ff             = 8192
0.00.098.336 I llm_load_print_meta: n_expert         = 0
0.00.098.337 I llm_load_print_meta: n_expert_used    = 0
0.00.098.338 I llm_load_print_meta: causal attn      = 1
0.00.098.338 I llm_load_print_meta: pooling type     = 0
0.00.098.338 I llm_load_print_meta: rope type        = 2
0.00.098.338 I llm_load_print_meta: rope scaling     = linear
0.00.098.338 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.339 I llm_load_print_meta: freq_scale_train = 1
0.00.098.339 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.341 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.341 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.341 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.341 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.341 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.341 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.341 I llm_load_print_meta: model type       = 1.4B
0.00.098.342 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.342 I llm_load_print_meta: model params     = 1.41 B
0.00.098.343 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.343 I llm_load_print_meta: general.name     = 1.4B
0.00.098.343 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.343 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.343 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.344 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.344 I llm_load_print_meta: LF token         = 128 ''
0.00.098.345 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.345 I llm_load_print_meta: max token length = 1024
0.00.100.921 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.921 I llm_load_tensors: offloading output layer to GPU
0.00.100.921 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.940 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.941 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.928 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.929 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.929 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.930 I llama_new_context_with_model: n_batch       = 2048
0.00.101.930 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.930 I llama_new_context_with_model: flash_attn    = 0
0.00.101.930 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.931 I llama_new_context_with_model: freq_scale    = 1
0.00.101.931 I ggml_metal_init: allocating
0.00.101.942 I ggml_metal_init: found device: Apple M4
0.00.101.944 I ggml_metal_init: picking default device: Apple M4
0.00.102.638 I ggml_metal_init: using embedded metal library
0.00.112.797 I ggml_metal_init: GPU name:   Apple M4
0.00.112.799 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.112.799 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.112.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.112.800 I ggml_metal_init: simdgroup reduction   = true
0.00.112.800 I ggml_metal_init: simdgroup matrix mul. = true
0.00.112.800 I ggml_metal_init: has bfloat            = true
0.00.112.800 I ggml_metal_init: use bfloat            = true
0.00.112.801 I ggml_metal_init: hasUnifiedMemory      = true
0.00.112.801 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.459 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.157.871 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.157.878 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.157.901 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.158.892 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.158.894 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.158.894 I llama_new_context_with_model: graph nodes  = 967
0.00.158.895 I llama_new_context_with_model: graph splits = 2
0.00.158.898 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.159.040 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.159.040 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.240.275 I main: llama threadpool init, n_threads = 4
0.00.240.321 I 
0.00.240.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.240.347 I 
0.00.240.420 I sampler seed: 1234
0.00.240.425 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.240.461 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.240.463 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.240.463 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.082.678 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.02.082.679 I llama_perf_context_print:        load time =     210.26 ms
0.02.082.680 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.57 tokens per second)
0.02.082.681 I llama_perf_context_print:        eval time =    1795.71 ms /    63 runs   (   28.50 ms per token,    35.08 tokens per second)
0.02.082.682 I llama_perf_context_print:       total time =    1842.41 ms /    70 tokens
0.02.082.926 I ggml_metal_free: deallocating

real	0m2.457s
user	0m0.146s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.568 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.702 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.140 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.167 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.169 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.169 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.171 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.172 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.175 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.359 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.424 I llama_model_loader: - type  f32:  194 tensors
0.00.053.424 I llama_model_loader: - type  f16:   98 tensors
0.00.083.434 I llm_load_vocab: special tokens cache size = 25
0.00.090.225 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.228 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.228 I llm_load_print_meta: arch             = gptneox
0.00.090.229 I llm_load_print_meta: vocab type       = BPE
0.00.090.229 I llm_load_print_meta: n_vocab          = 50304
0.00.090.229 I llm_load_print_meta: n_merges         = 50009
0.00.090.229 I llm_load_print_meta: vocab_only       = 0
0.00.090.229 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.229 I llm_load_print_meta: n_embd           = 2048
0.00.090.230 I llm_load_print_meta: n_layer          = 24
0.00.090.232 I llm_load_print_meta: n_head           = 16
0.00.090.233 I llm_load_print_meta: n_head_kv        = 16
0.00.090.233 I llm_load_print_meta: n_rot            = 32
0.00.090.233 I llm_load_print_meta: n_swa            = 0
0.00.090.233 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.233 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.234 I llm_load_print_meta: n_gqa            = 1
0.00.090.235 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.235 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.236 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.236 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.236 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.237 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.237 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.237 I llm_load_print_meta: n_ff             = 8192
0.00.090.237 I llm_load_print_meta: n_expert         = 0
0.00.090.238 I llm_load_print_meta: n_expert_used    = 0
0.00.090.238 I llm_load_print_meta: causal attn      = 1
0.00.090.238 I llm_load_print_meta: pooling type     = 0
0.00.090.238 I llm_load_print_meta: rope type        = 2
0.00.090.238 I llm_load_print_meta: rope scaling     = linear
0.00.090.239 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.239 I llm_load_print_meta: freq_scale_train = 1
0.00.090.239 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.242 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.242 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.242 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.242 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.242 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.242 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.242 I llm_load_print_meta: model type       = 1.4B
0.00.090.243 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.243 I llm_load_print_meta: model params     = 1.41 B
0.00.090.244 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.244 I llm_load_print_meta: general.name     = 1.4B
0.00.090.244 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.244 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.249 I llm_load_print_meta: LF token         = 128 ''
0.00.090.249 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.249 I llm_load_print_meta: max token length = 1024
0.00.092.109 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.109 I llm_load_tensors: offloading output layer to GPU
0.00.092.109 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.119 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.121 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.041 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.042 I llama_new_context_with_model: n_ctx         = 128
0.00.093.042 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.043 I llama_new_context_with_model: n_batch       = 128
0.00.093.043 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.043 I llama_new_context_with_model: flash_attn    = 0
0.00.093.043 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.044 I llama_new_context_with_model: freq_scale    = 1
0.00.093.044 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.045 I ggml_metal_init: allocating
0.00.093.051 I ggml_metal_init: found device: Apple M4
0.00.093.053 I ggml_metal_init: picking default device: Apple M4
0.00.093.699 I ggml_metal_init: using embedded metal library
0.00.096.389 I ggml_metal_init: GPU name:   Apple M4
0.00.096.391 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.392 I ggml_metal_init: simdgroup reduction   = true
0.00.096.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.392 I ggml_metal_init: has bfloat            = true
0.00.096.392 I ggml_metal_init: use bfloat            = true
0.00.096.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.917 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.167 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.170 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.184 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.080 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.081 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.081 I llama_new_context_with_model: graph nodes  = 967
0.00.108.081 I llama_new_context_with_model: graph splits = 2
0.00.108.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.235.914 I 
0.01.235.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.236.055 I perplexity: tokenizing the input ..
0.01.249.736 I perplexity: tokenization took 13.677 ms
0.01.249.752 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.371.543 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.373.270 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.373.290 I llama_perf_context_print:        load time =    1212.19 ms
0.01.373.292 I llama_perf_context_print: prompt eval time =     120.93 ms /   128 tokens (    0.94 ms per token,  1058.49 tokens per second)
0.01.373.294 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.373.295 I llama_perf_context_print:       total time =     137.39 ms /   129 tokens
0.01.374.040 I ggml_metal_free: deallocating

real	0m1.587s
user	0m0.124s
sys	0m0.233s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.783 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.665 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.672 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.677 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.677 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.678 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.679 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.679 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.685 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.685 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.662 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.677 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.678 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.678 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.679 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.679 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.679 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.680 I llama_model_loader: - type  f32:  194 tensors
0.00.035.680 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.926 I llm_load_vocab: special tokens cache size = 25
0.00.065.093 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.097 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.097 I llm_load_print_meta: arch             = gptneox
0.00.065.098 I llm_load_print_meta: vocab type       = BPE
0.00.065.098 I llm_load_print_meta: n_vocab          = 50304
0.00.065.098 I llm_load_print_meta: n_merges         = 50009
0.00.065.101 I llm_load_print_meta: vocab_only       = 0
0.00.065.101 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.101 I llm_load_print_meta: n_embd           = 2048
0.00.065.101 I llm_load_print_meta: n_layer          = 24
0.00.065.106 I llm_load_print_meta: n_head           = 16
0.00.065.107 I llm_load_print_meta: n_head_kv        = 16
0.00.065.107 I llm_load_print_meta: n_rot            = 32
0.00.065.107 I llm_load_print_meta: n_swa            = 0
0.00.065.107 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.107 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.108 I llm_load_print_meta: n_gqa            = 1
0.00.065.109 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.109 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.110 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.110 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.111 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.111 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.111 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.112 I llm_load_print_meta: n_ff             = 8192
0.00.065.112 I llm_load_print_meta: n_expert         = 0
0.00.065.112 I llm_load_print_meta: n_expert_used    = 0
0.00.065.112 I llm_load_print_meta: causal attn      = 1
0.00.065.112 I llm_load_print_meta: pooling type     = 0
0.00.065.112 I llm_load_print_meta: rope type        = 2
0.00.065.113 I llm_load_print_meta: rope scaling     = linear
0.00.065.114 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.114 I llm_load_print_meta: freq_scale_train = 1
0.00.065.114 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.114 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.114 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.115 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.115 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.115 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.115 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.115 I llm_load_print_meta: model type       = 1.4B
0.00.065.116 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.117 I llm_load_print_meta: model params     = 1.41 B
0.00.065.117 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.117 I llm_load_print_meta: general.name     = 1.4B
0.00.065.117 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.118 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.118 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.118 I llm_load_print_meta: LF token         = 128 ''
0.00.065.119 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.119 I llm_load_print_meta: max token length = 1024
0.00.067.626 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.626 I llm_load_tensors: offloading output layer to GPU
0.00.067.626 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.638 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.639 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.591 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.592 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.592 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.593 I llama_new_context_with_model: n_batch       = 2048
0.00.068.593 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.593 I llama_new_context_with_model: flash_attn    = 0
0.00.068.593 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.594 I llama_new_context_with_model: freq_scale    = 1
0.00.068.594 I ggml_metal_init: allocating
0.00.068.597 I ggml_metal_init: found device: Apple M4
0.00.068.599 I ggml_metal_init: picking default device: Apple M4
0.00.069.346 I ggml_metal_init: using embedded metal library
0.00.071.948 I ggml_metal_init: GPU name:   Apple M4
0.00.071.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.951 I ggml_metal_init: simdgroup reduction   = true
0.00.071.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.951 I ggml_metal_init: has bfloat            = true
0.00.071.951 I ggml_metal_init: use bfloat            = true
0.00.071.952 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.953 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.990 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.366 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.380 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.420 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.552 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.555 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.555 I llama_new_context_with_model: graph nodes  = 967
0.00.109.555 I llama_new_context_with_model: graph splits = 2
0.00.109.560 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.688 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.227.608 I main: llama threadpool init, n_threads = 4
0.01.227.646 I 
0.01.227.668 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.227.669 I 
0.01.227.898 I sampler seed: 1234
0.01.227.903 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.227.919 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.227.919 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.227.920 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.313.691 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.02.313.692 I llama_perf_context_print:        load time =    1217.82 ms
0.02.313.693 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.55 tokens per second)
0.02.313.694 I llama_perf_context_print:        eval time =    1039.25 ms /    63 runs   (   16.50 ms per token,    60.62 tokens per second)
0.02.313.694 I llama_perf_context_print:       total time =    1086.09 ms /    70 tokens
0.02.313.909 I ggml_metal_free: deallocating

real	0m2.333s
user	0m0.116s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.139 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.122 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.929 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.935 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.937 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.938 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.941 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.944 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.946 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.947 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.947 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.216 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.216 I llama_model_loader: - type  f32:  194 tensors
0.00.031.217 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.195 I llm_load_vocab: special tokens cache size = 25
0.00.061.648 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.650 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.651 I llm_load_print_meta: arch             = gptneox
0.00.061.651 I llm_load_print_meta: vocab type       = BPE
0.00.061.652 I llm_load_print_meta: n_vocab          = 50304
0.00.061.652 I llm_load_print_meta: n_merges         = 50009
0.00.061.652 I llm_load_print_meta: vocab_only       = 0
0.00.061.652 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.652 I llm_load_print_meta: n_embd           = 2048
0.00.061.652 I llm_load_print_meta: n_layer          = 24
0.00.061.656 I llm_load_print_meta: n_head           = 16
0.00.061.657 I llm_load_print_meta: n_head_kv        = 16
0.00.061.657 I llm_load_print_meta: n_rot            = 32
0.00.061.657 I llm_load_print_meta: n_swa            = 0
0.00.061.657 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.657 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.658 I llm_load_print_meta: n_gqa            = 1
0.00.061.659 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.659 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.660 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.660 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.660 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.661 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.661 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.661 I llm_load_print_meta: n_ff             = 8192
0.00.061.661 I llm_load_print_meta: n_expert         = 0
0.00.061.662 I llm_load_print_meta: n_expert_used    = 0
0.00.061.662 I llm_load_print_meta: causal attn      = 1
0.00.061.662 I llm_load_print_meta: pooling type     = 0
0.00.061.662 I llm_load_print_meta: rope type        = 2
0.00.061.662 I llm_load_print_meta: rope scaling     = linear
0.00.061.663 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.663 I llm_load_print_meta: freq_scale_train = 1
0.00.061.663 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.663 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.666 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.666 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.667 I llm_load_print_meta: model type       = 1.4B
0.00.061.667 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.667 I llm_load_print_meta: model params     = 1.41 B
0.00.061.668 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.668 I llm_load_print_meta: general.name     = 1.4B
0.00.061.669 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.670 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.670 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.670 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.671 I llm_load_print_meta: LF token         = 128 ''
0.00.061.671 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.671 I llm_load_print_meta: max token length = 1024
0.00.063.990 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.990 I llm_load_tensors: offloading output layer to GPU
0.00.063.990 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.002 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.003 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.930 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.931 I llama_new_context_with_model: n_ctx         = 128
0.00.064.931 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.932 I llama_new_context_with_model: n_batch       = 128
0.00.064.932 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.932 I llama_new_context_with_model: flash_attn    = 0
0.00.064.932 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.932 I llama_new_context_with_model: freq_scale    = 1
0.00.064.933 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.933 I ggml_metal_init: allocating
0.00.064.937 I ggml_metal_init: found device: Apple M4
0.00.064.938 I ggml_metal_init: picking default device: Apple M4
0.00.065.567 I ggml_metal_init: using embedded metal library
0.00.067.958 I ggml_metal_init: GPU name:   Apple M4
0.00.067.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.960 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.961 I ggml_metal_init: simdgroup reduction   = true
0.00.067.961 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.961 I ggml_metal_init: has bfloat            = true
0.00.067.961 I ggml_metal_init: use bfloat            = true
0.00.067.961 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.962 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.656 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.077.999 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.003 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.020 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.078.918 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.078.919 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.078.919 I llama_new_context_with_model: graph nodes  = 967
0.00.078.920 I llama_new_context_with_model: graph splits = 2
0.00.078.921 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.078.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.884.857 I 
0.00.884.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.884.899 I perplexity: tokenizing the input ..
0.00.892.728 I perplexity: tokenization took 7.828 ms
0.00.892.732 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.015.850 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.017.199 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.017.223 I llama_perf_context_print:        load time =     873.73 ms
0.01.017.224 I llama_perf_context_print: prompt eval time =     122.88 ms /   128 tokens (    0.96 ms per token,  1041.66 tokens per second)
0.01.017.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.017.225 I llama_perf_context_print:       total time =     132.37 ms /   129 tokens
0.01.017.593 I ggml_metal_free: deallocating

real	0m1.035s
user	0m0.090s
sys	0m0.152s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.013.892 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.238 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.240 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.240 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.241 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.241 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.242 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.244 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.158 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.152 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.152 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.153 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.153 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.153 I llama_model_loader: - type  f32:  194 tensors
0.00.032.154 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.154 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.775 I llm_load_vocab: special tokens cache size = 25
0.00.058.895 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.897 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.898 I llm_load_print_meta: arch             = gptneox
0.00.058.898 I llm_load_print_meta: vocab type       = BPE
0.00.058.898 I llm_load_print_meta: n_vocab          = 50304
0.00.058.899 I llm_load_print_meta: n_merges         = 50009
0.00.058.899 I llm_load_print_meta: vocab_only       = 0
0.00.058.899 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.899 I llm_load_print_meta: n_embd           = 2048
0.00.058.899 I llm_load_print_meta: n_layer          = 24
0.00.058.904 I llm_load_print_meta: n_head           = 16
0.00.058.905 I llm_load_print_meta: n_head_kv        = 16
0.00.058.905 I llm_load_print_meta: n_rot            = 32
0.00.058.905 I llm_load_print_meta: n_swa            = 0
0.00.058.905 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.907 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.908 I llm_load_print_meta: n_gqa            = 1
0.00.058.908 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.909 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.910 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.910 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.910 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.911 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.911 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.911 I llm_load_print_meta: n_ff             = 8192
0.00.058.912 I llm_load_print_meta: n_expert         = 0
0.00.058.912 I llm_load_print_meta: n_expert_used    = 0
0.00.058.912 I llm_load_print_meta: causal attn      = 1
0.00.058.912 I llm_load_print_meta: pooling type     = 0
0.00.058.912 I llm_load_print_meta: rope type        = 2
0.00.058.912 I llm_load_print_meta: rope scaling     = linear
0.00.058.913 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.913 I llm_load_print_meta: freq_scale_train = 1
0.00.058.914 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.914 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.914 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.916 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.916 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.916 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.916 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.917 I llm_load_print_meta: model type       = 1.4B
0.00.058.917 I llm_load_print_meta: model ftype      = Q4_0
0.00.058.918 I llm_load_print_meta: model params     = 1.41 B
0.00.058.918 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.058.918 I llm_load_print_meta: general.name     = 1.4B
0.00.058.919 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.919 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.919 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.919 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.923 I llm_load_print_meta: LF token         = 128 ''
0.00.058.924 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.924 I llm_load_print_meta: max token length = 1024
0.00.061.000 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.001 I llm_load_tensors: offloading output layer to GPU
0.00.061.001 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.008 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.061.009 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.062.105 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.106 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.106 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.106 I llama_new_context_with_model: n_batch       = 2048
0.00.062.106 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.107 I llama_new_context_with_model: flash_attn    = 0
0.00.062.107 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.107 I llama_new_context_with_model: freq_scale    = 1
0.00.062.108 I ggml_metal_init: allocating
0.00.062.111 I ggml_metal_init: found device: Apple M4
0.00.062.113 I ggml_metal_init: picking default device: Apple M4
0.00.062.842 I ggml_metal_init: using embedded metal library
0.00.065.393 I ggml_metal_init: GPU name:   Apple M4
0.00.065.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.395 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.395 I ggml_metal_init: simdgroup reduction   = true
0.00.065.396 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.396 I ggml_metal_init: has bfloat            = true
0.00.065.396 I ggml_metal_init: use bfloat            = true
0.00.065.396 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.241 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.849 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.860 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.885 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.099 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.100 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.101 I llama_new_context_with_model: graph nodes  = 967
0.00.104.101 I llama_new_context_with_model: graph splits = 2
0.00.104.107 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.212.837 I main: llama threadpool init, n_threads = 4
0.01.212.916 I 
0.01.212.991 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.212.993 I 
0.01.213.517 I sampler seed: 1234
0.01.213.528 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.213.578 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.213.581 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.213.581 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.896.527 I llama_perf_sampler_print:    sampling time =       1.61 ms /    71 runs   (    0.02 ms per token, 44044.67 tokens per second)
0.01.896.527 I llama_perf_context_print:        load time =    1198.93 ms
0.01.896.528 I llama_perf_context_print: prompt eval time =      50.60 ms /     7 tokens (    7.23 ms per token,   138.33 tokens per second)
0.01.896.529 I llama_perf_context_print:        eval time =     629.64 ms /    63 runs   (    9.99 ms per token,   100.06 tokens per second)
0.01.896.529 I llama_perf_context_print:       total time =     683.70 ms /    70 tokens
0.01.896.794 I ggml_metal_free: deallocating

real	0m1.915s
user	0m0.125s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.481 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.623 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.628 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.630 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.636 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.597 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.372 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.373 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.373 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.374 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.374 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.379 I llama_model_loader: - type  f32:  194 tensors
0.00.024.379 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.379 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.269 I llm_load_vocab: special tokens cache size = 25
0.00.051.389 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.394 I llm_load_print_meta: arch             = gptneox
0.00.051.394 I llm_load_print_meta: vocab type       = BPE
0.00.051.395 I llm_load_print_meta: n_vocab          = 50304
0.00.051.395 I llm_load_print_meta: n_merges         = 50009
0.00.051.395 I llm_load_print_meta: vocab_only       = 0
0.00.051.395 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.395 I llm_load_print_meta: n_embd           = 2048
0.00.051.397 I llm_load_print_meta: n_layer          = 24
0.00.051.401 I llm_load_print_meta: n_head           = 16
0.00.051.402 I llm_load_print_meta: n_head_kv        = 16
0.00.051.402 I llm_load_print_meta: n_rot            = 32
0.00.051.402 I llm_load_print_meta: n_swa            = 0
0.00.051.402 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.403 I llm_load_print_meta: n_gqa            = 1
0.00.051.406 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.406 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.407 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.407 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.407 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.408 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.408 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.408 I llm_load_print_meta: n_ff             = 8192
0.00.051.409 I llm_load_print_meta: n_expert         = 0
0.00.051.409 I llm_load_print_meta: n_expert_used    = 0
0.00.051.409 I llm_load_print_meta: causal attn      = 1
0.00.051.409 I llm_load_print_meta: pooling type     = 0
0.00.051.409 I llm_load_print_meta: rope type        = 2
0.00.051.409 I llm_load_print_meta: rope scaling     = linear
0.00.051.410 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.410 I llm_load_print_meta: freq_scale_train = 1
0.00.051.410 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.410 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.411 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.411 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.411 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.411 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.411 I llm_load_print_meta: model type       = 1.4B
0.00.051.412 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.412 I llm_load_print_meta: model params     = 1.41 B
0.00.051.412 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.413 I llm_load_print_meta: general.name     = 1.4B
0.00.051.413 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: LF token         = 128 ''
0.00.051.414 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.414 I llm_load_print_meta: max token length = 1024
0.00.053.276 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.277 I llm_load_tensors: offloading output layer to GPU
0.00.053.277 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.288 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.289 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.192 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.193 I llama_new_context_with_model: n_ctx         = 128
0.00.054.193 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.193 I llama_new_context_with_model: n_batch       = 128
0.00.054.193 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.193 I llama_new_context_with_model: flash_attn    = 0
0.00.054.194 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.194 I llama_new_context_with_model: freq_scale    = 1
0.00.054.195 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.195 I ggml_metal_init: allocating
0.00.054.203 I ggml_metal_init: found device: Apple M4
0.00.054.208 I ggml_metal_init: picking default device: Apple M4
0.00.054.853 I ggml_metal_init: using embedded metal library
0.00.057.247 I ggml_metal_init: GPU name:   Apple M4
0.00.057.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.250 I ggml_metal_init: simdgroup reduction   = true
0.00.057.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.250 I ggml_metal_init: has bfloat            = true
0.00.057.250 I ggml_metal_init: use bfloat            = true
0.00.057.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.827 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.239 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.241 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.256 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.040 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.041 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.042 I llama_new_context_with_model: graph nodes  = 967
0.00.070.042 I llama_new_context_with_model: graph splits = 2
0.00.070.043 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.086 I 
0.00.606.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.131 I perplexity: tokenizing the input ..
0.00.614.003 I perplexity: tokenization took 7.87 ms
0.00.614.007 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.076 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.737.567 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.737.582 I llama_perf_context_print:        load time =     596.60 ms
0.00.737.585 I llama_perf_context_print: prompt eval time =     121.83 ms /   128 tokens (    0.95 ms per token,  1050.63 tokens per second)
0.00.737.585 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.589 I llama_perf_context_print:       total time =     131.50 ms /   129 tokens
0.00.737.922 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.080s
sys	0m0.079s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.906 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.731 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.733 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.740 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.750 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.603 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.483 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.483 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.484 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.485 I llama_model_loader: - type  f32:  194 tensors
0.00.025.485 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.485 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.410 I llm_load_vocab: special tokens cache size = 25
0.00.052.261 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.264 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.264 I llm_load_print_meta: arch             = gptneox
0.00.052.265 I llm_load_print_meta: vocab type       = BPE
0.00.052.265 I llm_load_print_meta: n_vocab          = 50304
0.00.052.265 I llm_load_print_meta: n_merges         = 50009
0.00.052.265 I llm_load_print_meta: vocab_only       = 0
0.00.052.265 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.265 I llm_load_print_meta: n_embd           = 2048
0.00.052.266 I llm_load_print_meta: n_layer          = 24
0.00.052.269 I llm_load_print_meta: n_head           = 16
0.00.052.271 I llm_load_print_meta: n_head_kv        = 16
0.00.052.272 I llm_load_print_meta: n_rot            = 32
0.00.052.272 I llm_load_print_meta: n_swa            = 0
0.00.052.272 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.272 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.273 I llm_load_print_meta: n_gqa            = 1
0.00.052.274 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.274 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.275 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.275 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.275 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.276 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.276 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.276 I llm_load_print_meta: n_ff             = 8192
0.00.052.277 I llm_load_print_meta: n_expert         = 0
0.00.052.277 I llm_load_print_meta: n_expert_used    = 0
0.00.052.277 I llm_load_print_meta: causal attn      = 1
0.00.052.278 I llm_load_print_meta: pooling type     = 0
0.00.052.279 I llm_load_print_meta: rope type        = 2
0.00.052.279 I llm_load_print_meta: rope scaling     = linear
0.00.052.280 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.280 I llm_load_print_meta: freq_scale_train = 1
0.00.052.280 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.280 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.281 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.281 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.281 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.281 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.281 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.281 I llm_load_print_meta: model type       = 1.4B
0.00.052.282 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.282 I llm_load_print_meta: model params     = 1.41 B
0.00.052.283 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.283 I llm_load_print_meta: general.name     = 1.4B
0.00.052.283 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.283 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.284 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.284 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.284 I llm_load_print_meta: LF token         = 128 ''
0.00.052.284 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.284 I llm_load_print_meta: max token length = 1024
0.00.054.309 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.309 I llm_load_tensors: offloading output layer to GPU
0.00.054.309 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.320 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.321 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.230 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.231 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.231 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.231 I llama_new_context_with_model: n_batch       = 2048
0.00.055.232 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.232 I llama_new_context_with_model: flash_attn    = 0
0.00.055.232 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.233 I llama_new_context_with_model: freq_scale    = 1
0.00.055.233 I ggml_metal_init: allocating
0.00.055.240 I ggml_metal_init: found device: Apple M4
0.00.055.243 I ggml_metal_init: picking default device: Apple M4
0.00.055.857 I ggml_metal_init: using embedded metal library
0.00.058.227 I ggml_metal_init: GPU name:   Apple M4
0.00.058.228 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.229 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.229 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.229 I ggml_metal_init: simdgroup reduction   = true
0.00.058.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.230 I ggml_metal_init: has bfloat            = true
0.00.058.230 I ggml_metal_init: use bfloat            = true
0.00.058.230 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.231 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.347 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.707 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.716 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.736 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.723 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.725 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.725 I llama_new_context_with_model: graph nodes  = 967
0.00.088.726 I llama_new_context_with_model: graph splits = 2
0.00.088.729 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.378 I main: llama threadpool init, n_threads = 4
0.00.651.413 I 
0.00.651.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.438 I 
0.00.651.664 I sampler seed: 1234
0.00.651.668 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.651.718 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.651.719 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.651.720 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.368.338 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65740.74 tokens per second)
0.01.368.339 I llama_perf_context_print:        load time =     642.47 ms
0.01.368.340 I llama_perf_context_print: prompt eval time =      39.62 ms /     7 tokens (    5.66 ms per token,   176.70 tokens per second)
0.01.368.340 I llama_perf_context_print:        eval time =     674.27 ms /    63 runs   (   10.70 ms per token,    93.43 tokens per second)
0.01.368.341 I llama_perf_context_print:       total time =     716.96 ms /    70 tokens
0.01.368.583 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.069 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.749 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.754 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.757 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.758 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.762 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.765 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.766 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.769 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.769 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.645 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.673 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.677 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.678 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.678 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.679 I llama_model_loader: - type  f32:  194 tensors
0.00.023.679 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.679 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.668 I llm_load_vocab: special tokens cache size = 25
0.00.050.898 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.903 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.903 I llm_load_print_meta: arch             = gptneox
0.00.050.904 I llm_load_print_meta: vocab type       = BPE
0.00.050.904 I llm_load_print_meta: n_vocab          = 50304
0.00.050.904 I llm_load_print_meta: n_merges         = 50009
0.00.050.905 I llm_load_print_meta: vocab_only       = 0
0.00.050.905 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.905 I llm_load_print_meta: n_embd           = 2048
0.00.050.905 I llm_load_print_meta: n_layer          = 24
0.00.050.909 I llm_load_print_meta: n_head           = 16
0.00.050.910 I llm_load_print_meta: n_head_kv        = 16
0.00.050.910 I llm_load_print_meta: n_rot            = 32
0.00.050.910 I llm_load_print_meta: n_swa            = 0
0.00.050.910 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.911 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.911 I llm_load_print_meta: n_gqa            = 1
0.00.050.912 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.913 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.913 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.913 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.914 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.914 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.914 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.914 I llm_load_print_meta: n_ff             = 8192
0.00.050.915 I llm_load_print_meta: n_expert         = 0
0.00.050.915 I llm_load_print_meta: n_expert_used    = 0
0.00.050.915 I llm_load_print_meta: causal attn      = 1
0.00.050.915 I llm_load_print_meta: pooling type     = 0
0.00.050.916 I llm_load_print_meta: rope type        = 2
0.00.050.916 I llm_load_print_meta: rope scaling     = linear
0.00.050.917 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.918 I llm_load_print_meta: freq_scale_train = 1
0.00.050.918 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.918 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.918 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.918 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.918 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.919 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.919 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.919 I llm_load_print_meta: model type       = 1.4B
0.00.050.919 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.920 I llm_load_print_meta: model params     = 1.41 B
0.00.050.924 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.924 I llm_load_print_meta: general.name     = 1.4B
0.00.050.925 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.925 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.926 I llm_load_print_meta: LF token         = 128 ''
0.00.050.927 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.927 I llm_load_print_meta: max token length = 1024
0.00.052.963 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.963 I llm_load_tensors: offloading output layer to GPU
0.00.052.964 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.975 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.976 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.869 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.870 I llama_new_context_with_model: n_ctx         = 128
0.00.053.870 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.870 I llama_new_context_with_model: n_batch       = 128
0.00.053.870 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.871 I llama_new_context_with_model: flash_attn    = 0
0.00.053.871 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.871 I llama_new_context_with_model: freq_scale    = 1
0.00.053.872 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.872 I ggml_metal_init: allocating
0.00.053.878 I ggml_metal_init: found device: Apple M4
0.00.053.880 I ggml_metal_init: picking default device: Apple M4
0.00.054.548 I ggml_metal_init: using embedded metal library
0.00.056.975 I ggml_metal_init: GPU name:   Apple M4
0.00.056.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.977 I ggml_metal_init: simdgroup reduction   = true
0.00.056.978 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.978 I ggml_metal_init: has bfloat            = true
0.00.056.978 I ggml_metal_init: use bfloat            = true
0.00.056.979 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.356 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.680 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.683 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.698 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.571 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.573 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.573 I llama_new_context_with_model: graph nodes  = 967
0.00.069.573 I llama_new_context_with_model: graph splits = 2
0.00.069.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.575 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.986 I 
0.00.590.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.032 I perplexity: tokenizing the input ..
0.00.598.038 I perplexity: tokenization took 8.004 ms
0.00.598.044 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.961 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.722.150 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.722.168 I llama_perf_context_print:        load time =     580.91 ms
0.00.722.170 I llama_perf_context_print: prompt eval time =     122.69 ms /   128 tokens (    0.96 ms per token,  1043.28 tokens per second)
0.00.722.172 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.722.173 I llama_perf_context_print:       total time =     132.18 ms /   129 tokens
0.00.722.672 I ggml_metal_free: deallocating

real	0m0.738s
user	0m0.081s
sys	0m0.094s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.570 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.147 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.148 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.156 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.156 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.156 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.768 I llama_model_loader: - type  f32:  194 tensors
0.00.024.768 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.768 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.775 I llm_load_vocab: special tokens cache size = 25
0.00.051.830 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.832 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.833 I llm_load_print_meta: arch             = gptneox
0.00.051.833 I llm_load_print_meta: vocab type       = BPE
0.00.051.833 I llm_load_print_meta: n_vocab          = 50304
0.00.051.834 I llm_load_print_meta: n_merges         = 50009
0.00.051.834 I llm_load_print_meta: vocab_only       = 0
0.00.051.834 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.834 I llm_load_print_meta: n_embd           = 2048
0.00.051.834 I llm_load_print_meta: n_layer          = 24
0.00.051.837 I llm_load_print_meta: n_head           = 16
0.00.051.838 I llm_load_print_meta: n_head_kv        = 16
0.00.051.838 I llm_load_print_meta: n_rot            = 32
0.00.051.838 I llm_load_print_meta: n_swa            = 0
0.00.051.839 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.839 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.839 I llm_load_print_meta: n_gqa            = 1
0.00.051.840 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.841 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.842 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.842 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.843 I llm_load_print_meta: n_ff             = 8192
0.00.051.843 I llm_load_print_meta: n_expert         = 0
0.00.051.844 I llm_load_print_meta: n_expert_used    = 0
0.00.051.845 I llm_load_print_meta: causal attn      = 1
0.00.051.847 I llm_load_print_meta: pooling type     = 0
0.00.051.847 I llm_load_print_meta: rope type        = 2
0.00.051.848 I llm_load_print_meta: rope scaling     = linear
0.00.051.848 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.849 I llm_load_print_meta: freq_scale_train = 1
0.00.051.849 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.849 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.849 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.849 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.850 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.850 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.850 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.850 I llm_load_print_meta: model type       = 1.4B
0.00.051.850 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.851 I llm_load_print_meta: model params     = 1.41 B
0.00.051.852 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.852 I llm_load_print_meta: general.name     = 1.4B
0.00.051.852 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.852 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.852 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.854 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.854 I llm_load_print_meta: LF token         = 128 ''
0.00.051.854 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.855 I llm_load_print_meta: max token length = 1024
0.00.053.913 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.913 I llm_load_tensors: offloading output layer to GPU
0.00.053.913 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.924 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.925 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.938 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.939 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.939 I llama_new_context_with_model: n_batch       = 2048
0.00.054.939 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.939 I llama_new_context_with_model: flash_attn    = 0
0.00.054.940 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.940 I llama_new_context_with_model: freq_scale    = 1
0.00.054.941 I ggml_metal_init: allocating
0.00.054.944 I ggml_metal_init: found device: Apple M4
0.00.054.946 I ggml_metal_init: picking default device: Apple M4
0.00.055.552 I ggml_metal_init: using embedded metal library
0.00.057.910 I ggml_metal_init: GPU name:   Apple M4
0.00.057.912 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.912 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.913 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.913 I ggml_metal_init: simdgroup reduction   = true
0.00.057.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.913 I ggml_metal_init: has bfloat            = true
0.00.057.913 I ggml_metal_init: use bfloat            = true
0.00.057.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.916 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.823 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.491 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.498 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.517 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.570 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.571 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.572 I llama_new_context_with_model: graph nodes  = 967
0.00.088.572 I llama_new_context_with_model: graph splits = 2
0.00.088.574 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.905 I main: llama threadpool init, n_threads = 4
0.00.750.950 I 
0.00.750.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.974 I 
0.00.751.399 I sampler seed: 1234
0.00.751.408 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.431 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.431 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.432 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.545.688 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.01.545.688 I llama_perf_context_print:        load time =     741.33 ms
0.01.545.689 I llama_perf_context_print: prompt eval time =      47.84 ms /     7 tokens (    6.83 ms per token,   146.31 tokens per second)
0.01.545.690 I llama_perf_context_print:        eval time =     743.50 ms /    63 runs   (   11.80 ms per token,    84.73 tokens per second)
0.01.545.691 I llama_perf_context_print:       total time =     794.79 ms /    70 tokens
0.01.545.964 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.112s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.150 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.879 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.880 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.880 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.881 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.884 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.884 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.884 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.885 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.888 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.888 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.641 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.453 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.454 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.454 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.455 I llama_model_loader: - type  f32:  194 tensors
0.00.024.455 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.456 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.403 I llm_load_vocab: special tokens cache size = 25
0.00.050.462 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.465 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.465 I llm_load_print_meta: arch             = gptneox
0.00.050.466 I llm_load_print_meta: vocab type       = BPE
0.00.050.466 I llm_load_print_meta: n_vocab          = 50304
0.00.050.466 I llm_load_print_meta: n_merges         = 50009
0.00.050.466 I llm_load_print_meta: vocab_only       = 0
0.00.050.466 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.466 I llm_load_print_meta: n_embd           = 2048
0.00.050.467 I llm_load_print_meta: n_layer          = 24
0.00.050.469 I llm_load_print_meta: n_head           = 16
0.00.050.471 I llm_load_print_meta: n_head_kv        = 16
0.00.050.471 I llm_load_print_meta: n_rot            = 32
0.00.050.471 I llm_load_print_meta: n_swa            = 0
0.00.050.471 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.472 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.472 I llm_load_print_meta: n_gqa            = 1
0.00.050.473 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.474 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.474 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.474 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.475 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.475 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.475 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.476 I llm_load_print_meta: n_ff             = 8192
0.00.050.476 I llm_load_print_meta: n_expert         = 0
0.00.050.476 I llm_load_print_meta: n_expert_used    = 0
0.00.050.476 I llm_load_print_meta: causal attn      = 1
0.00.050.476 I llm_load_print_meta: pooling type     = 0
0.00.050.476 I llm_load_print_meta: rope type        = 2
0.00.050.477 I llm_load_print_meta: rope scaling     = linear
0.00.050.477 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.477 I llm_load_print_meta: freq_scale_train = 1
0.00.050.478 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.479 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.479 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.479 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.479 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.480 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.480 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.480 I llm_load_print_meta: model type       = 1.4B
0.00.050.480 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.481 I llm_load_print_meta: model params     = 1.41 B
0.00.050.481 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.483 I llm_load_print_meta: general.name     = 1.4B
0.00.050.483 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.483 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.484 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.484 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.484 I llm_load_print_meta: LF token         = 128 ''
0.00.050.484 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.485 I llm_load_print_meta: max token length = 1024
0.00.052.450 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.450 I llm_load_tensors: offloading output layer to GPU
0.00.052.450 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.461 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.462 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.349 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.350 I llama_new_context_with_model: n_ctx         = 128
0.00.053.350 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.350 I llama_new_context_with_model: n_batch       = 128
0.00.053.350 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.350 I llama_new_context_with_model: flash_attn    = 0
0.00.053.351 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.351 I llama_new_context_with_model: freq_scale    = 1
0.00.053.351 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.352 I ggml_metal_init: allocating
0.00.053.358 I ggml_metal_init: found device: Apple M4
0.00.053.362 I ggml_metal_init: picking default device: Apple M4
0.00.053.921 I ggml_metal_init: using embedded metal library
0.00.056.262 I ggml_metal_init: GPU name:   Apple M4
0.00.056.264 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.264 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.265 I ggml_metal_init: simdgroup reduction   = true
0.00.056.265 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.265 I ggml_metal_init: has bfloat            = true
0.00.056.265 I ggml_metal_init: use bfloat            = true
0.00.056.265 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.271 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.785 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.017 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.019 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.032 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.880 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.881 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.882 I llama_new_context_with_model: graph nodes  = 967
0.00.067.882 I llama_new_context_with_model: graph splits = 2
0.00.067.883 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.398 I 
0.00.690.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.447 I perplexity: tokenizing the input ..
0.00.698.293 I perplexity: tokenization took 7.845 ms
0.00.698.297 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.144 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.834.327 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.834.348 I llama_perf_context_print:        load time =     680.24 ms
0.00.834.352 I llama_perf_context_print: prompt eval time =     134.60 ms /   128 tokens (    1.05 ms per token,   950.97 tokens per second)
0.00.834.353 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.353 I llama_perf_context_print:       total time =     143.95 ms /   129 tokens
0.00.834.868 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.077s
sys	0m0.112s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.747 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.799 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.800 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.800 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.801 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.803 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.746 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.829 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.683 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.686 I llama_model_loader: - type  f32:  194 tensors
0.00.024.687 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.687 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.613 I llm_load_vocab: special tokens cache size = 25
0.00.051.552 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.555 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.556 I llm_load_print_meta: arch             = gptneox
0.00.051.556 I llm_load_print_meta: vocab type       = BPE
0.00.051.556 I llm_load_print_meta: n_vocab          = 50304
0.00.051.556 I llm_load_print_meta: n_merges         = 50009
0.00.051.557 I llm_load_print_meta: vocab_only       = 0
0.00.051.557 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.557 I llm_load_print_meta: n_embd           = 2048
0.00.051.557 I llm_load_print_meta: n_layer          = 24
0.00.051.560 I llm_load_print_meta: n_head           = 16
0.00.051.561 I llm_load_print_meta: n_head_kv        = 16
0.00.051.561 I llm_load_print_meta: n_rot            = 32
0.00.051.561 I llm_load_print_meta: n_swa            = 0
0.00.051.562 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.562 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.562 I llm_load_print_meta: n_gqa            = 1
0.00.051.563 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.564 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.564 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.565 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.565 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.565 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.565 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.566 I llm_load_print_meta: n_ff             = 8192
0.00.051.566 I llm_load_print_meta: n_expert         = 0
0.00.051.566 I llm_load_print_meta: n_expert_used    = 0
0.00.051.566 I llm_load_print_meta: causal attn      = 1
0.00.051.567 I llm_load_print_meta: pooling type     = 0
0.00.051.567 I llm_load_print_meta: rope type        = 2
0.00.051.567 I llm_load_print_meta: rope scaling     = linear
0.00.051.568 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.568 I llm_load_print_meta: freq_scale_train = 1
0.00.051.568 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.568 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.569 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.569 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.569 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.569 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.569 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.570 I llm_load_print_meta: model type       = 1.4B
0.00.051.570 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.570 I llm_load_print_meta: model params     = 1.41 B
0.00.051.571 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.573 I llm_load_print_meta: general.name     = 1.4B
0.00.051.573 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: LF token         = 128 ''
0.00.051.575 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.575 I llm_load_print_meta: max token length = 1024
0.00.053.709 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.709 I llm_load_tensors: offloading output layer to GPU
0.00.053.709 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.720 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.721 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.610 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.611 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.611 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.611 I llama_new_context_with_model: n_batch       = 2048
0.00.054.611 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.611 I llama_new_context_with_model: flash_attn    = 0
0.00.054.612 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.612 I llama_new_context_with_model: freq_scale    = 1
0.00.054.613 I ggml_metal_init: allocating
0.00.054.616 I ggml_metal_init: found device: Apple M4
0.00.054.619 I ggml_metal_init: picking default device: Apple M4
0.00.055.252 I ggml_metal_init: using embedded metal library
0.00.057.618 I ggml_metal_init: GPU name:   Apple M4
0.00.057.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.620 I ggml_metal_init: simdgroup reduction   = true
0.00.057.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.621 I ggml_metal_init: has bfloat            = true
0.00.057.621 I ggml_metal_init: use bfloat            = true
0.00.057.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.544 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.994 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.003 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.031 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.100 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.101 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.101 I llama_new_context_with_model: graph nodes  = 967
0.00.088.102 I llama_new_context_with_model: graph splits = 2
0.00.088.105 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.246 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.246 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.823 I main: llama threadpool init, n_threads = 4
0.00.727.867 I 
0.00.727.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.897 I 
0.00.728.126 I sampler seed: 1234
0.00.728.131 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.728.146 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.728.146 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.728.146 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.567.476 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.01.567.477 I llama_perf_context_print:        load time =     719.07 ms
0.01.567.478 I llama_perf_context_print: prompt eval time =      42.29 ms /     7 tokens (    6.04 ms per token,   165.51 tokens per second)
0.01.567.478 I llama_perf_context_print:        eval time =     794.11 ms /    63 runs   (   12.60 ms per token,    79.33 tokens per second)
0.01.567.479 I llama_perf_context_print:       total time =     839.66 ms /    70 tokens
0.01.567.684 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.921 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.867 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.871 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.872 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.873 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.886 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.888 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.890 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.781 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.690 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.691 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.692 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.692 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.692 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.693 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.693 I llama_model_loader: - type  f32:  194 tensors
0.00.023.694 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.694 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.458 I llm_load_vocab: special tokens cache size = 25
0.00.050.255 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.258 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.258 I llm_load_print_meta: arch             = gptneox
0.00.050.258 I llm_load_print_meta: vocab type       = BPE
0.00.050.259 I llm_load_print_meta: n_vocab          = 50304
0.00.050.259 I llm_load_print_meta: n_merges         = 50009
0.00.050.259 I llm_load_print_meta: vocab_only       = 0
0.00.050.259 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.259 I llm_load_print_meta: n_embd           = 2048
0.00.050.260 I llm_load_print_meta: n_layer          = 24
0.00.050.262 I llm_load_print_meta: n_head           = 16
0.00.050.263 I llm_load_print_meta: n_head_kv        = 16
0.00.050.263 I llm_load_print_meta: n_rot            = 32
0.00.050.263 I llm_load_print_meta: n_swa            = 0
0.00.050.264 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.264 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.265 I llm_load_print_meta: n_gqa            = 1
0.00.050.265 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.266 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.267 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.267 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.267 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.267 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.267 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.268 I llm_load_print_meta: n_ff             = 8192
0.00.050.268 I llm_load_print_meta: n_expert         = 0
0.00.050.268 I llm_load_print_meta: n_expert_used    = 0
0.00.050.268 I llm_load_print_meta: causal attn      = 1
0.00.050.269 I llm_load_print_meta: pooling type     = 0
0.00.050.269 I llm_load_print_meta: rope type        = 2
0.00.050.269 I llm_load_print_meta: rope scaling     = linear
0.00.050.269 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.270 I llm_load_print_meta: freq_scale_train = 1
0.00.050.270 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.270 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.270 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.271 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.271 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.271 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.271 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.273 I llm_load_print_meta: model type       = 1.4B
0.00.050.274 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.274 I llm_load_print_meta: model params     = 1.41 B
0.00.050.274 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.275 I llm_load_print_meta: general.name     = 1.4B
0.00.050.275 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.275 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.279 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.279 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.280 I llm_load_print_meta: LF token         = 128 ''
0.00.050.280 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.280 I llm_load_print_meta: max token length = 1024
0.00.052.362 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.362 I llm_load_tensors: offloading output layer to GPU
0.00.052.362 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.373 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.374 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.304 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.305 I llama_new_context_with_model: n_ctx         = 128
0.00.053.305 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.305 I llama_new_context_with_model: n_batch       = 128
0.00.053.305 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.306 I llama_new_context_with_model: flash_attn    = 0
0.00.053.306 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.306 I llama_new_context_with_model: freq_scale    = 1
0.00.053.307 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.307 I ggml_metal_init: allocating
0.00.053.311 I ggml_metal_init: found device: Apple M4
0.00.053.313 I ggml_metal_init: picking default device: Apple M4
0.00.053.900 I ggml_metal_init: using embedded metal library
0.00.056.235 I ggml_metal_init: GPU name:   Apple M4
0.00.056.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.237 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.237 I ggml_metal_init: simdgroup reduction   = true
0.00.056.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.237 I ggml_metal_init: has bfloat            = true
0.00.056.238 I ggml_metal_init: use bfloat            = true
0.00.056.238 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.239 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.096 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.389 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.393 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.408 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.335 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.336 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.336 I llama_new_context_with_model: graph nodes  = 967
0.00.068.337 I llama_new_context_with_model: graph splits = 2
0.00.068.338 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.338 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.867 I 
0.00.671.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.916 I perplexity: tokenizing the input ..
0.00.679.990 I perplexity: tokenization took 8.072 ms
0.00.679.996 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.492 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.815.662 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.815.679 I llama_perf_context_print:        load time =     662.94 ms
0.00.815.680 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.30 tokens per second)
0.00.815.681 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.682 I llama_perf_context_print:       total time =     143.81 ms /   129 tokens
0.00.816.082 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.079s
sys	0m0.114s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.929 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.973 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.816 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.861 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.702 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.704 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.704 I llama_model_loader: - type  f32:  194 tensors
0.00.025.704 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.704 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.705 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.839 I llm_load_vocab: special tokens cache size = 25
0.00.051.849 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.851 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.852 I llm_load_print_meta: arch             = gptneox
0.00.051.852 I llm_load_print_meta: vocab type       = BPE
0.00.051.852 I llm_load_print_meta: n_vocab          = 50304
0.00.051.853 I llm_load_print_meta: n_merges         = 50009
0.00.051.853 I llm_load_print_meta: vocab_only       = 0
0.00.051.853 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.853 I llm_load_print_meta: n_embd           = 2048
0.00.051.853 I llm_load_print_meta: n_layer          = 24
0.00.051.856 I llm_load_print_meta: n_head           = 16
0.00.051.857 I llm_load_print_meta: n_head_kv        = 16
0.00.051.857 I llm_load_print_meta: n_rot            = 32
0.00.051.857 I llm_load_print_meta: n_swa            = 0
0.00.051.857 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.857 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.860 I llm_load_print_meta: n_gqa            = 1
0.00.051.861 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.862 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.862 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.863 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.864 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.864 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.865 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.865 I llm_load_print_meta: n_ff             = 8192
0.00.051.866 I llm_load_print_meta: n_expert         = 0
0.00.051.866 I llm_load_print_meta: n_expert_used    = 0
0.00.051.866 I llm_load_print_meta: causal attn      = 1
0.00.051.866 I llm_load_print_meta: pooling type     = 0
0.00.051.866 I llm_load_print_meta: rope type        = 2
0.00.051.872 I llm_load_print_meta: rope scaling     = linear
0.00.051.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.875 I llm_load_print_meta: freq_scale_train = 1
0.00.051.875 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.875 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.876 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.876 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.877 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.877 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.878 I llm_load_print_meta: model type       = 1.4B
0.00.051.878 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.879 I llm_load_print_meta: model params     = 1.41 B
0.00.051.879 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.879 I llm_load_print_meta: general.name     = 1.4B
0.00.051.879 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.879 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.880 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.880 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.881 I llm_load_print_meta: LF token         = 128 ''
0.00.051.882 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.882 I llm_load_print_meta: max token length = 1024
0.00.053.763 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.763 I llm_load_tensors: offloading output layer to GPU
0.00.053.764 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.774 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.775 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.617 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.617 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.618 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.618 I llama_new_context_with_model: n_batch       = 2048
0.00.054.618 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.618 I llama_new_context_with_model: flash_attn    = 0
0.00.054.618 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.619 I llama_new_context_with_model: freq_scale    = 1
0.00.054.619 I ggml_metal_init: allocating
0.00.054.625 I ggml_metal_init: found device: Apple M4
0.00.054.627 I ggml_metal_init: picking default device: Apple M4
0.00.055.199 I ggml_metal_init: using embedded metal library
0.00.057.543 I ggml_metal_init: GPU name:   Apple M4
0.00.057.545 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.545 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.546 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.546 I ggml_metal_init: simdgroup reduction   = true
0.00.057.546 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.546 I ggml_metal_init: has bfloat            = true
0.00.057.546 I ggml_metal_init: use bfloat            = true
0.00.057.547 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.547 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.999 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.939 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.944 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.964 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.074 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.075 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.076 I llama_new_context_with_model: graph nodes  = 967
0.00.087.076 I llama_new_context_with_model: graph splits = 2
0.00.087.079 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.239 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.485.138 I main: llama threadpool init, n_threads = 4
0.00.485.183 I 
0.00.485.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.485.212 I 
0.00.485.441 I sampler seed: 1234
0.00.485.447 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.485.497 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.485.501 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.485.501 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.163.688 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64545.45 tokens per second)
0.01.163.689 I llama_perf_context_print:        load time =     475.20 ms
0.01.163.690 I llama_perf_context_print: prompt eval time =      35.99 ms /     7 tokens (    5.14 ms per token,   194.47 tokens per second)
0.01.163.690 I llama_perf_context_print:        eval time =     639.32 ms /    63 runs   (   10.15 ms per token,    98.54 tokens per second)
0.01.163.691 I llama_perf_context_print:       total time =     678.55 ms /    70 tokens
0.01.163.917 I ggml_metal_free: deallocating

real	0m1.183s
user	0m0.109s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.073 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.941 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.842 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.844 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.845 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.845 I llama_model_loader: - type  f32:  194 tensors
0.00.024.846 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.846 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.846 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.587 I llm_load_vocab: special tokens cache size = 25
0.00.051.650 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.653 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.654 I llm_load_print_meta: arch             = gptneox
0.00.051.654 I llm_load_print_meta: vocab type       = BPE
0.00.051.654 I llm_load_print_meta: n_vocab          = 50304
0.00.051.655 I llm_load_print_meta: n_merges         = 50009
0.00.051.655 I llm_load_print_meta: vocab_only       = 0
0.00.051.655 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.655 I llm_load_print_meta: n_embd           = 2048
0.00.051.655 I llm_load_print_meta: n_layer          = 24
0.00.051.658 I llm_load_print_meta: n_head           = 16
0.00.051.659 I llm_load_print_meta: n_head_kv        = 16
0.00.051.659 I llm_load_print_meta: n_rot            = 32
0.00.051.659 I llm_load_print_meta: n_swa            = 0
0.00.051.659 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.659 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.660 I llm_load_print_meta: n_gqa            = 1
0.00.051.661 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.662 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.662 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.663 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.663 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.663 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.663 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.664 I llm_load_print_meta: n_ff             = 8192
0.00.051.664 I llm_load_print_meta: n_expert         = 0
0.00.051.664 I llm_load_print_meta: n_expert_used    = 0
0.00.051.664 I llm_load_print_meta: causal attn      = 1
0.00.051.665 I llm_load_print_meta: pooling type     = 0
0.00.051.665 I llm_load_print_meta: rope type        = 2
0.00.051.665 I llm_load_print_meta: rope scaling     = linear
0.00.051.667 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.667 I llm_load_print_meta: freq_scale_train = 1
0.00.051.668 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.668 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.668 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.668 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.668 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.668 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.669 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.669 I llm_load_print_meta: model type       = 1.4B
0.00.051.669 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.670 I llm_load_print_meta: model params     = 1.41 B
0.00.051.670 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.670 I llm_load_print_meta: general.name     = 1.4B
0.00.051.675 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.675 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.675 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.675 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.676 I llm_load_print_meta: LF token         = 128 ''
0.00.051.676 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.676 I llm_load_print_meta: max token length = 1024
0.00.053.294 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.294 I llm_load_tensors: offloading output layer to GPU
0.00.053.295 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.305 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.306 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.131 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.132 I llama_new_context_with_model: n_ctx         = 128
0.00.054.132 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.132 I llama_new_context_with_model: n_batch       = 128
0.00.054.132 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.133 I llama_new_context_with_model: flash_attn    = 0
0.00.054.133 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.133 I llama_new_context_with_model: freq_scale    = 1
0.00.054.134 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.134 I ggml_metal_init: allocating
0.00.054.140 I ggml_metal_init: found device: Apple M4
0.00.054.143 I ggml_metal_init: picking default device: Apple M4
0.00.054.709 I ggml_metal_init: using embedded metal library
0.00.057.005 I ggml_metal_init: GPU name:   Apple M4
0.00.057.006 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.007 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.007 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.007 I ggml_metal_init: simdgroup reduction   = true
0.00.057.007 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.008 I ggml_metal_init: has bfloat            = true
0.00.057.008 I ggml_metal_init: use bfloat            = true
0.00.057.008 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.009 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.423 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.649 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.656 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.672 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.529 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.530 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.531 I llama_new_context_with_model: graph nodes  = 967
0.00.068.531 I llama_new_context_with_model: graph splits = 2
0.00.068.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.532 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.440.474 I 
0.00.440.522 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.440.536 I perplexity: tokenizing the input ..
0.00.448.380 I perplexity: tokenization took 7.843 ms
0.00.448.383 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.581.052 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.582.221 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.582.238 I llama_perf_context_print:        load time =     430.39 ms
0.00.582.238 I llama_perf_context_print: prompt eval time =     132.43 ms /   128 tokens (    1.03 ms per token,   966.56 tokens per second)
0.00.582.241 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.582.242 I llama_perf_context_print:       total time =     141.77 ms /   129 tokens
0.00.582.809 I ggml_metal_free: deallocating

real	0m0.598s
user	0m0.079s
sys	0m0.074s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.798 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.473 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.474 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.188 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.189 I llama_model_loader: - type  f32:  194 tensors
0.00.023.189 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.189 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.190 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.190 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.329 I llm_load_vocab: special tokens cache size = 25
0.00.050.344 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.347 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.348 I llm_load_print_meta: arch             = gptneox
0.00.050.348 I llm_load_print_meta: vocab type       = BPE
0.00.050.348 I llm_load_print_meta: n_vocab          = 50304
0.00.050.348 I llm_load_print_meta: n_merges         = 50009
0.00.050.349 I llm_load_print_meta: vocab_only       = 0
0.00.050.349 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.349 I llm_load_print_meta: n_embd           = 2048
0.00.050.349 I llm_load_print_meta: n_layer          = 24
0.00.050.352 I llm_load_print_meta: n_head           = 16
0.00.050.353 I llm_load_print_meta: n_head_kv        = 16
0.00.050.353 I llm_load_print_meta: n_rot            = 32
0.00.050.353 I llm_load_print_meta: n_swa            = 0
0.00.050.353 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.355 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.355 I llm_load_print_meta: n_gqa            = 1
0.00.050.356 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.357 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.357 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.358 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.359 I llm_load_print_meta: n_ff             = 8192
0.00.050.359 I llm_load_print_meta: n_expert         = 0
0.00.050.359 I llm_load_print_meta: n_expert_used    = 0
0.00.050.360 I llm_load_print_meta: causal attn      = 1
0.00.050.360 I llm_load_print_meta: pooling type     = 0
0.00.050.360 I llm_load_print_meta: rope type        = 2
0.00.050.360 I llm_load_print_meta: rope scaling     = linear
0.00.050.361 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.361 I llm_load_print_meta: freq_scale_train = 1
0.00.050.361 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.362 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.362 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.362 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.362 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.362 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.362 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.363 I llm_load_print_meta: model type       = 1.4B
0.00.050.363 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.364 I llm_load_print_meta: model params     = 1.41 B
0.00.050.364 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.364 I llm_load_print_meta: general.name     = 1.4B
0.00.050.365 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.366 I llm_load_print_meta: LF token         = 128 ''
0.00.050.366 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.366 I llm_load_print_meta: max token length = 1024
0.00.052.324 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.325 I llm_load_tensors: offloading output layer to GPU
0.00.052.325 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.335 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.337 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.242 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.242 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.243 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.243 I llama_new_context_with_model: n_batch       = 2048
0.00.053.243 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.243 I llama_new_context_with_model: flash_attn    = 0
0.00.053.244 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.244 I llama_new_context_with_model: freq_scale    = 1
0.00.053.245 I ggml_metal_init: allocating
0.00.053.251 I ggml_metal_init: found device: Apple M4
0.00.053.261 I ggml_metal_init: picking default device: Apple M4
0.00.053.837 I ggml_metal_init: using embedded metal library
0.00.056.153 I ggml_metal_init: GPU name:   Apple M4
0.00.056.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.155 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.155 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.156 I ggml_metal_init: simdgroup reduction   = true
0.00.056.156 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.156 I ggml_metal_init: has bfloat            = true
0.00.056.156 I ggml_metal_init: use bfloat            = true
0.00.056.156 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.157 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.096 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.484 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.488 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.516 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.560 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.561 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.561 I llama_new_context_with_model: graph nodes  = 967
0.00.086.561 I llama_new_context_with_model: graph splits = 2
0.00.086.564 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.860 I main: llama threadpool init, n_threads = 4
0.00.545.900 I 
0.00.545.923 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.924 I 
0.00.546.165 I sampler seed: 1234
0.00.546.169 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.546.185 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.546.185 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.546.185 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.281.408 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48563.61 tokens per second)
0.01.281.410 I llama_perf_context_print:        load time =     537.06 ms
0.01.281.410 I llama_perf_context_print: prompt eval time =      40.49 ms /     7 tokens (    5.78 ms per token,   172.89 tokens per second)
0.01.281.411 I llama_perf_context_print:        eval time =     692.04 ms /    63 runs   (   10.98 ms per token,    91.04 tokens per second)
0.01.281.411 I llama_perf_context_print:       total time =     735.55 ms /    70 tokens
0.01.281.666 I ggml_metal_free: deallocating

real	0m1.298s
user	0m0.111s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.838 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.414 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.419 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.420 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.420 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.420 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.420 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.421 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.421 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.422 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.422 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.423 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.423 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.425 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.425 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.286 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.089 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.090 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.091 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.091 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.092 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.092 I llama_model_loader: - type  f32:  194 tensors
0.00.023.092 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.093 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.093 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.093 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.883 I llm_load_vocab: special tokens cache size = 25
0.00.049.851 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.853 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.854 I llm_load_print_meta: arch             = gptneox
0.00.049.854 I llm_load_print_meta: vocab type       = BPE
0.00.049.854 I llm_load_print_meta: n_vocab          = 50304
0.00.049.854 I llm_load_print_meta: n_merges         = 50009
0.00.049.855 I llm_load_print_meta: vocab_only       = 0
0.00.049.855 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.855 I llm_load_print_meta: n_embd           = 2048
0.00.049.855 I llm_load_print_meta: n_layer          = 24
0.00.049.858 I llm_load_print_meta: n_head           = 16
0.00.049.859 I llm_load_print_meta: n_head_kv        = 16
0.00.049.859 I llm_load_print_meta: n_rot            = 32
0.00.049.860 I llm_load_print_meta: n_swa            = 0
0.00.049.860 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.860 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.861 I llm_load_print_meta: n_gqa            = 1
0.00.049.861 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.862 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.863 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.863 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.863 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.863 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.863 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.864 I llm_load_print_meta: n_ff             = 8192
0.00.049.864 I llm_load_print_meta: n_expert         = 0
0.00.049.866 I llm_load_print_meta: n_expert_used    = 0
0.00.049.866 I llm_load_print_meta: causal attn      = 1
0.00.049.866 I llm_load_print_meta: pooling type     = 0
0.00.049.866 I llm_load_print_meta: rope type        = 2
0.00.049.867 I llm_load_print_meta: rope scaling     = linear
0.00.049.867 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.867 I llm_load_print_meta: freq_scale_train = 1
0.00.049.868 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.868 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.868 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.868 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.868 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.869 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.869 I llm_load_print_meta: model type       = 1.4B
0.00.049.869 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.870 I llm_load_print_meta: model params     = 1.41 B
0.00.049.870 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.870 I llm_load_print_meta: general.name     = 1.4B
0.00.049.871 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: LF token         = 128 ''
0.00.049.872 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: max token length = 1024
0.00.051.818 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.819 I llm_load_tensors: offloading output layer to GPU
0.00.051.819 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.830 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.831 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.716 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.717 I llama_new_context_with_model: n_ctx         = 128
0.00.052.717 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.717 I llama_new_context_with_model: n_batch       = 128
0.00.052.717 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.717 I llama_new_context_with_model: flash_attn    = 0
0.00.052.718 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.718 I llama_new_context_with_model: freq_scale    = 1
0.00.052.718 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.719 I ggml_metal_init: allocating
0.00.052.725 I ggml_metal_init: found device: Apple M4
0.00.052.727 I ggml_metal_init: picking default device: Apple M4
0.00.053.288 I ggml_metal_init: using embedded metal library
0.00.055.602 I ggml_metal_init: GPU name:   Apple M4
0.00.055.604 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.604 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.604 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.605 I ggml_metal_init: simdgroup reduction   = true
0.00.055.605 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.605 I ggml_metal_init: has bfloat            = true
0.00.055.605 I ggml_metal_init: use bfloat            = true
0.00.055.606 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.606 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.929 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.193 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.209 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.071 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.072 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.072 I llama_new_context_with_model: graph nodes  = 967
0.00.067.073 I llama_new_context_with_model: graph splits = 2
0.00.067.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.074 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.247 I 
0.00.474.282 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.296 I perplexity: tokenizing the input ..
0.00.481.979 I perplexity: tokenization took 7.681 ms
0.00.481.982 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.908 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.615.089 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.615.102 I llama_perf_context_print:        load time =     465.40 ms
0.00.615.103 I llama_perf_context_print: prompt eval time =     131.70 ms /   128 tokens (    1.03 ms per token,   971.90 tokens per second)
0.00.615.104 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.104 I llama_perf_context_print:       total time =     140.86 ms /   129 tokens
0.00.615.630 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.078s
sys	0m0.084s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.664 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.390 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.396 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.397 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.397 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.398 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.400 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.400 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.401 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.401 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.402 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.404 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.322 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.341 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.154 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.155 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.155 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.156 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.156 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.156 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.157 I llama_model_loader: - type  f32:  194 tensors
0.00.023.157 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.157 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.157 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.137 I llm_load_vocab: special tokens cache size = 25
0.00.049.959 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.962 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.963 I llm_load_print_meta: arch             = gptneox
0.00.049.963 I llm_load_print_meta: vocab type       = BPE
0.00.049.963 I llm_load_print_meta: n_vocab          = 50304
0.00.049.963 I llm_load_print_meta: n_merges         = 50009
0.00.049.964 I llm_load_print_meta: vocab_only       = 0
0.00.049.964 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.964 I llm_load_print_meta: n_embd           = 2048
0.00.049.964 I llm_load_print_meta: n_layer          = 24
0.00.049.967 I llm_load_print_meta: n_head           = 16
0.00.049.968 I llm_load_print_meta: n_head_kv        = 16
0.00.049.970 I llm_load_print_meta: n_rot            = 32
0.00.049.970 I llm_load_print_meta: n_swa            = 0
0.00.049.970 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.970 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.971 I llm_load_print_meta: n_gqa            = 1
0.00.049.972 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.972 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.975 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.975 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.975 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.976 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.976 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.976 I llm_load_print_meta: n_ff             = 8192
0.00.049.977 I llm_load_print_meta: n_expert         = 0
0.00.049.977 I llm_load_print_meta: n_expert_used    = 0
0.00.049.977 I llm_load_print_meta: causal attn      = 1
0.00.049.977 I llm_load_print_meta: pooling type     = 0
0.00.049.977 I llm_load_print_meta: rope type        = 2
0.00.049.977 I llm_load_print_meta: rope scaling     = linear
0.00.049.978 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.978 I llm_load_print_meta: freq_scale_train = 1
0.00.049.979 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.979 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.979 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.979 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.979 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.981 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.981 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.981 I llm_load_print_meta: model type       = 1.4B
0.00.049.982 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.982 I llm_load_print_meta: model params     = 1.41 B
0.00.049.983 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.983 I llm_load_print_meta: general.name     = 1.4B
0.00.049.983 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.983 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.983 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.984 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.984 I llm_load_print_meta: LF token         = 128 ''
0.00.049.984 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.984 I llm_load_print_meta: max token length = 1024
0.00.052.001 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.001 I llm_load_tensors: offloading output layer to GPU
0.00.052.001 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.012 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.013 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.918 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.919 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.919 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.919 I llama_new_context_with_model: n_batch       = 2048
0.00.052.919 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.919 I llama_new_context_with_model: flash_attn    = 0
0.00.052.920 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.920 I llama_new_context_with_model: freq_scale    = 1
0.00.052.921 I ggml_metal_init: allocating
0.00.052.927 I ggml_metal_init: found device: Apple M4
0.00.052.929 I ggml_metal_init: picking default device: Apple M4
0.00.053.503 I ggml_metal_init: using embedded metal library
0.00.055.866 I ggml_metal_init: GPU name:   Apple M4
0.00.055.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.868 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.869 I ggml_metal_init: simdgroup reduction   = true
0.00.055.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.869 I ggml_metal_init: has bfloat            = true
0.00.055.869 I ggml_metal_init: use bfloat            = true
0.00.055.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.870 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.609 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.759 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.767 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.794 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.793 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.794 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.795 I llama_new_context_with_model: graph nodes  = 967
0.00.085.795 I llama_new_context_with_model: graph splits = 2
0.00.085.797 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.697 I main: llama threadpool init, n_threads = 4
0.00.614.739 I 
0.00.614.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.784 I 
0.00.615.038 I sampler seed: 1234
0.00.615.045 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.615.089 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.615.110 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.615.110 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.373.477 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.373.478 I llama_perf_context_print:        load time =     606.03 ms
0.01.373.478 I llama_perf_context_print: prompt eval time =      51.10 ms /     7 tokens (    7.30 ms per token,   136.98 tokens per second)
0.01.373.479 I llama_perf_context_print:        eval time =     704.20 ms /    63 runs   (   11.18 ms per token,    89.46 tokens per second)
0.01.373.483 I llama_perf_context_print:       total time =     758.78 ms /    70 tokens
0.01.373.716 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.111s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.813 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.312 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.314 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.071 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.962 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.963 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.964 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.964 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.964 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.965 I llama_model_loader: - type  f32:  194 tensors
0.00.022.965 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.965 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.966 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.075 I llm_load_vocab: special tokens cache size = 25
0.00.048.877 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.879 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.880 I llm_load_print_meta: arch             = gptneox
0.00.048.880 I llm_load_print_meta: vocab type       = BPE
0.00.048.880 I llm_load_print_meta: n_vocab          = 50304
0.00.048.881 I llm_load_print_meta: n_merges         = 50009
0.00.048.881 I llm_load_print_meta: vocab_only       = 0
0.00.048.881 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.881 I llm_load_print_meta: n_embd           = 2048
0.00.048.881 I llm_load_print_meta: n_layer          = 24
0.00.048.884 I llm_load_print_meta: n_head           = 16
0.00.048.885 I llm_load_print_meta: n_head_kv        = 16
0.00.048.885 I llm_load_print_meta: n_rot            = 32
0.00.048.885 I llm_load_print_meta: n_swa            = 0
0.00.048.885 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.886 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.887 I llm_load_print_meta: n_gqa            = 1
0.00.048.887 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.888 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.888 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.889 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.889 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.889 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.889 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.890 I llm_load_print_meta: n_ff             = 8192
0.00.048.890 I llm_load_print_meta: n_expert         = 0
0.00.048.890 I llm_load_print_meta: n_expert_used    = 0
0.00.048.891 I llm_load_print_meta: causal attn      = 1
0.00.048.891 I llm_load_print_meta: pooling type     = 0
0.00.048.891 I llm_load_print_meta: rope type        = 2
0.00.048.891 I llm_load_print_meta: rope scaling     = linear
0.00.048.893 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.893 I llm_load_print_meta: freq_scale_train = 1
0.00.048.893 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.894 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.894 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.896 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.896 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.896 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.896 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.897 I llm_load_print_meta: model type       = 1.4B
0.00.048.897 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.897 I llm_load_print_meta: model params     = 1.41 B
0.00.048.898 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.898 I llm_load_print_meta: general.name     = 1.4B
0.00.048.898 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.898 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.899 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.899 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.899 I llm_load_print_meta: LF token         = 128 ''
0.00.048.903 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.904 I llm_load_print_meta: max token length = 1024
0.00.050.489 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.490 I llm_load_tensors: offloading output layer to GPU
0.00.050.490 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.500 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.501 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.316 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.317 I llama_new_context_with_model: n_ctx         = 128
0.00.051.317 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.317 I llama_new_context_with_model: n_batch       = 128
0.00.051.317 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.317 I llama_new_context_with_model: flash_attn    = 0
0.00.051.318 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.318 I llama_new_context_with_model: freq_scale    = 1
0.00.051.319 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.319 I ggml_metal_init: allocating
0.00.051.322 I ggml_metal_init: found device: Apple M4
0.00.051.324 I ggml_metal_init: picking default device: Apple M4
0.00.051.895 I ggml_metal_init: using embedded metal library
0.00.054.284 I ggml_metal_init: GPU name:   Apple M4
0.00.054.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.286 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.286 I ggml_metal_init: simdgroup reduction   = true
0.00.054.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.287 I ggml_metal_init: has bfloat            = true
0.00.054.287 I ggml_metal_init: use bfloat            = true
0.00.054.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.288 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.961 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.193 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.207 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.138 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.139 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.140 I llama_new_context_with_model: graph nodes  = 967
0.00.066.140 I llama_new_context_with_model: graph splits = 2
0.00.066.141 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.562.824 I 
0.00.562.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.562.870 I perplexity: tokenizing the input ..
0.00.570.885 I perplexity: tokenization took 8.014 ms
0.00.570.888 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.177 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.706.346 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.706.367 I llama_perf_context_print:        load time =     554.01 ms
0.00.706.367 I llama_perf_context_print: prompt eval time =     134.06 ms /   128 tokens (    1.05 ms per token,   954.82 tokens per second)
0.00.706.368 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.706.369 I llama_perf_context_print:       total time =     143.54 ms /   129 tokens
0.00.706.828 I ggml_metal_free: deallocating

real	0m0.720s
user	0m0.077s
sys	0m0.107s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.217 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.858 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.863 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.865 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.866 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.866 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.867 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.867 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.868 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.868 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.869 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.523 I llama_model_loader: - type  f32:  194 tensors
0.00.023.523 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.523 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.724 I llm_load_vocab: special tokens cache size = 25
0.00.049.930 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.932 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.933 I llm_load_print_meta: arch             = gptneox
0.00.049.933 I llm_load_print_meta: vocab type       = BPE
0.00.049.933 I llm_load_print_meta: n_vocab          = 50304
0.00.049.933 I llm_load_print_meta: n_merges         = 50009
0.00.049.934 I llm_load_print_meta: vocab_only       = 0
0.00.049.934 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.934 I llm_load_print_meta: n_embd           = 2048
0.00.049.934 I llm_load_print_meta: n_layer          = 24
0.00.049.937 I llm_load_print_meta: n_head           = 16
0.00.049.937 I llm_load_print_meta: n_head_kv        = 16
0.00.049.938 I llm_load_print_meta: n_rot            = 32
0.00.049.938 I llm_load_print_meta: n_swa            = 0
0.00.049.938 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.938 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.939 I llm_load_print_meta: n_gqa            = 1
0.00.049.940 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.940 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.941 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.941 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.941 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.942 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.942 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.944 I llm_load_print_meta: n_ff             = 8192
0.00.049.944 I llm_load_print_meta: n_expert         = 0
0.00.049.944 I llm_load_print_meta: n_expert_used    = 0
0.00.049.944 I llm_load_print_meta: causal attn      = 1
0.00.049.944 I llm_load_print_meta: pooling type     = 0
0.00.049.944 I llm_load_print_meta: rope type        = 2
0.00.049.945 I llm_load_print_meta: rope scaling     = linear
0.00.049.945 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.945 I llm_load_print_meta: freq_scale_train = 1
0.00.049.946 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.946 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.946 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.946 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.946 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.946 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.946 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.947 I llm_load_print_meta: model type       = 1.4B
0.00.049.947 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.948 I llm_load_print_meta: model params     = 1.41 B
0.00.049.948 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.948 I llm_load_print_meta: general.name     = 1.4B
0.00.049.949 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.949 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.949 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.949 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: LF token         = 128 ''
0.00.049.950 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: max token length = 1024
0.00.051.980 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.980 I llm_load_tensors: offloading output layer to GPU
0.00.051.980 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.991 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.992 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.899 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.899 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.900 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.900 I llama_new_context_with_model: n_batch       = 2048
0.00.052.900 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.900 I llama_new_context_with_model: flash_attn    = 0
0.00.052.901 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.901 I llama_new_context_with_model: freq_scale    = 1
0.00.052.901 I ggml_metal_init: allocating
0.00.052.904 I ggml_metal_init: found device: Apple M4
0.00.052.906 I ggml_metal_init: picking default device: Apple M4
0.00.053.479 I ggml_metal_init: using embedded metal library
0.00.055.780 I ggml_metal_init: GPU name:   Apple M4
0.00.055.781 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.782 I ggml_metal_init: simdgroup reduction   = true
0.00.055.782 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.782 I ggml_metal_init: has bfloat            = true
0.00.055.783 I ggml_metal_init: use bfloat            = true
0.00.055.783 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.783 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.557 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.593 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.598 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.614 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.673 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.675 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.676 I llama_new_context_with_model: graph nodes  = 967
0.00.085.676 I llama_new_context_with_model: graph splits = 2
0.00.085.679 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.827 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.965 I main: llama threadpool init, n_threads = 4
0.00.688.008 I 
0.00.688.033 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.034 I 
0.00.688.271 I sampler seed: 1234
0.00.688.275 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.688.309 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.688.311 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.688.311 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.544.100 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.544.101 I llama_perf_context_print:        load time =     678.74 ms
0.01.544.102 I llama_perf_context_print: prompt eval time =      58.30 ms /     7 tokens (    8.33 ms per token,   120.06 tokens per second)
0.01.544.102 I llama_perf_context_print:        eval time =     794.55 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.544.103 I llama_perf_context_print:       total time =     856.14 ms /    70 tokens
0.01.544.318 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.493 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.884 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.673 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.742 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.549 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.550 I llama_model_loader: - type  f32:  194 tensors
0.00.023.550 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.550 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.473 I llm_load_vocab: special tokens cache size = 25
0.00.049.440 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.444 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.444 I llm_load_print_meta: arch             = gptneox
0.00.049.444 I llm_load_print_meta: vocab type       = BPE
0.00.049.445 I llm_load_print_meta: n_vocab          = 50304
0.00.049.445 I llm_load_print_meta: n_merges         = 50009
0.00.049.445 I llm_load_print_meta: vocab_only       = 0
0.00.049.447 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.447 I llm_load_print_meta: n_embd           = 2048
0.00.049.447 I llm_load_print_meta: n_layer          = 24
0.00.049.450 I llm_load_print_meta: n_head           = 16
0.00.049.451 I llm_load_print_meta: n_head_kv        = 16
0.00.049.451 I llm_load_print_meta: n_rot            = 32
0.00.049.451 I llm_load_print_meta: n_swa            = 0
0.00.049.451 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.451 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.452 I llm_load_print_meta: n_gqa            = 1
0.00.049.453 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.453 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.454 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.454 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.455 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.455 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.455 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.456 I llm_load_print_meta: n_ff             = 8192
0.00.049.456 I llm_load_print_meta: n_expert         = 0
0.00.049.456 I llm_load_print_meta: n_expert_used    = 0
0.00.049.456 I llm_load_print_meta: causal attn      = 1
0.00.049.456 I llm_load_print_meta: pooling type     = 0
0.00.049.456 I llm_load_print_meta: rope type        = 2
0.00.049.457 I llm_load_print_meta: rope scaling     = linear
0.00.049.459 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.459 I llm_load_print_meta: freq_scale_train = 1
0.00.049.459 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.460 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.460 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.460 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.460 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.460 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.460 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.460 I llm_load_print_meta: model type       = 1.4B
0.00.049.461 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.461 I llm_load_print_meta: model params     = 1.41 B
0.00.049.463 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.463 I llm_load_print_meta: general.name     = 1.4B
0.00.049.463 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.464 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.464 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.464 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.464 I llm_load_print_meta: LF token         = 128 ''
0.00.049.464 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.465 I llm_load_print_meta: max token length = 1024
0.00.051.440 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.440 I llm_load_tensors: offloading output layer to GPU
0.00.051.440 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.451 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.452 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.335 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.336 I llama_new_context_with_model: n_ctx         = 128
0.00.052.336 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.336 I llama_new_context_with_model: n_batch       = 128
0.00.052.336 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.337 I llama_new_context_with_model: flash_attn    = 0
0.00.052.337 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.337 I llama_new_context_with_model: freq_scale    = 1
0.00.052.338 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.338 I ggml_metal_init: allocating
0.00.052.341 I ggml_metal_init: found device: Apple M4
0.00.052.344 I ggml_metal_init: picking default device: Apple M4
0.00.052.923 I ggml_metal_init: using embedded metal library
0.00.055.245 I ggml_metal_init: GPU name:   Apple M4
0.00.055.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.248 I ggml_metal_init: simdgroup reduction   = true
0.00.055.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.248 I ggml_metal_init: has bfloat            = true
0.00.055.249 I ggml_metal_init: use bfloat            = true
0.00.055.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.759 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.087 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.090 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.107 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.019 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.020 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.021 I llama_new_context_with_model: graph nodes  = 967
0.00.067.021 I llama_new_context_with_model: graph splits = 2
0.00.067.022 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.835 I 
0.00.625.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.886 I perplexity: tokenizing the input ..
0.00.633.635 I perplexity: tokenization took 7.747 ms
0.00.633.638 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.464 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.775.702 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.775.735 I llama_perf_context_print:        load time =     616.34 ms
0.00.775.738 I llama_perf_context_print: prompt eval time =     140.60 ms /   128 tokens (    1.10 ms per token,   910.40 tokens per second)
0.00.775.739 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.775.739 I llama_perf_context_print:       total time =     149.90 ms /   129 tokens
0.00.776.239 I ggml_metal_free: deallocating

real	0m0.791s
user	0m0.077s
sys	0m0.107s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.744 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.423 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.427 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.428 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.435 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.436 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.437 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.437 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.437 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.440 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.344 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.188 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.189 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.189 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.190 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.190 I llama_model_loader: - type  f32:  194 tensors
0.00.023.191 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.975 I llm_load_vocab: special tokens cache size = 25
0.00.050.076 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.079 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.079 I llm_load_print_meta: arch             = gptneox
0.00.050.079 I llm_load_print_meta: vocab type       = BPE
0.00.050.080 I llm_load_print_meta: n_vocab          = 50304
0.00.050.080 I llm_load_print_meta: n_merges         = 50009
0.00.050.080 I llm_load_print_meta: vocab_only       = 0
0.00.050.080 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.080 I llm_load_print_meta: n_embd           = 2048
0.00.050.080 I llm_load_print_meta: n_layer          = 24
0.00.050.083 I llm_load_print_meta: n_head           = 16
0.00.050.084 I llm_load_print_meta: n_head_kv        = 16
0.00.050.084 I llm_load_print_meta: n_rot            = 32
0.00.050.084 I llm_load_print_meta: n_swa            = 0
0.00.050.085 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.085 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.085 I llm_load_print_meta: n_gqa            = 1
0.00.050.086 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.087 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.087 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.088 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.088 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.088 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.088 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.089 I llm_load_print_meta: n_ff             = 8192
0.00.050.089 I llm_load_print_meta: n_expert         = 0
0.00.050.089 I llm_load_print_meta: n_expert_used    = 0
0.00.050.089 I llm_load_print_meta: causal attn      = 1
0.00.050.089 I llm_load_print_meta: pooling type     = 0
0.00.050.089 I llm_load_print_meta: rope type        = 2
0.00.050.090 I llm_load_print_meta: rope scaling     = linear
0.00.050.092 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.092 I llm_load_print_meta: freq_scale_train = 1
0.00.050.092 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.093 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.093 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.094 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.094 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.094 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.095 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.095 I llm_load_print_meta: model type       = 1.4B
0.00.050.095 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.096 I llm_load_print_meta: model params     = 1.41 B
0.00.050.096 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.096 I llm_load_print_meta: general.name     = 1.4B
0.00.050.096 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: LF token         = 128 ''
0.00.050.101 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.101 I llm_load_print_meta: max token length = 1024
0.00.052.162 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.163 I llm_load_tensors: offloading output layer to GPU
0.00.052.163 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.173 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.174 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.088 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.089 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.090 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.090 I llama_new_context_with_model: n_batch       = 2048
0.00.053.090 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.090 I llama_new_context_with_model: flash_attn    = 0
0.00.053.091 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.091 I llama_new_context_with_model: freq_scale    = 1
0.00.053.091 I ggml_metal_init: allocating
0.00.053.095 I ggml_metal_init: found device: Apple M4
0.00.053.097 I ggml_metal_init: picking default device: Apple M4
0.00.053.687 I ggml_metal_init: using embedded metal library
0.00.056.011 I ggml_metal_init: GPU name:   Apple M4
0.00.056.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.013 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.013 I ggml_metal_init: simdgroup reduction   = true
0.00.056.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.013 I ggml_metal_init: has bfloat            = true
0.00.056.014 I ggml_metal_init: use bfloat            = true
0.00.056.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.908 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.753 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.760 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.781 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.794 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.796 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.796 I llama_new_context_with_model: graph nodes  = 967
0.00.085.796 I llama_new_context_with_model: graph splits = 2
0.00.085.799 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.931 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.691 I main: llama threadpool init, n_threads = 4
0.00.731.739 I 
0.00.731.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.782 I 
0.00.731.942 I sampler seed: 1234
0.00.731.946 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.979 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.981 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.981 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.613.491 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.613.491 I llama_perf_context_print:        load time =     722.94 ms
0.01.613.492 I llama_perf_context_print: prompt eval time =      54.32 ms /     7 tokens (    7.76 ms per token,   128.86 tokens per second)
0.01.613.493 I llama_perf_context_print:        eval time =     824.25 ms /    63 runs   (   13.08 ms per token,    76.43 tokens per second)
0.01.613.493 I llama_perf_context_print:       total time =     881.80 ms /    70 tokens
0.01.613.699 I ggml_metal_free: deallocating

real	0m1.631s
user	0m0.111s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4428 (bbb17be3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.991 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.300 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.301 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.302 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.302 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.303 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.304 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.304 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.305 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.308 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.308 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.309 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.310 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.310 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.311 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.088 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.054 I llama_model_loader: - type  f32:  194 tensors
0.00.023.054 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.064 I llm_load_vocab: special tokens cache size = 25
0.00.048.982 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.986 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.986 I llm_load_print_meta: arch             = gptneox
0.00.048.986 I llm_load_print_meta: vocab type       = BPE
0.00.048.987 I llm_load_print_meta: n_vocab          = 50304
0.00.048.987 I llm_load_print_meta: n_merges         = 50009
0.00.048.989 I llm_load_print_meta: vocab_only       = 0
0.00.048.989 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.989 I llm_load_print_meta: n_embd           = 2048
0.00.048.989 I llm_load_print_meta: n_layer          = 24
0.00.048.992 I llm_load_print_meta: n_head           = 16
0.00.048.993 I llm_load_print_meta: n_head_kv        = 16
0.00.048.993 I llm_load_print_meta: n_rot            = 32
0.00.048.993 I llm_load_print_meta: n_swa            = 0
0.00.048.994 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.998 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.998 I llm_load_print_meta: n_gqa            = 1
0.00.048.999 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.001 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.002 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.003 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.003 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.003 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.003 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.004 I llm_load_print_meta: n_ff             = 8192
0.00.049.004 I llm_load_print_meta: n_expert         = 0
0.00.049.004 I llm_load_print_meta: n_expert_used    = 0
0.00.049.007 I llm_load_print_meta: causal attn      = 1
0.00.049.007 I llm_load_print_meta: pooling type     = 0
0.00.049.007 I llm_load_print_meta: rope type        = 2
0.00.049.007 I llm_load_print_meta: rope scaling     = linear
0.00.049.007 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.007 I llm_load_print_meta: freq_scale_train = 1
0.00.049.008 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.008 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.008 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.008 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.008 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.008 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.009 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.009 I llm_load_print_meta: model type       = 1.4B
0.00.049.009 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.010 I llm_load_print_meta: model params     = 1.41 B
0.00.049.010 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.010 I llm_load_print_meta: general.name     = 1.4B
0.00.049.010 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.011 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.011 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.011 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.011 I llm_load_print_meta: LF token         = 128 ''
0.00.049.011 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.012 I llm_load_print_meta: max token length = 1024
0.00.050.851 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.851 I llm_load_tensors: offloading output layer to GPU
0.00.050.852 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.857 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.858 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.810 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.811 I llama_new_context_with_model: n_ctx         = 128
0.00.051.811 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.812 I llama_new_context_with_model: n_batch       = 128
0.00.051.812 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.812 I llama_new_context_with_model: flash_attn    = 0
0.00.051.812 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.813 I llama_new_context_with_model: freq_scale    = 1
0.00.051.813 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.813 I ggml_metal_init: allocating
0.00.051.820 I ggml_metal_init: found device: Apple M4
0.00.051.824 I ggml_metal_init: picking default device: Apple M4
0.00.052.418 I ggml_metal_init: using embedded metal library
0.00.054.757 I ggml_metal_init: GPU name:   Apple M4
0.00.054.759 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.760 I ggml_metal_init: simdgroup reduction   = true
0.00.054.760 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.760 I ggml_metal_init: has bfloat            = true
0.00.054.760 I ggml_metal_init: use bfloat            = true
0.00.054.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.335 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.955 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.961 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.976 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.826 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.827 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.828 I llama_new_context_with_model: graph nodes  = 967
0.00.066.828 I llama_new_context_with_model: graph splits = 2
0.00.066.829 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.829 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.469 I 
0.00.521.490 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.504 I perplexity: tokenizing the input ..
0.00.529.358 I perplexity: tokenization took 7.852 ms
0.00.529.362 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.669.395 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.670.572 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.670.595 I llama_perf_context_print:        load time =     512.48 ms
0.00.670.596 I llama_perf_context_print: prompt eval time =     139.80 ms /   128 tokens (    1.09 ms per token,   915.59 tokens per second)
0.00.670.597 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.670.597 I llama_perf_context_print:       total time =     149.12 ms /   129 tokens
0.00.671.087 I ggml_metal_free: deallocating

real	0m0.684s
user	0m0.077s
sys	0m0.095s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4428 (bbb17be3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c60a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c60a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c60af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c60b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c60bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c60c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c60c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c60cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c60d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c60d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c60db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c60e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c60eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c60f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c60fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c610270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c610990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c6110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c6117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c611fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c6126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c612de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c613500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c613da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c6144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c614780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c614d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c615a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c615f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c6166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c6171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c617730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c6179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c617e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c618330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c6187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c618c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c619110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c6195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c619a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c619ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c61a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c61a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c61ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c61b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c61bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c61c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c61c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c61cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c61d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c61d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c61dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c61e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c61ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c61f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c61f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c61f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c6201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c6204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c620940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c621280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c622060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c622500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c6229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c6232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c623780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c623c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c624170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c6246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c624c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c625160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c6256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c625c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c626150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c6266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c626bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c627140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c627690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c627be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c628130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c628680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c628bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c629120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c629670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c629bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c62a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c62a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c62abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c62b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c62b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c62bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c61b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c62c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c62c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c62cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c62d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c62d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c62dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c62e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c62e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c62ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c62f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c62f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c62fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c630230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c630780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c630cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c631170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c631610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c631ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c631f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c6323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c632890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c632d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c6331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c633670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c633b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c633fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c634450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c6348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c634d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c6356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c635b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c636010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c6364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c636950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c636df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c637290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c637730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c637bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c638070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c638510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c6389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c638e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c6392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c639790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c639c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c63a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c63a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c63aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c63aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c63b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c63b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c63bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c63c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c63c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c63ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c63cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c63d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c63d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c63dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c63e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c63e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c63ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c63ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c63f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c63f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c63fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c6401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c640690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c640b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c640fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c641470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c641910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c641db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c642250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c6426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c642b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c643030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c6434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c643970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c643e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c6442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c644750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c644bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c645090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c645530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c6459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c645e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c646310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c6467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c646c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c6470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c647590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c647a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c647ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c648420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c648970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c648ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c649410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c6496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c649ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c64a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c64a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c64b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c64b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c64b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c64be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c64c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c64cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c64d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c64d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c64da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c64e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c64e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c64ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c64f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c64f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c64fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c6501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c650720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c650c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c6511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c651710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c651c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c6521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c652700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c604080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c6044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c604960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c604dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c605240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c6056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c605b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c605f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c606400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c606870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c606ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c607150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c6075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c607a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c607ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c608310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c608780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c608bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c609060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c6094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c609940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c609db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c60a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c60a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c60ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c60af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c60b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c60b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c60bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c60c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c60c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c60ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c60ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c60d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c60d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c60dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c60e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c60e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c60e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c60ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c60f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c60f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c60fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c60ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c6103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c610830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c610ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c611110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c611580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c6119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c611e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c6122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c612740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c612bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c613020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c613490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c613900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c613d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c6141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c614d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c615440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c615b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c616280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c616800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c616c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c6170e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.149.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.149.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c649990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c649fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c64a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c61d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c61d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c61f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c64c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c614a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c61b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c61be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c61c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c61af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c61dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c613a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c61fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c62c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c616ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c64c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c64abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c615310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c6155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c6529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c652c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c652f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c653200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c6534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c653780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c653a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c653d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c653fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c654280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c654540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c654800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c654ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c654d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c655040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c655300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c6555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c655880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c655b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c655e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c6560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c656380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c656900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c656bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c656e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c657140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c6576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c657980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c657c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c657f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c6581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c658480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c658740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c658a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c658cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c658f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c659240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c659500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c6597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c659a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c659d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c65a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c65a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c65a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c65a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c65ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c65adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c65b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c65b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c65b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c65b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c65bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c65be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c65c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c65c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c65c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c65c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c65cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c65cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c65d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c65d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c65d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c65d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c65dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c65df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c65e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c65e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c65e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c65ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c65ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c65efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c65f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c65f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c65f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c65fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c65fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c660040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c660300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c6605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c660880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c660b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c660e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c6610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c661380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c661640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c661900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c661bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c661e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c662140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c662400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c6626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c662980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c662c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c662f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c6631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c663480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c663740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c663a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c663cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c663f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c664240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c664500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c6647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c664a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c664d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c665000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c6652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c665580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c665840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c665b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c665dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c666080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c666340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c666600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c6668c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c666b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c666e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c667100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c6673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c667940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c667c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c6680a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c668540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c668940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c668c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c669100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c669610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c669b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c66a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c66a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c66aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c66af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c66b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c66b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c66be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c66c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c66c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c66cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c66d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c66d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c66dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c66e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c66e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c66ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c66eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c66f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c66f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c66fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c670070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c6704e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c670950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c670dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c671230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c6716a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c671b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c671f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c6723f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c672860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c672cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c673140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c6735b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c673a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c673e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c674300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c674770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c674be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c675050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c6754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c675930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c675da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c676210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c676680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c676af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c676f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c6773d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c677840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c677cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c678120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c678590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c678a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c678e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c6793f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c679860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c679cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c67a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c67aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c67ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c67b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c67b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c67baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c67bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c67c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c67c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c67ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c67d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c67d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c67da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c67de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c67e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c67e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c67ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c67f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c67f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c67f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c67fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c6801f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c680660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c680ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c680f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c6813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c681820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c681c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c682100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c682570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c6829e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c682e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c6832c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c683730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c683ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c684010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c684480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c6848f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c684d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c6851d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c685640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c685ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c685f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c686390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c686800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c686c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c6870e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c687550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c6879c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c687e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c6882a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c688710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c688b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c688ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c689460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c6898d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c689d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c68a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c68a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c68aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c68af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c68b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c68b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c68bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c68c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c68c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c68c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c68ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c68d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c68d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c68db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c68dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c68e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c68eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c68f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c68fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c690410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c6906d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c690b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c691140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c691750 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c690e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c64bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c691400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c68e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c679f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c691bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c691e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c692130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c6923f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c6926b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c692970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c692c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c693200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c6937d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c693e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c6940c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c694380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c694640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c694900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c694bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c694e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c695140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c695400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c6956c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c695980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c695c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c695f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c6961c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c696480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c696740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c696a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c696cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c696f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c697240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c697500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c6977c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c697a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c697d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c698000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c6982c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c698580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c698840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c698b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c698dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c699080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c699340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c699600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c6998c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c699b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c699e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c69a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c69a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c69a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c69a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c69ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c69aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c69b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c69b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c69b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c69b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c69bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c69bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c69c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c69c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c69c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c69ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c69cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c69cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c69d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c69d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c69d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c69dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c69dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c69e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c69e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c69e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c69e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c69eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c69ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c69f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c69f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c69f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c69f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c69fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c69fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c6a0140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c6a0400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c6a06c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c6a0980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c6a0c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c6a0f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c6a11c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c6a1480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c6a1740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c6a1a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c6a1cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c6a1f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c6a2240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c6a2500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c6a27c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c6a2a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c6a2d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c6a3000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c6a32c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c6a3580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c6a3840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c6a3b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c6a3dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c6a4080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c6a4340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c6a4600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c6a48c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c6a4b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c6a4e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c6a5100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c6a53c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c6a5680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c6a5940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c6a5c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c6a5ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c6a6180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c6a6440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c6a6700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c6a69c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c6a6c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c6a6f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c6a7200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c6a74c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c6a7780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c6a7a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c6a7d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c6a7fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c6a8280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c6a8540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c6a8800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c6a8ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c6a8d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c6a9040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c6a9300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c6a95c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c6a9880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c6a9b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c6a9e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c6aa0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c6aa380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c6aa640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c6aa900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c6aabc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c6aae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c6ab140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c6ab400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c6ab6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c6ab980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c6abc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c6abf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c6ac1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c6ac480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c6ac740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c6aca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c6accc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c6acf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c6ad240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c6ad500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c6ad7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c6ada80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c6add40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c6ae000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c6ae2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c6ae580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c6ae840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c6aeb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c6aedc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c6af080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c6af340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c6af600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c6af8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c6afb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c6afe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c6b0100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c6b03c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c6b0680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c6b0940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c6b0c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c6b0ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c6b1180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c6b1440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c6b1700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c6b19c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c6b1c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c6b1f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c6b2200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c6b24c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c6b2780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c6b2a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c6b2d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c6b2fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c6b3280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c6b3540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c6b3800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c6b3ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c6b3d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c6b4040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c6b4300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c6b45c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c6b4880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c6b4b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c6b4e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c6b50c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c6b5380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c6b5640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c6b5c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c6b5ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c6b6190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c6b6450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c6b6710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c6b69d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c6b6c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c6b6f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c6b7210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c6b74d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c6b7790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c6b7a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c6b7d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c6b7fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c6b8290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c6b8550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c6b8810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c6b8ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c6b8d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c6b9050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c6b9310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c6b95d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c6b9890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c6b9b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c6b9e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c6ba0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c6ba390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c6ba650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c6ba910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c6babd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c6bae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c6bb150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c6bb410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c6bb6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c6bb990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c6bbc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c6bbf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c6bc1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c6bc490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c6bc750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c6bca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c6bccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c6bcf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c6bd250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c6bd510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c6bd7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c6bda90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c6bdd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c6be010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c6be2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c6be590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c6be850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c6beb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c6bedd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c6bf090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c6bf350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c6bf610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c6bf8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c6bfb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c6bfe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c6c0110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c6c0580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c6c09f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c6c0e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c6c12d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c6c1740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c6c1bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c6c2020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c6c2490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c6c2900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c6c2d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c6c31e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c6c3650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c6c4190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c6c48b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c6c4fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c6c56f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c6c59b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c6c5c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c6c60e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c6c6550 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.846s
user	0m0.299s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4428 (bbb17be3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1557103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155710af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1557110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155711650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155711c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1557121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155712760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155712d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1557132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1557137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155713cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1557141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155714ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155715490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155715ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1557163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155716ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155717200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155717920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1557180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155718810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155718f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155719650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155719ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15571a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15571a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15571aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15571bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15571c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15571c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15571c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15571cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15571d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15571d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15571db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15571dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15571e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15571e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15571edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15571f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15571f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15571fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155720040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1557204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1557207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155720db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1557213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155721ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1557222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155722900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155722f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155723520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155723b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155724140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155724930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155724dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155725270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155725530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155725b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155726330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1557265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155726a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155726f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1557273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155727870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155727d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1557281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155728650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155728af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155728f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155729430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1557298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155729d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15572a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15572a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15572ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15572b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15572b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15572bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15572c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15572c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15572cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15572d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15572d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15572dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15572e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15572e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15572ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15572f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15572f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15572fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155730260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1557307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155730d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155731250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1557317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155731cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1557219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155732160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155732910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155732e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1557333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155733900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155733e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1557343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1557348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155734e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155735390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1557358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155735e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155736380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1557368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155736e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1557372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155737760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155737c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1557380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155738540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1557389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155738e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155739320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1557397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155739c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15573a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15573a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15573aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15573aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15573b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15573b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15573bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15573c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15573c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15573caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15573cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15573d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15573d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15573dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15573e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15573e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15573eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15573efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15573f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15573f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15573fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155740220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1557406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155740b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155741000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1557414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155741940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155741de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155742280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155742720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155742bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155743060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155743500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1557439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155743e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1557442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155744780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155744c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1557450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155745560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155745a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155745ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155746340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1557467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155747120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1557475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155747a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155747f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1557483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155748840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155748ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155749180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155749620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155749ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155749f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15574a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15574a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15574ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15574b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15574b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15574bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15574bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15574c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15574c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15574cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15574d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15574d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15574db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15574e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15574e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15574eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15574f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15574f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15574f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15574fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155750440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155750a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155751240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1557516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1557519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155751fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1557525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155752db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155753250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1557536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155753b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155754340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155754890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155754de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155755330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155755880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155755dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155756320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155756870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155756dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155757310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155757860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155757db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155758300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155758850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155758da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1557592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155759840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155759d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15575a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15575a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15575ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15575b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15575b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15575bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15575c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15575c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15575cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15575d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15575d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15575dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15575e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15575e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15575ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15575f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15575f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15575fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155760280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1557607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155760d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155761270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1557617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155761d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155762260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1557627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155762d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155763250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1557637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155763cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155764240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155764790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155764ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155765230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155765780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155765cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155766220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155766770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155766cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155767160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155767600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155767aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155767f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1557683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155768880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155768d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1557691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155769660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155769b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155769fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15576a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15576a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15576ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15576b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15576b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15576be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15576c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15576ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15576d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15576d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15576dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15576e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15576e770 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.198 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15576e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1557500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15574fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155750700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1557237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1557231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1557257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15571ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155721680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155721fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1557225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155720a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155722bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155719b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155725e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155732420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15576d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15571cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15571d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155752880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155750d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15571b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15571b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15571b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15576ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15576ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15576f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15576f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15576f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15576f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15576fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15576ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1557701d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155770490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155770750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155770a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155770cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155770f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155771250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155771510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1557717d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155771a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155771d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155772010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1557722d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155772590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155772850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155772b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155772dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155773090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155773350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155773610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1557738d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155773b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155773e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155774110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1557743d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155774690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155774950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155774c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155774ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155775190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155775450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155775710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1557759d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155775c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155775f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155776210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1557764d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155776790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155776a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155776d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155776fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155777290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155777550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155777810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155777ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155777d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155778050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155778310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1557785d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155778890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155778b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155778e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1557790d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155779390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155779650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155779910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155779bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155779e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15577a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15577a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15577a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15577a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15577ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15577af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15577b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15577b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15577b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15577ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15577bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15577bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15577c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15577c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15577c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15577ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15577cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15577d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15577d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15577d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15577d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15577db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15577ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15577e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15577e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15577e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15577e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15577eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15577ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15577f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15577f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15577f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15577f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15577fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15577fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155780190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155780450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155780710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1557809d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155780c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155780f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155781210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1557814d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155781790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155781a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155781d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155781fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155782290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155782550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155782810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155782ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155782d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155783050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155783310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1557835d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155783890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155783b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155783e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1557840d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155784390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155784650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155784910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155784bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155784e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155785150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155785410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1557856d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155785990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155785c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155785f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1557861d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155786490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155786750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155786a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155786cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155786f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155787250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155787510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1557877d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155787a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155787d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155788010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1557882d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155788590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155788850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155788b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155788dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155789090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155789350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155789610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1557898d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155789b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155789e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15578a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15578a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15578a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15578a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15578ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15578aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15578b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15578b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15578b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15578b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15578bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15578bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15578c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15578c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15578c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15578ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15578cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15578cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15578d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15578d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15578d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15578dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15578dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15578e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15578e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15578e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15578eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15578ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15578f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15578f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15578fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15578fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1557902e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155790750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155790bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155791030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1557914a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155791910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155791d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1557921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155792660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155792ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155792f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1557933b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155793820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155793c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155794100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155794570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1557949e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155794e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1557952c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155795730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155795ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155796010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155796480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1557968f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155796d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1557971d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155797640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155797ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155797f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155798390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155798800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155798c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1557990e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155799550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1557999c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155799e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15579a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15579a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15579ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15579aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15579b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15579b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15579bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15579c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15579c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15579ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15579cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15579d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15579d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15579dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15579e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15579e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15579e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15579ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15579f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15579f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15579fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15579ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1557a0440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1557a08b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1557a0d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1557a1190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1557a1600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1557a1a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1557a1ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1557a2350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1557a27c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1557a3230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1557a3950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1557a4070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1557a4790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1557a4a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1557a5240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1557a5500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1557a5b10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156a046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156a04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156a04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156a05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156a058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156a05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156a06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156a065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156a06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156a06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156a07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156a079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156a08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156a08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156a094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156a09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156a0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156a0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156a0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156a0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156a0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156a0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156a0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156a0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156a0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156a0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156a0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156a0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156a0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156a0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156a0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156a0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156a0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156a10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156a104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156a10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156a10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156a11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156a11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156a11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156a11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156a123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156a12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156a12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156a13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156a13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156a13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156a13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156a142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156a14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156a14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156a15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156a154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156a15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156a15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156a161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156a16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156a16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156a170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156a17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156a179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156a17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156a18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156a18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156a18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156a18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156a19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156a198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156a19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156a1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156a1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156a1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156a1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156a1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156a1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156a1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156a1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156a1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156a1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156a1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156a1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156a1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156a1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156a1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156a1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156a1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156a1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156a1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156a1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156a1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156a1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156a20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156a207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156a20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156a21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156a21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156a21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156a21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156a22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156a226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156a22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156a22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156a23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156a23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156a23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156a243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156a24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156a24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156a25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156a25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156a25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156a25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156a262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156a26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156a26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156a27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156a274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156a27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156a27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156a281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156a28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156a28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156a28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156a293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156a29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156a29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156a2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156a2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156a2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156a2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156a2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156a2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156a2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156a2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156a2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156a2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156a2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156a2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156a2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156a2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156a2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156a2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156a2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156a2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156a2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156a2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156a2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156a2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156a302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156a30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156a30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156a30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156a31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156a318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156a31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156a321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156a32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156a32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156a32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156a33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156a337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156a33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156a340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156a34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156a349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156a34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156a35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156a356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156a35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156a35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156a36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156a368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156a36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156a37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156a37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156a37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156a37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156a38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156a387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156a38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156a390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156a39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156a39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156a39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156a3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156a3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156a3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156a3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156a3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156a3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156a3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156a3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156a3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156a3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156a3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156a3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156a3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156a3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156a3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156a3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156a3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156a3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156a3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156a3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156a3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156a3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156a40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156a40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156a40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156a41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156a41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156a41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156a42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156a426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156a42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156a42fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156a43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156a43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156a43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156a44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156a445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156a44a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156a44eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156a45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156a45790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156a45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156a46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156a464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156a46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156a46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156a47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156a476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156a47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156a47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156a483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156a48860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156a48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156a49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156a495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156a49a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156a49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156a4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156a4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156a4abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156a4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156a4b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156a4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156a4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156a4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156a4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156a4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156a4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156a4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156a4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156a4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156a4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156a4e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156a4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156a4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156a4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156a4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156a4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156a50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156a504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156a50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156a50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156a511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156a51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156a51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156a51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156a523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156a52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156a52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156a53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156a53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156a539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156a53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156a542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156a54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156a54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156a55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156a55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156a558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156a56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156a56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156a571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156a578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156a57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156a57ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156a585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156a58c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.912s
user	0m0.241s
sys	0m0.134s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.62 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.16 sec*proc (2 tests)

Total Test time (real) =   1.17 sec
        1.19 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.15 user         0.04 sys
```
