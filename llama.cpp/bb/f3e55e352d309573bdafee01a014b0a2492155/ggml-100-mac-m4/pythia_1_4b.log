Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.524s
user	0m0.882s
sys	0m1.211s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Linking C executable ../bin/test-c
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Built target llava_static
[ 43%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-sampling
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Built target test-log
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Built target test-arg-parser
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-autorelease
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-quantize-perf
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target test-rope
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-batched
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gritlm
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-eval-callback
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Built target llama-infill
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Generating loading.html.hpp
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Built target llama-lookup-create
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-lookup-merge
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-parallel
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-run
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-tts
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.077s
user	0m6.028s
sys	0m9.649s

main: quantize time =  4925.48 ms
main:    total time =  4925.48 ms

main: quantize time =  1955.00 ms
main:    total time =  1955.00 ms

main: quantize time =  2259.89 ms
main:    total time =  2259.89 ms

main: quantize time =  2788.58 ms
main:    total time =  2788.58 ms

main: quantize time =  2851.97 ms
main:    total time =  2851.97 ms

main: quantize time =  5361.41 ms
main:    total time =  5361.42 ms

main: quantize time =  5736.06 ms
main:    total time =  5736.06 ms

main: quantize time =  6787.70 ms
main:    total time =  6787.70 ms

main: quantize time =  5856.64 ms
main:    total time =  5856.64 ms

main: quantize time =  4559.89 ms
main:    total time =  4559.89 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.209 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.327 I main: llama backend init
0.00.000.333 I main: load the model and apply lora adapter, if any
0.00.036.698 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.050.234 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.050.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.050.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.050.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.050.255 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.050.256 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.050.256 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.050.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.050.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.050.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.050.262 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.050.263 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.050.263 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.050.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.050.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.050.269 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.050.270 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.061.241 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.068.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.068.380 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.068.381 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.068.381 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.068.381 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.068.383 I llama_model_loader: - type  f32:  194 tensors
0.00.068.383 I llama_model_loader: - type  f16:   98 tensors
0.00.068.384 I print_info: file format = GGUF V3 (latest)
0.00.068.385 I print_info: file type   = all F32 (guessed)
0.00.068.388 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.427 I load: special tokens cache size = 25
0.00.103.188 I load: token to piece cache size = 0.2984 MB
0.00.103.191 I print_info: arch             = gptneox
0.00.103.191 I print_info: vocab_only       = 0
0.00.103.191 I print_info: n_ctx_train      = 2048
0.00.103.191 I print_info: n_embd           = 2048
0.00.103.191 I print_info: n_layer          = 24
0.00.103.194 I print_info: n_head           = 16
0.00.103.195 I print_info: n_head_kv        = 16
0.00.103.197 I print_info: n_rot            = 32
0.00.103.197 I print_info: n_swa            = 0
0.00.103.197 I print_info: n_embd_head_k    = 128
0.00.103.197 I print_info: n_embd_head_v    = 128
0.00.103.198 I print_info: n_gqa            = 1
0.00.103.199 I print_info: n_embd_k_gqa     = 2048
0.00.103.199 I print_info: n_embd_v_gqa     = 2048
0.00.103.200 I print_info: f_norm_eps       = 1.0e-05
0.00.103.200 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.200 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.201 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.201 I print_info: f_logit_scale    = 0.0e+00
0.00.103.201 I print_info: n_ff             = 8192
0.00.103.202 I print_info: n_expert         = 0
0.00.103.202 I print_info: n_expert_used    = 0
0.00.103.203 I print_info: causal attn      = 1
0.00.103.203 I print_info: pooling type     = 0
0.00.103.203 I print_info: rope type        = 2
0.00.103.203 I print_info: rope scaling     = linear
0.00.103.203 I print_info: freq_base_train  = 10000.0
0.00.103.204 I print_info: freq_scale_train = 1
0.00.103.204 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.204 I print_info: rope_finetuned   = unknown
0.00.103.204 I print_info: ssm_d_conv       = 0
0.00.103.204 I print_info: ssm_d_inner      = 0
0.00.103.205 I print_info: ssm_d_state      = 0
0.00.103.205 I print_info: ssm_dt_rank      = 0
0.00.103.205 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.205 I print_info: model type       = 1.4B
0.00.103.205 I print_info: model params     = 1.41 B
0.00.103.207 I print_info: general.name     = 1.4B
0.00.103.207 I print_info: vocab type       = BPE
0.00.103.207 I print_info: n_vocab          = 50304
0.00.103.208 I print_info: n_merges         = 50009
0.00.103.208 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.208 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.208 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.208 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.103.209 I print_info: LF token         = 128 'Ä'
0.00.103.209 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.103.209 I print_info: max token length = 1024
0.00.105.845 I load_tensors: offloading 24 repeating layers to GPU
0.00.105.845 I load_tensors: offloading output layer to GPU
0.00.105.845 I load_tensors: offloaded 25/25 layers to GPU
0.00.105.864 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.105.865 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.106.148 I llama_init_from_model: n_seq_max     = 1
0.00.106.149 I llama_init_from_model: n_ctx         = 2048
0.00.106.149 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.106.149 I llama_init_from_model: n_batch       = 2048
0.00.106.149 I llama_init_from_model: n_ubatch      = 512
0.00.106.149 I llama_init_from_model: flash_attn    = 0
0.00.106.150 I llama_init_from_model: freq_base     = 10000.0
0.00.106.150 I llama_init_from_model: freq_scale    = 1
0.00.106.150 I ggml_metal_init: allocating
0.00.106.153 I ggml_metal_init: found device: Apple M4
0.00.106.155 I ggml_metal_init: picking default device: Apple M4
0.00.106.827 I ggml_metal_init: using embedded metal library
0.00.116.324 I ggml_metal_init: GPU name:   Apple M4
0.00.116.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.327 I ggml_metal_init: simdgroup reduction   = true
0.00.116.327 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.327 I ggml_metal_init: has bfloat            = true
0.00.116.327 I ggml_metal_init: use bfloat            = true
0.00.116.328 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.140.873 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.161.445 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.161.454 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.161.478 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.162.438 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.162.440 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.162.440 I llama_init_from_model: graph nodes  = 967
0.00.162.440 I llama_init_from_model: graph splits = 2
0.00.162.444 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.162.573 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.162.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.717 I main: llama threadpool init, n_threads = 4
0.00.243.759 I 
0.00.243.781 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.243.782 I 
0.00.243.855 I sampler seed: 1234
0.00.243.859 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.243.885 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.243.886 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.243.886 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.071.981 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.02.071.982 I llama_perf_context_print:        load time =     207.01 ms
0.02.071.983 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.55 tokens per second)
0.02.071.983 I llama_perf_context_print:        eval time =    1781.61 ms /    63 runs   (   28.28 ms per token,    35.36 tokens per second)
0.02.071.985 I llama_perf_context_print:       total time =    1828.27 ms /    70 tokens
0.02.072.202 I ggml_metal_free: deallocating

real	0m2.371s
user	0m0.143s
sys	0m0.104s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.820 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.018 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.025 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.026 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.026 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.026 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.029 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.029 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.037 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.037 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.646 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.647 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.647 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.647 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.648 I llama_model_loader: - type  f32:  194 tensors
0.00.026.648 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.649 I print_info: file format = GGUF V3 (latest)
0.00.026.650 I print_info: file type   = Q8_0
0.00.026.651 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.219 I load: special tokens cache size = 25
0.00.052.950 I load: token to piece cache size = 0.2984 MB
0.00.052.955 I print_info: arch             = gptneox
0.00.052.955 I print_info: vocab_only       = 0
0.00.052.955 I print_info: n_ctx_train      = 2048
0.00.052.955 I print_info: n_embd           = 2048
0.00.052.958 I print_info: n_layer          = 24
0.00.052.964 I print_info: n_head           = 16
0.00.052.965 I print_info: n_head_kv        = 16
0.00.052.965 I print_info: n_rot            = 32
0.00.052.966 I print_info: n_swa            = 0
0.00.052.966 I print_info: n_embd_head_k    = 128
0.00.052.966 I print_info: n_embd_head_v    = 128
0.00.052.967 I print_info: n_gqa            = 1
0.00.052.967 I print_info: n_embd_k_gqa     = 2048
0.00.052.968 I print_info: n_embd_v_gqa     = 2048
0.00.052.969 I print_info: f_norm_eps       = 1.0e-05
0.00.052.970 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.970 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.970 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.970 I print_info: f_logit_scale    = 0.0e+00
0.00.052.972 I print_info: n_ff             = 8192
0.00.052.972 I print_info: n_expert         = 0
0.00.052.972 I print_info: n_expert_used    = 0
0.00.052.972 I print_info: causal attn      = 1
0.00.052.972 I print_info: pooling type     = 0
0.00.052.972 I print_info: rope type        = 2
0.00.052.973 I print_info: rope scaling     = linear
0.00.052.973 I print_info: freq_base_train  = 10000.0
0.00.052.974 I print_info: freq_scale_train = 1
0.00.052.974 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.974 I print_info: rope_finetuned   = unknown
0.00.052.974 I print_info: ssm_d_conv       = 0
0.00.052.974 I print_info: ssm_d_inner      = 0
0.00.052.975 I print_info: ssm_d_state      = 0
0.00.052.975 I print_info: ssm_dt_rank      = 0
0.00.052.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.975 I print_info: model type       = 1.4B
0.00.052.976 I print_info: model params     = 1.41 B
0.00.052.976 I print_info: general.name     = 1.4B
0.00.052.977 I print_info: vocab type       = BPE
0.00.052.977 I print_info: n_vocab          = 50304
0.00.052.977 I print_info: n_merges         = 50009
0.00.052.977 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.978 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.978 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.978 I print_info: LF token         = 128 'Ä'
0.00.052.978 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.980 I print_info: max token length = 1024
0.00.055.058 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.059 I load_tensors: offloading output layer to GPU
0.00.055.059 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.070 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.071 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.055.415 I llama_init_from_model: n_seq_max     = 1
0.00.055.416 I llama_init_from_model: n_ctx         = 2048
0.00.055.416 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.416 I llama_init_from_model: n_batch       = 2048
0.00.055.417 I llama_init_from_model: n_ubatch      = 512
0.00.055.417 I llama_init_from_model: flash_attn    = 0
0.00.055.417 I llama_init_from_model: freq_base     = 10000.0
0.00.055.417 I llama_init_from_model: freq_scale    = 1
0.00.055.418 I ggml_metal_init: allocating
0.00.055.421 I ggml_metal_init: found device: Apple M4
0.00.055.423 I ggml_metal_init: picking default device: Apple M4
0.00.056.210 I ggml_metal_init: using embedded metal library
0.00.058.847 I ggml_metal_init: GPU name:   Apple M4
0.00.058.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.849 I ggml_metal_init: simdgroup reduction   = true
0.00.058.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.850 I ggml_metal_init: has bfloat            = true
0.00.058.850 I ggml_metal_init: use bfloat            = true
0.00.058.850 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.851 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.477 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.471 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.482 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.508 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.095.598 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.095.600 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.095.600 I llama_init_from_model: graph nodes  = 967
0.00.095.601 I llama_init_from_model: graph splits = 2
0.00.095.605 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.721 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.028.195 I main: llama threadpool init, n_threads = 4
0.01.028.234 I 
0.01.028.254 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.028.254 I 
0.01.028.478 I sampler seed: 1234
0.01.028.484 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.028.495 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.028.496 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.028.497 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.115.884 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.02.115.885 I llama_perf_context_print:        load time =    1018.37 ms
0.02.115.886 I llama_perf_context_print: prompt eval time =      43.32 ms /     7 tokens (    6.19 ms per token,   161.60 tokens per second)
0.02.115.886 I llama_perf_context_print:        eval time =    1041.14 ms /    63 runs   (   16.53 ms per token,    60.51 tokens per second)
0.02.115.888 I llama_perf_context_print:       total time =    1087.69 ms /    70 tokens
0.02.116.089 I ggml_metal_free: deallocating

real	0m2.135s
user	0m0.112s
sys	0m0.218s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.978 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.720 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.720 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.721 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.721 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.721 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.722 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.723 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.723 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.724 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.724 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.724 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.725 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.727 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.339 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.339 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.340 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.341 I llama_model_loader: - type  f32:  194 tensors
0.00.028.341 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.341 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.342 I print_info: file format = GGUF V3 (latest)
0.00.028.343 I print_info: file type   = Q4_0
0.00.028.344 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.492 I load: special tokens cache size = 25
0.00.053.399 I load: token to piece cache size = 0.2984 MB
0.00.053.402 I print_info: arch             = gptneox
0.00.053.402 I print_info: vocab_only       = 0
0.00.053.403 I print_info: n_ctx_train      = 2048
0.00.053.403 I print_info: n_embd           = 2048
0.00.053.403 I print_info: n_layer          = 24
0.00.053.407 I print_info: n_head           = 16
0.00.053.408 I print_info: n_head_kv        = 16
0.00.053.408 I print_info: n_rot            = 32
0.00.053.408 I print_info: n_swa            = 0
0.00.053.408 I print_info: n_embd_head_k    = 128
0.00.053.408 I print_info: n_embd_head_v    = 128
0.00.053.409 I print_info: n_gqa            = 1
0.00.053.410 I print_info: n_embd_k_gqa     = 2048
0.00.053.411 I print_info: n_embd_v_gqa     = 2048
0.00.053.411 I print_info: f_norm_eps       = 1.0e-05
0.00.053.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.414 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.415 I print_info: f_logit_scale    = 0.0e+00
0.00.053.419 I print_info: n_ff             = 8192
0.00.053.419 I print_info: n_expert         = 0
0.00.053.419 I print_info: n_expert_used    = 0
0.00.053.419 I print_info: causal attn      = 1
0.00.053.419 I print_info: pooling type     = 0
0.00.053.419 I print_info: rope type        = 2
0.00.053.420 I print_info: rope scaling     = linear
0.00.053.420 I print_info: freq_base_train  = 10000.0
0.00.053.420 I print_info: freq_scale_train = 1
0.00.053.421 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.421 I print_info: rope_finetuned   = unknown
0.00.053.421 I print_info: ssm_d_conv       = 0
0.00.053.421 I print_info: ssm_d_inner      = 0
0.00.053.421 I print_info: ssm_d_state      = 0
0.00.053.421 I print_info: ssm_dt_rank      = 0
0.00.053.421 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.422 I print_info: model type       = 1.4B
0.00.053.422 I print_info: model params     = 1.41 B
0.00.053.422 I print_info: general.name     = 1.4B
0.00.053.423 I print_info: vocab type       = BPE
0.00.053.423 I print_info: n_vocab          = 50304
0.00.053.423 I print_info: n_merges         = 50009
0.00.053.423 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.424 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.424 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.424 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.424 I print_info: LF token         = 128 'Ä'
0.00.053.425 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.425 I print_info: max token length = 1024
0.00.055.653 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.653 I load_tensors: offloading output layer to GPU
0.00.055.653 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.665 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.666 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.994 I llama_init_from_model: n_seq_max     = 1
0.00.055.995 I llama_init_from_model: n_ctx         = 2048
0.00.055.995 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.995 I llama_init_from_model: n_batch       = 2048
0.00.055.995 I llama_init_from_model: n_ubatch      = 512
0.00.055.996 I llama_init_from_model: flash_attn    = 0
0.00.055.996 I llama_init_from_model: freq_base     = 10000.0
0.00.055.996 I llama_init_from_model: freq_scale    = 1
0.00.055.997 I ggml_metal_init: allocating
0.00.056.000 I ggml_metal_init: found device: Apple M4
0.00.056.001 I ggml_metal_init: picking default device: Apple M4
0.00.056.739 I ggml_metal_init: using embedded metal library
0.00.059.299 I ggml_metal_init: GPU name:   Apple M4
0.00.059.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.301 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.302 I ggml_metal_init: simdgroup reduction   = true
0.00.059.302 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.302 I ggml_metal_init: has bfloat            = true
0.00.059.302 I ggml_metal_init: use bfloat            = true
0.00.059.302 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.303 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.739 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.274 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.286 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.312 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.488 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.490 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.491 I llama_init_from_model: graph nodes  = 967
0.00.097.491 I llama_init_from_model: graph splits = 2
0.00.097.495 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.611 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.145 I main: llama threadpool init, n_threads = 4
0.00.642.196 I 
0.00.642.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.229 I 
0.00.642.458 I sampler seed: 1234
0.00.642.463 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.642.503 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.642.504 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.642.505 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.326.194 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56349.21 tokens per second)
0.01.326.195 I llama_perf_context_print:        load time =     630.16 ms
0.01.326.196 I llama_perf_context_print: prompt eval time =      43.21 ms /     7 tokens (    6.17 ms per token,   161.99 tokens per second)
0.01.326.196 I llama_perf_context_print:        eval time =     637.39 ms /    63 runs   (   10.12 ms per token,    98.84 tokens per second)
0.01.326.197 I llama_perf_context_print:       total time =     684.05 ms /    70 tokens
0.01.326.441 I ggml_metal_free: deallocating

real	0m1.346s
user	0m0.110s
sys	0m0.145s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.104 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.868 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.873 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.876 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.883 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.595 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.143 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.145 I llama_model_loader: - type  f32:  194 tensors
0.00.026.145 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.146 I print_info: file format = GGUF V3 (latest)
0.00.026.146 I print_info: file type   = Q4_1
0.00.026.147 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.049 I load: special tokens cache size = 25
0.00.051.799 I load: token to piece cache size = 0.2984 MB
0.00.051.801 I print_info: arch             = gptneox
0.00.051.802 I print_info: vocab_only       = 0
0.00.051.802 I print_info: n_ctx_train      = 2048
0.00.051.802 I print_info: n_embd           = 2048
0.00.051.802 I print_info: n_layer          = 24
0.00.051.805 I print_info: n_head           = 16
0.00.051.806 I print_info: n_head_kv        = 16
0.00.051.806 I print_info: n_rot            = 32
0.00.051.808 I print_info: n_swa            = 0
0.00.051.808 I print_info: n_embd_head_k    = 128
0.00.051.808 I print_info: n_embd_head_v    = 128
0.00.051.809 I print_info: n_gqa            = 1
0.00.051.809 I print_info: n_embd_k_gqa     = 2048
0.00.051.810 I print_info: n_embd_v_gqa     = 2048
0.00.051.811 I print_info: f_norm_eps       = 1.0e-05
0.00.051.811 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.811 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.812 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.812 I print_info: f_logit_scale    = 0.0e+00
0.00.051.812 I print_info: n_ff             = 8192
0.00.051.813 I print_info: n_expert         = 0
0.00.051.813 I print_info: n_expert_used    = 0
0.00.051.815 I print_info: causal attn      = 1
0.00.051.816 I print_info: pooling type     = 0
0.00.051.816 I print_info: rope type        = 2
0.00.051.816 I print_info: rope scaling     = linear
0.00.051.817 I print_info: freq_base_train  = 10000.0
0.00.051.817 I print_info: freq_scale_train = 1
0.00.051.817 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.818 I print_info: rope_finetuned   = unknown
0.00.051.818 I print_info: ssm_d_conv       = 0
0.00.051.818 I print_info: ssm_d_inner      = 0
0.00.051.818 I print_info: ssm_d_state      = 0
0.00.051.818 I print_info: ssm_dt_rank      = 0
0.00.051.818 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.819 I print_info: model type       = 1.4B
0.00.051.819 I print_info: model params     = 1.41 B
0.00.051.819 I print_info: general.name     = 1.4B
0.00.051.820 I print_info: vocab type       = BPE
0.00.051.820 I print_info: n_vocab          = 50304
0.00.051.820 I print_info: n_merges         = 50009
0.00.051.822 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.822 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.822 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.822 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.822 I print_info: LF token         = 128 'Ä'
0.00.051.823 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.823 I print_info: max token length = 1024
0.00.053.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.860 I load_tensors: offloading output layer to GPU
0.00.053.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.871 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.873 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.172 I llama_init_from_model: n_seq_max     = 1
0.00.054.173 I llama_init_from_model: n_ctx         = 2048
0.00.054.173 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.173 I llama_init_from_model: n_batch       = 2048
0.00.054.174 I llama_init_from_model: n_ubatch      = 512
0.00.054.174 I llama_init_from_model: flash_attn    = 0
0.00.054.174 I llama_init_from_model: freq_base     = 10000.0
0.00.054.174 I llama_init_from_model: freq_scale    = 1
0.00.054.175 I ggml_metal_init: allocating
0.00.054.178 I ggml_metal_init: found device: Apple M4
0.00.054.180 I ggml_metal_init: picking default device: Apple M4
0.00.054.800 I ggml_metal_init: using embedded metal library
0.00.057.231 I ggml_metal_init: GPU name:   Apple M4
0.00.057.233 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.234 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.234 I ggml_metal_init: simdgroup reduction   = true
0.00.057.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.234 I ggml_metal_init: has bfloat            = true
0.00.057.234 I ggml_metal_init: use bfloat            = true
0.00.057.235 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.322 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.685 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.692 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.710 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.844 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.845 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.846 I llama_init_from_model: graph nodes  = 967
0.00.088.846 I llama_init_from_model: graph splits = 2
0.00.088.849 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.979 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.580 I main: llama threadpool init, n_threads = 4
0.00.681.617 I 
0.00.681.663 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.664 I 
0.00.681.892 I sampler seed: 1234
0.00.681.898 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.939 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.943 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.943 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.414.590 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64253.39 tokens per second)
0.01.414.591 I llama_perf_context_print:        load time =     671.47 ms
0.01.414.591 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.47 tokens per second)
0.01.414.592 I llama_perf_context_print:        eval time =     686.16 ms /    63 runs   (   10.89 ms per token,    91.82 tokens per second)
0.01.414.592 I llama_perf_context_print:       total time =     733.01 ms /    70 tokens
0.01.414.826 I ggml_metal_free: deallocating

real	0m1.433s
user	0m0.109s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.012.253 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.763 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.765 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.767 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.769 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.769 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.468 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.129 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.130 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.130 I llama_model_loader: - type  f32:  194 tensors
0.00.028.130 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.131 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.131 I print_info: file format = GGUF V3 (latest)
0.00.028.132 I print_info: file type   = Q5_0
0.00.028.133 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.047.051 I load: special tokens cache size = 25
0.00.052.945 I load: token to piece cache size = 0.2984 MB
0.00.052.948 I print_info: arch             = gptneox
0.00.052.948 I print_info: vocab_only       = 0
0.00.052.948 I print_info: n_ctx_train      = 2048
0.00.052.948 I print_info: n_embd           = 2048
0.00.052.948 I print_info: n_layer          = 24
0.00.052.951 I print_info: n_head           = 16
0.00.052.952 I print_info: n_head_kv        = 16
0.00.052.952 I print_info: n_rot            = 32
0.00.052.952 I print_info: n_swa            = 0
0.00.052.952 I print_info: n_embd_head_k    = 128
0.00.052.953 I print_info: n_embd_head_v    = 128
0.00.052.955 I print_info: n_gqa            = 1
0.00.052.956 I print_info: n_embd_k_gqa     = 2048
0.00.052.957 I print_info: n_embd_v_gqa     = 2048
0.00.052.957 I print_info: f_norm_eps       = 1.0e-05
0.00.052.958 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.958 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.958 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.958 I print_info: f_logit_scale    = 0.0e+00
0.00.052.959 I print_info: n_ff             = 8192
0.00.052.959 I print_info: n_expert         = 0
0.00.052.959 I print_info: n_expert_used    = 0
0.00.052.960 I print_info: causal attn      = 1
0.00.052.961 I print_info: pooling type     = 0
0.00.052.961 I print_info: rope type        = 2
0.00.052.962 I print_info: rope scaling     = linear
0.00.052.963 I print_info: freq_base_train  = 10000.0
0.00.052.963 I print_info: freq_scale_train = 1
0.00.052.964 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.964 I print_info: rope_finetuned   = unknown
0.00.052.964 I print_info: ssm_d_conv       = 0
0.00.052.964 I print_info: ssm_d_inner      = 0
0.00.052.964 I print_info: ssm_d_state      = 0
0.00.052.964 I print_info: ssm_dt_rank      = 0
0.00.052.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.969 I print_info: model type       = 1.4B
0.00.052.971 I print_info: model params     = 1.41 B
0.00.052.971 I print_info: general.name     = 1.4B
0.00.052.971 I print_info: vocab type       = BPE
0.00.052.971 I print_info: n_vocab          = 50304
0.00.052.971 I print_info: n_merges         = 50009
0.00.052.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.972 I print_info: LF token         = 128 'Ä'
0.00.052.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.973 I print_info: max token length = 1024
0.00.054.965 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.965 I load_tensors: offloading output layer to GPU
0.00.054.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.976 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.977 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.264 I llama_init_from_model: n_seq_max     = 1
0.00.055.265 I llama_init_from_model: n_ctx         = 2048
0.00.055.265 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.265 I llama_init_from_model: n_batch       = 2048
0.00.055.265 I llama_init_from_model: n_ubatch      = 512
0.00.055.265 I llama_init_from_model: flash_attn    = 0
0.00.055.266 I llama_init_from_model: freq_base     = 10000.0
0.00.055.266 I llama_init_from_model: freq_scale    = 1
0.00.055.266 I ggml_metal_init: allocating
0.00.055.269 I ggml_metal_init: found device: Apple M4
0.00.055.271 I ggml_metal_init: picking default device: Apple M4
0.00.055.884 I ggml_metal_init: using embedded metal library
0.00.058.293 I ggml_metal_init: GPU name:   Apple M4
0.00.058.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.295 I ggml_metal_init: simdgroup reduction   = true
0.00.058.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.295 I ggml_metal_init: has bfloat            = true
0.00.058.296 I ggml_metal_init: use bfloat            = true
0.00.058.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.023 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.072 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.082 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.103 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.167 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.168 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.169 I llama_init_from_model: graph nodes  = 967
0.00.089.169 I llama_init_from_model: graph splits = 2
0.00.089.172 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.306 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.307 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.484 I main: llama threadpool init, n_threads = 4
0.00.677.524 I 
0.00.677.551 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.551 I 
0.00.677.779 I sampler seed: 1234
0.00.677.783 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.828 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.832 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.833 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.473.998 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.473.999 I llama_perf_context_print:        load time =     665.23 ms
0.01.474.000 I llama_perf_context_print: prompt eval time =      47.05 ms /     7 tokens (    6.72 ms per token,   148.78 tokens per second)
0.01.474.000 I llama_perf_context_print:        eval time =     746.01 ms /    63 runs   (   11.84 ms per token,    84.45 tokens per second)
0.01.474.001 I llama_perf_context_print:       total time =     796.52 ms /    70 tokens
0.01.474.264 I ggml_metal_free: deallocating

real	0m1.492s
user	0m0.109s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.815 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.341 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.341 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.342 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.342 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.343 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.343 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.343 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.344 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.345 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.345 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.347 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.348 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.348 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.124 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.871 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.874 I llama_model_loader: - type  f32:  194 tensors
0.00.025.874 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.874 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.875 I print_info: file format = GGUF V3 (latest)
0.00.025.875 I print_info: file type   = Q5_1
0.00.025.876 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.969 I load: special tokens cache size = 25
0.00.051.036 I load: token to piece cache size = 0.2984 MB
0.00.051.038 I print_info: arch             = gptneox
0.00.051.039 I print_info: vocab_only       = 0
0.00.051.039 I print_info: n_ctx_train      = 2048
0.00.051.039 I print_info: n_embd           = 2048
0.00.051.039 I print_info: n_layer          = 24
0.00.051.042 I print_info: n_head           = 16
0.00.051.043 I print_info: n_head_kv        = 16
0.00.051.043 I print_info: n_rot            = 32
0.00.051.045 I print_info: n_swa            = 0
0.00.051.045 I print_info: n_embd_head_k    = 128
0.00.051.045 I print_info: n_embd_head_v    = 128
0.00.051.046 I print_info: n_gqa            = 1
0.00.051.047 I print_info: n_embd_k_gqa     = 2048
0.00.051.048 I print_info: n_embd_v_gqa     = 2048
0.00.051.048 I print_info: f_norm_eps       = 1.0e-05
0.00.051.048 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.049 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.049 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.049 I print_info: f_logit_scale    = 0.0e+00
0.00.051.050 I print_info: n_ff             = 8192
0.00.051.050 I print_info: n_expert         = 0
0.00.051.050 I print_info: n_expert_used    = 0
0.00.051.050 I print_info: causal attn      = 1
0.00.051.050 I print_info: pooling type     = 0
0.00.051.050 I print_info: rope type        = 2
0.00.051.051 I print_info: rope scaling     = linear
0.00.051.052 I print_info: freq_base_train  = 10000.0
0.00.051.052 I print_info: freq_scale_train = 1
0.00.051.052 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.053 I print_info: rope_finetuned   = unknown
0.00.051.053 I print_info: ssm_d_conv       = 0
0.00.051.053 I print_info: ssm_d_inner      = 0
0.00.051.053 I print_info: ssm_d_state      = 0
0.00.051.053 I print_info: ssm_dt_rank      = 0
0.00.051.053 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.054 I print_info: model type       = 1.4B
0.00.051.054 I print_info: model params     = 1.41 B
0.00.051.054 I print_info: general.name     = 1.4B
0.00.051.055 I print_info: vocab type       = BPE
0.00.051.056 I print_info: n_vocab          = 50304
0.00.051.057 I print_info: n_merges         = 50009
0.00.051.057 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.057 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.057 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.057 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.058 I print_info: LF token         = 128 'Ä'
0.00.051.058 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.059 I print_info: max token length = 1024
0.00.053.026 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.026 I load_tensors: offloading output layer to GPU
0.00.053.027 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.037 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.038 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.333 I llama_init_from_model: n_seq_max     = 1
0.00.053.334 I llama_init_from_model: n_ctx         = 2048
0.00.053.334 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.334 I llama_init_from_model: n_batch       = 2048
0.00.053.335 I llama_init_from_model: n_ubatch      = 512
0.00.053.335 I llama_init_from_model: flash_attn    = 0
0.00.053.335 I llama_init_from_model: freq_base     = 10000.0
0.00.053.335 I llama_init_from_model: freq_scale    = 1
0.00.053.336 I ggml_metal_init: allocating
0.00.053.339 I ggml_metal_init: found device: Apple M4
0.00.053.341 I ggml_metal_init: picking default device: Apple M4
0.00.053.912 I ggml_metal_init: using embedded metal library
0.00.056.320 I ggml_metal_init: GPU name:   Apple M4
0.00.056.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.323 I ggml_metal_init: simdgroup reduction   = true
0.00.056.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.323 I ggml_metal_init: has bfloat            = true
0.00.056.323 I ggml_metal_init: use bfloat            = true
0.00.056.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.125 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.544 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.555 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.585 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.630 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.632 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.632 I llama_init_from_model: graph nodes  = 967
0.00.087.632 I llama_init_from_model: graph splits = 2
0.00.087.635 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.167 I main: llama threadpool init, n_threads = 4
0.00.740.233 I 
0.00.740.252 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.254 I 
0.00.740.487 I sampler seed: 1234
0.00.740.491 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.502 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.502 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.580.884 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.01.580.885 I llama_perf_context_print:        load time =     730.35 ms
0.01.580.886 I llama_perf_context_print: prompt eval time =      48.38 ms /     7 tokens (    6.91 ms per token,   144.69 tokens per second)
0.01.580.886 I llama_perf_context_print:        eval time =     788.99 ms /    63 runs   (   12.52 ms per token,    79.85 tokens per second)
0.01.580.887 I llama_perf_context_print:       total time =     840.72 ms /    70 tokens
0.01.581.086 I ggml_metal_free: deallocating

real	0m1.599s
user	0m0.109s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.237 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.257 I llama_model_loader: - type  f32:  194 tensors
0.00.024.257 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.257 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.257 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.258 I print_info: file format = GGUF V3 (latest)
0.00.024.258 I print_info: file type   = Q2_K - Medium
0.00.024.259 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.252 I load: special tokens cache size = 25
0.00.049.162 I load: token to piece cache size = 0.2984 MB
0.00.049.165 I print_info: arch             = gptneox
0.00.049.165 I print_info: vocab_only       = 0
0.00.049.165 I print_info: n_ctx_train      = 2048
0.00.049.165 I print_info: n_embd           = 2048
0.00.049.166 I print_info: n_layer          = 24
0.00.049.168 I print_info: n_head           = 16
0.00.049.169 I print_info: n_head_kv        = 16
0.00.049.169 I print_info: n_rot            = 32
0.00.049.169 I print_info: n_swa            = 0
0.00.049.171 I print_info: n_embd_head_k    = 128
0.00.049.171 I print_info: n_embd_head_v    = 128
0.00.049.172 I print_info: n_gqa            = 1
0.00.049.173 I print_info: n_embd_k_gqa     = 2048
0.00.049.173 I print_info: n_embd_v_gqa     = 2048
0.00.049.179 I print_info: f_norm_eps       = 1.0e-05
0.00.049.181 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.181 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.182 I print_info: f_logit_scale    = 0.0e+00
0.00.049.183 I print_info: n_ff             = 8192
0.00.049.183 I print_info: n_expert         = 0
0.00.049.183 I print_info: n_expert_used    = 0
0.00.049.184 I print_info: causal attn      = 1
0.00.049.184 I print_info: pooling type     = 0
0.00.049.185 I print_info: rope type        = 2
0.00.049.187 I print_info: rope scaling     = linear
0.00.049.187 I print_info: freq_base_train  = 10000.0
0.00.049.187 I print_info: freq_scale_train = 1
0.00.049.187 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.188 I print_info: rope_finetuned   = unknown
0.00.049.188 I print_info: ssm_d_conv       = 0
0.00.049.188 I print_info: ssm_d_inner      = 0
0.00.049.188 I print_info: ssm_d_state      = 0
0.00.049.189 I print_info: ssm_dt_rank      = 0
0.00.049.189 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.189 I print_info: model type       = 1.4B
0.00.049.190 I print_info: model params     = 1.41 B
0.00.049.190 I print_info: general.name     = 1.4B
0.00.049.190 I print_info: vocab type       = BPE
0.00.049.190 I print_info: n_vocab          = 50304
0.00.049.191 I print_info: n_merges         = 50009
0.00.049.191 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.191 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.191 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.191 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.195 I print_info: LF token         = 128 'Ä'
0.00.049.198 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.198 I print_info: max token length = 1024
0.00.051.034 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.035 I load_tensors: offloading output layer to GPU
0.00.051.035 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.045 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.046 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.322 I llama_init_from_model: n_seq_max     = 1
0.00.051.323 I llama_init_from_model: n_ctx         = 2048
0.00.051.323 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.323 I llama_init_from_model: n_batch       = 2048
0.00.051.323 I llama_init_from_model: n_ubatch      = 512
0.00.051.324 I llama_init_from_model: flash_attn    = 0
0.00.051.324 I llama_init_from_model: freq_base     = 10000.0
0.00.051.324 I llama_init_from_model: freq_scale    = 1
0.00.051.325 I ggml_metal_init: allocating
0.00.051.327 I ggml_metal_init: found device: Apple M4
0.00.051.329 I ggml_metal_init: picking default device: Apple M4
0.00.051.896 I ggml_metal_init: using embedded metal library
0.00.054.195 I ggml_metal_init: GPU name:   Apple M4
0.00.054.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.198 I ggml_metal_init: simdgroup reduction   = true
0.00.054.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.198 I ggml_metal_init: has bfloat            = true
0.00.054.198 I ggml_metal_init: use bfloat            = true
0.00.054.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.200 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.973 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.471 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.476 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.446 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.447 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.447 I llama_init_from_model: graph nodes  = 967
0.00.084.448 I llama_init_from_model: graph splits = 2
0.00.084.451 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.322 I main: llama threadpool init, n_threads = 4
0.00.510.364 I 
0.00.510.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.510.389 I 
0.00.510.624 I sampler seed: 1234
0.00.510.631 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.510.666 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.510.676 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.510.677 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.186.974 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63963.96 tokens per second)
0.01.186.974 I llama_perf_context_print:        load time =     501.08 ms
0.01.186.975 I llama_perf_context_print: prompt eval time =      35.85 ms /     7 tokens (    5.12 ms per token,   195.27 tokens per second)
0.01.186.976 I llama_perf_context_print:        eval time =     637.61 ms /    63 runs   (   10.12 ms per token,    98.81 tokens per second)
0.01.186.976 I llama_perf_context_print:       total time =     676.65 ms /    70 tokens
0.01.187.224 I ggml_metal_free: deallocating

real	0m1.205s
user	0m0.108s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.735 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.144 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.150 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.155 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.156 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.156 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.157 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.157 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.158 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.158 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.159 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.159 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.159 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.160 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.161 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.162 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.162 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.977 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.730 I llama_model_loader: - type  f32:  194 tensors
0.00.024.730 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.730 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.730 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.731 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.731 I print_info: file format = GGUF V3 (latest)
0.00.024.731 I print_info: file type   = Q3_K - Medium
0.00.024.732 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.673 I load: special tokens cache size = 25
0.00.049.739 I load: token to piece cache size = 0.2984 MB
0.00.049.742 I print_info: arch             = gptneox
0.00.049.743 I print_info: vocab_only       = 0
0.00.049.743 I print_info: n_ctx_train      = 2048
0.00.049.743 I print_info: n_embd           = 2048
0.00.049.743 I print_info: n_layer          = 24
0.00.049.746 I print_info: n_head           = 16
0.00.049.746 I print_info: n_head_kv        = 16
0.00.049.747 I print_info: n_rot            = 32
0.00.049.747 I print_info: n_swa            = 0
0.00.049.747 I print_info: n_embd_head_k    = 128
0.00.049.747 I print_info: n_embd_head_v    = 128
0.00.049.748 I print_info: n_gqa            = 1
0.00.049.749 I print_info: n_embd_k_gqa     = 2048
0.00.049.750 I print_info: n_embd_v_gqa     = 2048
0.00.049.750 I print_info: f_norm_eps       = 1.0e-05
0.00.049.750 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.752 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.752 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.752 I print_info: f_logit_scale    = 0.0e+00
0.00.049.753 I print_info: n_ff             = 8192
0.00.049.753 I print_info: n_expert         = 0
0.00.049.755 I print_info: n_expert_used    = 0
0.00.049.755 I print_info: causal attn      = 1
0.00.049.755 I print_info: pooling type     = 0
0.00.049.755 I print_info: rope type        = 2
0.00.049.755 I print_info: rope scaling     = linear
0.00.049.756 I print_info: freq_base_train  = 10000.0
0.00.049.757 I print_info: freq_scale_train = 1
0.00.049.757 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.757 I print_info: rope_finetuned   = unknown
0.00.049.757 I print_info: ssm_d_conv       = 0
0.00.049.758 I print_info: ssm_d_inner      = 0
0.00.049.758 I print_info: ssm_d_state      = 0
0.00.049.759 I print_info: ssm_dt_rank      = 0
0.00.049.759 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.759 I print_info: model type       = 1.4B
0.00.049.760 I print_info: model params     = 1.41 B
0.00.049.761 I print_info: general.name     = 1.4B
0.00.049.762 I print_info: vocab type       = BPE
0.00.049.762 I print_info: n_vocab          = 50304
0.00.049.762 I print_info: n_merges         = 50009
0.00.049.762 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.762 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.763 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.763 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.763 I print_info: LF token         = 128 'Ä'
0.00.049.763 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.764 I print_info: max token length = 1024
0.00.051.314 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.314 I load_tensors: offloading output layer to GPU
0.00.051.314 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.324 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.325 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.614 I llama_init_from_model: n_seq_max     = 1
0.00.051.615 I llama_init_from_model: n_ctx         = 2048
0.00.051.616 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.616 I llama_init_from_model: n_batch       = 2048
0.00.051.616 I llama_init_from_model: n_ubatch      = 512
0.00.051.616 I llama_init_from_model: flash_attn    = 0
0.00.051.617 I llama_init_from_model: freq_base     = 10000.0
0.00.051.617 I llama_init_from_model: freq_scale    = 1
0.00.051.618 I ggml_metal_init: allocating
0.00.051.621 I ggml_metal_init: found device: Apple M4
0.00.051.623 I ggml_metal_init: picking default device: Apple M4
0.00.052.212 I ggml_metal_init: using embedded metal library
0.00.054.539 I ggml_metal_init: GPU name:   Apple M4
0.00.054.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.541 I ggml_metal_init: simdgroup reduction   = true
0.00.054.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.541 I ggml_metal_init: has bfloat            = true
0.00.054.542 I ggml_metal_init: use bfloat            = true
0.00.054.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.398 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.669 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.677 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.705 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.654 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.655 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.656 I llama_init_from_model: graph nodes  = 967
0.00.085.656 I llama_init_from_model: graph splits = 2
0.00.085.659 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.555.034 I main: llama threadpool init, n_threads = 4
0.00.555.073 I 
0.00.555.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.555.098 I 
0.00.555.328 I sampler seed: 1234
0.00.555.333 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.555.374 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.555.374 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.555.374 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.295.712 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 62008.73 tokens per second)
0.01.295.712 I llama_perf_context_print:        load time =     546.29 ms
0.01.295.713 I llama_perf_context_print: prompt eval time =      40.45 ms /     7 tokens (    5.78 ms per token,   173.03 tokens per second)
0.01.295.714 I llama_perf_context_print:        eval time =     697.01 ms /    63 runs   (   11.06 ms per token,    90.39 tokens per second)
0.01.295.714 I llama_perf_context_print:       total time =     740.68 ms /    70 tokens
0.01.295.957 I ggml_metal_free: deallocating

real	0m1.312s
user	0m0.109s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.011.498 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.896 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.901 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.908 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.910 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.911 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.911 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.912 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.696 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.440 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.440 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.441 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.441 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.441 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.442 I llama_model_loader: - type  f32:  194 tensors
0.00.027.442 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.443 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.443 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.443 I print_info: file format = GGUF V3 (latest)
0.00.027.444 I print_info: file type   = Q4_K - Medium
0.00.027.445 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.478 I load: special tokens cache size = 25
0.00.052.447 I load: token to piece cache size = 0.2984 MB
0.00.052.450 I print_info: arch             = gptneox
0.00.052.450 I print_info: vocab_only       = 0
0.00.052.450 I print_info: n_ctx_train      = 2048
0.00.052.450 I print_info: n_embd           = 2048
0.00.052.451 I print_info: n_layer          = 24
0.00.052.453 I print_info: n_head           = 16
0.00.052.454 I print_info: n_head_kv        = 16
0.00.052.454 I print_info: n_rot            = 32
0.00.052.454 I print_info: n_swa            = 0
0.00.052.454 I print_info: n_embd_head_k    = 128
0.00.052.457 I print_info: n_embd_head_v    = 128
0.00.052.457 I print_info: n_gqa            = 1
0.00.052.458 I print_info: n_embd_k_gqa     = 2048
0.00.052.459 I print_info: n_embd_v_gqa     = 2048
0.00.052.459 I print_info: f_norm_eps       = 1.0e-05
0.00.052.460 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.460 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.460 I print_info: f_logit_scale    = 0.0e+00
0.00.052.461 I print_info: n_ff             = 8192
0.00.052.461 I print_info: n_expert         = 0
0.00.052.461 I print_info: n_expert_used    = 0
0.00.052.461 I print_info: causal attn      = 1
0.00.052.462 I print_info: pooling type     = 0
0.00.052.462 I print_info: rope type        = 2
0.00.052.462 I print_info: rope scaling     = linear
0.00.052.462 I print_info: freq_base_train  = 10000.0
0.00.052.462 I print_info: freq_scale_train = 1
0.00.052.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.463 I print_info: rope_finetuned   = unknown
0.00.052.465 I print_info: ssm_d_conv       = 0
0.00.052.465 I print_info: ssm_d_inner      = 0
0.00.052.465 I print_info: ssm_d_state      = 0
0.00.052.465 I print_info: ssm_dt_rank      = 0
0.00.052.465 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.465 I print_info: model type       = 1.4B
0.00.052.466 I print_info: model params     = 1.41 B
0.00.052.466 I print_info: general.name     = 1.4B
0.00.052.466 I print_info: vocab type       = BPE
0.00.052.467 I print_info: n_vocab          = 50304
0.00.052.467 I print_info: n_merges         = 50009
0.00.052.467 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.467 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.468 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.468 I print_info: LF token         = 128 'Ä'
0.00.052.469 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.470 I print_info: max token length = 1024
0.00.054.268 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.268 I load_tensors: offloading output layer to GPU
0.00.054.269 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.274 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.275 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.692 I llama_init_from_model: n_seq_max     = 1
0.00.054.692 I llama_init_from_model: n_ctx         = 2048
0.00.054.692 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.693 I llama_init_from_model: n_batch       = 2048
0.00.054.693 I llama_init_from_model: n_ubatch      = 512
0.00.054.693 I llama_init_from_model: flash_attn    = 0
0.00.054.693 I llama_init_from_model: freq_base     = 10000.0
0.00.054.694 I llama_init_from_model: freq_scale    = 1
0.00.054.694 I ggml_metal_init: allocating
0.00.054.697 I ggml_metal_init: found device: Apple M4
0.00.054.699 I ggml_metal_init: picking default device: Apple M4
0.00.055.290 I ggml_metal_init: using embedded metal library
0.00.057.610 I ggml_metal_init: GPU name:   Apple M4
0.00.057.611 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.612 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.612 I ggml_metal_init: simdgroup reduction   = true
0.00.057.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.613 I ggml_metal_init: has bfloat            = true
0.00.057.613 I ggml_metal_init: use bfloat            = true
0.00.057.613 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.614 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.332 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.532 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.537 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.555 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.621 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.623 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.623 I llama_init_from_model: graph nodes  = 967
0.00.087.623 I llama_init_from_model: graph splits = 2
0.00.087.626 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.756 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.757 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.047 I main: llama threadpool init, n_threads = 4
0.00.614.088 I 
0.00.614.125 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.127 I 
0.00.614.348 I sampler seed: 1234
0.00.614.355 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.367 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.367 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.367 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.369.668 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.369.668 I llama_perf_context_print:        load time =     602.54 ms
0.01.369.669 I llama_perf_context_print: prompt eval time =      47.10 ms /     7 tokens (    6.73 ms per token,   148.61 tokens per second)
0.01.369.670 I llama_perf_context_print:        eval time =     705.19 ms /    63 runs   (   11.19 ms per token,    89.34 tokens per second)
0.01.369.670 I llama_perf_context_print:       total time =     755.62 ms /    70 tokens
0.01.369.854 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.107s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.912 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.708 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.709 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.709 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.709 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.712 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.712 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.719 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.462 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.241 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.242 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.243 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.243 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.243 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.244 I llama_model_loader: - type  f32:  194 tensors
0.00.026.244 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.244 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.245 I print_info: file format = GGUF V3 (latest)
0.00.026.246 I print_info: file type   = Q5_K - Medium
0.00.026.246 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.365 I load: special tokens cache size = 25
0.00.051.290 I load: token to piece cache size = 0.2984 MB
0.00.051.293 I print_info: arch             = gptneox
0.00.051.293 I print_info: vocab_only       = 0
0.00.051.293 I print_info: n_ctx_train      = 2048
0.00.051.294 I print_info: n_embd           = 2048
0.00.051.294 I print_info: n_layer          = 24
0.00.051.297 I print_info: n_head           = 16
0.00.051.298 I print_info: n_head_kv        = 16
0.00.051.298 I print_info: n_rot            = 32
0.00.051.298 I print_info: n_swa            = 0
0.00.051.298 I print_info: n_embd_head_k    = 128
0.00.051.299 I print_info: n_embd_head_v    = 128
0.00.051.299 I print_info: n_gqa            = 1
0.00.051.300 I print_info: n_embd_k_gqa     = 2048
0.00.051.301 I print_info: n_embd_v_gqa     = 2048
0.00.051.301 I print_info: f_norm_eps       = 1.0e-05
0.00.051.302 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.302 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.302 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.302 I print_info: f_logit_scale    = 0.0e+00
0.00.051.303 I print_info: n_ff             = 8192
0.00.051.303 I print_info: n_expert         = 0
0.00.051.303 I print_info: n_expert_used    = 0
0.00.051.306 I print_info: causal attn      = 1
0.00.051.308 I print_info: pooling type     = 0
0.00.051.308 I print_info: rope type        = 2
0.00.051.308 I print_info: rope scaling     = linear
0.00.051.308 I print_info: freq_base_train  = 10000.0
0.00.051.309 I print_info: freq_scale_train = 1
0.00.051.309 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.309 I print_info: rope_finetuned   = unknown
0.00.051.309 I print_info: ssm_d_conv       = 0
0.00.051.309 I print_info: ssm_d_inner      = 0
0.00.051.310 I print_info: ssm_d_state      = 0
0.00.051.310 I print_info: ssm_dt_rank      = 0
0.00.051.310 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.310 I print_info: model type       = 1.4B
0.00.051.310 I print_info: model params     = 1.41 B
0.00.051.311 I print_info: general.name     = 1.4B
0.00.051.311 I print_info: vocab type       = BPE
0.00.051.311 I print_info: n_vocab          = 50304
0.00.051.312 I print_info: n_merges         = 50009
0.00.051.312 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.313 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.313 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.314 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.314 I print_info: LF token         = 128 'Ä'
0.00.051.315 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.315 I print_info: max token length = 1024
0.00.053.375 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.376 I load_tensors: offloading output layer to GPU
0.00.053.376 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.386 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.387 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.707 I llama_init_from_model: n_seq_max     = 1
0.00.053.708 I llama_init_from_model: n_ctx         = 2048
0.00.053.709 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.709 I llama_init_from_model: n_batch       = 2048
0.00.053.709 I llama_init_from_model: n_ubatch      = 512
0.00.053.709 I llama_init_from_model: flash_attn    = 0
0.00.053.709 I llama_init_from_model: freq_base     = 10000.0
0.00.053.710 I llama_init_from_model: freq_scale    = 1
0.00.053.710 I ggml_metal_init: allocating
0.00.053.713 I ggml_metal_init: found device: Apple M4
0.00.053.715 I ggml_metal_init: picking default device: Apple M4
0.00.054.301 I ggml_metal_init: using embedded metal library
0.00.056.670 I ggml_metal_init: GPU name:   Apple M4
0.00.056.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.672 I ggml_metal_init: simdgroup reduction   = true
0.00.056.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.672 I ggml_metal_init: has bfloat            = true
0.00.056.672 I ggml_metal_init: use bfloat            = true
0.00.056.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.673 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.440 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.049 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.056 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.073 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.081 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.082 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.082 I llama_init_from_model: graph nodes  = 967
0.00.088.083 I llama_init_from_model: graph splits = 2
0.00.088.086 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.215 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.474 I main: llama threadpool init, n_threads = 4
0.00.690.532 I 
0.00.690.555 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.555 I 
0.00.690.778 I sampler seed: 1234
0.00.690.783 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.794 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.795 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.795 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.546.123 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63906.39 tokens per second)
0.01.546.123 I llama_perf_context_print:        load time =     681.56 ms
0.01.546.124 I llama_perf_context_print: prompt eval time =      56.42 ms /     7 tokens (    8.06 ms per token,   124.08 tokens per second)
0.01.546.127 I llama_perf_context_print:        eval time =     796.07 ms /    63 runs   (   12.64 ms per token,    79.14 tokens per second)
0.01.546.132 I llama_perf_context_print:       total time =     855.65 ms /    70 tokens
0.01.546.323 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.875 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.882 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.896 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.896 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.900 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.900 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.907 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.555 I llama_model_loader: - type  f32:  194 tensors
0.00.026.555 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.556 I print_info: file format = GGUF V3 (latest)
0.00.026.556 I print_info: file type   = Q6_K
0.00.026.557 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.389 I load: special tokens cache size = 25
0.00.052.486 I load: token to piece cache size = 0.2984 MB
0.00.052.488 I print_info: arch             = gptneox
0.00.052.489 I print_info: vocab_only       = 0
0.00.052.489 I print_info: n_ctx_train      = 2048
0.00.052.489 I print_info: n_embd           = 2048
0.00.052.489 I print_info: n_layer          = 24
0.00.052.492 I print_info: n_head           = 16
0.00.052.492 I print_info: n_head_kv        = 16
0.00.052.493 I print_info: n_rot            = 32
0.00.052.493 I print_info: n_swa            = 0
0.00.052.493 I print_info: n_embd_head_k    = 128
0.00.052.493 I print_info: n_embd_head_v    = 128
0.00.052.494 I print_info: n_gqa            = 1
0.00.052.494 I print_info: n_embd_k_gqa     = 2048
0.00.052.495 I print_info: n_embd_v_gqa     = 2048
0.00.052.496 I print_info: f_norm_eps       = 1.0e-05
0.00.052.496 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.496 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.496 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.497 I print_info: f_logit_scale    = 0.0e+00
0.00.052.497 I print_info: n_ff             = 8192
0.00.052.497 I print_info: n_expert         = 0
0.00.052.498 I print_info: n_expert_used    = 0
0.00.052.498 I print_info: causal attn      = 1
0.00.052.499 I print_info: pooling type     = 0
0.00.052.499 I print_info: rope type        = 2
0.00.052.499 I print_info: rope scaling     = linear
0.00.052.500 I print_info: freq_base_train  = 10000.0
0.00.052.500 I print_info: freq_scale_train = 1
0.00.052.500 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.500 I print_info: rope_finetuned   = unknown
0.00.052.501 I print_info: ssm_d_conv       = 0
0.00.052.501 I print_info: ssm_d_inner      = 0
0.00.052.501 I print_info: ssm_d_state      = 0
0.00.052.503 I print_info: ssm_dt_rank      = 0
0.00.052.503 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.504 I print_info: model type       = 1.4B
0.00.052.504 I print_info: model params     = 1.41 B
0.00.052.504 I print_info: general.name     = 1.4B
0.00.052.505 I print_info: vocab type       = BPE
0.00.052.505 I print_info: n_vocab          = 50304
0.00.052.505 I print_info: n_merges         = 50009
0.00.052.505 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.505 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.505 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.506 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.506 I print_info: LF token         = 128 'Ä'
0.00.052.506 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.506 I print_info: max token length = 1024
0.00.054.611 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.611 I load_tensors: offloading output layer to GPU
0.00.054.611 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.622 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.624 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.916 I llama_init_from_model: n_seq_max     = 1
0.00.054.917 I llama_init_from_model: n_ctx         = 2048
0.00.054.917 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.917 I llama_init_from_model: n_batch       = 2048
0.00.054.918 I llama_init_from_model: n_ubatch      = 512
0.00.054.918 I llama_init_from_model: flash_attn    = 0
0.00.054.918 I llama_init_from_model: freq_base     = 10000.0
0.00.054.918 I llama_init_from_model: freq_scale    = 1
0.00.054.919 I ggml_metal_init: allocating
0.00.054.922 I ggml_metal_init: found device: Apple M4
0.00.054.924 I ggml_metal_init: picking default device: Apple M4
0.00.055.528 I ggml_metal_init: using embedded metal library
0.00.057.929 I ggml_metal_init: GPU name:   Apple M4
0.00.057.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.931 I ggml_metal_init: simdgroup reduction   = true
0.00.057.931 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.931 I ggml_metal_init: has bfloat            = true
0.00.057.931 I ggml_metal_init: use bfloat            = true
0.00.057.932 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.932 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.014 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.400 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.406 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.425 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.498 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.499 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.499 I llama_init_from_model: graph nodes  = 967
0.00.090.500 I llama_init_from_model: graph splits = 2
0.00.090.503 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.631 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.737 I main: llama threadpool init, n_threads = 4
0.00.744.775 I 
0.00.744.802 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.802 I 
0.00.745.032 I sampler seed: 1234
0.00.745.037 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.102 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.105 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.105 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.628.063 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.628.063 I llama_perf_context_print:        load time =     734.86 ms
0.01.628.064 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.46 tokens per second)
0.01.628.065 I llama_perf_context_print:        eval time =     825.44 ms /    63 runs   (   13.10 ms per token,    76.32 tokens per second)
0.01.628.065 I llama_perf_context_print:       total time =     883.33 ms /    70 tokens
0.01.628.299 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.522 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.301 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.596 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.605 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.608 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.612 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.613 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.613 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.614 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.614 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.615 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.621 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.171 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.232 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.235 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.236 I llama_model_loader: - type  f32:  194 tensors
0.00.055.236 I llama_model_loader: - type  f16:   98 tensors
0.00.055.237 I print_info: file format = GGUF V3 (latest)
0.00.055.238 I print_info: file type   = all F32 (guessed)
0.00.055.239 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.082.865 I load: special tokens cache size = 25
0.00.089.782 I load: token to piece cache size = 0.2984 MB
0.00.089.785 I print_info: arch             = gptneox
0.00.089.785 I print_info: vocab_only       = 0
0.00.089.785 I print_info: n_ctx_train      = 2048
0.00.089.785 I print_info: n_embd           = 2048
0.00.089.785 I print_info: n_layer          = 24
0.00.089.788 I print_info: n_head           = 16
0.00.089.789 I print_info: n_head_kv        = 16
0.00.089.789 I print_info: n_rot            = 32
0.00.089.790 I print_info: n_swa            = 0
0.00.089.790 I print_info: n_embd_head_k    = 128
0.00.089.790 I print_info: n_embd_head_v    = 128
0.00.089.791 I print_info: n_gqa            = 1
0.00.089.791 I print_info: n_embd_k_gqa     = 2048
0.00.089.792 I print_info: n_embd_v_gqa     = 2048
0.00.089.792 I print_info: f_norm_eps       = 1.0e-05
0.00.089.792 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.794 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.795 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.795 I print_info: f_logit_scale    = 0.0e+00
0.00.089.795 I print_info: n_ff             = 8192
0.00.089.795 I print_info: n_expert         = 0
0.00.089.796 I print_info: n_expert_used    = 0
0.00.089.796 I print_info: causal attn      = 1
0.00.089.796 I print_info: pooling type     = 0
0.00.089.796 I print_info: rope type        = 2
0.00.089.796 I print_info: rope scaling     = linear
0.00.089.797 I print_info: freq_base_train  = 10000.0
0.00.089.797 I print_info: freq_scale_train = 1
0.00.089.797 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.798 I print_info: rope_finetuned   = unknown
0.00.089.798 I print_info: ssm_d_conv       = 0
0.00.089.798 I print_info: ssm_d_inner      = 0
0.00.089.798 I print_info: ssm_d_state      = 0
0.00.089.798 I print_info: ssm_dt_rank      = 0
0.00.089.800 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.800 I print_info: model type       = 1.4B
0.00.089.800 I print_info: model params     = 1.41 B
0.00.089.800 I print_info: general.name     = 1.4B
0.00.089.801 I print_info: vocab type       = BPE
0.00.089.801 I print_info: n_vocab          = 50304
0.00.089.801 I print_info: n_merges         = 50009
0.00.089.801 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.802 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.802 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.802 I print_info: LF token         = 128 'Ä'
0.00.089.806 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.806 I print_info: max token length = 1024
0.00.092.325 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.326 I load_tensors: offloading output layer to GPU
0.00.092.326 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.336 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.337 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.605 I llama_init_from_model: n_seq_max     = 1
0.00.092.606 I llama_init_from_model: n_ctx         = 128
0.00.092.606 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.607 I llama_init_from_model: n_batch       = 128
0.00.092.607 I llama_init_from_model: n_ubatch      = 128
0.00.092.607 I llama_init_from_model: flash_attn    = 0
0.00.092.607 I llama_init_from_model: freq_base     = 10000.0
0.00.092.608 I llama_init_from_model: freq_scale    = 1
0.00.092.608 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.608 I ggml_metal_init: allocating
0.00.092.611 I ggml_metal_init: found device: Apple M4
0.00.092.613 I ggml_metal_init: picking default device: Apple M4
0.00.093.220 I ggml_metal_init: using embedded metal library
0.00.095.828 I ggml_metal_init: GPU name:   Apple M4
0.00.095.829 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.830 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.830 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.830 I ggml_metal_init: simdgroup reduction   = true
0.00.095.830 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.831 I ggml_metal_init: has bfloat            = true
0.00.095.831 I ggml_metal_init: use bfloat            = true
0.00.095.831 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.832 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.123 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.424 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.426 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.440 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.333 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.108.334 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.108.334 I llama_init_from_model: graph nodes  = 967
0.00.108.335 I llama_init_from_model: graph splits = 2
0.00.108.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.336 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.917.585 I 
0.00.917.644 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.917.653 I perplexity: tokenizing the input ..
0.00.930.592 I perplexity: tokenization took 12.934 ms
0.00.930.597 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.051.422 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.053.267 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.053.325 I llama_perf_context_print:        load time =     894.26 ms
0.01.053.327 I llama_perf_context_print: prompt eval time =     120.13 ms /   128 tokens (    0.94 ms per token,  1065.49 tokens per second)
0.01.053.328 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.053.336 I llama_perf_context_print:       total time =     135.75 ms /   129 tokens
0.01.054.133 I ggml_metal_free: deallocating

real	0m1.252s
user	0m0.125s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.248 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.131 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.131 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.132 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.132 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.133 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.133 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.919 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.457 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.200 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.200 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.201 I llama_model_loader: - type  f32:  194 tensors
0.00.036.201 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.202 I print_info: file format = GGUF V3 (latest)
0.00.036.203 I print_info: file type   = Q8_0
0.00.036.204 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.061.355 I load: special tokens cache size = 25
0.00.067.900 I load: token to piece cache size = 0.2984 MB
0.00.067.903 I print_info: arch             = gptneox
0.00.067.903 I print_info: vocab_only       = 0
0.00.067.903 I print_info: n_ctx_train      = 2048
0.00.067.904 I print_info: n_embd           = 2048
0.00.067.904 I print_info: n_layer          = 24
0.00.067.907 I print_info: n_head           = 16
0.00.067.908 I print_info: n_head_kv        = 16
0.00.067.908 I print_info: n_rot            = 32
0.00.067.908 I print_info: n_swa            = 0
0.00.067.909 I print_info: n_embd_head_k    = 128
0.00.067.909 I print_info: n_embd_head_v    = 128
0.00.067.909 I print_info: n_gqa            = 1
0.00.067.910 I print_info: n_embd_k_gqa     = 2048
0.00.067.911 I print_info: n_embd_v_gqa     = 2048
0.00.067.911 I print_info: f_norm_eps       = 1.0e-05
0.00.067.911 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.912 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.912 I print_info: f_logit_scale    = 0.0e+00
0.00.067.912 I print_info: n_ff             = 8192
0.00.067.913 I print_info: n_expert         = 0
0.00.067.913 I print_info: n_expert_used    = 0
0.00.067.913 I print_info: causal attn      = 1
0.00.067.913 I print_info: pooling type     = 0
0.00.067.913 I print_info: rope type        = 2
0.00.067.913 I print_info: rope scaling     = linear
0.00.067.914 I print_info: freq_base_train  = 10000.0
0.00.067.914 I print_info: freq_scale_train = 1
0.00.067.914 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.914 I print_info: rope_finetuned   = unknown
0.00.067.914 I print_info: ssm_d_conv       = 0
0.00.067.915 I print_info: ssm_d_inner      = 0
0.00.067.915 I print_info: ssm_d_state      = 0
0.00.067.915 I print_info: ssm_dt_rank      = 0
0.00.067.915 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.915 I print_info: model type       = 1.4B
0.00.067.915 I print_info: model params     = 1.41 B
0.00.067.916 I print_info: general.name     = 1.4B
0.00.067.917 I print_info: vocab type       = BPE
0.00.067.917 I print_info: n_vocab          = 50304
0.00.067.917 I print_info: n_merges         = 50009
0.00.067.917 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.917 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.917 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.918 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.919 I print_info: LF token         = 128 'Ä'
0.00.067.919 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.920 I print_info: max token length = 1024
0.00.070.274 I load_tensors: offloading 24 repeating layers to GPU
0.00.070.274 I load_tensors: offloading output layer to GPU
0.00.070.274 I load_tensors: offloaded 25/25 layers to GPU
0.00.070.285 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.286 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.070.604 I llama_init_from_model: n_seq_max     = 1
0.00.070.604 I llama_init_from_model: n_ctx         = 128
0.00.070.605 I llama_init_from_model: n_ctx_per_seq = 128
0.00.070.605 I llama_init_from_model: n_batch       = 128
0.00.070.605 I llama_init_from_model: n_ubatch      = 128
0.00.070.605 I llama_init_from_model: flash_attn    = 0
0.00.070.605 I llama_init_from_model: freq_base     = 10000.0
0.00.070.606 I llama_init_from_model: freq_scale    = 1
0.00.070.606 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.606 I ggml_metal_init: allocating
0.00.070.609 I ggml_metal_init: found device: Apple M4
0.00.070.611 I ggml_metal_init: picking default device: Apple M4
0.00.071.283 I ggml_metal_init: using embedded metal library
0.00.073.859 I ggml_metal_init: GPU name:   Apple M4
0.00.073.861 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.861 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.862 I ggml_metal_init: simdgroup reduction   = true
0.00.073.862 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.862 I ggml_metal_init: has bfloat            = true
0.00.073.862 I ggml_metal_init: use bfloat            = true
0.00.073.863 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.379 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.702 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.708 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.725 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.626 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.085.627 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.085.628 I llama_init_from_model: graph nodes  = 967
0.00.085.628 I llama_init_from_model: graph splits = 2
0.00.085.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.629 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.835.671 I 
0.00.835.695 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.835.698 I perplexity: tokenizing the input ..
0.00.843.759 I perplexity: tokenization took 8.06 ms
0.00.843.763 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.967.720 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.968.880 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.968.903 I llama_perf_context_print:        load time =     823.42 ms
0.00.968.905 I llama_perf_context_print: prompt eval time =     123.70 ms /   128 tokens (    0.97 ms per token,  1034.74 tokens per second)
0.00.968.905 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.968.906 I llama_perf_context_print:       total time =     133.23 ms /   129 tokens
0.00.969.324 I ggml_metal_free: deallocating

real	0m0.987s
user	0m0.095s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.853 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.545 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.546 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.552 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.553 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.313 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.350 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.036 I llama_model_loader: - type  f32:  194 tensors
0.00.025.036 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.036 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.037 I print_info: file format = GGUF V3 (latest)
0.00.025.038 I print_info: file type   = Q4_0
0.00.025.039 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.043.899 I load: special tokens cache size = 25
0.00.049.811 I load: token to piece cache size = 0.2984 MB
0.00.049.814 I print_info: arch             = gptneox
0.00.049.815 I print_info: vocab_only       = 0
0.00.049.815 I print_info: n_ctx_train      = 2048
0.00.049.815 I print_info: n_embd           = 2048
0.00.049.815 I print_info: n_layer          = 24
0.00.049.818 I print_info: n_head           = 16
0.00.049.819 I print_info: n_head_kv        = 16
0.00.049.819 I print_info: n_rot            = 32
0.00.049.819 I print_info: n_swa            = 0
0.00.049.820 I print_info: n_embd_head_k    = 128
0.00.049.821 I print_info: n_embd_head_v    = 128
0.00.049.822 I print_info: n_gqa            = 1
0.00.049.823 I print_info: n_embd_k_gqa     = 2048
0.00.049.824 I print_info: n_embd_v_gqa     = 2048
0.00.049.824 I print_info: f_norm_eps       = 1.0e-05
0.00.049.825 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.825 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.825 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.825 I print_info: f_logit_scale    = 0.0e+00
0.00.049.826 I print_info: n_ff             = 8192
0.00.049.827 I print_info: n_expert         = 0
0.00.049.828 I print_info: n_expert_used    = 0
0.00.049.828 I print_info: causal attn      = 1
0.00.049.828 I print_info: pooling type     = 0
0.00.049.828 I print_info: rope type        = 2
0.00.049.828 I print_info: rope scaling     = linear
0.00.049.829 I print_info: freq_base_train  = 10000.0
0.00.049.829 I print_info: freq_scale_train = 1
0.00.049.829 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.829 I print_info: rope_finetuned   = unknown
0.00.049.829 I print_info: ssm_d_conv       = 0
0.00.049.829 I print_info: ssm_d_inner      = 0
0.00.049.830 I print_info: ssm_d_state      = 0
0.00.049.830 I print_info: ssm_dt_rank      = 0
0.00.049.830 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.830 I print_info: model type       = 1.4B
0.00.049.830 I print_info: model params     = 1.41 B
0.00.049.830 I print_info: general.name     = 1.4B
0.00.049.831 I print_info: vocab type       = BPE
0.00.049.835 I print_info: n_vocab          = 50304
0.00.049.835 I print_info: n_merges         = 50009
0.00.049.835 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.836 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.836 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.838 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.838 I print_info: LF token         = 128 'Ä'
0.00.049.838 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.839 I print_info: max token length = 1024
0.00.051.779 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.779 I load_tensors: offloading output layer to GPU
0.00.051.779 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.790 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.791 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.078 I llama_init_from_model: n_seq_max     = 1
0.00.052.079 I llama_init_from_model: n_ctx         = 128
0.00.052.079 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.079 I llama_init_from_model: n_batch       = 128
0.00.052.079 I llama_init_from_model: n_ubatch      = 128
0.00.052.079 I llama_init_from_model: flash_attn    = 0
0.00.052.080 I llama_init_from_model: freq_base     = 10000.0
0.00.052.080 I llama_init_from_model: freq_scale    = 1
0.00.052.080 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.081 I ggml_metal_init: allocating
0.00.052.084 I ggml_metal_init: found device: Apple M4
0.00.052.086 I ggml_metal_init: picking default device: Apple M4
0.00.052.658 I ggml_metal_init: using embedded metal library
0.00.055.034 I ggml_metal_init: GPU name:   Apple M4
0.00.055.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.036 I ggml_metal_init: simdgroup reduction   = true
0.00.055.037 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.037 I ggml_metal_init: has bfloat            = true
0.00.055.037 I ggml_metal_init: use bfloat            = true
0.00.055.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.784 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.012 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.014 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.029 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.962 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.963 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.963 I llama_init_from_model: graph nodes  = 967
0.00.066.963 I llama_init_from_model: graph splits = 2
0.00.066.964 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.226 I 
0.00.591.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.298 I perplexity: tokenizing the input ..
0.00.599.632 I perplexity: tokenization took 8.332 ms
0.00.599.640 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.722.512 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.723.737 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.723.766 I llama_perf_context_print:        load time =     581.36 ms
0.00.723.767 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.65 tokens per second)
0.00.723.767 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.723.768 I llama_perf_context_print:       total time =     132.55 ms /   129 tokens
0.00.724.225 I ggml_metal_free: deallocating

real	0m0.739s
user	0m0.077s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.818 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.989 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.994 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.995 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.996 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.996 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.997 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.997 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.998 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.998 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.999 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.999 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.000 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.002 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.755 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.788 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.530 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.530 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.531 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.531 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.532 I llama_model_loader: - type  f32:  194 tensors
0.00.024.532 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.532 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.533 I print_info: file format = GGUF V3 (latest)
0.00.024.533 I print_info: file type   = Q4_1
0.00.024.534 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.437 I load: special tokens cache size = 25
0.00.049.543 I load: token to piece cache size = 0.2984 MB
0.00.049.546 I print_info: arch             = gptneox
0.00.049.547 I print_info: vocab_only       = 0
0.00.049.547 I print_info: n_ctx_train      = 2048
0.00.049.547 I print_info: n_embd           = 2048
0.00.049.547 I print_info: n_layer          = 24
0.00.049.550 I print_info: n_head           = 16
0.00.049.551 I print_info: n_head_kv        = 16
0.00.049.553 I print_info: n_rot            = 32
0.00.049.554 I print_info: n_swa            = 0
0.00.049.554 I print_info: n_embd_head_k    = 128
0.00.049.554 I print_info: n_embd_head_v    = 128
0.00.049.555 I print_info: n_gqa            = 1
0.00.049.555 I print_info: n_embd_k_gqa     = 2048
0.00.049.556 I print_info: n_embd_v_gqa     = 2048
0.00.049.561 I print_info: f_norm_eps       = 1.0e-05
0.00.049.562 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.562 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.562 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.562 I print_info: f_logit_scale    = 0.0e+00
0.00.049.563 I print_info: n_ff             = 8192
0.00.049.563 I print_info: n_expert         = 0
0.00.049.564 I print_info: n_expert_used    = 0
0.00.049.564 I print_info: causal attn      = 1
0.00.049.564 I print_info: pooling type     = 0
0.00.049.564 I print_info: rope type        = 2
0.00.049.564 I print_info: rope scaling     = linear
0.00.049.565 I print_info: freq_base_train  = 10000.0
0.00.049.565 I print_info: freq_scale_train = 1
0.00.049.565 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.566 I print_info: rope_finetuned   = unknown
0.00.049.566 I print_info: ssm_d_conv       = 0
0.00.049.566 I print_info: ssm_d_inner      = 0
0.00.049.566 I print_info: ssm_d_state      = 0
0.00.049.568 I print_info: ssm_dt_rank      = 0
0.00.049.568 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.568 I print_info: model type       = 1.4B
0.00.049.568 I print_info: model params     = 1.41 B
0.00.049.569 I print_info: general.name     = 1.4B
0.00.049.569 I print_info: vocab type       = BPE
0.00.049.569 I print_info: n_vocab          = 50304
0.00.049.569 I print_info: n_merges         = 50009
0.00.049.569 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.570 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.570 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.570 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.570 I print_info: LF token         = 128 'Ä'
0.00.049.570 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.570 I print_info: max token length = 1024
0.00.051.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.602 I load_tensors: offloading output layer to GPU
0.00.051.602 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.613 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.614 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.891 I llama_init_from_model: n_seq_max     = 1
0.00.051.892 I llama_init_from_model: n_ctx         = 128
0.00.051.892 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.892 I llama_init_from_model: n_batch       = 128
0.00.051.892 I llama_init_from_model: n_ubatch      = 128
0.00.051.893 I llama_init_from_model: flash_attn    = 0
0.00.051.893 I llama_init_from_model: freq_base     = 10000.0
0.00.051.893 I llama_init_from_model: freq_scale    = 1
0.00.051.893 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.894 I ggml_metal_init: allocating
0.00.051.897 I ggml_metal_init: found device: Apple M4
0.00.051.899 I ggml_metal_init: picking default device: Apple M4
0.00.052.487 I ggml_metal_init: using embedded metal library
0.00.054.809 I ggml_metal_init: GPU name:   Apple M4
0.00.054.811 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.811 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.812 I ggml_metal_init: simdgroup reduction   = true
0.00.054.812 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.812 I ggml_metal_init: has bfloat            = true
0.00.054.812 I ggml_metal_init: use bfloat            = true
0.00.054.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.813 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.470 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.735 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.737 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.751 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.614 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.615 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.615 I llama_init_from_model: graph nodes  = 967
0.00.066.615 I llama_init_from_model: graph splits = 2
0.00.066.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.638 I 
0.00.616.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.680 I perplexity: tokenizing the input ..
0.00.624.499 I perplexity: tokenization took 7.817 ms
0.00.624.502 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.717 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.747.978 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.748.001 I llama_perf_context_print:        load time =     607.81 ms
0.00.748.004 I llama_perf_context_print: prompt eval time =     121.99 ms /   128 tokens (    0.95 ms per token,  1049.28 tokens per second)
0.00.748.005 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.005 I llama_perf_context_print:       total time =     131.37 ms /   129 tokens
0.00.748.385 I ggml_metal_free: deallocating

real	0m0.761s
user	0m0.077s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.177 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.153 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.158 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.164 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.164 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.165 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.166 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.167 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.167 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.167 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.168 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.168 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.169 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.554 I llama_model_loader: - type  f32:  194 tensors
0.00.025.554 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.554 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.555 I print_info: file format = GGUF V3 (latest)
0.00.025.555 I print_info: file type   = Q5_0
0.00.025.556 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.469 I load: special tokens cache size = 25
0.00.050.566 I load: token to piece cache size = 0.2984 MB
0.00.050.569 I print_info: arch             = gptneox
0.00.050.569 I print_info: vocab_only       = 0
0.00.050.569 I print_info: n_ctx_train      = 2048
0.00.050.569 I print_info: n_embd           = 2048
0.00.050.570 I print_info: n_layer          = 24
0.00.050.572 I print_info: n_head           = 16
0.00.050.573 I print_info: n_head_kv        = 16
0.00.050.573 I print_info: n_rot            = 32
0.00.050.573 I print_info: n_swa            = 0
0.00.050.573 I print_info: n_embd_head_k    = 128
0.00.050.573 I print_info: n_embd_head_v    = 128
0.00.050.574 I print_info: n_gqa            = 1
0.00.050.575 I print_info: n_embd_k_gqa     = 2048
0.00.050.576 I print_info: n_embd_v_gqa     = 2048
0.00.050.576 I print_info: f_norm_eps       = 1.0e-05
0.00.050.578 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.579 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.579 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.579 I print_info: f_logit_scale    = 0.0e+00
0.00.050.580 I print_info: n_ff             = 8192
0.00.050.580 I print_info: n_expert         = 0
0.00.050.580 I print_info: n_expert_used    = 0
0.00.050.580 I print_info: causal attn      = 1
0.00.050.580 I print_info: pooling type     = 0
0.00.050.580 I print_info: rope type        = 2
0.00.050.581 I print_info: rope scaling     = linear
0.00.050.581 I print_info: freq_base_train  = 10000.0
0.00.050.581 I print_info: freq_scale_train = 1
0.00.050.582 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.582 I print_info: rope_finetuned   = unknown
0.00.050.582 I print_info: ssm_d_conv       = 0
0.00.050.582 I print_info: ssm_d_inner      = 0
0.00.050.582 I print_info: ssm_d_state      = 0
0.00.050.582 I print_info: ssm_dt_rank      = 0
0.00.050.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.583 I print_info: model type       = 1.4B
0.00.050.583 I print_info: model params     = 1.41 B
0.00.050.583 I print_info: general.name     = 1.4B
0.00.050.584 I print_info: vocab type       = BPE
0.00.050.584 I print_info: n_vocab          = 50304
0.00.050.584 I print_info: n_merges         = 50009
0.00.050.584 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.584 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.585 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.585 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.585 I print_info: LF token         = 128 'Ä'
0.00.050.585 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.585 I print_info: max token length = 1024
0.00.052.549 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.549 I load_tensors: offloading output layer to GPU
0.00.052.549 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.560 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.561 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.843 I llama_init_from_model: n_seq_max     = 1
0.00.052.843 I llama_init_from_model: n_ctx         = 128
0.00.052.844 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.844 I llama_init_from_model: n_batch       = 128
0.00.052.844 I llama_init_from_model: n_ubatch      = 128
0.00.052.844 I llama_init_from_model: flash_attn    = 0
0.00.052.845 I llama_init_from_model: freq_base     = 10000.0
0.00.052.845 I llama_init_from_model: freq_scale    = 1
0.00.052.845 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.846 I ggml_metal_init: allocating
0.00.052.849 I ggml_metal_init: found device: Apple M4
0.00.052.851 I ggml_metal_init: picking default device: Apple M4
0.00.053.412 I ggml_metal_init: using embedded metal library
0.00.055.763 I ggml_metal_init: GPU name:   Apple M4
0.00.055.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.766 I ggml_metal_init: simdgroup reduction   = true
0.00.055.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.766 I ggml_metal_init: has bfloat            = true
0.00.055.766 I ggml_metal_init: use bfloat            = true
0.00.055.766 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.394 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.632 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.635 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.648 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.552 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.552 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.553 I llama_init_from_model: graph nodes  = 967
0.00.067.553 I llama_init_from_model: graph splits = 2
0.00.067.554 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.554 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.805 I 
0.00.625.837 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.842 I perplexity: tokenizing the input ..
0.00.634.068 I perplexity: tokenization took 8.225 ms
0.00.634.073 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.911 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.770.059 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.770.088 I llama_perf_context_print:        load time =     615.62 ms
0.00.770.089 I llama_perf_context_print: prompt eval time =     134.61 ms /   128 tokens (    1.05 ms per token,   950.88 tokens per second)
0.00.770.089 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.090 I llama_perf_context_print:       total time =     144.29 ms /   129 tokens
0.00.770.397 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.573 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.474 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.478 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.480 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.481 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.483 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.486 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.211 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.939 I llama_model_loader: - type  f32:  194 tensors
0.00.023.939 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.940 I print_info: file format = GGUF V3 (latest)
0.00.023.940 I print_info: file type   = Q5_1
0.00.023.941 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.547 I load: special tokens cache size = 25
0.00.049.640 I load: token to piece cache size = 0.2984 MB
0.00.049.643 I print_info: arch             = gptneox
0.00.049.644 I print_info: vocab_only       = 0
0.00.049.644 I print_info: n_ctx_train      = 2048
0.00.049.644 I print_info: n_embd           = 2048
0.00.049.644 I print_info: n_layer          = 24
0.00.049.647 I print_info: n_head           = 16
0.00.049.648 I print_info: n_head_kv        = 16
0.00.049.648 I print_info: n_rot            = 32
0.00.049.648 I print_info: n_swa            = 0
0.00.049.648 I print_info: n_embd_head_k    = 128
0.00.049.651 I print_info: n_embd_head_v    = 128
0.00.049.651 I print_info: n_gqa            = 1
0.00.049.652 I print_info: n_embd_k_gqa     = 2048
0.00.049.653 I print_info: n_embd_v_gqa     = 2048
0.00.049.654 I print_info: f_norm_eps       = 1.0e-05
0.00.049.655 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.656 I print_info: f_logit_scale    = 0.0e+00
0.00.049.656 I print_info: n_ff             = 8192
0.00.049.657 I print_info: n_expert         = 0
0.00.049.657 I print_info: n_expert_used    = 0
0.00.049.657 I print_info: causal attn      = 1
0.00.049.657 I print_info: pooling type     = 0
0.00.049.657 I print_info: rope type        = 2
0.00.049.657 I print_info: rope scaling     = linear
0.00.049.658 I print_info: freq_base_train  = 10000.0
0.00.049.658 I print_info: freq_scale_train = 1
0.00.049.658 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.659 I print_info: rope_finetuned   = unknown
0.00.049.659 I print_info: ssm_d_conv       = 0
0.00.049.659 I print_info: ssm_d_inner      = 0
0.00.049.659 I print_info: ssm_d_state      = 0
0.00.049.659 I print_info: ssm_dt_rank      = 0
0.00.049.659 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.659 I print_info: model type       = 1.4B
0.00.049.664 I print_info: model params     = 1.41 B
0.00.049.664 I print_info: general.name     = 1.4B
0.00.049.665 I print_info: vocab type       = BPE
0.00.049.665 I print_info: n_vocab          = 50304
0.00.049.665 I print_info: n_merges         = 50009
0.00.049.665 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.665 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.665 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.666 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.666 I print_info: LF token         = 128 'Ä'
0.00.049.666 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.666 I print_info: max token length = 1024
0.00.051.726 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.726 I load_tensors: offloading output layer to GPU
0.00.051.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.737 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.739 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.029 I llama_init_from_model: n_seq_max     = 1
0.00.052.030 I llama_init_from_model: n_ctx         = 128
0.00.052.030 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.030 I llama_init_from_model: n_batch       = 128
0.00.052.030 I llama_init_from_model: n_ubatch      = 128
0.00.052.031 I llama_init_from_model: flash_attn    = 0
0.00.052.031 I llama_init_from_model: freq_base     = 10000.0
0.00.052.031 I llama_init_from_model: freq_scale    = 1
0.00.052.032 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.032 I ggml_metal_init: allocating
0.00.052.035 I ggml_metal_init: found device: Apple M4
0.00.052.037 I ggml_metal_init: picking default device: Apple M4
0.00.052.615 I ggml_metal_init: using embedded metal library
0.00.054.969 I ggml_metal_init: GPU name:   Apple M4
0.00.054.971 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.971 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.972 I ggml_metal_init: simdgroup reduction   = true
0.00.054.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.972 I ggml_metal_init: has bfloat            = true
0.00.054.972 I ggml_metal_init: use bfloat            = true
0.00.054.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.172 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.175 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.189 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.110 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.111 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.111 I llama_init_from_model: graph nodes  = 967
0.00.067.111 I llama_init_from_model: graph splits = 2
0.00.067.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.663 I 
0.00.675.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.701 I perplexity: tokenizing the input ..
0.00.682.989 I perplexity: tokenization took 7.287 ms
0.00.682.996 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.304 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.819.545 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.819.575 I llama_perf_context_print:        load time =     667.09 ms
0.00.819.576 I llama_perf_context_print: prompt eval time =     135.08 ms /   128 tokens (    1.06 ms per token,   947.59 tokens per second)
0.00.819.577 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.577 I llama_perf_context_print:       total time =     143.91 ms /   129 tokens
0.00.820.005 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.078s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.935 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.833 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.838 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.843 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.846 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.846 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.848 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.506 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.183 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.184 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.186 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.186 I llama_model_loader: - type  f32:  194 tensors
0.00.025.187 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.187 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.187 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.188 I print_info: file format = GGUF V3 (latest)
0.00.025.188 I print_info: file type   = Q2_K - Medium
0.00.025.189 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.127 I load: special tokens cache size = 25
0.00.049.977 I load: token to piece cache size = 0.2984 MB
0.00.049.980 I print_info: arch             = gptneox
0.00.049.981 I print_info: vocab_only       = 0
0.00.049.981 I print_info: n_ctx_train      = 2048
0.00.049.981 I print_info: n_embd           = 2048
0.00.049.981 I print_info: n_layer          = 24
0.00.049.984 I print_info: n_head           = 16
0.00.049.985 I print_info: n_head_kv        = 16
0.00.049.985 I print_info: n_rot            = 32
0.00.049.987 I print_info: n_swa            = 0
0.00.049.987 I print_info: n_embd_head_k    = 128
0.00.049.987 I print_info: n_embd_head_v    = 128
0.00.049.988 I print_info: n_gqa            = 1
0.00.049.989 I print_info: n_embd_k_gqa     = 2048
0.00.049.989 I print_info: n_embd_v_gqa     = 2048
0.00.049.990 I print_info: f_norm_eps       = 1.0e-05
0.00.049.990 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.990 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.991 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.991 I print_info: f_logit_scale    = 0.0e+00
0.00.049.991 I print_info: n_ff             = 8192
0.00.049.992 I print_info: n_expert         = 0
0.00.049.992 I print_info: n_expert_used    = 0
0.00.049.992 I print_info: causal attn      = 1
0.00.049.992 I print_info: pooling type     = 0
0.00.049.992 I print_info: rope type        = 2
0.00.049.992 I print_info: rope scaling     = linear
0.00.049.997 I print_info: freq_base_train  = 10000.0
0.00.049.997 I print_info: freq_scale_train = 1
0.00.049.997 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.998 I print_info: rope_finetuned   = unknown
0.00.049.998 I print_info: ssm_d_conv       = 0
0.00.049.999 I print_info: ssm_d_inner      = 0
0.00.049.999 I print_info: ssm_d_state      = 0
0.00.050.000 I print_info: ssm_dt_rank      = 0
0.00.050.000 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.000 I print_info: model type       = 1.4B
0.00.050.000 I print_info: model params     = 1.41 B
0.00.050.000 I print_info: general.name     = 1.4B
0.00.050.001 I print_info: vocab type       = BPE
0.00.050.001 I print_info: n_vocab          = 50304
0.00.050.001 I print_info: n_merges         = 50009
0.00.050.002 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.002 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.002 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.002 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.002 I print_info: LF token         = 128 'Ä'
0.00.050.003 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.004 I print_info: max token length = 1024
0.00.051.920 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.920 I load_tensors: offloading output layer to GPU
0.00.051.920 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.931 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.932 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.218 I llama_init_from_model: n_seq_max     = 1
0.00.052.219 I llama_init_from_model: n_ctx         = 128
0.00.052.219 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.219 I llama_init_from_model: n_batch       = 128
0.00.052.219 I llama_init_from_model: n_ubatch      = 128
0.00.052.219 I llama_init_from_model: flash_attn    = 0
0.00.052.220 I llama_init_from_model: freq_base     = 10000.0
0.00.052.220 I llama_init_from_model: freq_scale    = 1
0.00.052.220 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.221 I ggml_metal_init: allocating
0.00.052.224 I ggml_metal_init: found device: Apple M4
0.00.052.226 I ggml_metal_init: picking default device: Apple M4
0.00.052.792 I ggml_metal_init: using embedded metal library
0.00.055.188 I ggml_metal_init: GPU name:   Apple M4
0.00.055.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.190 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.190 I ggml_metal_init: simdgroup reduction   = true
0.00.055.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.190 I ggml_metal_init: has bfloat            = true
0.00.055.190 I ggml_metal_init: use bfloat            = true
0.00.055.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.191 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.725 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.072 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.075 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.089 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.003 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.004 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.004 I llama_init_from_model: graph nodes  = 967
0.00.067.004 I llama_init_from_model: graph splits = 2
0.00.067.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.559 I 
0.00.439.604 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.611 I perplexity: tokenizing the input ..
0.00.447.635 I perplexity: tokenization took 8.021 ms
0.00.447.638 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.580.078 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.581.244 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.581.269 I llama_perf_context_print:        load time =     429.61 ms
0.00.581.270 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.15 tokens per second)
0.00.581.271 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.581.271 I llama_perf_context_print:       total time =     141.72 ms /   129 tokens
0.00.581.788 I ggml_metal_free: deallocating

real	0m0.596s
user	0m0.077s
sys	0m0.066s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.757 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.065 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.071 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.073 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.073 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.073 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.074 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.075 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.076 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.076 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.076 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.077 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.077 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.077 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.539 I llama_model_loader: - type  f32:  194 tensors
0.00.024.539 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.540 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.540 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.540 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.541 I print_info: file format = GGUF V3 (latest)
0.00.024.541 I print_info: file type   = Q3_K - Medium
0.00.024.542 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.436 I load: special tokens cache size = 25
0.00.049.477 I load: token to piece cache size = 0.2984 MB
0.00.049.480 I print_info: arch             = gptneox
0.00.049.480 I print_info: vocab_only       = 0
0.00.049.481 I print_info: n_ctx_train      = 2048
0.00.049.481 I print_info: n_embd           = 2048
0.00.049.481 I print_info: n_layer          = 24
0.00.049.484 I print_info: n_head           = 16
0.00.049.485 I print_info: n_head_kv        = 16
0.00.049.485 I print_info: n_rot            = 32
0.00.049.485 I print_info: n_swa            = 0
0.00.049.485 I print_info: n_embd_head_k    = 128
0.00.049.485 I print_info: n_embd_head_v    = 128
0.00.049.486 I print_info: n_gqa            = 1
0.00.049.487 I print_info: n_embd_k_gqa     = 2048
0.00.049.488 I print_info: n_embd_v_gqa     = 2048
0.00.049.488 I print_info: f_norm_eps       = 1.0e-05
0.00.049.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.489 I print_info: f_logit_scale    = 0.0e+00
0.00.049.490 I print_info: n_ff             = 8192
0.00.049.490 I print_info: n_expert         = 0
0.00.049.490 I print_info: n_expert_used    = 0
0.00.049.490 I print_info: causal attn      = 1
0.00.049.490 I print_info: pooling type     = 0
0.00.049.493 I print_info: rope type        = 2
0.00.049.493 I print_info: rope scaling     = linear
0.00.049.494 I print_info: freq_base_train  = 10000.0
0.00.049.494 I print_info: freq_scale_train = 1
0.00.049.494 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.494 I print_info: rope_finetuned   = unknown
0.00.049.495 I print_info: ssm_d_conv       = 0
0.00.049.495 I print_info: ssm_d_inner      = 0
0.00.049.495 I print_info: ssm_d_state      = 0
0.00.049.495 I print_info: ssm_dt_rank      = 0
0.00.049.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.495 I print_info: model type       = 1.4B
0.00.049.496 I print_info: model params     = 1.41 B
0.00.049.496 I print_info: general.name     = 1.4B
0.00.049.496 I print_info: vocab type       = BPE
0.00.049.497 I print_info: n_vocab          = 50304
0.00.049.497 I print_info: n_merges         = 50009
0.00.049.497 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.498 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.498 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.502 I print_info: LF token         = 128 'Ä'
0.00.049.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.504 I print_info: max token length = 1024
0.00.051.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.408 I load_tensors: offloading output layer to GPU
0.00.051.408 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.419 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.420 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.710 I llama_init_from_model: n_seq_max     = 1
0.00.051.711 I llama_init_from_model: n_ctx         = 128
0.00.051.711 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.711 I llama_init_from_model: n_batch       = 128
0.00.051.711 I llama_init_from_model: n_ubatch      = 128
0.00.051.712 I llama_init_from_model: flash_attn    = 0
0.00.051.712 I llama_init_from_model: freq_base     = 10000.0
0.00.051.712 I llama_init_from_model: freq_scale    = 1
0.00.051.713 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.713 I ggml_metal_init: allocating
0.00.051.716 I ggml_metal_init: found device: Apple M4
0.00.051.718 I ggml_metal_init: picking default device: Apple M4
0.00.052.300 I ggml_metal_init: using embedded metal library
0.00.054.677 I ggml_metal_init: GPU name:   Apple M4
0.00.054.678 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.679 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.679 I ggml_metal_init: simdgroup reduction   = true
0.00.054.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.680 I ggml_metal_init: has bfloat            = true
0.00.054.680 I ggml_metal_init: use bfloat            = true
0.00.054.680 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.304 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.660 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.666 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.681 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.632 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.633 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.633 I llama_init_from_model: graph nodes  = 967
0.00.066.634 I llama_init_from_model: graph splits = 2
0.00.066.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.430 I 
0.00.500.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.466 I perplexity: tokenizing the input ..
0.00.507.845 I perplexity: tokenization took 7.377 ms
0.00.507.849 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.640.379 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.641.638 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.641.670 I llama_perf_context_print:        load time =     491.67 ms
0.00.641.671 I llama_perf_context_print: prompt eval time =     132.30 ms /   128 tokens (    1.03 ms per token,   967.51 tokens per second)
0.00.641.672 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.641.672 I llama_perf_context_print:       total time =     141.24 ms /   129 tokens
0.00.642.264 I ggml_metal_free: deallocating

real	0m0.656s
user	0m0.077s
sys	0m0.079s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.884 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.937 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.944 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.945 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.945 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.947 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.948 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.948 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.949 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.950 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.950 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.744 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.450 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.451 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.452 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.453 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.453 I llama_model_loader: - type  f32:  194 tensors
0.00.025.453 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.454 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.454 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.454 I print_info: file format = GGUF V3 (latest)
0.00.025.455 I print_info: file type   = Q4_K - Medium
0.00.025.457 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.419 I load: special tokens cache size = 25
0.00.050.171 I load: token to piece cache size = 0.2984 MB
0.00.050.174 I print_info: arch             = gptneox
0.00.050.174 I print_info: vocab_only       = 0
0.00.050.175 I print_info: n_ctx_train      = 2048
0.00.050.175 I print_info: n_embd           = 2048
0.00.050.175 I print_info: n_layer          = 24
0.00.050.178 I print_info: n_head           = 16
0.00.050.179 I print_info: n_head_kv        = 16
0.00.050.179 I print_info: n_rot            = 32
0.00.050.179 I print_info: n_swa            = 0
0.00.050.179 I print_info: n_embd_head_k    = 128
0.00.050.180 I print_info: n_embd_head_v    = 128
0.00.050.180 I print_info: n_gqa            = 1
0.00.050.181 I print_info: n_embd_k_gqa     = 2048
0.00.050.182 I print_info: n_embd_v_gqa     = 2048
0.00.050.182 I print_info: f_norm_eps       = 1.0e-05
0.00.050.182 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.185 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.185 I print_info: f_logit_scale    = 0.0e+00
0.00.050.186 I print_info: n_ff             = 8192
0.00.050.186 I print_info: n_expert         = 0
0.00.050.186 I print_info: n_expert_used    = 0
0.00.050.186 I print_info: causal attn      = 1
0.00.050.186 I print_info: pooling type     = 0
0.00.050.188 I print_info: rope type        = 2
0.00.050.188 I print_info: rope scaling     = linear
0.00.050.188 I print_info: freq_base_train  = 10000.0
0.00.050.189 I print_info: freq_scale_train = 1
0.00.050.189 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.189 I print_info: rope_finetuned   = unknown
0.00.050.189 I print_info: ssm_d_conv       = 0
0.00.050.189 I print_info: ssm_d_inner      = 0
0.00.050.190 I print_info: ssm_d_state      = 0
0.00.050.190 I print_info: ssm_dt_rank      = 0
0.00.050.190 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.190 I print_info: model type       = 1.4B
0.00.050.190 I print_info: model params     = 1.41 B
0.00.050.191 I print_info: general.name     = 1.4B
0.00.050.195 I print_info: vocab type       = BPE
0.00.050.196 I print_info: n_vocab          = 50304
0.00.050.196 I print_info: n_merges         = 50009
0.00.050.196 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.196 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.196 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.197 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.197 I print_info: LF token         = 128 'Ä'
0.00.050.197 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.197 I print_info: max token length = 1024
0.00.052.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.170 I load_tensors: offloading output layer to GPU
0.00.052.170 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.181 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.183 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.469 I llama_init_from_model: n_seq_max     = 1
0.00.052.470 I llama_init_from_model: n_ctx         = 128
0.00.052.470 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.470 I llama_init_from_model: n_batch       = 128
0.00.052.470 I llama_init_from_model: n_ubatch      = 128
0.00.052.471 I llama_init_from_model: flash_attn    = 0
0.00.052.471 I llama_init_from_model: freq_base     = 10000.0
0.00.052.471 I llama_init_from_model: freq_scale    = 1
0.00.052.472 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.472 I ggml_metal_init: allocating
0.00.052.475 I ggml_metal_init: found device: Apple M4
0.00.052.477 I ggml_metal_init: picking default device: Apple M4
0.00.053.073 I ggml_metal_init: using embedded metal library
0.00.055.425 I ggml_metal_init: GPU name:   Apple M4
0.00.055.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.427 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.427 I ggml_metal_init: simdgroup reduction   = true
0.00.055.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.428 I ggml_metal_init: has bfloat            = true
0.00.055.428 I ggml_metal_init: use bfloat            = true
0.00.055.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.080 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.448 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.452 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.469 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.330 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.331 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.331 I llama_init_from_model: graph nodes  = 967
0.00.067.331 I llama_init_from_model: graph splits = 2
0.00.067.332 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.516 I 
0.00.549.558 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.565 I perplexity: tokenizing the input ..
0.00.557.926 I perplexity: tokenization took 8.359 ms
0.00.557.929 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.692.623 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.693.782 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.693.811 I llama_perf_context_print:        load time =     539.62 ms
0.00.693.812 I llama_perf_context_print: prompt eval time =     134.47 ms /   128 tokens (    1.05 ms per token,   951.91 tokens per second)
0.00.693.813 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.693.814 I llama_perf_context_print:       total time =     144.30 ms /   129 tokens
0.00.694.337 I ggml_metal_free: deallocating

real	0m0.709s
user	0m0.078s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.879 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.001 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.008 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.010 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.010 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.011 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.011 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.012 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.012 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.014 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.542 I llama_model_loader: - type  f32:  194 tensors
0.00.024.542 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.542 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.543 I print_info: file format = GGUF V3 (latest)
0.00.024.543 I print_info: file type   = Q5_K - Medium
0.00.024.544 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.042 I load: special tokens cache size = 25
0.00.050.104 I load: token to piece cache size = 0.2984 MB
0.00.050.107 I print_info: arch             = gptneox
0.00.050.107 I print_info: vocab_only       = 0
0.00.050.108 I print_info: n_ctx_train      = 2048
0.00.050.108 I print_info: n_embd           = 2048
0.00.050.108 I print_info: n_layer          = 24
0.00.050.111 I print_info: n_head           = 16
0.00.050.111 I print_info: n_head_kv        = 16
0.00.050.112 I print_info: n_rot            = 32
0.00.050.112 I print_info: n_swa            = 0
0.00.050.112 I print_info: n_embd_head_k    = 128
0.00.050.112 I print_info: n_embd_head_v    = 128
0.00.050.113 I print_info: n_gqa            = 1
0.00.050.114 I print_info: n_embd_k_gqa     = 2048
0.00.050.114 I print_info: n_embd_v_gqa     = 2048
0.00.050.115 I print_info: f_norm_eps       = 1.0e-05
0.00.050.115 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.117 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.117 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.117 I print_info: f_logit_scale    = 0.0e+00
0.00.050.118 I print_info: n_ff             = 8192
0.00.050.118 I print_info: n_expert         = 0
0.00.050.118 I print_info: n_expert_used    = 0
0.00.050.119 I print_info: causal attn      = 1
0.00.050.119 I print_info: pooling type     = 0
0.00.050.119 I print_info: rope type        = 2
0.00.050.119 I print_info: rope scaling     = linear
0.00.050.121 I print_info: freq_base_train  = 10000.0
0.00.050.121 I print_info: freq_scale_train = 1
0.00.050.122 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.122 I print_info: rope_finetuned   = unknown
0.00.050.122 I print_info: ssm_d_conv       = 0
0.00.050.122 I print_info: ssm_d_inner      = 0
0.00.050.122 I print_info: ssm_d_state      = 0
0.00.050.122 I print_info: ssm_dt_rank      = 0
0.00.050.123 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.123 I print_info: model type       = 1.4B
0.00.050.123 I print_info: model params     = 1.41 B
0.00.050.123 I print_info: general.name     = 1.4B
0.00.050.124 I print_info: vocab type       = BPE
0.00.050.124 I print_info: n_vocab          = 50304
0.00.050.124 I print_info: n_merges         = 50009
0.00.050.124 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.125 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.125 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.125 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.125 I print_info: LF token         = 128 'Ä'
0.00.050.126 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.126 I print_info: max token length = 1024
0.00.052.161 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.161 I load_tensors: offloading output layer to GPU
0.00.052.162 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.172 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.173 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.461 I llama_init_from_model: n_seq_max     = 1
0.00.052.462 I llama_init_from_model: n_ctx         = 128
0.00.052.462 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.462 I llama_init_from_model: n_batch       = 128
0.00.052.463 I llama_init_from_model: n_ubatch      = 128
0.00.052.463 I llama_init_from_model: flash_attn    = 0
0.00.052.463 I llama_init_from_model: freq_base     = 10000.0
0.00.052.463 I llama_init_from_model: freq_scale    = 1
0.00.052.464 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.464 I ggml_metal_init: allocating
0.00.052.467 I ggml_metal_init: found device: Apple M4
0.00.052.469 I ggml_metal_init: picking default device: Apple M4
0.00.053.047 I ggml_metal_init: using embedded metal library
0.00.055.399 I ggml_metal_init: GPU name:   Apple M4
0.00.055.400 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.401 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.401 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.401 I ggml_metal_init: simdgroup reduction   = true
0.00.055.402 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.402 I ggml_metal_init: has bfloat            = true
0.00.055.402 I ggml_metal_init: use bfloat            = true
0.00.055.402 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.403 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.224 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.491 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.494 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.510 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.352 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.353 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.353 I llama_init_from_model: graph nodes  = 967
0.00.067.353 I llama_init_from_model: graph splits = 2
0.00.067.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.393 I 
0.00.612.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.434 I perplexity: tokenizing the input ..
0.00.620.513 I perplexity: tokenization took 8.078 ms
0.00.620.517 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.611 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.762.937 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.762.967 I llama_perf_context_print:        load time =     603.51 ms
0.00.762.969 I llama_perf_context_print: prompt eval time =     140.83 ms /   128 tokens (    1.10 ms per token,   908.93 tokens per second)
0.00.762.969 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.972 I llama_perf_context_print:       total time =     150.58 ms /   129 tokens
0.00.763.480 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.078s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.945 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.816 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.827 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.828 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.833 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.835 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.570 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.254 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.255 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.256 I llama_model_loader: - type  f32:  194 tensors
0.00.026.256 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.256 I print_info: file format = GGUF V3 (latest)
0.00.026.257 I print_info: file type   = Q6_K
0.00.026.257 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.048 I load: special tokens cache size = 25
0.00.050.809 I load: token to piece cache size = 0.2984 MB
0.00.050.812 I print_info: arch             = gptneox
0.00.050.812 I print_info: vocab_only       = 0
0.00.050.813 I print_info: n_ctx_train      = 2048
0.00.050.813 I print_info: n_embd           = 2048
0.00.050.813 I print_info: n_layer          = 24
0.00.050.815 I print_info: n_head           = 16
0.00.050.816 I print_info: n_head_kv        = 16
0.00.050.816 I print_info: n_rot            = 32
0.00.050.816 I print_info: n_swa            = 0
0.00.050.817 I print_info: n_embd_head_k    = 128
0.00.050.817 I print_info: n_embd_head_v    = 128
0.00.050.819 I print_info: n_gqa            = 1
0.00.050.820 I print_info: n_embd_k_gqa     = 2048
0.00.050.821 I print_info: n_embd_v_gqa     = 2048
0.00.050.825 I print_info: f_norm_eps       = 1.0e-05
0.00.050.826 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.826 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.826 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.827 I print_info: f_logit_scale    = 0.0e+00
0.00.050.828 I print_info: n_ff             = 8192
0.00.050.828 I print_info: n_expert         = 0
0.00.050.829 I print_info: n_expert_used    = 0
0.00.050.829 I print_info: causal attn      = 1
0.00.050.829 I print_info: pooling type     = 0
0.00.050.829 I print_info: rope type        = 2
0.00.050.829 I print_info: rope scaling     = linear
0.00.050.830 I print_info: freq_base_train  = 10000.0
0.00.050.830 I print_info: freq_scale_train = 1
0.00.050.830 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.830 I print_info: rope_finetuned   = unknown
0.00.050.831 I print_info: ssm_d_conv       = 0
0.00.050.831 I print_info: ssm_d_inner      = 0
0.00.050.831 I print_info: ssm_d_state      = 0
0.00.050.832 I print_info: ssm_dt_rank      = 0
0.00.050.835 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.836 I print_info: model type       = 1.4B
0.00.050.836 I print_info: model params     = 1.41 B
0.00.050.836 I print_info: general.name     = 1.4B
0.00.050.838 I print_info: vocab type       = BPE
0.00.050.838 I print_info: n_vocab          = 50304
0.00.050.838 I print_info: n_merges         = 50009
0.00.050.838 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.838 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.838 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.839 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.839 I print_info: LF token         = 128 'Ä'
0.00.050.839 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.839 I print_info: max token length = 1024
0.00.052.542 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.542 I load_tensors: offloading output layer to GPU
0.00.052.542 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.548 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.548 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.861 I llama_init_from_model: n_seq_max     = 1
0.00.052.861 I llama_init_from_model: n_ctx         = 128
0.00.052.862 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.862 I llama_init_from_model: n_batch       = 128
0.00.052.862 I llama_init_from_model: n_ubatch      = 128
0.00.052.862 I llama_init_from_model: flash_attn    = 0
0.00.052.862 I llama_init_from_model: freq_base     = 10000.0
0.00.052.863 I llama_init_from_model: freq_scale    = 1
0.00.052.863 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.863 I ggml_metal_init: allocating
0.00.052.866 I ggml_metal_init: found device: Apple M4
0.00.052.868 I ggml_metal_init: picking default device: Apple M4
0.00.053.442 I ggml_metal_init: using embedded metal library
0.00.055.713 I ggml_metal_init: GPU name:   Apple M4
0.00.055.715 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.715 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.715 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.715 I ggml_metal_init: simdgroup reduction   = true
0.00.055.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.716 I ggml_metal_init: has bfloat            = true
0.00.055.716 I ggml_metal_init: use bfloat            = true
0.00.055.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.717 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.198 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.493 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.497 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.513 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.347 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.348 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.349 I llama_init_from_model: graph nodes  = 967
0.00.067.349 I llama_init_from_model: graph splits = 2
0.00.067.350 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.350 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.242.733 I 
0.00.242.762 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.242.766 I perplexity: tokenizing the input ..
0.00.250.055 I perplexity: tokenization took 7.288 ms
0.00.250.059 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.390.064 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.391.276 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.391.295 I llama_perf_context_print:        load time =     231.78 ms
0.00.391.297 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.69 tokens per second)
0.00.391.297 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.391.298 I llama_perf_context_print:       total time =     148.57 ms /   129 tokens
0.00.391.641 I ggml_metal_free: deallocating

real	0m0.406s
user	0m0.076s
sys	0m0.044s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.248 I build: 4483 (bbf3e55e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.936 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.537 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.552 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.552 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.555 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.082 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.906 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.856 I llama_model_loader: - type  f32:  194 tensors
0.00.056.856 I llama_model_loader: - type  f16:   98 tensors
0.00.056.857 I print_info: file format = GGUF V3 (latest)
0.00.056.858 I print_info: file type   = all F32 (guessed)
0.00.056.859 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.284 I load: special tokens cache size = 25
0.00.087.444 I load: token to piece cache size = 0.2984 MB
0.00.087.447 I print_info: arch             = gptneox
0.00.087.447 I print_info: vocab_only       = 0
0.00.087.447 I print_info: n_ctx_train      = 2048
0.00.087.447 I print_info: n_embd           = 2048
0.00.087.447 I print_info: n_layer          = 24
0.00.087.450 I print_info: n_head           = 16
0.00.087.451 I print_info: n_head_kv        = 16
0.00.087.451 I print_info: n_rot            = 32
0.00.087.453 I print_info: n_swa            = 0
0.00.087.453 I print_info: n_embd_head_k    = 128
0.00.087.453 I print_info: n_embd_head_v    = 128
0.00.087.454 I print_info: n_gqa            = 1
0.00.087.455 I print_info: n_embd_k_gqa     = 2048
0.00.087.462 I print_info: n_embd_v_gqa     = 2048
0.00.087.464 I print_info: f_norm_eps       = 1.0e-05
0.00.087.464 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.465 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.465 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.465 I print_info: f_logit_scale    = 0.0e+00
0.00.087.472 I print_info: n_ff             = 8192
0.00.087.472 I print_info: n_expert         = 0
0.00.087.472 I print_info: n_expert_used    = 0
0.00.087.472 I print_info: causal attn      = 1
0.00.087.472 I print_info: pooling type     = 0
0.00.087.473 I print_info: rope type        = 2
0.00.087.474 I print_info: rope scaling     = linear
0.00.087.474 I print_info: freq_base_train  = 10000.0
0.00.087.474 I print_info: freq_scale_train = 1
0.00.087.474 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.475 I print_info: rope_finetuned   = unknown
0.00.087.475 I print_info: ssm_d_conv       = 0
0.00.087.475 I print_info: ssm_d_inner      = 0
0.00.087.475 I print_info: ssm_d_state      = 0
0.00.087.475 I print_info: ssm_dt_rank      = 0
0.00.087.475 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.475 I print_info: model type       = 1.4B
0.00.087.476 I print_info: model params     = 1.41 B
0.00.087.476 I print_info: general.name     = 1.4B
0.00.087.477 I print_info: vocab type       = BPE
0.00.087.477 I print_info: n_vocab          = 50304
0.00.087.477 I print_info: n_merges         = 50009
0.00.087.478 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.478 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.478 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.478 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.479 I print_info: LF token         = 128 'Ä'
0.00.087.480 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.480 I print_info: max token length = 1024
0.00.090.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.061 I load_tensors: offloading output layer to GPU
0.00.090.061 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.072 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.073 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.347 I llama_init_from_model: n_seq_max     = 1
0.00.090.348 I llama_init_from_model: n_ctx         = 128
0.00.090.348 I llama_init_from_model: n_ctx_per_seq = 128
0.00.090.348 I llama_init_from_model: n_batch       = 128
0.00.090.349 I llama_init_from_model: n_ubatch      = 128
0.00.090.349 I llama_init_from_model: flash_attn    = 0
0.00.090.349 I llama_init_from_model: freq_base     = 10000.0
0.00.090.349 I llama_init_from_model: freq_scale    = 1
0.00.090.350 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.350 I ggml_metal_init: allocating
0.00.090.353 I ggml_metal_init: found device: Apple M4
0.00.090.355 I ggml_metal_init: picking default device: Apple M4
0.00.090.930 I ggml_metal_init: using embedded metal library
0.00.093.392 I ggml_metal_init: GPU name:   Apple M4
0.00.093.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.394 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.395 I ggml_metal_init: simdgroup reduction   = true
0.00.093.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.395 I ggml_metal_init: has bfloat            = true
0.00.093.395 I ggml_metal_init: use bfloat            = true
0.00.093.396 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.085 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.316 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.322 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.337 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.213 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.104.214 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.104.214 I llama_init_from_model: graph nodes  = 967
0.00.104.215 I llama_init_from_model: graph splits = 2
0.00.104.216 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.216 I 
0.00.104.241 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.242 I compute_imatrix: tokenizing the input ..
0.00.111.145 I compute_imatrix: tokenization took 6.903 ms
0.00.111.147 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.494.246 I compute_imatrix: 1.38 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.496.685 I llama_perf_context_print:        load time =    1471.31 ms
0.01.496.686 I llama_perf_context_print: prompt eval time =    1382.46 ms /   128 tokens (   10.80 ms per token,    92.59 tokens per second)
0.01.496.687 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.496.687 I llama_perf_context_print:       total time =    1473.74 ms /   129 tokens
0.01.497.231 I ggml_metal_free: deallocating

real	0m1.683s
user	0m0.165s
sys	0m0.237s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4483 (bbf3e55e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137f0a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137f0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137f0b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137f0bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137f0c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137f0c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137f0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137f0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137f0d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137f0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137f0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137f0ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137f0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137f0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137f10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137f10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137f11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137f11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137f12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137f12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137f12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137f135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137f13e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137f14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137f14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137f14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137f15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137f15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137f162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137f16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137f16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137f17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137f177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137f17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137f17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137f183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137f18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137f18d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137f191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137f19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137f19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137f19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137f1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137f1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137f1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137f1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137f1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137f1c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137f1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137f1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137f1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137f1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137f1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137f1ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137f1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137f1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137f1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137f20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137f20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137f209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137f20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137f21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137f217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137f21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137f22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137f225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137f22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137f22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137f23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137f23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137f23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137f24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137f24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137f24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137f25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137f25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137f25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137f261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137f26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137f26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137f271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137f27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137f27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137f28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137f29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137f29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137f2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137f2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137f2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137f2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137f2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137f1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137f2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137f2c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137f2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137f2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137f2d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137f2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137f2e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137f2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137f2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137f2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137f2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137f302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137f30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137f30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137f31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137f316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137f31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137f32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137f32930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137f32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137f33270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137f33710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137f33bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137f34050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137f34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137f34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137f352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137f35770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137f35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137f360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137f36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137f369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137f37330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137f37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137f38110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137f385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137f38a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137f38ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f39390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f39830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137f39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137f3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137f3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137f3aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137f3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137f3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137f3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137f3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137f3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137f3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137f3cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137f3d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137f3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137f3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137f3e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137f3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137f3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137f3f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137f3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137f3f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137f3fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137f40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137f40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137f41070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137f41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137f419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137f41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137f422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137f42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137f42c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137f43570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f43a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137f43eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f44350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137f447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137f44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137f45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137f455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137f45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137f45f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137f463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137f46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137f46cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137f47190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137f47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137f47ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137f47f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137f484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137f48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137f48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137f494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137f49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137f49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137f4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137f4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137f4b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137f4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137f4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137f4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137f4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137f4cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137f4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137f4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137f4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137f4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137f4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137f4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137f4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137f4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137f4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137f50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137f507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137f50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137f51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137f517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137f51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137f52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137f527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137f52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137f53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137f53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137f53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137f54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137f54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137f54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137f55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137f55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137f55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137f56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137f56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137f56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137f57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137f57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137f57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137f581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137f58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137f58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137f591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137f59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137f59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137f5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137f5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137f5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137f5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137f5b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137f5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137f5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137f5c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137f5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137f5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137f5d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137f5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137f5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137f5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137f5ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137f5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137f5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137f5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137f60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137f606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137f60c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137f610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137f61550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137f619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137f61e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137f62330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137f627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137f62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137f63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137f635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137f63a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137f63ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137f64390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137f64830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137f64cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137f65170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137f656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137f65de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137f66500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137f66c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137f67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137f67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137f67df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137f680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137f686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.144.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133f5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133f5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133f5b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133f4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133f4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133f48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133f45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133f55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133f52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133f4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133f46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133f43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133f48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133f49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133f4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133f4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133f53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133f47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133f51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133f4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133f4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133f475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133f55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133f447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133f430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133f45340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133f55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133f4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133f53380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133f49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133f4bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133f4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133f47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133f4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133f516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133f45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133f544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133f51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133f4d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133f56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133f44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133f56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133f44200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133f54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133f50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133f53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133f52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133f4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133f41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133f04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133f5da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133f0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133f5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133f5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133f5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133f5f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133f5fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133f5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133f5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133f60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133f60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133f607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133f60a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133f60d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133f61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133f612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133f61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133f61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133f61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133f61dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133f62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133f62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133f62610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133f628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133f62b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133f62e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133f63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133f633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133f63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133f63950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133f63c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133f63ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133f64190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133f64450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133f64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133f649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133f64c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133f64f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133f65210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133f654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133f65790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133f65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133f65d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133f65fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133f66290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133f66550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133f66810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133f66ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133f66d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133f67050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133f67310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133f675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133f67890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133f67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133f67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133f680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133f68390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133f68650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133f68910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133f68bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133f68e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133f69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133f69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133f696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133f69990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133f69c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133f69f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133f6a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133f6a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133f6a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133f6aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133f6acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133f6af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133f6b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133f6b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133f6b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133f6ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133f6bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133f6c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133f6c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133f6c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133f6c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133f6cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133f6cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133f6d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133f6d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133f6d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133f6d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133f6db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133f6de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133f6e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133f6e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133f6e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133f6e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133f6ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133f6eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133f6f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133f6f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133f6f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133f6f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133f6fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133f6ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133f70210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133f704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133f70790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133f70a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133f70d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133f70fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133f71290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133f71550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133f71810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133f71ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133f71d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133f72050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133f72310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133f725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133f72890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133f72b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133f72e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133f730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133f73390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133f73650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133f73910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133f73bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133f73e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133f74150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133f74410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133f746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133f74990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133f74c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133f74f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133f751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133f75490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133f75750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133f75a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133f75cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133f75f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133f76250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133f76510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133f767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133f76a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133f76d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133f77010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133f772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133f77590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133f77850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133f77b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133f77dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133f78090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133f78350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133f78610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133f788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133f78b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133f78e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133f79110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133f793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133f79690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133f79950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133f79c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133f79ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133f7a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133f7a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133f7aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133f7ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133f7afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133f7b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133f7b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133f7b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133f7baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133f7bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133f7c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133f7ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133f7cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133f7d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133f7da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133f7dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133f7e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133f7ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133f7efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133f7f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133f7fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133f7ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133f80500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133f80a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133f80fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133f814f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133f81a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133f81f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133f824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133f82a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133f82f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133f834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133f83a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133f83f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133f844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133f84a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133f84f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133f854b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133f85a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133f85f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133f864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133f869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133f86f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133f87490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133f879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133f87f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133f88480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133f889d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133f88f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133f89470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133f899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133f89f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133f8a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133f8a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133f8af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133f8b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133f8b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133f8bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133f8bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133f8c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133f8c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133f8cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133f8cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133f8d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133f8d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133f8dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133f8e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133f8e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133f8e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133f8ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133f8f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133f8f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133f8fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133f90000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133f90cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133f91410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133f91b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133f91df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133f92260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133f92860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133f92e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.772s
user	0m0.298s
sys	0m0.309s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4483 (bbf3e55e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14160ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14160d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14160d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14160dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14160e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14160e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14160edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14160f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14160f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14160fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141610350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141610850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141611370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141611b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141612330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141612a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141613170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141613fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141614780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141614ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1416155c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141615ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141616580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141616ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141616f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141617570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1416181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141618720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1416189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141618e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141619140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1416199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141619f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14161a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14161a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14161ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14161afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14161b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14161b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14161bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14161c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14161c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14161cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14161ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14161d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14161da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14161e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14161e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14161ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14161f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14161fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1416201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1416207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141620fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141621460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141621900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1416221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1416229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141623120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1416235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141623a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141623f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1416243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141624ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141625620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141625ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141625f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141626400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141626950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141626ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1416273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141627940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141627e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1416283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141628930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141628e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1416293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141629920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141629e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14162a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14162a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14162ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14162b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14162b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14162be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14162c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14162c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14162ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14162d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14162d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14162de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14162e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14161e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14162e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14162efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14162f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14162fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14162ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1416304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141630a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141630f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1416314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141631a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141631f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1416324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141632a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141632f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1416334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141633950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141633df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141634290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141634730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141634bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141635070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141635510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1416359b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141635e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1416362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141636790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141636c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1416370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141637570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141637a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141637eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141638350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1416387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141638c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141639130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1416395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141639a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141639f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14163a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14163a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14163acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14163b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14163b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14163bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14163bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14163c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14163c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14163cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14163d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14163d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14163db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14163dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14163e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14163e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14163edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14163f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14163f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14163fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141640030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1416404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141640970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141640e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1416412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141641750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141641bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141642090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141642530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1416429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141642e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141643310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1416437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141643c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1416440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141644590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141644a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141644ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141645370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141645810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141645cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141646150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1416465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141646a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141646f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1416473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141647870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141647d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1416481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141648650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141648af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141648f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141649430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1416498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141649d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14164a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14164a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14164ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14164b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14164b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14164bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14164beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14164c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14164cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14164d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14164d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14164dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14164e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14164e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14164ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14164f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14164f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14164fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141650220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1416509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141650f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141651470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1416519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141651f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141652460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1416529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141652f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141653450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1416539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141653ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141654440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141654990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141654ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141655430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141655980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141655ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141656420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141656970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141656ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141657410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141657960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141657eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141658400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141658950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141658ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1416593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141659940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141659e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14165a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14165a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14165ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14165b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14165b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14165be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14165c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14165c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14165ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14165d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14165d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14165de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14165e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14165e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14165ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14165f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14165f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14165fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141660380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1416608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141660e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141661370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1416618c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141661e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141662360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1416628b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141662e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141663350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1416637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141663c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141664130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1416645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141664a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141664f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1416653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141665850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141665cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141666190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141666630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141666ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141666f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141667410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1416678b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141667e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141668520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141668c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141669360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141669a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141669d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14166a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14166a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14166ae00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.253 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.256 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1417099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141709e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14170a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14170a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14170abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14170b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14170b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14170bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14170c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14170c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14170cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14170d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14170db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14170e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14170eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14170f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14170f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141710070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141710790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141710f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141711680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1417124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141712be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141713300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1417135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141713bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1417141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1417147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141714fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141715740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141715fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141716510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1417167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141716c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141717110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1417175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141717a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141717ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141718390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141718830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141718cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141719170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141719430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141719a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14171a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14171a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14171ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14171b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14171b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14171bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14171c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14171cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14171d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14171d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14171dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14171deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14171e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14171ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14171f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14171f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14171fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14171ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1417203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141720870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141720d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1417211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141721650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141721af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141721f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141722430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1417228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141722e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1417238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141723e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141724360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1417248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141724e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141725350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1417258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141725df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141726340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141726890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141726de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141727330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141727880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141727dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141728320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141728870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141728dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141729310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141729860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141729db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14172a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14172a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14172ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14172b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14172b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14172bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14172c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14172c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14172cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14172d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14172d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14172dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14172e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14172e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14172ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14172f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14172f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14172fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1417301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141730690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141730b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141730fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141731470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141731910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141731db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141732250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1417326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141732b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141733030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1417334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141733970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141733e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1417342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141734750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141734bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141735090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141735530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1417359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141735e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141736310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1417367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141736c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1417370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141737590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141737a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141737ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141738370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141738810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141738cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141739150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1417395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141739a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141739f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14173a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14173a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14173ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14173b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14173b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14173baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14173bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14173c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14173c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14173cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14173d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14173d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14173db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14173dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14173e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14173e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14173edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14173f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14173f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14173fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141740050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1417404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141740990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141740e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1417412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141741770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141741c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1417420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141742550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1417429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141742e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141743330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1417437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141743c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141744110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1417445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141744a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141744ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141745390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141745830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141745cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141746170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141746610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141746ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141746f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1417474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1417479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141747f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141748490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141748750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141748d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141749370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141749980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14174a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14174a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14174a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14174aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14174b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14174bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14174c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14174c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14174cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14174d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14174d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14174dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14174e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14174e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14174ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14174f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14174f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14174fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141750240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141750790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141750ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141751230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141751780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141751cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141752220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141752770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141752cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141753210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141753760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141753cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141754200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141754750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141754ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1417551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141755740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141755c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1417561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141756730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141756c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1417571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141757720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141757c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1417581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141758710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141758c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1417591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141759700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141759c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14175a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14175a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14175ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14175b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14175b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14175bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14175c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14175c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14175cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14175d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14175d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14175dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14175e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14175e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14175ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14175f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14175f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14175fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141760090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141760530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1417609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141760e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141761310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1417617b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141761c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1417620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141762590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141762a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141762ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141763370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141763810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141763cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141764150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1417646a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141764dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1417654e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141765c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141766320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1417665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141766dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141767090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1417676a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1428044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1428056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1428063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1428092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14280a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14280a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14280af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14280b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14280be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14280c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14280cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14280d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14280dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14280dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14280e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14280e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14280e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14280edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14280f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14280f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14280fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14280fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1428102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1428114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1428133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1428149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1428152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1428177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1428180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1428189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1428196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14281a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14281a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14281ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14281b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14281b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14281ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14281bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14281c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14281c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14281cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14281d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14281d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14281d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14281ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14281e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14281e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14281eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14281efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14281f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14281f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14281fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1428205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1428217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1428224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1428253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1428269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1428272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1428291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14282a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14282a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14282ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14282b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14282b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14282b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14282be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14282c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14282c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14282cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14282d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14282d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14282d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14282dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14282e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14282e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14282eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14282ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14282f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14282f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14282fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1428300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1428309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1428328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1428331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1428347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1428350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1428366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1428378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1428385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1428397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14283a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14283a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14283a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14283ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14283b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14283b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14283bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14283bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14283c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14283c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14283ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14283d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14283d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14283da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14283deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14283e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14283e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14283ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14283f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14283f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14283f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14283fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1428406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1428424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1428436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1428455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1428474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1428493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14284a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14284a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14284aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14284ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14284b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14284b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14284bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14284c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14284c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14284c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14284cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14284d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14284d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14284dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14284df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14284e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14284e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14284eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14284f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14284f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14284f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14284fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1428502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1428521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1428533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1428540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1428549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1428552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1428568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1428576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1428579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.911s
user	0m0.242s
sys	0m0.133s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
