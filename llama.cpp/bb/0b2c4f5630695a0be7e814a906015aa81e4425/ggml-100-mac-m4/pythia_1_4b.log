Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.542s
user	0m0.864s
sys	0m1.240s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target xxhash
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 33%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-sampling
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-grammar-parser
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Built target test-llama-grammar
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-gguf
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-chat-template
[ 63%] Built target test-gguf
[ 63%] Built target test-model-load-cancel
[ 64%] Built target test-backend-ops
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-barrier
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 64%] Built target test-rope
[ 64%] Built target test-quantize-perf
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Built target llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gritlm
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Built target llama-batched
[ 74%] Built target llama-eval-callback
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Built target llama-bench
[ 75%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-cli
[ 83%] Built target llama-parallel
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-passkey
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Generating index.html.gz.hpp
[ 90%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Built target llama-quantize
[ 90%] Built target llama-perplexity
[ 90%] Linking CXX executable ../../bin/llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-vdot
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Built target llama-gen-docs
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.003s
user	0m5.930s
sys	0m9.770s

main: quantize time =  5207.53 ms
main:    total time =  5207.53 ms

main: quantize time =  1917.24 ms
main:    total time =  1917.24 ms

main: quantize time =  1835.31 ms
main:    total time =  1835.31 ms

main: quantize time =  2482.56 ms
main:    total time =  2482.56 ms

main: quantize time =  2162.62 ms
main:    total time =  2162.62 ms

main: quantize time =  5314.42 ms
main:    total time =  5314.42 ms

main: quantize time =  5767.79 ms
main:    total time =  5767.79 ms

main: quantize time =  6811.05 ms
main:    total time =  6811.05 ms

main: quantize time =  5892.95 ms
main:    total time =  5892.95 ms

main: quantize time =  4631.01 ms
main:    total time =  4631.01 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.124 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.252 I main: llama backend init
0.00.000.259 I main: load the model and apply lora adapter, if any
0.00.039.317 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.049.972 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.049.983 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.049.988 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.049.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.049.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.049.990 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.049.990 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.049.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.049.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.049.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.049.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.049.995 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.049.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.050.004 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.050.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.050.011 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.050.011 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.058.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.060.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.068.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.068.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.068.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.068.528 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.068.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.068.530 I llama_model_loader: - type  f32:  194 tensors
0.00.068.530 I llama_model_loader: - type  f16:   98 tensors
0.00.100.242 I llm_load_vocab: special tokens cache size = 25
0.00.106.949 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.106.951 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.106.952 I llm_load_print_meta: arch             = gptneox
0.00.106.952 I llm_load_print_meta: vocab type       = BPE
0.00.106.952 I llm_load_print_meta: n_vocab          = 50304
0.00.106.952 I llm_load_print_meta: n_merges         = 50009
0.00.106.952 I llm_load_print_meta: vocab_only       = 0
0.00.106.953 I llm_load_print_meta: n_ctx_train      = 2048
0.00.106.953 I llm_load_print_meta: n_embd           = 2048
0.00.106.953 I llm_load_print_meta: n_layer          = 24
0.00.106.956 I llm_load_print_meta: n_head           = 16
0.00.106.957 I llm_load_print_meta: n_head_kv        = 16
0.00.106.957 I llm_load_print_meta: n_rot            = 32
0.00.106.957 I llm_load_print_meta: n_swa            = 0
0.00.106.957 I llm_load_print_meta: n_embd_head_k    = 128
0.00.106.958 I llm_load_print_meta: n_embd_head_v    = 128
0.00.106.958 I llm_load_print_meta: n_gqa            = 1
0.00.106.959 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.106.959 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.106.960 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.106.960 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.106.960 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.106.961 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.106.961 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.106.961 I llm_load_print_meta: n_ff             = 8192
0.00.106.973 I llm_load_print_meta: n_expert         = 0
0.00.106.974 I llm_load_print_meta: n_expert_used    = 0
0.00.106.974 I llm_load_print_meta: causal attn      = 1
0.00.106.974 I llm_load_print_meta: pooling type     = 0
0.00.106.975 I llm_load_print_meta: rope type        = 2
0.00.106.975 I llm_load_print_meta: rope scaling     = linear
0.00.106.976 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.106.976 I llm_load_print_meta: freq_scale_train = 1
0.00.106.976 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.106.976 I llm_load_print_meta: rope_finetuned   = unknown
0.00.106.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.106.978 I llm_load_print_meta: ssm_d_inner      = 0
0.00.106.979 I llm_load_print_meta: ssm_d_state      = 0
0.00.106.979 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.106.979 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.106.979 I llm_load_print_meta: model type       = 1.4B
0.00.106.980 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.106.980 I llm_load_print_meta: model params     = 1.41 B
0.00.106.981 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.106.981 I llm_load_print_meta: general.name     = 1.4B
0.00.106.981 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.106.981 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.106.982 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.106.982 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.106.982 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.106.982 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.106.983 I llm_load_print_meta: max token length = 1024
0.00.109.601 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.109.601 I llm_load_tensors: offloading output layer to GPU
0.00.109.602 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.109.620 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.109.621 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.110.563 I llama_new_context_with_model: n_seq_max     = 1
0.00.110.564 I llama_new_context_with_model: n_ctx         = 2048
0.00.110.564 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.110.564 I llama_new_context_with_model: n_batch       = 2048
0.00.110.565 I llama_new_context_with_model: n_ubatch      = 512
0.00.110.565 I llama_new_context_with_model: flash_attn    = 0
0.00.110.565 I llama_new_context_with_model: freq_base     = 10000.0
0.00.110.565 I llama_new_context_with_model: freq_scale    = 1
0.00.110.566 I ggml_metal_init: allocating
0.00.110.569 I ggml_metal_init: found device: Apple M4
0.00.110.571 I ggml_metal_init: picking default device: Apple M4
0.00.111.244 I ggml_metal_init: using embedded metal library
0.00.121.859 I ggml_metal_init: GPU name:   Apple M4
0.00.121.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.861 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.861 I ggml_metal_init: simdgroup reduction   = true
0.00.121.862 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.862 I ggml_metal_init: has bfloat            = true
0.00.121.862 I ggml_metal_init: use bfloat            = true
0.00.121.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.146.049 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.167.762 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.167.771 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.167.814 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.168.802 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.168.804 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.168.805 I llama_new_context_with_model: graph nodes  = 967
0.00.168.805 I llama_new_context_with_model: graph splits = 2
0.00.168.808 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.168.950 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.168.950 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.249.692 I main: llama threadpool init, n_threads = 4
0.00.249.732 I 
0.00.249.775 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.249.777 I 
0.00.249.848 I sampler seed: 1234
0.00.249.852 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.249.876 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.249.877 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.249.878 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.082.844 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.02.082.845 I llama_perf_context_print:        load time =     210.36 ms
0.02.082.846 I llama_perf_context_print: prompt eval time =      43.79 ms /     7 tokens (    6.26 ms per token,   159.87 tokens per second)
0.02.082.847 I llama_perf_context_print:        eval time =    1786.16 ms /    63 runs   (   28.35 ms per token,    35.27 tokens per second)
0.02.082.847 I llama_perf_context_print:       total time =    1833.15 ms /    70 tokens
0.02.083.044 I ggml_metal_free: deallocating

real	0m2.391s
user	0m0.145s
sys	0m0.103s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.971 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.673 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.218 I llama_model_loader: - type  f32:  194 tensors
0.00.026.218 I llama_model_loader: - type q8_0:   98 tensors
0.00.048.052 I llm_load_vocab: special tokens cache size = 25
0.00.054.162 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.166 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.167 I llm_load_print_meta: arch             = gptneox
0.00.054.167 I llm_load_print_meta: vocab type       = BPE
0.00.054.168 I llm_load_print_meta: n_vocab          = 50304
0.00.054.168 I llm_load_print_meta: n_merges         = 50009
0.00.054.168 I llm_load_print_meta: vocab_only       = 0
0.00.054.170 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.170 I llm_load_print_meta: n_embd           = 2048
0.00.054.170 I llm_load_print_meta: n_layer          = 24
0.00.054.175 I llm_load_print_meta: n_head           = 16
0.00.054.176 I llm_load_print_meta: n_head_kv        = 16
0.00.054.176 I llm_load_print_meta: n_rot            = 32
0.00.054.176 I llm_load_print_meta: n_swa            = 0
0.00.054.177 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.177 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.177 I llm_load_print_meta: n_gqa            = 1
0.00.054.178 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.179 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.180 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.182 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.183 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.183 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.183 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.184 I llm_load_print_meta: n_ff             = 8192
0.00.054.199 I llm_load_print_meta: n_expert         = 0
0.00.054.199 I llm_load_print_meta: n_expert_used    = 0
0.00.054.199 I llm_load_print_meta: causal attn      = 1
0.00.054.200 I llm_load_print_meta: pooling type     = 0
0.00.054.200 I llm_load_print_meta: rope type        = 2
0.00.054.200 I llm_load_print_meta: rope scaling     = linear
0.00.054.201 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.201 I llm_load_print_meta: freq_scale_train = 1
0.00.054.202 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.202 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.202 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.202 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.203 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.203 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.203 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.204 I llm_load_print_meta: model type       = 1.4B
0.00.054.204 I llm_load_print_meta: model ftype      = Q8_0
0.00.054.204 I llm_load_print_meta: model params     = 1.41 B
0.00.054.205 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.054.205 I llm_load_print_meta: general.name     = 1.4B
0.00.054.205 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.205 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.207 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.207 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.207 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.208 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.208 I llm_load_print_meta: max token length = 1024
0.00.056.780 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.780 I llm_load_tensors: offloading output layer to GPU
0.00.056.780 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.792 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.056.793 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.057.832 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.833 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.833 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.833 I llama_new_context_with_model: n_batch       = 2048
0.00.057.833 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.834 I llama_new_context_with_model: flash_attn    = 0
0.00.057.834 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.834 I llama_new_context_with_model: freq_scale    = 1
0.00.057.835 I ggml_metal_init: allocating
0.00.057.840 I ggml_metal_init: found device: Apple M4
0.00.057.842 I ggml_metal_init: picking default device: Apple M4
0.00.058.606 I ggml_metal_init: using embedded metal library
0.00.061.172 I ggml_metal_init: GPU name:   Apple M4
0.00.061.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.175 I ggml_metal_init: simdgroup reduction   = true
0.00.061.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.175 I ggml_metal_init: has bfloat            = true
0.00.061.176 I ggml_metal_init: use bfloat            = true
0.00.061.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.177 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.693 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.979 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.000 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.041 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.209 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.211 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.211 I llama_new_context_with_model: graph nodes  = 967
0.00.099.211 I llama_new_context_with_model: graph splits = 2
0.00.099.215 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.340 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.123.798 I main: llama threadpool init, n_threads = 4
0.01.123.842 I 
0.01.123.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.123.875 I 
0.01.124.105 I sampler seed: 1234
0.01.124.111 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.124.122 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.124.122 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.124.122 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.210.632 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48596.85 tokens per second)
0.02.210.633 I llama_perf_context_print:        load time =    1112.82 ms
0.02.210.634 I llama_perf_context_print: prompt eval time =      43.06 ms /     7 tokens (    6.15 ms per token,   162.56 tokens per second)
0.02.210.635 I llama_perf_context_print:        eval time =    1040.82 ms /    63 runs   (   16.52 ms per token,    60.53 tokens per second)
0.02.210.635 I llama_perf_context_print:       total time =    1086.84 ms /    70 tokens
0.02.210.892 I ggml_metal_free: deallocating

real	0m2.231s
user	0m0.112s
sys	0m0.212s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.954 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.430 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.435 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.437 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.437 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.438 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.052 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.054 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.054 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.055 I llama_model_loader: - type  f32:  194 tensors
0.00.026.056 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.056 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.537 I llm_load_vocab: special tokens cache size = 25
0.00.052.574 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.577 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.577 I llm_load_print_meta: arch             = gptneox
0.00.052.577 I llm_load_print_meta: vocab type       = BPE
0.00.052.578 I llm_load_print_meta: n_vocab          = 50304
0.00.052.578 I llm_load_print_meta: n_merges         = 50009
0.00.052.578 I llm_load_print_meta: vocab_only       = 0
0.00.052.578 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.578 I llm_load_print_meta: n_embd           = 2048
0.00.052.579 I llm_load_print_meta: n_layer          = 24
0.00.052.583 I llm_load_print_meta: n_head           = 16
0.00.052.584 I llm_load_print_meta: n_head_kv        = 16
0.00.052.584 I llm_load_print_meta: n_rot            = 32
0.00.052.584 I llm_load_print_meta: n_swa            = 0
0.00.052.584 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.585 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.585 I llm_load_print_meta: n_gqa            = 1
0.00.052.586 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.587 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.587 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.588 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.588 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.588 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.589 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.589 I llm_load_print_meta: n_ff             = 8192
0.00.052.601 I llm_load_print_meta: n_expert         = 0
0.00.052.601 I llm_load_print_meta: n_expert_used    = 0
0.00.052.602 I llm_load_print_meta: causal attn      = 1
0.00.052.602 I llm_load_print_meta: pooling type     = 0
0.00.052.602 I llm_load_print_meta: rope type        = 2
0.00.052.602 I llm_load_print_meta: rope scaling     = linear
0.00.052.605 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.605 I llm_load_print_meta: freq_scale_train = 1
0.00.052.605 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.605 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.606 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.606 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.606 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.606 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.606 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.607 I llm_load_print_meta: model type       = 1.4B
0.00.052.607 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.607 I llm_load_print_meta: model params     = 1.41 B
0.00.052.608 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.608 I llm_load_print_meta: general.name     = 1.4B
0.00.052.608 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.609 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.610 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.610 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.610 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.610 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.610 I llm_load_print_meta: max token length = 1024
0.00.054.729 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.729 I llm_load_tensors: offloading output layer to GPU
0.00.054.730 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.741 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.742 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.682 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.682 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.683 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.683 I llama_new_context_with_model: n_batch       = 2048
0.00.055.683 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.683 I llama_new_context_with_model: flash_attn    = 0
0.00.055.684 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.684 I llama_new_context_with_model: freq_scale    = 1
0.00.055.685 I ggml_metal_init: allocating
0.00.055.688 I ggml_metal_init: found device: Apple M4
0.00.055.690 I ggml_metal_init: picking default device: Apple M4
0.00.056.368 I ggml_metal_init: using embedded metal library
0.00.058.784 I ggml_metal_init: GPU name:   Apple M4
0.00.058.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.786 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.786 I ggml_metal_init: simdgroup reduction   = true
0.00.058.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.787 I ggml_metal_init: has bfloat            = true
0.00.058.787 I ggml_metal_init: use bfloat            = true
0.00.058.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.788 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.248 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.254 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.287 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.360 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.362 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.362 I llama_new_context_with_model: graph nodes  = 967
0.00.091.363 I llama_new_context_with_model: graph splits = 2
0.00.091.366 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.130 I main: llama threadpool init, n_threads = 4
0.00.659.169 I 
0.00.659.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.204 I 
0.00.659.347 I sampler seed: 1234
0.00.659.351 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.659.369 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.659.369 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.659.369 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.339.061 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.01.339.061 I llama_perf_context_print:        load time =     648.17 ms
0.01.339.062 I llama_perf_context_print: prompt eval time =      40.73 ms /     7 tokens (    5.82 ms per token,   171.86 tokens per second)
0.01.339.063 I llama_perf_context_print:        eval time =     635.87 ms /    63 runs   (   10.09 ms per token,    99.08 tokens per second)
0.01.339.063 I llama_perf_context_print:       total time =     679.93 ms /    70 tokens
0.01.339.287 I ggml_metal_free: deallocating

real	0m1.358s
user	0m0.111s
sys	0m0.136s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.746 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.409 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.416 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.420 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.420 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.422 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.422 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.429 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.788 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.789 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.789 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.790 I llama_model_loader: - type  f32:  194 tensors
0.00.023.790 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.791 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.935 I llm_load_vocab: special tokens cache size = 25
0.00.049.872 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.875 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.875 I llm_load_print_meta: arch             = gptneox
0.00.049.876 I llm_load_print_meta: vocab type       = BPE
0.00.049.876 I llm_load_print_meta: n_vocab          = 50304
0.00.049.876 I llm_load_print_meta: n_merges         = 50009
0.00.049.876 I llm_load_print_meta: vocab_only       = 0
0.00.049.876 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.877 I llm_load_print_meta: n_embd           = 2048
0.00.049.877 I llm_load_print_meta: n_layer          = 24
0.00.049.879 I llm_load_print_meta: n_head           = 16
0.00.049.880 I llm_load_print_meta: n_head_kv        = 16
0.00.049.880 I llm_load_print_meta: n_rot            = 32
0.00.049.881 I llm_load_print_meta: n_swa            = 0
0.00.049.881 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.881 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.882 I llm_load_print_meta: n_gqa            = 1
0.00.049.883 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.883 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.884 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.885 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.885 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.885 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.885 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.886 I llm_load_print_meta: n_ff             = 8192
0.00.049.897 I llm_load_print_meta: n_expert         = 0
0.00.049.898 I llm_load_print_meta: n_expert_used    = 0
0.00.049.899 I llm_load_print_meta: causal attn      = 1
0.00.049.901 I llm_load_print_meta: pooling type     = 0
0.00.049.901 I llm_load_print_meta: rope type        = 2
0.00.049.901 I llm_load_print_meta: rope scaling     = linear
0.00.049.901 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.901 I llm_load_print_meta: freq_scale_train = 1
0.00.049.902 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.902 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.902 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.902 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.902 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.902 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.902 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.903 I llm_load_print_meta: model type       = 1.4B
0.00.049.904 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.904 I llm_load_print_meta: model params     = 1.41 B
0.00.049.905 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.905 I llm_load_print_meta: general.name     = 1.4B
0.00.049.905 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.905 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.906 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.906 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.906 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.906 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.906 I llm_load_print_meta: max token length = 1024
0.00.051.821 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.821 I llm_load_tensors: offloading output layer to GPU
0.00.051.821 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.832 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.833 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.730 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.731 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.731 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.731 I llama_new_context_with_model: n_batch       = 2048
0.00.052.731 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.731 I llama_new_context_with_model: flash_attn    = 0
0.00.052.732 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.732 I llama_new_context_with_model: freq_scale    = 1
0.00.052.733 I ggml_metal_init: allocating
0.00.052.739 I ggml_metal_init: found device: Apple M4
0.00.052.741 I ggml_metal_init: picking default device: Apple M4
0.00.053.340 I ggml_metal_init: using embedded metal library
0.00.055.688 I ggml_metal_init: GPU name:   Apple M4
0.00.055.689 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.690 I ggml_metal_init: simdgroup reduction   = true
0.00.055.690 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.690 I ggml_metal_init: has bfloat            = true
0.00.055.692 I ggml_metal_init: use bfloat            = true
0.00.055.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.207 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.321 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.329 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.361 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.327 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.329 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.329 I llama_new_context_with_model: graph nodes  = 967
0.00.085.329 I llama_new_context_with_model: graph splits = 2
0.00.085.331 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.475 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.671 I main: llama threadpool init, n_threads = 4
0.00.734.711 I 
0.00.734.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.763 I 
0.00.735.010 I sampler seed: 1234
0.00.735.015 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.071 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.076 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.076 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.457.658 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64369.90 tokens per second)
0.01.457.659 I llama_perf_context_print:        load time =     725.92 ms
0.01.457.659 I llama_perf_context_print: prompt eval time =      43.55 ms /     7 tokens (    6.22 ms per token,   160.74 tokens per second)
0.01.457.660 I llama_perf_context_print:        eval time =     676.22 ms /    63 runs   (   10.73 ms per token,    93.17 tokens per second)
0.01.457.660 I llama_perf_context_print:       total time =     722.99 ms /    70 tokens
0.01.457.880 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.109s
sys	0m0.153s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.871 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.481 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.486 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.488 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.488 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.489 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.489 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.490 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.491 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.491 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.491 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.500 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.500 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.330 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.046 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.048 I llama_model_loader: - type  f32:  194 tensors
0.00.025.048 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.049 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.374 I llm_load_vocab: special tokens cache size = 25
0.00.051.289 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.291 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.292 I llm_load_print_meta: arch             = gptneox
0.00.051.292 I llm_load_print_meta: vocab type       = BPE
0.00.051.292 I llm_load_print_meta: n_vocab          = 50304
0.00.051.292 I llm_load_print_meta: n_merges         = 50009
0.00.051.293 I llm_load_print_meta: vocab_only       = 0
0.00.051.293 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.293 I llm_load_print_meta: n_embd           = 2048
0.00.051.293 I llm_load_print_meta: n_layer          = 24
0.00.051.296 I llm_load_print_meta: n_head           = 16
0.00.051.296 I llm_load_print_meta: n_head_kv        = 16
0.00.051.299 I llm_load_print_meta: n_rot            = 32
0.00.051.299 I llm_load_print_meta: n_swa            = 0
0.00.051.299 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.299 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.300 I llm_load_print_meta: n_gqa            = 1
0.00.051.301 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.301 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.302 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.302 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.302 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.302 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.303 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.303 I llm_load_print_meta: n_ff             = 8192
0.00.051.315 I llm_load_print_meta: n_expert         = 0
0.00.051.315 I llm_load_print_meta: n_expert_used    = 0
0.00.051.317 I llm_load_print_meta: causal attn      = 1
0.00.051.318 I llm_load_print_meta: pooling type     = 0
0.00.051.318 I llm_load_print_meta: rope type        = 2
0.00.051.318 I llm_load_print_meta: rope scaling     = linear
0.00.051.318 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.319 I llm_load_print_meta: freq_scale_train = 1
0.00.051.319 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.319 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.319 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.319 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.319 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.319 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.319 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.320 I llm_load_print_meta: model type       = 1.4B
0.00.051.321 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.322 I llm_load_print_meta: model params     = 1.41 B
0.00.051.322 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.322 I llm_load_print_meta: general.name     = 1.4B
0.00.051.323 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.323 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.323 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.323 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.323 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.324 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: max token length = 1024
0.00.053.344 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.344 I llm_load_tensors: offloading output layer to GPU
0.00.053.345 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.355 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.356 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.297 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.298 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.298 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.299 I llama_new_context_with_model: n_batch       = 2048
0.00.054.299 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.299 I llama_new_context_with_model: flash_attn    = 0
0.00.054.300 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.300 I llama_new_context_with_model: freq_scale    = 1
0.00.054.300 I ggml_metal_init: allocating
0.00.054.307 I ggml_metal_init: found device: Apple M4
0.00.054.310 I ggml_metal_init: picking default device: Apple M4
0.00.054.887 I ggml_metal_init: using embedded metal library
0.00.057.269 I ggml_metal_init: GPU name:   Apple M4
0.00.057.271 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.271 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.272 I ggml_metal_init: simdgroup reduction   = true
0.00.057.272 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.272 I ggml_metal_init: has bfloat            = true
0.00.057.272 I ggml_metal_init: use bfloat            = true
0.00.057.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.274 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.835 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.789 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.795 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.826 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.795 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.796 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.797 I llama_new_context_with_model: graph nodes  = 967
0.00.087.797 I llama_new_context_with_model: graph splits = 2
0.00.087.799 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.943 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.032 I main: llama threadpool init, n_threads = 4
0.00.709.068 I 
0.00.709.104 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.104 I 
0.00.709.328 I sampler seed: 1234
0.00.709.333 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.709.375 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.709.376 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.709.376 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.497.775 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.497.775 I llama_perf_context_print:        load time =     699.16 ms
0.01.497.776 I llama_perf_context_print: prompt eval time =      43.05 ms /     7 tokens (    6.15 ms per token,   162.61 tokens per second)
0.01.497.777 I llama_perf_context_print:        eval time =     742.42 ms /    63 runs   (   11.78 ms per token,    84.86 tokens per second)
0.01.497.777 I llama_perf_context_print:       total time =     788.74 ms /    70 tokens
0.01.498.015 I ggml_metal_free: deallocating

real	0m1.516s
user	0m0.109s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.771 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.203 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.212 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.213 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.216 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.217 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.648 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.648 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.649 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.649 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.650 I llama_model_loader: - type  f32:  194 tensors
0.00.024.650 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.650 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.899 I llm_load_vocab: special tokens cache size = 25
0.00.050.823 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.826 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.826 I llm_load_print_meta: arch             = gptneox
0.00.050.826 I llm_load_print_meta: vocab type       = BPE
0.00.050.827 I llm_load_print_meta: n_vocab          = 50304
0.00.050.827 I llm_load_print_meta: n_merges         = 50009
0.00.050.827 I llm_load_print_meta: vocab_only       = 0
0.00.050.827 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.827 I llm_load_print_meta: n_embd           = 2048
0.00.050.828 I llm_load_print_meta: n_layer          = 24
0.00.050.831 I llm_load_print_meta: n_head           = 16
0.00.050.831 I llm_load_print_meta: n_head_kv        = 16
0.00.050.832 I llm_load_print_meta: n_rot            = 32
0.00.050.832 I llm_load_print_meta: n_swa            = 0
0.00.050.832 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.832 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.833 I llm_load_print_meta: n_gqa            = 1
0.00.050.834 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.834 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.835 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.835 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.835 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.836 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.836 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.837 I llm_load_print_meta: n_ff             = 8192
0.00.050.849 I llm_load_print_meta: n_expert         = 0
0.00.050.849 I llm_load_print_meta: n_expert_used    = 0
0.00.050.850 I llm_load_print_meta: causal attn      = 1
0.00.050.852 I llm_load_print_meta: pooling type     = 0
0.00.050.852 I llm_load_print_meta: rope type        = 2
0.00.050.852 I llm_load_print_meta: rope scaling     = linear
0.00.050.853 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.853 I llm_load_print_meta: freq_scale_train = 1
0.00.050.853 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.853 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.853 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.853 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.854 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.854 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.854 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.854 I llm_load_print_meta: model type       = 1.4B
0.00.050.856 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.856 I llm_load_print_meta: model params     = 1.41 B
0.00.050.856 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.856 I llm_load_print_meta: general.name     = 1.4B
0.00.050.857 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.857 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.857 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.857 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.857 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.858 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.858 I llm_load_print_meta: max token length = 1024
0.00.052.827 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.827 I llm_load_tensors: offloading output layer to GPU
0.00.052.827 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.838 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.839 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.750 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.751 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.752 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.752 I llama_new_context_with_model: n_batch       = 2048
0.00.053.752 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.752 I llama_new_context_with_model: flash_attn    = 0
0.00.053.752 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.753 I llama_new_context_with_model: freq_scale    = 1
0.00.053.753 I ggml_metal_init: allocating
0.00.053.756 I ggml_metal_init: found device: Apple M4
0.00.053.758 I ggml_metal_init: picking default device: Apple M4
0.00.054.355 I ggml_metal_init: using embedded metal library
0.00.056.658 I ggml_metal_init: GPU name:   Apple M4
0.00.056.659 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.660 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.660 I ggml_metal_init: simdgroup reduction   = true
0.00.056.661 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.662 I ggml_metal_init: has bfloat            = true
0.00.056.662 I ggml_metal_init: use bfloat            = true
0.00.056.662 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.664 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.294 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.919 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.929 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.973 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.972 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.974 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.974 I llama_new_context_with_model: graph nodes  = 967
0.00.086.974 I llama_new_context_with_model: graph splits = 2
0.00.086.977 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.105 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.106 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.107 I main: llama threadpool init, n_threads = 4
0.00.784.144 I 
0.00.784.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.193 I 
0.00.784.441 I sampler seed: 1234
0.00.784.445 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.491 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.492 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.492 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.617.839 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 60996.56 tokens per second)
0.01.617.840 I llama_perf_context_print:        load time =     775.33 ms
0.01.617.840 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.57 tokens per second)
0.01.617.844 I llama_perf_context_print:        eval time =     788.20 ms /    63 runs   (   12.51 ms per token,    79.93 tokens per second)
0.01.617.845 I llama_perf_context_print:       total time =     833.73 ms /    70 tokens
0.01.618.042 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.108s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.964 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.439 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.445 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.445 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.446 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.446 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.447 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.447 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.447 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.448 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.448 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.159 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.161 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.161 I llama_model_loader: - type  f32:  194 tensors
0.00.024.161 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.162 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.162 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.131 I llm_load_vocab: special tokens cache size = 25
0.00.051.190 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.193 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.193 I llm_load_print_meta: arch             = gptneox
0.00.051.193 I llm_load_print_meta: vocab type       = BPE
0.00.051.193 I llm_load_print_meta: n_vocab          = 50304
0.00.051.194 I llm_load_print_meta: n_merges         = 50009
0.00.051.194 I llm_load_print_meta: vocab_only       = 0
0.00.051.194 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.194 I llm_load_print_meta: n_embd           = 2048
0.00.051.194 I llm_load_print_meta: n_layer          = 24
0.00.051.197 I llm_load_print_meta: n_head           = 16
0.00.051.198 I llm_load_print_meta: n_head_kv        = 16
0.00.051.198 I llm_load_print_meta: n_rot            = 32
0.00.051.198 I llm_load_print_meta: n_swa            = 0
0.00.051.198 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.198 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.199 I llm_load_print_meta: n_gqa            = 1
0.00.051.200 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.201 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.201 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.201 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.201 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.202 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.202 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.202 I llm_load_print_meta: n_ff             = 8192
0.00.051.214 I llm_load_print_meta: n_expert         = 0
0.00.051.216 I llm_load_print_meta: n_expert_used    = 0
0.00.051.216 I llm_load_print_meta: causal attn      = 1
0.00.051.216 I llm_load_print_meta: pooling type     = 0
0.00.051.216 I llm_load_print_meta: rope type        = 2
0.00.051.217 I llm_load_print_meta: rope scaling     = linear
0.00.051.217 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.218 I llm_load_print_meta: freq_scale_train = 1
0.00.051.218 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.218 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.219 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.219 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.219 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.219 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.220 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.221 I llm_load_print_meta: model type       = 1.4B
0.00.051.221 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.221 I llm_load_print_meta: model params     = 1.41 B
0.00.051.222 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.222 I llm_load_print_meta: general.name     = 1.4B
0.00.051.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.223 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.223 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.224 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.225 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.225 I llm_load_print_meta: max token length = 1024
0.00.053.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.189 I llm_load_tensors: offloading output layer to GPU
0.00.053.189 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.200 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.201 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.143 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.144 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.144 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.144 I llama_new_context_with_model: n_batch       = 2048
0.00.054.145 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.145 I llama_new_context_with_model: flash_attn    = 0
0.00.054.145 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.146 I llama_new_context_with_model: freq_scale    = 1
0.00.054.146 I ggml_metal_init: allocating
0.00.054.149 I ggml_metal_init: found device: Apple M4
0.00.054.152 I ggml_metal_init: picking default device: Apple M4
0.00.054.740 I ggml_metal_init: using embedded metal library
0.00.057.072 I ggml_metal_init: GPU name:   Apple M4
0.00.057.074 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.075 I ggml_metal_init: simdgroup reduction   = true
0.00.057.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.075 I ggml_metal_init: has bfloat            = true
0.00.057.075 I ggml_metal_init: use bfloat            = true
0.00.057.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.031 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.517 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.523 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.556 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.618 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.620 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.620 I llama_new_context_with_model: graph nodes  = 967
0.00.089.620 I llama_new_context_with_model: graph splits = 2
0.00.089.623 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.766 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.449.146 I main: llama threadpool init, n_threads = 4
0.00.449.182 I 
0.00.449.219 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.449.220 I 
0.00.449.436 I sampler seed: 1234
0.00.449.440 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.449.451 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.449.451 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.449.452 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.128.847 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.128.848 I llama_perf_context_print:        load time =     439.18 ms
0.01.128.849 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.65 tokens per second)
0.01.128.849 I llama_perf_context_print:        eval time =     640.64 ms /    63 runs   (   10.17 ms per token,    98.34 tokens per second)
0.01.128.850 I llama_perf_context_print:       total time =     679.70 ms /    70 tokens
0.01.129.095 I ggml_metal_free: deallocating

real	0m1.146s
user	0m0.110s
sys	0m0.112s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.850 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.377 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.383 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.384 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.386 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.386 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.387 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.387 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.390 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.306 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.144 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.145 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.145 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.145 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.146 I llama_model_loader: - type  f32:  194 tensors
0.00.025.146 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.146 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.147 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.147 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.154 I llm_load_vocab: special tokens cache size = 25
0.00.052.203 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.205 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.206 I llm_load_print_meta: arch             = gptneox
0.00.052.206 I llm_load_print_meta: vocab type       = BPE
0.00.052.206 I llm_load_print_meta: n_vocab          = 50304
0.00.052.207 I llm_load_print_meta: n_merges         = 50009
0.00.052.207 I llm_load_print_meta: vocab_only       = 0
0.00.052.207 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.207 I llm_load_print_meta: n_embd           = 2048
0.00.052.207 I llm_load_print_meta: n_layer          = 24
0.00.052.210 I llm_load_print_meta: n_head           = 16
0.00.052.211 I llm_load_print_meta: n_head_kv        = 16
0.00.052.211 I llm_load_print_meta: n_rot            = 32
0.00.052.211 I llm_load_print_meta: n_swa            = 0
0.00.052.211 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.211 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.213 I llm_load_print_meta: n_gqa            = 1
0.00.052.214 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.214 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.215 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.215 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.215 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.216 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.216 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.216 I llm_load_print_meta: n_ff             = 8192
0.00.052.228 I llm_load_print_meta: n_expert         = 0
0.00.052.230 I llm_load_print_meta: n_expert_used    = 0
0.00.052.230 I llm_load_print_meta: causal attn      = 1
0.00.052.230 I llm_load_print_meta: pooling type     = 0
0.00.052.230 I llm_load_print_meta: rope type        = 2
0.00.052.230 I llm_load_print_meta: rope scaling     = linear
0.00.052.230 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.231 I llm_load_print_meta: freq_scale_train = 1
0.00.052.231 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.231 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.231 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.231 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.231 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.231 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.232 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.232 I llm_load_print_meta: model type       = 1.4B
0.00.052.232 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.234 I llm_load_print_meta: model params     = 1.41 B
0.00.052.234 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.235 I llm_load_print_meta: general.name     = 1.4B
0.00.052.235 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.235 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.235 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.235 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.236 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.236 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.236 I llm_load_print_meta: max token length = 1024
0.00.054.185 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.185 I llm_load_tensors: offloading output layer to GPU
0.00.054.186 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.196 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.198 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.129 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.130 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.130 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.130 I llama_new_context_with_model: n_batch       = 2048
0.00.055.131 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.131 I llama_new_context_with_model: flash_attn    = 0
0.00.055.131 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.132 I llama_new_context_with_model: freq_scale    = 1
0.00.055.132 I ggml_metal_init: allocating
0.00.055.136 I ggml_metal_init: found device: Apple M4
0.00.055.138 I ggml_metal_init: picking default device: Apple M4
0.00.055.728 I ggml_metal_init: using embedded metal library
0.00.058.029 I ggml_metal_init: GPU name:   Apple M4
0.00.058.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.032 I ggml_metal_init: simdgroup reduction   = true
0.00.058.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.032 I ggml_metal_init: has bfloat            = true
0.00.058.032 I ggml_metal_init: use bfloat            = true
0.00.058.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.033 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.846 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.649 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.679 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.646 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.648 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.648 I llama_new_context_with_model: graph nodes  = 967
0.00.087.648 I llama_new_context_with_model: graph splits = 2
0.00.087.652 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.782 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.918 I main: llama threadpool init, n_threads = 4
0.00.532.959 I 
0.00.533.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.017 I 
0.00.533.251 I sampler seed: 1234
0.00.533.256 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.533.268 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.533.269 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.533.269 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.283.510 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.01.283.510 I llama_perf_context_print:        load time =     523.06 ms
0.01.283.511 I llama_perf_context_print: prompt eval time =      44.91 ms /     7 tokens (    6.42 ms per token,   155.85 tokens per second)
0.01.283.512 I llama_perf_context_print:        eval time =     702.30 ms /    63 runs   (   11.15 ms per token,    89.70 tokens per second)
0.01.283.512 I llama_perf_context_print:       total time =     750.60 ms /    70 tokens
0.01.283.704 I ggml_metal_free: deallocating

real	0m1.301s
user	0m0.110s
sys	0m0.124s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.367 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.596 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.597 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.599 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.599 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.599 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.600 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.600 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.600 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.435 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.224 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.226 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.226 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.226 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.227 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.227 I llama_model_loader: - type  f32:  194 tensors
0.00.024.227 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.228 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.228 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.464 I llm_load_vocab: special tokens cache size = 25
0.00.050.406 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.408 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.409 I llm_load_print_meta: arch             = gptneox
0.00.050.409 I llm_load_print_meta: vocab type       = BPE
0.00.050.409 I llm_load_print_meta: n_vocab          = 50304
0.00.050.409 I llm_load_print_meta: n_merges         = 50009
0.00.050.410 I llm_load_print_meta: vocab_only       = 0
0.00.050.410 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.410 I llm_load_print_meta: n_embd           = 2048
0.00.050.410 I llm_load_print_meta: n_layer          = 24
0.00.050.413 I llm_load_print_meta: n_head           = 16
0.00.050.413 I llm_load_print_meta: n_head_kv        = 16
0.00.050.414 I llm_load_print_meta: n_rot            = 32
0.00.050.414 I llm_load_print_meta: n_swa            = 0
0.00.050.414 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.414 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.415 I llm_load_print_meta: n_gqa            = 1
0.00.050.416 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.416 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.417 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.417 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.417 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.417 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.418 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.418 I llm_load_print_meta: n_ff             = 8192
0.00.050.430 I llm_load_print_meta: n_expert         = 0
0.00.050.430 I llm_load_print_meta: n_expert_used    = 0
0.00.050.430 I llm_load_print_meta: causal attn      = 1
0.00.050.431 I llm_load_print_meta: pooling type     = 0
0.00.050.431 I llm_load_print_meta: rope type        = 2
0.00.050.431 I llm_load_print_meta: rope scaling     = linear
0.00.050.431 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.433 I llm_load_print_meta: freq_scale_train = 1
0.00.050.434 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.434 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.435 I llm_load_print_meta: model type       = 1.4B
0.00.050.435 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.436 I llm_load_print_meta: model params     = 1.41 B
0.00.050.436 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.436 I llm_load_print_meta: general.name     = 1.4B
0.00.050.438 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.438 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.438 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.438 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.438 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.438 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.439 I llm_load_print_meta: max token length = 1024
0.00.052.382 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.382 I llm_load_tensors: offloading output layer to GPU
0.00.052.383 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.393 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.394 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.271 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.271 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.271 I llama_new_context_with_model: n_batch       = 2048
0.00.053.271 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.272 I llama_new_context_with_model: flash_attn    = 0
0.00.053.272 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.272 I llama_new_context_with_model: freq_scale    = 1
0.00.053.273 I ggml_metal_init: allocating
0.00.053.276 I ggml_metal_init: found device: Apple M4
0.00.053.278 I ggml_metal_init: picking default device: Apple M4
0.00.053.860 I ggml_metal_init: using embedded metal library
0.00.056.147 I ggml_metal_init: GPU name:   Apple M4
0.00.056.148 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.149 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.149 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.149 I ggml_metal_init: simdgroup reduction   = true
0.00.056.150 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.150 I ggml_metal_init: has bfloat            = true
0.00.056.150 I ggml_metal_init: use bfloat            = true
0.00.056.150 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.744 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.415 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.424 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.458 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.413 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.414 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.414 I llama_new_context_with_model: graph nodes  = 967
0.00.085.415 I llama_new_context_with_model: graph splits = 2
0.00.085.417 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.560 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.561 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.787 I main: llama threadpool init, n_threads = 4
0.00.629.823 I 
0.00.629.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.855 I 
0.00.630.072 I sampler seed: 1234
0.00.630.076 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.112 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.114 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.114 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.389.698 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.389.699 I llama_perf_context_print:        load time =     620.42 ms
0.01.389.699 I llama_perf_context_print: prompt eval time =      50.93 ms /     7 tokens (    7.28 ms per token,   137.45 tokens per second)
0.01.389.700 I llama_perf_context_print:        eval time =     705.59 ms /    63 runs   (   11.20 ms per token,    89.29 tokens per second)
0.01.389.702 I llama_perf_context_print:       total time =     759.91 ms /    70 tokens
0.01.389.902 I ggml_metal_free: deallocating

real	0m1.408s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.851 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.127 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.138 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.138 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.142 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.142 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.142 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.143 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.146 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.146 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.148 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.998 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.026 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.824 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.825 I llama_model_loader: - type  f32:  194 tensors
0.00.024.825 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.825 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.782 I llm_load_vocab: special tokens cache size = 25
0.00.051.855 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.858 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.859 I llm_load_print_meta: arch             = gptneox
0.00.051.859 I llm_load_print_meta: vocab type       = BPE
0.00.051.859 I llm_load_print_meta: n_vocab          = 50304
0.00.051.859 I llm_load_print_meta: n_merges         = 50009
0.00.051.859 I llm_load_print_meta: vocab_only       = 0
0.00.051.860 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.860 I llm_load_print_meta: n_embd           = 2048
0.00.051.860 I llm_load_print_meta: n_layer          = 24
0.00.051.862 I llm_load_print_meta: n_head           = 16
0.00.051.863 I llm_load_print_meta: n_head_kv        = 16
0.00.051.863 I llm_load_print_meta: n_rot            = 32
0.00.051.863 I llm_load_print_meta: n_swa            = 0
0.00.051.863 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.864 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.864 I llm_load_print_meta: n_gqa            = 1
0.00.051.865 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.866 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.866 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.867 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.867 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.867 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.867 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.868 I llm_load_print_meta: n_ff             = 8192
0.00.051.879 I llm_load_print_meta: n_expert         = 0
0.00.051.880 I llm_load_print_meta: n_expert_used    = 0
0.00.051.880 I llm_load_print_meta: causal attn      = 1
0.00.051.882 I llm_load_print_meta: pooling type     = 0
0.00.051.882 I llm_load_print_meta: rope type        = 2
0.00.051.882 I llm_load_print_meta: rope scaling     = linear
0.00.051.882 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.883 I llm_load_print_meta: freq_scale_train = 1
0.00.051.883 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.883 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.883 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.883 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.883 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.883 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.883 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.884 I llm_load_print_meta: model type       = 1.4B
0.00.051.884 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.885 I llm_load_print_meta: model params     = 1.41 B
0.00.051.885 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.885 I llm_load_print_meta: general.name     = 1.4B
0.00.051.886 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.886 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.886 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.886 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.886 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.887 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.887 I llm_load_print_meta: max token length = 1024
0.00.053.979 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.980 I llm_load_tensors: offloading output layer to GPU
0.00.053.980 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.990 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.992 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.972 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.973 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.974 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.974 I llama_new_context_with_model: n_batch       = 2048
0.00.054.974 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.974 I llama_new_context_with_model: flash_attn    = 0
0.00.054.975 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.975 I llama_new_context_with_model: freq_scale    = 1
0.00.054.975 I ggml_metal_init: allocating
0.00.054.979 I ggml_metal_init: found device: Apple M4
0.00.054.981 I ggml_metal_init: picking default device: Apple M4
0.00.055.569 I ggml_metal_init: using embedded metal library
0.00.057.933 I ggml_metal_init: GPU name:   Apple M4
0.00.057.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.935 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.935 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.936 I ggml_metal_init: simdgroup reduction   = true
0.00.057.936 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.936 I ggml_metal_init: has bfloat            = true
0.00.057.936 I ggml_metal_init: use bfloat            = true
0.00.057.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.834 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.503 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.511 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.544 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.568 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.569 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.569 I llama_new_context_with_model: graph nodes  = 967
0.00.088.570 I llama_new_context_with_model: graph splits = 2
0.00.088.572 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.276 I main: llama threadpool init, n_threads = 4
0.00.689.313 I 
0.00.689.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.359 I 
0.00.689.574 I sampler seed: 1234
0.00.689.578 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.589 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.590 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.590 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.543.717 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.543.719 I llama_perf_context_print:        load time =     680.42 ms
0.01.543.719 I llama_perf_context_print: prompt eval time =      55.48 ms /     7 tokens (    7.93 ms per token,   126.18 tokens per second)
0.01.543.722 I llama_perf_context_print:        eval time =     795.84 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.543.722 I llama_perf_context_print:       total time =     854.44 ms /    70 tokens
0.01.543.975 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.110s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.019.457 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.041.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.482 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.483 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.484 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.486 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.486 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.487 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.488 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.490 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.846 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.481 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.054.191 I llama_model_loader: - type  f32:  194 tensors
0.00.054.192 I llama_model_loader: - type q6_K:   98 tensors
0.00.092.709 I llm_load_vocab: special tokens cache size = 25
0.00.102.554 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.102.557 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.102.558 I llm_load_print_meta: arch             = gptneox
0.00.102.558 I llm_load_print_meta: vocab type       = BPE
0.00.102.558 I llm_load_print_meta: n_vocab          = 50304
0.00.102.558 I llm_load_print_meta: n_merges         = 50009
0.00.102.559 I llm_load_print_meta: vocab_only       = 0
0.00.102.559 I llm_load_print_meta: n_ctx_train      = 2048
0.00.102.559 I llm_load_print_meta: n_embd           = 2048
0.00.102.559 I llm_load_print_meta: n_layer          = 24
0.00.102.563 I llm_load_print_meta: n_head           = 16
0.00.102.564 I llm_load_print_meta: n_head_kv        = 16
0.00.102.564 I llm_load_print_meta: n_rot            = 32
0.00.102.567 I llm_load_print_meta: n_swa            = 0
0.00.102.567 I llm_load_print_meta: n_embd_head_k    = 128
0.00.102.568 I llm_load_print_meta: n_embd_head_v    = 128
0.00.102.568 I llm_load_print_meta: n_gqa            = 1
0.00.102.570 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.102.570 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.102.571 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.102.571 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.102.572 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.102.573 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.102.574 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.102.576 I llm_load_print_meta: n_ff             = 8192
0.00.102.588 I llm_load_print_meta: n_expert         = 0
0.00.102.588 I llm_load_print_meta: n_expert_used    = 0
0.00.102.589 I llm_load_print_meta: causal attn      = 1
0.00.102.589 I llm_load_print_meta: pooling type     = 0
0.00.102.590 I llm_load_print_meta: rope type        = 2
0.00.102.590 I llm_load_print_meta: rope scaling     = linear
0.00.102.591 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.102.591 I llm_load_print_meta: freq_scale_train = 1
0.00.102.592 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.102.592 I llm_load_print_meta: rope_finetuned   = unknown
0.00.102.592 I llm_load_print_meta: ssm_d_conv       = 0
0.00.102.592 I llm_load_print_meta: ssm_d_inner      = 0
0.00.102.592 I llm_load_print_meta: ssm_d_state      = 0
0.00.102.593 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.102.593 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.102.594 I llm_load_print_meta: model type       = 1.4B
0.00.102.595 I llm_load_print_meta: model ftype      = Q6_K
0.00.102.595 I llm_load_print_meta: model params     = 1.41 B
0.00.102.596 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.102.596 I llm_load_print_meta: general.name     = 1.4B
0.00.102.596 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.102.596 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.102.598 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.102.598 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.102.599 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.102.599 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.102.599 I llm_load_print_meta: max token length = 1024
0.00.105.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.105.269 I llm_load_tensors: offloading output layer to GPU
0.00.105.269 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.105.280 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.105.282 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.106.503 I llama_new_context_with_model: n_seq_max     = 1
0.00.106.504 I llama_new_context_with_model: n_ctx         = 2048
0.00.106.504 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.106.504 I llama_new_context_with_model: n_batch       = 2048
0.00.106.505 I llama_new_context_with_model: n_ubatch      = 512
0.00.106.505 I llama_new_context_with_model: flash_attn    = 0
0.00.106.505 I llama_new_context_with_model: freq_base     = 10000.0
0.00.106.506 I llama_new_context_with_model: freq_scale    = 1
0.00.106.506 I ggml_metal_init: allocating
0.00.106.510 I ggml_metal_init: found device: Apple M4
0.00.106.513 I ggml_metal_init: picking default device: Apple M4
0.00.107.316 I ggml_metal_init: using embedded metal library
0.00.110.550 I ggml_metal_init: GPU name:   Apple M4
0.00.110.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.110.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.110.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.110.554 I ggml_metal_init: simdgroup reduction   = true
0.00.110.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.110.554 I ggml_metal_init: has bfloat            = true
0.00.110.554 I ggml_metal_init: use bfloat            = true
0.00.110.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.110.555 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.121.419 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.143.205 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.143.217 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.143.251 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.144.226 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.144.228 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.144.228 I llama_new_context_with_model: graph nodes  = 967
0.00.144.228 I llama_new_context_with_model: graph splits = 2
0.00.144.231 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.144.378 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.047.560 I main: llama threadpool init, n_threads = 4
0.01.047.645 I 
0.01.047.717 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.047.719 I 
0.01.048.228 I sampler seed: 1234
0.01.048.236 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.048.267 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.048.269 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.048.269 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.933.424 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.933.425 I llama_perf_context_print:        load time =    1028.09 ms
0.01.933.426 I llama_perf_context_print: prompt eval time =      55.01 ms /     7 tokens (    7.86 ms per token,   127.24 tokens per second)
0.01.933.427 I llama_perf_context_print:        eval time =     827.23 ms /    63 runs   (   13.13 ms per token,    76.16 tokens per second)
0.01.933.427 I llama_perf_context_print:       total time =     885.87 ms /    70 tokens
0.01.933.660 I ggml_metal_free: deallocating

real	0m1.970s
user	0m0.152s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.760 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.455 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.798 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.810 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.814 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.815 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.819 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.820 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.829 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.830 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.328 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.336 I llama_model_loader: - type  f32:  194 tensors
0.00.053.336 I llama_model_loader: - type  f16:   98 tensors
0.00.088.166 I llm_load_vocab: special tokens cache size = 25
0.00.095.187 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.190 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.191 I llm_load_print_meta: arch             = gptneox
0.00.095.191 I llm_load_print_meta: vocab type       = BPE
0.00.095.191 I llm_load_print_meta: n_vocab          = 50304
0.00.095.192 I llm_load_print_meta: n_merges         = 50009
0.00.095.192 I llm_load_print_meta: vocab_only       = 0
0.00.095.192 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.192 I llm_load_print_meta: n_embd           = 2048
0.00.095.192 I llm_load_print_meta: n_layer          = 24
0.00.095.196 I llm_load_print_meta: n_head           = 16
0.00.095.196 I llm_load_print_meta: n_head_kv        = 16
0.00.095.198 I llm_load_print_meta: n_rot            = 32
0.00.095.198 I llm_load_print_meta: n_swa            = 0
0.00.095.198 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.198 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.199 I llm_load_print_meta: n_gqa            = 1
0.00.095.200 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.200 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.201 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.201 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.202 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.202 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.203 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.203 I llm_load_print_meta: n_ff             = 8192
0.00.095.215 I llm_load_print_meta: n_expert         = 0
0.00.095.215 I llm_load_print_meta: n_expert_used    = 0
0.00.095.215 I llm_load_print_meta: causal attn      = 1
0.00.095.215 I llm_load_print_meta: pooling type     = 0
0.00.095.215 I llm_load_print_meta: rope type        = 2
0.00.095.216 I llm_load_print_meta: rope scaling     = linear
0.00.095.216 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.216 I llm_load_print_meta: freq_scale_train = 1
0.00.095.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.217 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.217 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.219 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.219 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.219 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.219 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.220 I llm_load_print_meta: model type       = 1.4B
0.00.095.220 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.221 I llm_load_print_meta: model params     = 1.41 B
0.00.095.221 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.221 I llm_load_print_meta: general.name     = 1.4B
0.00.095.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.222 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.222 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.222 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.095.223 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.223 I llm_load_print_meta: max token length = 1024
0.00.097.751 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.751 I llm_load_tensors: offloading output layer to GPU
0.00.097.751 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.761 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.763 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.778 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.779 I llama_new_context_with_model: n_ctx         = 128
0.00.098.779 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.098.779 I llama_new_context_with_model: n_batch       = 128
0.00.098.779 I llama_new_context_with_model: n_ubatch      = 128
0.00.098.779 I llama_new_context_with_model: flash_attn    = 0
0.00.098.780 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.780 I llama_new_context_with_model: freq_scale    = 1
0.00.098.780 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.098.781 I ggml_metal_init: allocating
0.00.098.791 I ggml_metal_init: found device: Apple M4
0.00.098.793 I ggml_metal_init: picking default device: Apple M4
0.00.099.452 I ggml_metal_init: using embedded metal library
0.00.102.225 I ggml_metal_init: GPU name:   Apple M4
0.00.102.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.228 I ggml_metal_init: simdgroup reduction   = true
0.00.102.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.228 I ggml_metal_init: has bfloat            = true
0.00.102.228 I ggml_metal_init: use bfloat            = true
0.00.102.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.685 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.113.009 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.113.013 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.113.041 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.938 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.113.939 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.113.939 I llama_new_context_with_model: graph nodes  = 967
0.00.113.940 I llama_new_context_with_model: graph splits = 2
0.00.113.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.113.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.096.907 I 
0.01.096.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.097.005 I perplexity: tokenizing the input ..
0.01.110.439 I perplexity: tokenization took 13.425 ms
0.01.110.445 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.232.563 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.234.578 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.234.602 I llama_perf_context_print:        load time =    1073.43 ms
0.01.234.604 I llama_perf_context_print: prompt eval time =     121.19 ms /   128 tokens (    0.95 ms per token,  1056.18 tokens per second)
0.01.234.605 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.234.606 I llama_perf_context_print:       total time =     137.70 ms /   129 tokens
0.01.235.208 I ggml_metal_free: deallocating

real	0m1.432s
user	0m0.130s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.146 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.074 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.918 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.920 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.921 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.922 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.923 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.925 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.926 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.926 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.929 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.929 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.929 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.564 I llama_model_loader: - type  f32:  194 tensors
0.00.037.564 I llama_model_loader: - type q8_0:   98 tensors
0.00.064.898 I llm_load_vocab: special tokens cache size = 25
0.00.071.571 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.574 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.575 I llm_load_print_meta: arch             = gptneox
0.00.071.575 I llm_load_print_meta: vocab type       = BPE
0.00.071.575 I llm_load_print_meta: n_vocab          = 50304
0.00.071.575 I llm_load_print_meta: n_merges         = 50009
0.00.071.575 I llm_load_print_meta: vocab_only       = 0
0.00.071.576 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.576 I llm_load_print_meta: n_embd           = 2048
0.00.071.577 I llm_load_print_meta: n_layer          = 24
0.00.071.581 I llm_load_print_meta: n_head           = 16
0.00.071.581 I llm_load_print_meta: n_head_kv        = 16
0.00.071.582 I llm_load_print_meta: n_rot            = 32
0.00.071.582 I llm_load_print_meta: n_swa            = 0
0.00.071.582 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.583 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.583 I llm_load_print_meta: n_gqa            = 1
0.00.071.584 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.585 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.585 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.586 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.586 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.586 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.587 I llm_load_print_meta: n_ff             = 8192
0.00.071.599 I llm_load_print_meta: n_expert         = 0
0.00.071.599 I llm_load_print_meta: n_expert_used    = 0
0.00.071.599 I llm_load_print_meta: causal attn      = 1
0.00.071.600 I llm_load_print_meta: pooling type     = 0
0.00.071.600 I llm_load_print_meta: rope type        = 2
0.00.071.600 I llm_load_print_meta: rope scaling     = linear
0.00.071.600 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.600 I llm_load_print_meta: freq_scale_train = 1
0.00.071.601 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.601 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.601 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.601 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.603 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.603 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.603 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.604 I llm_load_print_meta: model type       = 1.4B
0.00.071.605 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.605 I llm_load_print_meta: model params     = 1.41 B
0.00.071.605 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.606 I llm_load_print_meta: general.name     = 1.4B
0.00.071.606 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.606 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.606 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.606 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.607 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.071.607 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.607 I llm_load_print_meta: max token length = 1024
0.00.073.972 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.972 I llm_load_tensors: offloading output layer to GPU
0.00.073.972 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.983 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.984 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.074.955 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.956 I llama_new_context_with_model: n_ctx         = 128
0.00.074.956 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.074.956 I llama_new_context_with_model: n_batch       = 128
0.00.074.956 I llama_new_context_with_model: n_ubatch      = 128
0.00.074.956 I llama_new_context_with_model: flash_attn    = 0
0.00.074.957 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.957 I llama_new_context_with_model: freq_scale    = 1
0.00.074.958 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.074.958 I ggml_metal_init: allocating
0.00.074.964 I ggml_metal_init: found device: Apple M4
0.00.074.966 I ggml_metal_init: picking default device: Apple M4
0.00.075.639 I ggml_metal_init: using embedded metal library
0.00.078.391 I ggml_metal_init: GPU name:   Apple M4
0.00.078.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.394 I ggml_metal_init: simdgroup reduction   = true
0.00.078.394 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.394 I ggml_metal_init: has bfloat            = true
0.00.078.394 I ggml_metal_init: use bfloat            = true
0.00.078.395 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.661 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.129 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.090.133 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.090.162 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.081 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.091.082 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.091.082 I llama_new_context_with_model: graph nodes  = 967
0.00.091.083 I llama_new_context_with_model: graph splits = 2
0.00.091.085 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.091.085 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.907.775 I 
0.00.907.851 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.907.880 I perplexity: tokenizing the input ..
0.00.925.394 I perplexity: tokenization took 17.512 ms
0.00.925.401 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.067.683 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.069.356 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.069.391 I llama_perf_context_print:        load time =     893.69 ms
0.01.069.393 I llama_perf_context_print: prompt eval time =     141.31 ms /   128 tokens (    1.10 ms per token,   905.78 tokens per second)
0.01.069.394 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.069.394 I llama_perf_context_print:       total time =     161.62 ms /   129 tokens
0.01.069.986 I ggml_metal_free: deallocating

real	0m1.093s
user	0m0.114s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.128 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.132 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.207 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.212 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.215 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.216 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.217 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.222 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.009 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.648 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.338 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.342 I llama_model_loader: - type  f32:  194 tensors
0.00.034.342 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.342 I llama_model_loader: - type q6_K:    1 tensors
0.00.062.282 I llm_load_vocab: special tokens cache size = 25
0.00.068.877 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.880 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.880 I llm_load_print_meta: arch             = gptneox
0.00.068.880 I llm_load_print_meta: vocab type       = BPE
0.00.068.880 I llm_load_print_meta: n_vocab          = 50304
0.00.068.881 I llm_load_print_meta: n_merges         = 50009
0.00.068.881 I llm_load_print_meta: vocab_only       = 0
0.00.068.881 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.881 I llm_load_print_meta: n_embd           = 2048
0.00.068.881 I llm_load_print_meta: n_layer          = 24
0.00.068.884 I llm_load_print_meta: n_head           = 16
0.00.068.885 I llm_load_print_meta: n_head_kv        = 16
0.00.068.885 I llm_load_print_meta: n_rot            = 32
0.00.068.885 I llm_load_print_meta: n_swa            = 0
0.00.068.887 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.888 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.888 I llm_load_print_meta: n_gqa            = 1
0.00.068.889 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.889 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.890 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.891 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.891 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.891 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.895 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.898 I llm_load_print_meta: n_ff             = 8192
0.00.068.911 I llm_load_print_meta: n_expert         = 0
0.00.068.912 I llm_load_print_meta: n_expert_used    = 0
0.00.068.912 I llm_load_print_meta: causal attn      = 1
0.00.068.914 I llm_load_print_meta: pooling type     = 0
0.00.068.914 I llm_load_print_meta: rope type        = 2
0.00.068.914 I llm_load_print_meta: rope scaling     = linear
0.00.068.915 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.915 I llm_load_print_meta: freq_scale_train = 1
0.00.068.915 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.916 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.916 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.916 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.916 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.916 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.916 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.917 I llm_load_print_meta: model type       = 1.4B
0.00.068.917 I llm_load_print_meta: model ftype      = Q4_0
0.00.068.917 I llm_load_print_meta: model params     = 1.41 B
0.00.068.918 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.068.918 I llm_load_print_meta: general.name     = 1.4B
0.00.068.918 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.918 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.919 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.919 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.919 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.919 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.919 I llm_load_print_meta: max token length = 1024
0.00.071.072 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.072 I llm_load_tensors: offloading output layer to GPU
0.00.071.072 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.083 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.071.084 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.072.068 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.069 I llama_new_context_with_model: n_ctx         = 128
0.00.072.070 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.072.070 I llama_new_context_with_model: n_batch       = 128
0.00.072.070 I llama_new_context_with_model: n_ubatch      = 128
0.00.072.070 I llama_new_context_with_model: flash_attn    = 0
0.00.072.071 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.071 I llama_new_context_with_model: freq_scale    = 1
0.00.072.071 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.072 I ggml_metal_init: allocating
0.00.072.074 I ggml_metal_init: found device: Apple M4
0.00.072.076 I ggml_metal_init: picking default device: Apple M4
0.00.072.677 I ggml_metal_init: using embedded metal library
0.00.075.438 I ggml_metal_init: GPU name:   Apple M4
0.00.075.440 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.441 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.441 I ggml_metal_init: simdgroup reduction   = true
0.00.075.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.442 I ggml_metal_init: has bfloat            = true
0.00.075.442 I ggml_metal_init: use bfloat            = true
0.00.075.442 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.822 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.341 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.345 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.370 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.348 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.349 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.349 I llama_new_context_with_model: graph nodes  = 967
0.00.088.349 I llama_new_context_with_model: graph splits = 2
0.00.088.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.380 I 
0.00.630.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.454 I perplexity: tokenizing the input ..
0.00.638.144 I perplexity: tokenization took 7.689 ms
0.00.638.153 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.716 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.761.885 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.761.907 I llama_perf_context_print:        load time =     618.24 ms
0.00.761.910 I llama_perf_context_print: prompt eval time =     122.34 ms /   128 tokens (    0.96 ms per token,  1046.31 tokens per second)
0.00.761.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.912 I llama_perf_context_print:       total time =     131.53 ms /   129 tokens
0.00.762.337 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.097s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.553 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.496 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.505 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.505 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.506 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.507 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.508 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.509 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.509 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.300 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.358 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.089 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.090 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.091 I llama_model_loader: - type  f32:  194 tensors
0.00.023.091 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.092 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.070 I llm_load_vocab: special tokens cache size = 25
0.00.050.136 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.138 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.139 I llm_load_print_meta: arch             = gptneox
0.00.050.139 I llm_load_print_meta: vocab type       = BPE
0.00.050.139 I llm_load_print_meta: n_vocab          = 50304
0.00.050.140 I llm_load_print_meta: n_merges         = 50009
0.00.050.140 I llm_load_print_meta: vocab_only       = 0
0.00.050.140 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.140 I llm_load_print_meta: n_embd           = 2048
0.00.050.140 I llm_load_print_meta: n_layer          = 24
0.00.050.143 I llm_load_print_meta: n_head           = 16
0.00.050.144 I llm_load_print_meta: n_head_kv        = 16
0.00.050.144 I llm_load_print_meta: n_rot            = 32
0.00.050.144 I llm_load_print_meta: n_swa            = 0
0.00.050.147 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.147 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.147 I llm_load_print_meta: n_gqa            = 1
0.00.050.148 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.149 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.150 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.150 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.150 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.150 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.150 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.151 I llm_load_print_meta: n_ff             = 8192
0.00.050.163 I llm_load_print_meta: n_expert         = 0
0.00.050.163 I llm_load_print_meta: n_expert_used    = 0
0.00.050.163 I llm_load_print_meta: causal attn      = 1
0.00.050.163 I llm_load_print_meta: pooling type     = 0
0.00.050.163 I llm_load_print_meta: rope type        = 2
0.00.050.165 I llm_load_print_meta: rope scaling     = linear
0.00.050.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.165 I llm_load_print_meta: freq_scale_train = 1
0.00.050.165 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.165 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.166 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.166 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.166 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.166 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.166 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.167 I llm_load_print_meta: model type       = 1.4B
0.00.050.167 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.167 I llm_load_print_meta: model params     = 1.41 B
0.00.050.168 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.168 I llm_load_print_meta: general.name     = 1.4B
0.00.050.168 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.168 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.168 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.168 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.169 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.169 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.169 I llm_load_print_meta: max token length = 1024
0.00.052.148 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.148 I llm_load_tensors: offloading output layer to GPU
0.00.052.149 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.159 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.161 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.079 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.079 I llama_new_context_with_model: n_ctx         = 128
0.00.053.079 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.080 I llama_new_context_with_model: n_batch       = 128
0.00.053.080 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.080 I llama_new_context_with_model: flash_attn    = 0
0.00.053.080 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.080 I llama_new_context_with_model: freq_scale    = 1
0.00.053.081 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.081 I ggml_metal_init: allocating
0.00.053.084 I ggml_metal_init: found device: Apple M4
0.00.053.086 I ggml_metal_init: picking default device: Apple M4
0.00.053.668 I ggml_metal_init: using embedded metal library
0.00.055.996 I ggml_metal_init: GPU name:   Apple M4
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.998 I ggml_metal_init: simdgroup reduction   = true
0.00.055.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.999 I ggml_metal_init: has bfloat            = true
0.00.055.999 I ggml_metal_init: use bfloat            = true
0.00.055.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.724 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.014 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.017 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.041 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.967 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.968 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.968 I llama_new_context_with_model: graph nodes  = 967
0.00.067.968 I llama_new_context_with_model: graph splits = 2
0.00.067.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.900 I 
0.00.670.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.952 I perplexity: tokenizing the input ..
0.00.679.082 I perplexity: tokenization took 8.128 ms
0.00.679.085 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.921 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.803.084 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.803.099 I llama_perf_context_print:        load time =     662.34 ms
0.00.803.100 I llama_perf_context_print: prompt eval time =     122.61 ms /   128 tokens (    0.96 ms per token,  1043.96 tokens per second)
0.00.803.101 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.101 I llama_perf_context_print:       total time =     132.20 ms /   129 tokens
0.00.803.444 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.079s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.353 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.022 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.026 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.033 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.034 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.036 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.036 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.847 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.738 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.739 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.739 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.740 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.740 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.741 I llama_model_loader: - type  f32:  194 tensors
0.00.024.741 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.741 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.963 I llm_load_vocab: special tokens cache size = 25
0.00.050.798 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.800 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.800 I llm_load_print_meta: arch             = gptneox
0.00.050.801 I llm_load_print_meta: vocab type       = BPE
0.00.050.801 I llm_load_print_meta: n_vocab          = 50304
0.00.050.801 I llm_load_print_meta: n_merges         = 50009
0.00.050.801 I llm_load_print_meta: vocab_only       = 0
0.00.050.802 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.802 I llm_load_print_meta: n_embd           = 2048
0.00.050.802 I llm_load_print_meta: n_layer          = 24
0.00.050.805 I llm_load_print_meta: n_head           = 16
0.00.050.805 I llm_load_print_meta: n_head_kv        = 16
0.00.050.807 I llm_load_print_meta: n_rot            = 32
0.00.050.808 I llm_load_print_meta: n_swa            = 0
0.00.050.808 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.808 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.809 I llm_load_print_meta: n_gqa            = 1
0.00.050.809 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.810 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.811 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.811 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.811 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.811 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.811 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.812 I llm_load_print_meta: n_ff             = 8192
0.00.050.824 I llm_load_print_meta: n_expert         = 0
0.00.050.824 I llm_load_print_meta: n_expert_used    = 0
0.00.050.824 I llm_load_print_meta: causal attn      = 1
0.00.050.824 I llm_load_print_meta: pooling type     = 0
0.00.050.824 I llm_load_print_meta: rope type        = 2
0.00.050.824 I llm_load_print_meta: rope scaling     = linear
0.00.050.825 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.825 I llm_load_print_meta: freq_scale_train = 1
0.00.050.825 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.825 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.825 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.825 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.826 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.826 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.826 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.828 I llm_load_print_meta: model type       = 1.4B
0.00.050.828 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.828 I llm_load_print_meta: model params     = 1.41 B
0.00.050.829 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.829 I llm_load_print_meta: general.name     = 1.4B
0.00.050.829 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.829 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.829 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.830 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.830 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.830 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.830 I llm_load_print_meta: max token length = 1024
0.00.052.792 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.792 I llm_load_tensors: offloading output layer to GPU
0.00.052.793 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.803 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.804 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.694 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.695 I llama_new_context_with_model: n_ctx         = 128
0.00.053.695 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.696 I llama_new_context_with_model: n_batch       = 128
0.00.053.696 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.696 I llama_new_context_with_model: flash_attn    = 0
0.00.053.696 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.696 I llama_new_context_with_model: freq_scale    = 1
0.00.053.697 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.697 I ggml_metal_init: allocating
0.00.053.701 I ggml_metal_init: found device: Apple M4
0.00.053.703 I ggml_metal_init: picking default device: Apple M4
0.00.054.272 I ggml_metal_init: using embedded metal library
0.00.056.574 I ggml_metal_init: GPU name:   Apple M4
0.00.056.576 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.577 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.577 I ggml_metal_init: simdgroup reduction   = true
0.00.056.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.577 I ggml_metal_init: has bfloat            = true
0.00.056.577 I ggml_metal_init: use bfloat            = true
0.00.056.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.579 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.059 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.345 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.347 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.373 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.222 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.223 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.224 I llama_new_context_with_model: graph nodes  = 967
0.00.068.224 I llama_new_context_with_model: graph splits = 2
0.00.068.225 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.394 I 
0.00.651.432 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.444 I perplexity: tokenizing the input ..
0.00.659.330 I perplexity: tokenization took 7.885 ms
0.00.659.333 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.668 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.795.999 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.796.013 I llama_perf_context_print:        load time =     641.04 ms
0.00.796.014 I llama_perf_context_print: prompt eval time =     135.10 ms /   128 tokens (    1.06 ms per token,   947.47 tokens per second)
0.00.796.014 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.015 I llama_perf_context_print:       total time =     144.62 ms /   129 tokens
0.00.796.410 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.077s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.663 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.595 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.605 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.606 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.611 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.611 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.611 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.612 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.612 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.612 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.614 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.614 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.367 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.369 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.371 I llama_model_loader: - type  f32:  194 tensors
0.00.025.371 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.371 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.227 I llm_load_vocab: special tokens cache size = 25
0.00.052.212 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.215 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.215 I llm_load_print_meta: arch             = gptneox
0.00.052.215 I llm_load_print_meta: vocab type       = BPE
0.00.052.215 I llm_load_print_meta: n_vocab          = 50304
0.00.052.216 I llm_load_print_meta: n_merges         = 50009
0.00.052.216 I llm_load_print_meta: vocab_only       = 0
0.00.052.216 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.216 I llm_load_print_meta: n_embd           = 2048
0.00.052.216 I llm_load_print_meta: n_layer          = 24
0.00.052.219 I llm_load_print_meta: n_head           = 16
0.00.052.220 I llm_load_print_meta: n_head_kv        = 16
0.00.052.220 I llm_load_print_meta: n_rot            = 32
0.00.052.221 I llm_load_print_meta: n_swa            = 0
0.00.052.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.222 I llm_load_print_meta: n_gqa            = 1
0.00.052.223 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.223 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.224 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.224 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.225 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.225 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.225 I llm_load_print_meta: n_ff             = 8192
0.00.052.237 I llm_load_print_meta: n_expert         = 0
0.00.052.237 I llm_load_print_meta: n_expert_used    = 0
0.00.052.238 I llm_load_print_meta: causal attn      = 1
0.00.052.238 I llm_load_print_meta: pooling type     = 0
0.00.052.238 I llm_load_print_meta: rope type        = 2
0.00.052.238 I llm_load_print_meta: rope scaling     = linear
0.00.052.238 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.239 I llm_load_print_meta: freq_scale_train = 1
0.00.052.239 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.239 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.239 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.239 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.239 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.240 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.240 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.240 I llm_load_print_meta: model type       = 1.4B
0.00.052.240 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.241 I llm_load_print_meta: model params     = 1.41 B
0.00.052.241 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.241 I llm_load_print_meta: general.name     = 1.4B
0.00.052.242 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.242 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.242 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.242 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.242 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.243 I llm_load_print_meta: max token length = 1024
0.00.054.599 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.599 I llm_load_tensors: offloading output layer to GPU
0.00.054.600 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.610 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.611 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.591 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.592 I llama_new_context_with_model: n_ctx         = 128
0.00.055.592 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.593 I llama_new_context_with_model: n_batch       = 128
0.00.055.593 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.593 I llama_new_context_with_model: flash_attn    = 0
0.00.055.593 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.594 I llama_new_context_with_model: freq_scale    = 1
0.00.055.594 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.595 I ggml_metal_init: allocating
0.00.055.601 I ggml_metal_init: found device: Apple M4
0.00.055.603 I ggml_metal_init: picking default device: Apple M4
0.00.056.180 I ggml_metal_init: using embedded metal library
0.00.058.888 I ggml_metal_init: GPU name:   Apple M4
0.00.058.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.890 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.890 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.891 I ggml_metal_init: simdgroup reduction   = true
0.00.058.891 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.891 I ggml_metal_init: has bfloat            = true
0.00.058.891 I ggml_metal_init: use bfloat            = true
0.00.058.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.892 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.900 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.203 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.207 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.245 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.151 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.152 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.152 I llama_new_context_with_model: graph nodes  = 967
0.00.071.152 I llama_new_context_with_model: graph splits = 2
0.00.071.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.370 I 
0.00.709.399 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.409 I perplexity: tokenizing the input ..
0.00.718.046 I perplexity: tokenization took 8.635 ms
0.00.718.050 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.852.371 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.853.861 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.853.878 I llama_perf_context_print:        load time =     698.70 ms
0.00.853.878 I llama_perf_context_print: prompt eval time =     134.08 ms /   128 tokens (    1.05 ms per token,   954.63 tokens per second)
0.00.853.879 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.881 I llama_perf_context_print:       total time =     144.51 ms /   129 tokens
0.00.854.244 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.079s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.963 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.655 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.019.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.663 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.663 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.663 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.664 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.665 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.665 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.666 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.666 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.666 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.668 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.668 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.583 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.543 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.544 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.544 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.028.545 I llama_model_loader: - type  f32:  194 tensors
0.00.028.545 I llama_model_loader: - type q2_K:   49 tensors
0.00.028.546 I llama_model_loader: - type q3_K:   48 tensors
0.00.028.546 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.364 I llm_load_vocab: special tokens cache size = 25
0.00.056.418 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.422 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.423 I llm_load_print_meta: arch             = gptneox
0.00.056.423 I llm_load_print_meta: vocab type       = BPE
0.00.056.423 I llm_load_print_meta: n_vocab          = 50304
0.00.056.423 I llm_load_print_meta: n_merges         = 50009
0.00.056.424 I llm_load_print_meta: vocab_only       = 0
0.00.056.424 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.424 I llm_load_print_meta: n_embd           = 2048
0.00.056.424 I llm_load_print_meta: n_layer          = 24
0.00.056.431 I llm_load_print_meta: n_head           = 16
0.00.056.432 I llm_load_print_meta: n_head_kv        = 16
0.00.056.432 I llm_load_print_meta: n_rot            = 32
0.00.056.432 I llm_load_print_meta: n_swa            = 0
0.00.056.432 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.432 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.434 I llm_load_print_meta: n_gqa            = 1
0.00.056.434 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.435 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.435 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.436 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.436 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.436 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.436 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.436 I llm_load_print_meta: n_ff             = 8192
0.00.056.450 I llm_load_print_meta: n_expert         = 0
0.00.056.450 I llm_load_print_meta: n_expert_used    = 0
0.00.056.450 I llm_load_print_meta: causal attn      = 1
0.00.056.450 I llm_load_print_meta: pooling type     = 0
0.00.056.450 I llm_load_print_meta: rope type        = 2
0.00.056.451 I llm_load_print_meta: rope scaling     = linear
0.00.056.451 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.451 I llm_load_print_meta: freq_scale_train = 1
0.00.056.451 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.452 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.452 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.452 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.452 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.452 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.452 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.453 I llm_load_print_meta: model type       = 1.4B
0.00.056.453 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.056.453 I llm_load_print_meta: model params     = 1.41 B
0.00.056.454 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.056.454 I llm_load_print_meta: general.name     = 1.4B
0.00.056.454 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.454 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.454 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.455 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.455 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.455 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.455 I llm_load_print_meta: max token length = 1024
0.00.058.306 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.307 I llm_load_tensors: offloading output layer to GPU
0.00.058.307 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.318 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.058.319 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.059.313 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.313 I llama_new_context_with_model: n_ctx         = 128
0.00.059.313 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.059.314 I llama_new_context_with_model: n_batch       = 128
0.00.059.314 I llama_new_context_with_model: n_ubatch      = 128
0.00.059.314 I llama_new_context_with_model: flash_attn    = 0
0.00.059.314 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.315 I llama_new_context_with_model: freq_scale    = 1
0.00.059.315 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.059.316 I ggml_metal_init: allocating
0.00.059.322 I ggml_metal_init: found device: Apple M4
0.00.059.325 I ggml_metal_init: picking default device: Apple M4
0.00.059.903 I ggml_metal_init: using embedded metal library
0.00.062.277 I ggml_metal_init: GPU name:   Apple M4
0.00.062.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.280 I ggml_metal_init: simdgroup reduction   = true
0.00.062.280 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.280 I ggml_metal_init: has bfloat            = true
0.00.062.280 I ggml_metal_init: use bfloat            = true
0.00.062.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.282 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.378 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.073.669 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.672 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.698 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.074.547 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.074.548 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.074.548 I llama_new_context_with_model: graph nodes  = 967
0.00.074.548 I llama_new_context_with_model: graph splits = 2
0.00.074.550 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.074.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.388.018 I 
0.00.388.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.062 I perplexity: tokenizing the input ..
0.00.395.995 I perplexity: tokenization took 7.931 ms
0.00.395.999 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.527.565 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.528.985 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.001 I llama_perf_context_print:        load time =     374.05 ms
0.00.529.002 I llama_perf_context_print: prompt eval time =     131.31 ms /   128 tokens (    1.03 ms per token,   974.76 tokens per second)
0.00.529.002 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.003 I llama_perf_context_print:       total time =     140.98 ms /   129 tokens
0.00.529.365 I ggml_metal_free: deallocating

real	0m0.548s
user	0m0.081s
sys	0m0.063s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.682 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.501 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.506 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.513 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.513 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.514 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.516 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.518 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.523 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.396 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.396 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.397 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.398 I llama_model_loader: - type  f32:  194 tensors
0.00.023.398 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.398 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.398 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.399 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.901 I llm_load_vocab: special tokens cache size = 25
0.00.051.011 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.017 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.019 I llm_load_print_meta: arch             = gptneox
0.00.051.020 I llm_load_print_meta: vocab type       = BPE
0.00.051.020 I llm_load_print_meta: n_vocab          = 50304
0.00.051.020 I llm_load_print_meta: n_merges         = 50009
0.00.051.020 I llm_load_print_meta: vocab_only       = 0
0.00.051.020 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.021 I llm_load_print_meta: n_embd           = 2048
0.00.051.021 I llm_load_print_meta: n_layer          = 24
0.00.051.025 I llm_load_print_meta: n_head           = 16
0.00.051.026 I llm_load_print_meta: n_head_kv        = 16
0.00.051.026 I llm_load_print_meta: n_rot            = 32
0.00.051.026 I llm_load_print_meta: n_swa            = 0
0.00.051.026 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.027 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.027 I llm_load_print_meta: n_gqa            = 1
0.00.051.028 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.028 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.029 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.029 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.029 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.029 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.029 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.030 I llm_load_print_meta: n_ff             = 8192
0.00.051.044 I llm_load_print_meta: n_expert         = 0
0.00.051.044 I llm_load_print_meta: n_expert_used    = 0
0.00.051.044 I llm_load_print_meta: causal attn      = 1
0.00.051.044 I llm_load_print_meta: pooling type     = 0
0.00.051.044 I llm_load_print_meta: rope type        = 2
0.00.051.045 I llm_load_print_meta: rope scaling     = linear
0.00.051.046 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.047 I llm_load_print_meta: freq_scale_train = 1
0.00.051.047 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.047 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.047 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.047 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.049 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.049 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.049 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.049 I llm_load_print_meta: model type       = 1.4B
0.00.051.050 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.050 I llm_load_print_meta: model params     = 1.41 B
0.00.051.051 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.051 I llm_load_print_meta: general.name     = 1.4B
0.00.051.051 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.051 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.053 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.053 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.053 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.053 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.053 I llm_load_print_meta: max token length = 1024
0.00.052.867 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.867 I llm_load_tensors: offloading output layer to GPU
0.00.052.867 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.878 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.880 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.795 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.796 I llama_new_context_with_model: n_ctx         = 128
0.00.053.796 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.796 I llama_new_context_with_model: n_batch       = 128
0.00.053.796 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.796 I llama_new_context_with_model: flash_attn    = 0
0.00.053.797 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.798 I llama_new_context_with_model: freq_scale    = 1
0.00.053.799 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.799 I ggml_metal_init: allocating
0.00.053.804 I ggml_metal_init: found device: Apple M4
0.00.053.806 I ggml_metal_init: picking default device: Apple M4
0.00.054.408 I ggml_metal_init: using embedded metal library
0.00.056.753 I ggml_metal_init: GPU name:   Apple M4
0.00.056.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.755 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.756 I ggml_metal_init: simdgroup reduction   = true
0.00.056.756 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.756 I ggml_metal_init: has bfloat            = true
0.00.056.756 I ggml_metal_init: use bfloat            = true
0.00.056.757 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.141 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.536 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.539 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.567 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.463 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.464 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.464 I llama_new_context_with_model: graph nodes  = 967
0.00.069.464 I llama_new_context_with_model: graph splits = 2
0.00.069.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.489.966 I 
0.00.490.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.027 I perplexity: tokenizing the input ..
0.00.498.111 I perplexity: tokenization took 8.082 ms
0.00.498.114 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.629.485 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.630.661 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.630.670 I llama_perf_context_print:        load time =     481.28 ms
0.00.630.672 I llama_perf_context_print: prompt eval time =     131.15 ms /   128 tokens (    1.02 ms per token,   976.02 tokens per second)
0.00.630.673 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.630.673 I llama_perf_context_print:       total time =     140.71 ms /   129 tokens
0.00.631.130 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.080s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.819 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.336 I llama_model_loader: - type  f32:  194 tensors
0.00.023.337 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.337 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.337 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.506 I llm_load_vocab: special tokens cache size = 25
0.00.049.307 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.310 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.310 I llm_load_print_meta: arch             = gptneox
0.00.049.310 I llm_load_print_meta: vocab type       = BPE
0.00.049.310 I llm_load_print_meta: n_vocab          = 50304
0.00.049.311 I llm_load_print_meta: n_merges         = 50009
0.00.049.311 I llm_load_print_meta: vocab_only       = 0
0.00.049.311 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.311 I llm_load_print_meta: n_embd           = 2048
0.00.049.311 I llm_load_print_meta: n_layer          = 24
0.00.049.314 I llm_load_print_meta: n_head           = 16
0.00.049.315 I llm_load_print_meta: n_head_kv        = 16
0.00.049.315 I llm_load_print_meta: n_rot            = 32
0.00.049.315 I llm_load_print_meta: n_swa            = 0
0.00.049.315 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.316 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.316 I llm_load_print_meta: n_gqa            = 1
0.00.049.317 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.320 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.321 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.321 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.328 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.330 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.331 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.338 I llm_load_print_meta: n_ff             = 8192
0.00.049.350 I llm_load_print_meta: n_expert         = 0
0.00.049.350 I llm_load_print_meta: n_expert_used    = 0
0.00.049.350 I llm_load_print_meta: causal attn      = 1
0.00.049.350 I llm_load_print_meta: pooling type     = 0
0.00.049.351 I llm_load_print_meta: rope type        = 2
0.00.049.352 I llm_load_print_meta: rope scaling     = linear
0.00.049.352 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.353 I llm_load_print_meta: freq_scale_train = 1
0.00.049.353 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.353 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.353 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.353 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.355 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.355 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.355 I llm_load_print_meta: model type       = 1.4B
0.00.049.355 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.356 I llm_load_print_meta: model params     = 1.41 B
0.00.049.356 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.356 I llm_load_print_meta: general.name     = 1.4B
0.00.049.356 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.357 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.357 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.357 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.357 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.357 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.357 I llm_load_print_meta: max token length = 1024
0.00.051.307 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.307 I llm_load_tensors: offloading output layer to GPU
0.00.051.307 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.318 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.319 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.198 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.199 I llama_new_context_with_model: n_ctx         = 128
0.00.052.199 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.200 I llama_new_context_with_model: n_batch       = 128
0.00.052.200 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.200 I llama_new_context_with_model: flash_attn    = 0
0.00.052.200 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.201 I llama_new_context_with_model: freq_scale    = 1
0.00.052.201 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.202 I ggml_metal_init: allocating
0.00.052.208 I ggml_metal_init: found device: Apple M4
0.00.052.211 I ggml_metal_init: picking default device: Apple M4
0.00.052.793 I ggml_metal_init: using embedded metal library
0.00.055.132 I ggml_metal_init: GPU name:   Apple M4
0.00.055.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.134 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.134 I ggml_metal_init: simdgroup reduction   = true
0.00.055.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.134 I ggml_metal_init: has bfloat            = true
0.00.055.134 I ggml_metal_init: use bfloat            = true
0.00.055.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.557 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.858 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.863 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.890 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.753 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.754 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.754 I llama_new_context_with_model: graph nodes  = 967
0.00.066.755 I llama_new_context_with_model: graph splits = 2
0.00.066.756 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.756 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.562.439 I 
0.00.562.476 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.562.488 I perplexity: tokenizing the input ..
0.00.570.240 I perplexity: tokenization took 7.75 ms
0.00.570.243 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.037 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.706.325 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.706.338 I llama_perf_context_print:        load time =     553.62 ms
0.00.706.339 I llama_perf_context_print: prompt eval time =     134.57 ms /   128 tokens (    1.05 ms per token,   951.21 tokens per second)
0.00.706.340 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.706.340 I llama_perf_context_print:       total time =     143.90 ms /   129 tokens
0.00.706.845 I ggml_metal_free: deallocating

real	0m0.721s
user	0m0.077s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.521 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.247 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.258 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.258 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.259 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.973 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.978 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.731 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.733 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.733 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.734 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.734 I llama_model_loader: - type  f32:  194 tensors
0.00.023.734 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.735 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.783 I llm_load_vocab: special tokens cache size = 25
0.00.049.612 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.615 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.616 I llm_load_print_meta: arch             = gptneox
0.00.049.616 I llm_load_print_meta: vocab type       = BPE
0.00.049.616 I llm_load_print_meta: n_vocab          = 50304
0.00.049.616 I llm_load_print_meta: n_merges         = 50009
0.00.049.617 I llm_load_print_meta: vocab_only       = 0
0.00.049.617 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.617 I llm_load_print_meta: n_embd           = 2048
0.00.049.617 I llm_load_print_meta: n_layer          = 24
0.00.049.620 I llm_load_print_meta: n_head           = 16
0.00.049.621 I llm_load_print_meta: n_head_kv        = 16
0.00.049.621 I llm_load_print_meta: n_rot            = 32
0.00.049.621 I llm_load_print_meta: n_swa            = 0
0.00.049.621 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.622 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.622 I llm_load_print_meta: n_gqa            = 1
0.00.049.623 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.624 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.626 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.626 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.627 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.627 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.627 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.628 I llm_load_print_meta: n_ff             = 8192
0.00.049.640 I llm_load_print_meta: n_expert         = 0
0.00.049.640 I llm_load_print_meta: n_expert_used    = 0
0.00.049.640 I llm_load_print_meta: causal attn      = 1
0.00.049.640 I llm_load_print_meta: pooling type     = 0
0.00.049.640 I llm_load_print_meta: rope type        = 2
0.00.049.641 I llm_load_print_meta: rope scaling     = linear
0.00.049.642 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.644 I llm_load_print_meta: freq_scale_train = 1
0.00.049.644 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.644 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.645 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.645 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.645 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.645 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.645 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.646 I llm_load_print_meta: model type       = 1.4B
0.00.049.646 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.646 I llm_load_print_meta: model params     = 1.41 B
0.00.049.647 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.647 I llm_load_print_meta: general.name     = 1.4B
0.00.049.647 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.648 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.648 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.648 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.648 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.649 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.649 I llm_load_print_meta: max token length = 1024
0.00.051.676 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.676 I llm_load_tensors: offloading output layer to GPU
0.00.051.676 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.687 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.688 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.655 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.656 I llama_new_context_with_model: n_ctx         = 128
0.00.052.656 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.656 I llama_new_context_with_model: n_batch       = 128
0.00.052.656 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.657 I llama_new_context_with_model: flash_attn    = 0
0.00.052.657 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.657 I llama_new_context_with_model: freq_scale    = 1
0.00.052.658 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.658 I ggml_metal_init: allocating
0.00.052.664 I ggml_metal_init: found device: Apple M4
0.00.052.668 I ggml_metal_init: picking default device: Apple M4
0.00.053.244 I ggml_metal_init: using embedded metal library
0.00.055.638 I ggml_metal_init: GPU name:   Apple M4
0.00.055.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.640 I ggml_metal_init: simdgroup reduction   = true
0.00.055.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.642 I ggml_metal_init: has bfloat            = true
0.00.055.642 I ggml_metal_init: use bfloat            = true
0.00.055.643 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.126 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.432 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.435 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.462 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.402 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.403 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.403 I llama_new_context_with_model: graph nodes  = 967
0.00.067.404 I llama_new_context_with_model: graph splits = 2
0.00.067.405 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.405 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.409 I 
0.00.638.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.494 I perplexity: tokenizing the input ..
0.00.646.618 I perplexity: tokenization took 8.123 ms
0.00.646.626 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.337 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.788.490 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.788.505 I llama_perf_context_print:        load time =     628.88 ms
0.00.788.506 I llama_perf_context_print: prompt eval time =     140.48 ms /   128 tokens (    1.10 ms per token,   911.18 tokens per second)
0.00.788.507 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.507 I llama_perf_context_print:       total time =     150.10 ms /   129 tokens
0.00.788.966 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.077s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.293 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.986 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.991 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.992 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.993 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.994 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.994 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.996 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.997 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.997 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.005 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.685 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.686 I llama_model_loader: - type  f32:  194 tensors
0.00.024.686 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.743 I llm_load_vocab: special tokens cache size = 25
0.00.051.692 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.695 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.695 I llm_load_print_meta: arch             = gptneox
0.00.051.695 I llm_load_print_meta: vocab type       = BPE
0.00.051.696 I llm_load_print_meta: n_vocab          = 50304
0.00.051.696 I llm_load_print_meta: n_merges         = 50009
0.00.051.696 I llm_load_print_meta: vocab_only       = 0
0.00.051.696 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.696 I llm_load_print_meta: n_embd           = 2048
0.00.051.696 I llm_load_print_meta: n_layer          = 24
0.00.051.700 I llm_load_print_meta: n_head           = 16
0.00.051.700 I llm_load_print_meta: n_head_kv        = 16
0.00.051.701 I llm_load_print_meta: n_rot            = 32
0.00.051.701 I llm_load_print_meta: n_swa            = 0
0.00.051.703 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.703 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.704 I llm_load_print_meta: n_gqa            = 1
0.00.051.705 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.706 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.706 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.707 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.707 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.707 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.707 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.708 I llm_load_print_meta: n_ff             = 8192
0.00.051.719 I llm_load_print_meta: n_expert         = 0
0.00.051.720 I llm_load_print_meta: n_expert_used    = 0
0.00.051.720 I llm_load_print_meta: causal attn      = 1
0.00.051.721 I llm_load_print_meta: pooling type     = 0
0.00.051.721 I llm_load_print_meta: rope type        = 2
0.00.051.721 I llm_load_print_meta: rope scaling     = linear
0.00.051.721 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.722 I llm_load_print_meta: freq_scale_train = 1
0.00.051.722 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.722 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.722 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.722 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.722 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.723 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.723 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.723 I llm_load_print_meta: model type       = 1.4B
0.00.051.724 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.724 I llm_load_print_meta: model params     = 1.41 B
0.00.051.724 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.724 I llm_load_print_meta: general.name     = 1.4B
0.00.051.725 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.726 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.726 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.726 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.726 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.727 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.728 I llm_load_print_meta: max token length = 1024
0.00.053.732 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.732 I llm_load_tensors: offloading output layer to GPU
0.00.053.733 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.743 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.744 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.637 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.637 I llama_new_context_with_model: n_ctx         = 128
0.00.054.638 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.638 I llama_new_context_with_model: n_batch       = 128
0.00.054.638 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.638 I llama_new_context_with_model: flash_attn    = 0
0.00.054.638 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.639 I llama_new_context_with_model: freq_scale    = 1
0.00.054.639 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.640 I ggml_metal_init: allocating
0.00.054.643 I ggml_metal_init: found device: Apple M4
0.00.054.645 I ggml_metal_init: picking default device: Apple M4
0.00.055.233 I ggml_metal_init: using embedded metal library
0.00.057.547 I ggml_metal_init: GPU name:   Apple M4
0.00.057.548 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.549 I ggml_metal_init: simdgroup reduction   = true
0.00.057.549 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.549 I ggml_metal_init: has bfloat            = true
0.00.057.550 I ggml_metal_init: use bfloat            = true
0.00.057.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.344 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.601 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.604 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.629 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.534 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.535 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.535 I llama_new_context_with_model: graph nodes  = 967
0.00.069.536 I llama_new_context_with_model: graph splits = 2
0.00.069.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.537 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.492 I 
0.00.356.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.537 I perplexity: tokenizing the input ..
0.00.364.502 I perplexity: tokenization took 7.964 ms
0.00.364.505 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.504.944 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.506.184 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.506.211 I llama_perf_context_print:        load time =     346.19 ms
0.00.506.212 I llama_perf_context_print: prompt eval time =     140.13 ms /   128 tokens (    1.09 ms per token,   913.46 tokens per second)
0.00.506.214 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.506.214 I llama_perf_context_print:       total time =     149.72 ms /   129 tokens
0.00.506.696 I ggml_metal_free: deallocating

real	0m0.520s
user	0m0.078s
sys	0m0.074s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.250 I build: 4400 (bb0b2c4f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.986 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.255 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.266 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.266 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.267 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.268 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.271 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.271 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.274 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.275 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.760 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.384 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.386 I llama_model_loader: - type  f32:  194 tensors
0.00.051.386 I llama_model_loader: - type  f16:   98 tensors
0.00.079.033 I llm_load_vocab: special tokens cache size = 25
0.00.085.172 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.175 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.175 I llm_load_print_meta: arch             = gptneox
0.00.085.175 I llm_load_print_meta: vocab type       = BPE
0.00.085.175 I llm_load_print_meta: n_vocab          = 50304
0.00.085.176 I llm_load_print_meta: n_merges         = 50009
0.00.085.176 I llm_load_print_meta: vocab_only       = 0
0.00.085.176 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.176 I llm_load_print_meta: n_embd           = 2048
0.00.085.176 I llm_load_print_meta: n_layer          = 24
0.00.085.179 I llm_load_print_meta: n_head           = 16
0.00.085.180 I llm_load_print_meta: n_head_kv        = 16
0.00.085.180 I llm_load_print_meta: n_rot            = 32
0.00.085.180 I llm_load_print_meta: n_swa            = 0
0.00.085.180 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.180 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.182 I llm_load_print_meta: n_gqa            = 1
0.00.085.183 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.183 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.184 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.184 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.184 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.184 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.185 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.185 I llm_load_print_meta: n_ff             = 8192
0.00.085.196 I llm_load_print_meta: n_expert         = 0
0.00.085.197 I llm_load_print_meta: n_expert_used    = 0
0.00.085.197 I llm_load_print_meta: causal attn      = 1
0.00.085.198 I llm_load_print_meta: pooling type     = 0
0.00.085.199 I llm_load_print_meta: rope type        = 2
0.00.085.199 I llm_load_print_meta: rope scaling     = linear
0.00.085.199 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.199 I llm_load_print_meta: freq_scale_train = 1
0.00.085.199 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.200 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.200 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.200 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.200 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.200 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.200 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.204 I llm_load_print_meta: model type       = 1.4B
0.00.085.205 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.205 I llm_load_print_meta: model params     = 1.41 B
0.00.085.205 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.206 I llm_load_print_meta: general.name     = 1.4B
0.00.085.207 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.207 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.207 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.207 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.207 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.208 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.208 I llm_load_print_meta: max token length = 1024
0.00.087.656 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.656 I llm_load_tensors: offloading output layer to GPU
0.00.087.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.666 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.667 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.627 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.628 I llama_new_context_with_model: n_ctx         = 128
0.00.088.628 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.629 I llama_new_context_with_model: n_batch       = 128
0.00.088.629 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.629 I llama_new_context_with_model: flash_attn    = 0
0.00.088.629 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.629 I llama_new_context_with_model: freq_scale    = 1
0.00.088.630 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.630 I ggml_metal_init: allocating
0.00.088.633 I ggml_metal_init: found device: Apple M4
0.00.088.635 I ggml_metal_init: picking default device: Apple M4
0.00.089.224 I ggml_metal_init: using embedded metal library
0.00.091.671 I ggml_metal_init: GPU name:   Apple M4
0.00.091.673 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.673 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.674 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.674 I ggml_metal_init: simdgroup reduction   = true
0.00.091.674 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.674 I ggml_metal_init: has bfloat            = true
0.00.091.674 I ggml_metal_init: use bfloat            = true
0.00.091.675 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.676 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.410 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.695 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.699 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.728 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.596 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.597 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.597 I llama_new_context_with_model: graph nodes  = 967
0.00.102.597 I llama_new_context_with_model: graph splits = 2
0.00.102.599 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.599 I 
0.00.102.635 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.102.636 I compute_imatrix: tokenizing the input ..
0.00.109.354 I compute_imatrix: tokenization took 6.717 ms
0.00.109.356 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.603.953 I compute_imatrix: 1.49 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.606.354 I llama_perf_context_print:        load time =    1581.97 ms
0.01.606.355 I llama_perf_context_print: prompt eval time =    1493.95 ms /   128 tokens (   11.67 ms per token,    85.68 tokens per second)
0.01.606.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.606.356 I llama_perf_context_print:       total time =    1584.36 ms /   129 tokens
0.01.606.961 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.163s
sys	0m0.243s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4400 (bb0b2c4f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122f0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122f0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122f0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122f0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122f0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122f0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122f0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122f0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122f0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122f0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122f0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122f0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122f0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122f0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122f101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122f10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122f11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122f11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122f11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122f12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122f12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122f13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122f13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122f14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122f14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122f14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122f15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122f15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122f16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122f168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122f17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122f176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122f17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122f17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122f182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122f18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122f18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122f19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122f19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122f199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122f19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122f1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122f1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122f1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122f1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122f1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122f1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122f1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122f1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122f1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122f1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122f1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122f1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122f1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122f1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122f1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122f20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122f20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122f208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122f20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122f21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122f216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122f21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122f21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122f22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122f22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122f22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122f23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122f23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122f23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122f240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122f24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122f24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122f250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122f25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122f260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122f26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122f26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122f270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122f27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122f27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122f28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122f28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122f290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122f295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122f29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122f2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122f2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122f2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122f2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122f2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122f2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122f1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122f2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122f2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122f2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122f2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122f2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122f2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122f2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122f2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122f2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122f2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122f2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122f2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122f301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122f30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122f30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122f310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122f31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122f31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122f31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122f32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122f32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122f32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122f33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122f335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122f33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122f33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122f343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122f34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122f34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122f351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122f35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122f35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122f35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122f36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122f368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122f36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122f37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122f376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122f37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122f37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122f38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122f38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122f39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122f39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122f39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122f3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122f3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122f3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122f3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122f3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122f3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122f3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122f3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122f3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122f3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122f3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122f3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122f3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122f3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122f3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122f3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122f3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122f3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122f3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122f3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122f3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122f40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122f40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122f40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122f40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122f413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122f41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122f41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122f421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122f42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122f42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122f42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122f43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122f438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122f43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122f44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122f446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122f44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122f45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122f454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122f45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122f45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122f46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122f46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122f46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122f47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122f47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122f479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122f47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122f483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122f488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122f48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122f49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122f49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122f49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122f4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122f4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122f4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122f4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122f4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122f4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122f4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122f4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122f4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122f4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122f4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122f4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122f4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122f4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122f4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122f4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122f4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122f50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122f506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122f50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122f51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122f51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122f51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122f52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122f52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122f52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122f53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122f53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122f53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122f54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122f54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122f54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122f55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122f55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122f55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122f560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122f56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122f56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122f570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122f57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122f57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122f580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122f58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122f58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122f590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122f59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122f59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122f5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122f5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122f5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122f5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122f5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122f5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122f5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122f5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122f5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122f5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122f5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122f5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122f5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122f5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122f5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122f5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122f5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122f5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122f60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122f605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122f60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122f60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122f61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122f618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122f61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122f62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122f626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122f62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122f62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122f63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122f63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122f63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122f64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122f64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122f64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122f65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122f655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122f65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122f663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122f66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122f67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122f674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122f67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122f67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122f685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122e04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122e04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122e053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122e05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122e05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122e06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122e06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122e069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122e06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122e07380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122e077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122e07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122e08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122e09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122e09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122e0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122e0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122e0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122e0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122e0bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122e0c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122e0cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122e0d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122e0da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122e0e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122e0e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122e0e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122e0eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122e0efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122e0f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122e0f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122e0fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122e10220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122e104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122e10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122e10dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122e11230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122e116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122e11b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122e11f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122e123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122e12860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122e12cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122e13140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122e135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122e13a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122e13e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122e14300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122e14770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122e14be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122e15050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122e154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122e15930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122e15da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122e16210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122e16680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122e16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122e170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122e17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122e179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122e17e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122e182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122e18720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122e18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122e19000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122e19470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122e198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122e19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122e1a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122e1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122e1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122e1af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122e1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122e1b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122e1bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122e1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122e1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122e1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122e1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122e1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122e1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122e1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122e1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122e1e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122e1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122e1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122e1f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122e1f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122e1fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122e1fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122e20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122e207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122e20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122e210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122e21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122e21990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122e21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122e22270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122e226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122e22b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122e22fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122e23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122e238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122e23d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122e24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122e245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122e24a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122e24ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122e25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122e257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122e25c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122e26090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122e26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122e26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122e26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122e27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122e276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122e27b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122e27fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122e28410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122e28880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122e28cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122e29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122e295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122e29a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122e29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122e2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122e2a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122e2ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122e2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122e2b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122e2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122e2bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122e2c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122e2c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122e2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122e2cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122e2d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122e2d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122e2dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122e2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122e2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122e2ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122e2ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122e2f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122e2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122e2fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122e30050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122e304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122e30930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122e30da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122e31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122e31680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122e31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122e31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122e323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122e32840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122e32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122e33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122e33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122e33a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122e33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122e342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122e34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122e34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122e35030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122e35c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122e35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122e361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122e36650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122e36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122e36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122e373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122e37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122e37c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122e380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122e38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122e389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122e38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122e392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122e39720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122e39b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122e3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122e3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122e3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122e3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122e3b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122e3b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122e3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122e3bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122e3c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122e3c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122e3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122e3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122e3d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122e3d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122e3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122e3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122e3e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122e3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122e3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122e3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122e3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122e3fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122e40330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122e407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122e40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122e41080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122e415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122e41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122e42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122e428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122e42ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122e43460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122e43a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122e43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122e445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122e44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122e45120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122e456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122e45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122e46260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122e46820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122e46de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122e473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122e47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122e47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122e484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122e48aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122e49060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122e49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122e49be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122e4a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122e4a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122e4ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122e4b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122e4b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122e4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122e4c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122e4c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122e4cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122e4d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122e4db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122e4e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122e4e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122e4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122e4f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122e4f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122e4fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122e50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122e50920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122e50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122e514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122e51a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122e52020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122e525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122e52ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122e53160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122e53720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122e53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122e542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122e54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122e54e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122e553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122e559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122e55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122e56520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122e56ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122e56fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122e574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122e579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122e57ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122e583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122e588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122e58de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122e592e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122e597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122e59ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122e5a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122e5a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122e5abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122e5b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122e5b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122e5bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122e5c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122e5ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122e5d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122e5d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122e5e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122e5e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122e5e8d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122f68250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122f49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122f49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122f4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122f1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122f1d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122f1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122f4c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122f149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122f1b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122f1bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122f1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122f1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122f139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122f1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122f2c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122f677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122f16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122f16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122f4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122f4ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122f14fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122f15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122f15550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122f68a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122f68cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122f68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122f69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122f69500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122f697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122f69a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122f69d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122f6a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122f6a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122f6a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122f6a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122f6ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122f6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122f6b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122f6b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122f6b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122f6b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122f6bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122f6be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122f6c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122f6c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122f6c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122f6c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122f6cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122f6cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122f6d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122f6d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122f6d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122f6d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122f6dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122f6df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122f6e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122f6e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122f6e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122f6ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122f6ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122f6efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122f6f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122f6f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122f6f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122f6fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122f6fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122f70040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122f70300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122f705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122f70880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122f70b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122f70e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122f710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122f71380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122f71640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122f71900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122f71bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122f71e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122f72140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122f72400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122f726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122f72980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122f72c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122f72f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122f731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122f73480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122f73740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122f73a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122f73cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122f73f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122f74240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122f74500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122f747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122f74a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122f74d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122f75000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122f752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122f75580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122f75840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122f75b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122f75dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122f76080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122f76340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122f76600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122f768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122f76b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122f76e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122f77100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122f773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122f77680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122f77940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122f77c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122f77ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122f78180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122f78440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122f78700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122f789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122f78c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122f78f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122f79200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122f794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122f79780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122f79a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122f79d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122f79fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122f7a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122f7a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122f7a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122f7aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122f7ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122f7b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122f7b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122f7b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122f7b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122f7bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122f7be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122f7c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122f7c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122f7c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122f7c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122f7cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122f7ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122f7d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122f7d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122f7d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122f7d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122f7dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122f7df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122f7e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122f7e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122f7e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122f7ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122f7ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122f7ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122f7f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122f7f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122f7f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122f7fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122f7fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122f80000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122f802c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122f80580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122f80840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122f80b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122f80dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122f81080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122f81340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122f81600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122f818c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122f81b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122f81e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122f82100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122f823c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122f82680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122f82940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122f82c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122f82ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122f83180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122f83440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122f83700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122f839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122f83c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122f83f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122f84200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122f844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122f84780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122f84a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122f84d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122f84fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122f85280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122f85540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122f85800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122f85ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122f85d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122f86040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122f86300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122f865c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122f86880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122f86b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122f86e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122f870c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122f87380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122f87640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122f87900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122f87bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122f87e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122f88140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122f88400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122f889d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122f88f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122f89470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122f899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122f89f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122f8a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122f8a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122f8af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122f8b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122f8b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122f8bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122f8c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122f8c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122f8cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122f8d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122f8d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122f8ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122f8e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122f8e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122f8eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122f8f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122f8f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122f8feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122f90400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122f90950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122f90ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122f913f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122f91940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122f91e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122f923e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122f92930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122f92e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122f933d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122f93920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122f93e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122f943c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122f94910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122f94e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122f953b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122f95900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122f95e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122f963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122f968f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122f96e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122f97390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122f978e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122f97e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122f98380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122f988d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122f98e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122f99370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122f998c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122f99e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122f9a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122f9a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122f9ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122f9b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122f9b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122f9b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122f9bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122f9c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122f9c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122f9c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122f9cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122f9d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122f9d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122f9daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122f9df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122f9e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122f9e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122f9ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122f9f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122f9f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122f9f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122fa06a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122fa0dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122fa14e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122fa17a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122fa1c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122fa2210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122fa2820 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.832s
user	0m0.292s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4400 (bb0b2c4f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f60b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f60bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f60c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f60c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f60ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f60d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f60d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f60dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f60e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f60e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f60ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f60f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f60fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f610560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f6122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f6129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f6131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f6138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f614000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f614720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f614fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f6156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f6159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f617160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f617420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f6178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f617b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f618410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f618c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f6190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f6199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f619e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f61a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f61a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f61ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f61b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f61b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f61b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f61be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f61c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f61cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f61d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f61d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f61dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f61e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f61ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f61f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f61fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f61fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f620600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f620c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f621400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f6216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f621b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f6224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f622940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f622de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f6249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f6258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f6268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f627370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f6278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f627e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f628360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f6288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f6298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f629df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f62a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f62a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f62ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f62b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f62b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f62bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f62c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f62c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f62cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f61caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f62d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f62d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f62df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f62e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f62e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f62ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f62f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f62f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f62ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f630460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f6309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f630f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f631450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f6319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f6343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f6351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f6368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f6376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f6384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f63a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f63a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f63a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f63ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f63b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f63b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f63bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f63c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f63c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f63ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f63ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f63d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f63d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f63dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f63e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f63e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f63ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f63ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f63f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f63f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f63fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f6418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f6421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f642b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f643470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f643910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f643db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f6446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f6454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f645970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f645e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f6462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f6479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f647e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f6487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f648c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f6490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f649640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f649b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f64a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f64a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f64a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f64af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f64b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f64bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14f64c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14f64c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f64ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f64d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14f64d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f64de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f64e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f64e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f64ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f64f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f64f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f64feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f650400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f650950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f650ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f6513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f651940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f651e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f6523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f652930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f652e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f6533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f653920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f653e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f6543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f654910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f654e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f6553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f655900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f655e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f6563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f6568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f656e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f6578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f657e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f658380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f6588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f658e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f659370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f6598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f659e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f65a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f65a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f65ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f65b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f65b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f65bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f65c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f65c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f65cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f65d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f65d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f65ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f65e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f65e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f65edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f65f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f65f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f65fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f660300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f660850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f660da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f6612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f661840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f661d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14f662230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14f6626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f662b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f663010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f6634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f663950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f663df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f664290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f664730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f664bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f665510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f6659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f665e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f6662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f666840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f666f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f6684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f668780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14f668f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f669230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f669840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.153 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f704d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f7051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f705630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f705aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f705f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f706380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f7067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f706c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f7070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f707540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f7079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f7080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f708bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f709370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f709b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f70a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f70a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f70b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f70b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f70bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f70c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f70cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f70d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f70dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f70e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f70e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f70e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f70ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f70f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f70f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f70fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f70ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f7103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f710670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f710ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f710f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f7113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f711830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f711ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f712110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f712580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f7129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f712e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f7132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f713740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f713bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f714020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f714490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f714900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f714d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f7151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f715650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f715ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f715f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f7163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f716810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f716d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f717280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f7176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f717b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f718440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f7188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f718d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f719190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f719600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f719a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f719ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f71a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f71a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f71ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f71b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f71b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f71b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f71bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f71c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f71c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f71cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f71cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f71d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f71d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f71dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f71e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f71e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f71ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f71eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f71f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f71f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f71fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f720080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f7204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f720960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f720dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f721240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f7216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f721b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f721f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f722400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f722870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f722ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f723150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f7235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f723a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f723ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f724310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f724780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f724bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f725060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f7254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f725940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f725db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f726220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f726690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f726b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f726f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f7273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f727850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f727cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f728130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f7285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f728a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f728e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f7292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f729760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f729bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f72a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f72a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f72a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f72ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f72b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f72b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f72bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f72bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f72c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f72c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f72cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f72d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f72d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f72d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f72de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f72e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f72e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f72ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f72f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f72f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f72f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f72fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f7301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f730650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f730ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f730f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f7313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f731810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f731c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f7320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f732560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f7329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f732e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f7332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f733720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f733b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f734000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f734470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f7348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f734d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f7351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f735df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f7360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f736370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f7367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f736c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f7370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f737530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f7379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f737e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f738280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f7386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f738b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f738fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f739440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f7398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f739d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f73a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f73a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f73aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f73aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f73b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f73b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f73bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f73c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f73c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f73c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f73cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f73d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f73d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f73db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f73dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f73e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f73e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f73ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f73f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f73f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f73fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f740050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f7404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f740930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f740da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f741210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f741730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f741c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f7427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f742a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f743030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f7435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f744170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f744730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f744cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f7452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f745870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f7463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f7469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f746f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f747530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f747af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f7480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f748670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f748c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f7491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f7497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f749d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f74a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f74a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f74aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f74b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f74ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f74bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f74c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f74cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f74d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f74d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f74dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f74e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f74e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f74edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f74f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f74f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f74ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f7504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f750ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f751070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f751630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f751bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f7521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f752770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f752d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f7532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f7538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f753e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f754430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f7549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f754fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f755570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f755b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f7560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f7566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f756c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f757170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f757670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f757b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f758070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f758570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f758a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f758f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f759470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f759970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f75a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f75a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f75ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f75b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f75b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f75c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f75c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f75cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f75d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f75d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f75e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f75e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f75ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f75ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f74c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f74b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f748370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f745b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f755270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f752a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f7507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f74e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f7466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f743e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f748ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f74a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f74f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f74c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f754130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f747db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f751330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f74abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f74ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f7477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f755830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f7449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f7432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f745570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f755df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f74b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f7535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f7494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f74bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f74fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f747230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f7501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f7518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f7460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f7546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f751eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f74d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f756970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f744fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f7563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f744430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f754cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f74eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f750d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f753b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f752470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f74a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f741f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f704880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f75dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f70bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f75ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f75efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f75f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f75f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f75f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f75fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f760140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f760400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f7606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f760980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f760c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f760f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f7611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f761480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f761740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f761a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f761cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f761f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f762240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f762500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f7627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f762d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f762fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f763290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f763550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f763810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f763ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f763d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f764050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f764310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f7645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f764890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f764b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f764e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f7650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f765390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f765650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f765910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f765bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f765e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f766150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f766410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f7666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f766990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f766c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f766f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f7671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f767490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f767750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f767a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f767cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f767f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f768250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f768510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f7687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f768a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f768d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f769010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f7692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f769590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f769850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f769b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f769dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f76a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f76a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f76a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f76a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f76ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f76ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f76b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f76b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f76b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f76b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f76bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f76bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f76c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f76c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f76c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f76c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f76cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f76cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f76d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f76d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f76d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f76da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f76dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f76dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f76e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f76e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f76e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f76ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f76ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f76f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f76f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f76f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f76f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f76fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f76fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f7700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f770390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f770650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f770910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f770bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f770e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f771150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f771410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f7716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f771990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f771c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f771f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f7721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f772490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f772750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f772a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f772cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f772f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f773250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f773510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f7737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f773a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f773d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f774010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f7742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f774590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f774850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f774b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f774dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f775090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f775350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f775610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f7758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f775b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f775e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f776110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f7763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f776690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f776950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f776c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f776ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f777190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f777450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f777710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f7779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f777c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f777f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f778210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f7784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f778790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f778a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f778d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f778fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f779290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f779550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f779810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f779ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f779d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f77a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f77a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f77a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f77aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f77ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f77b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f77b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f77b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f77b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f77bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f77bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f77c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f77c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f77c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f77cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f77d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f77d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f77dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f77e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f77e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f77ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f77f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f77f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f77fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f780190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f7806e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f780c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f781180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f7816d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f781c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f782170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f7826c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f782c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f783160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f7836b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f783c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f784150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f7846a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f784bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f785140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f785690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f785be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f786130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f786680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f786bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f787120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f787670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f787bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f788110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f788660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f788bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f789100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f789650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f789ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f78a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f78a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f78ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f78b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f78b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f78b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f78bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f78be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f78c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f78c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f78cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f78d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f78d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f78d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f78dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f78e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f78e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f78ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f78ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f78f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f78f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f78fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f790980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f7910a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f7917c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f791a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f791ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f7924f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f792b00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.922s
user	0m0.245s
sys	0m0.141s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
