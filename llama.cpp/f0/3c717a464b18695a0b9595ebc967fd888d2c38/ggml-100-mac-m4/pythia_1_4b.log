Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.567s
user	0m0.846s
sys	0m1.253s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target llama-simple-chat
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Built target llava_static
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Built target test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-backend-ops
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target llama-batched-bench
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-passkey
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-parallel
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-cli
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative-simple
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-tts
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.074s
user	0m5.959s
sys	0m9.513s

main: quantize time =  4390.49 ms
main:    total time =  4390.49 ms

main: quantize time =  1962.02 ms
main:    total time =  1962.02 ms

main: quantize time =  1884.26 ms
main:    total time =  1884.26 ms

main: quantize time =  2625.34 ms
main:    total time =  2625.34 ms

main: quantize time =  2301.28 ms
main:    total time =  2301.28 ms

main: quantize time =  5010.31 ms
main:    total time =  5010.31 ms

main: quantize time =  5648.37 ms
main:    total time =  5648.37 ms

main: quantize time =  6757.51 ms
main:    total time =  6757.51 ms

main: quantize time =  5961.95 ms
main:    total time =  5961.95 ms

main: quantize time =  4631.52 ms
main:    total time =  4631.52 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.149 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.255 I main: llama backend init
0.00.000.260 I main: load the model and apply lora adapter, if any
0.00.027.016 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.865 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.878 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.882 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.883 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.902 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.903 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.903 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.176 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.216 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.218 I llama_model_loader: - type  f32:  194 tensors
0.00.056.218 I llama_model_loader: - type  f16:   98 tensors
0.00.087.889 I llm_load_vocab: special tokens cache size = 25
0.00.094.891 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.894 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.895 I llm_load_print_meta: arch             = gptneox
0.00.094.895 I llm_load_print_meta: vocab type       = BPE
0.00.094.895 I llm_load_print_meta: n_vocab          = 50304
0.00.094.895 I llm_load_print_meta: n_merges         = 50009
0.00.094.895 I llm_load_print_meta: vocab_only       = 0
0.00.094.896 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.896 I llm_load_print_meta: n_embd           = 2048
0.00.094.896 I llm_load_print_meta: n_layer          = 24
0.00.094.899 I llm_load_print_meta: n_head           = 16
0.00.094.900 I llm_load_print_meta: n_head_kv        = 16
0.00.094.900 I llm_load_print_meta: n_rot            = 32
0.00.094.901 I llm_load_print_meta: n_swa            = 0
0.00.094.901 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.903 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.904 I llm_load_print_meta: n_gqa            = 1
0.00.094.904 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.905 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.906 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.906 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.906 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.906 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.907 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.907 I llm_load_print_meta: n_ff             = 8192
0.00.094.907 I llm_load_print_meta: n_expert         = 0
0.00.094.908 I llm_load_print_meta: n_expert_used    = 0
0.00.094.908 I llm_load_print_meta: causal attn      = 1
0.00.094.908 I llm_load_print_meta: pooling type     = 0
0.00.094.908 I llm_load_print_meta: rope type        = 2
0.00.094.908 I llm_load_print_meta: rope scaling     = linear
0.00.094.909 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.909 I llm_load_print_meta: freq_scale_train = 1
0.00.094.909 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.909 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.910 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.910 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.910 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.910 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.910 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.910 I llm_load_print_meta: model type       = 1.4B
0.00.094.930 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.930 I llm_load_print_meta: model params     = 1.41 B
0.00.094.931 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.931 I llm_load_print_meta: general.name     = 1.4B
0.00.094.931 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.932 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.932 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.932 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.932 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.094.932 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.933 I llm_load_print_meta: max token length = 1024
0.00.097.544 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.544 I llm_load_tensors: offloading output layer to GPU
0.00.097.544 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.563 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.564 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.541 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.542 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.542 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.542 I llama_new_context_with_model: n_batch       = 2048
0.00.098.542 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.542 I llama_new_context_with_model: flash_attn    = 0
0.00.098.543 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.543 I llama_new_context_with_model: freq_scale    = 1
0.00.098.544 I ggml_metal_init: allocating
0.00.098.551 I ggml_metal_init: found device: Apple M4
0.00.098.554 I ggml_metal_init: picking default device: Apple M4
0.00.099.247 I ggml_metal_init: using embedded metal library
0.00.118.468 I ggml_metal_init: GPU name:   Apple M4
0.00.118.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.118.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.118.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.118.472 I ggml_metal_init: simdgroup reduction   = true
0.00.118.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.118.472 I ggml_metal_init: has bfloat            = true
0.00.118.472 I ggml_metal_init: use bfloat            = true
0.00.118.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.118.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.163.050 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.185.910 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.185.917 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.185.942 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.186.974 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.186.977 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.186.977 I llama_new_context_with_model: graph nodes  = 967
0.00.186.978 I llama_new_context_with_model: graph splits = 2
0.00.186.981 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.187.110 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.187.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.269.288 I main: llama threadpool init, n_threads = 4
0.00.269.323 I 
0.00.269.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.269.344 I 
0.00.269.424 I sampler seed: 1234
0.00.269.429 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.269.452 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.269.454 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.269.454 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.113.058 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.02.113.058 I llama_perf_context_print:        load time =     242.26 ms
0.02.113.059 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.02.113.060 I llama_perf_context_print:        eval time =    1796.95 ms /    63 runs   (   28.52 ms per token,    35.06 tokens per second)
0.02.113.060 I llama_perf_context_print:       total time =    1843.77 ms /    70 tokens
0.02.113.299 I ggml_metal_free: deallocating

real	0m2.395s
user	0m0.146s
sys	0m0.105s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.613 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.578 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.583 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.585 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.588 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.589 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.589 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.590 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.590 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.591 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.591 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.591 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.592 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.592 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.595 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.595 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.793 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.367 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.368 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.368 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.369 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.369 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.370 I llama_model_loader: - type  f32:  194 tensors
0.00.036.370 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.801 I llm_load_vocab: special tokens cache size = 25
0.00.065.715 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.719 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.719 I llm_load_print_meta: arch             = gptneox
0.00.065.720 I llm_load_print_meta: vocab type       = BPE
0.00.065.720 I llm_load_print_meta: n_vocab          = 50304
0.00.065.720 I llm_load_print_meta: n_merges         = 50009
0.00.065.722 I llm_load_print_meta: vocab_only       = 0
0.00.065.722 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.722 I llm_load_print_meta: n_embd           = 2048
0.00.065.724 I llm_load_print_meta: n_layer          = 24
0.00.065.729 I llm_load_print_meta: n_head           = 16
0.00.065.730 I llm_load_print_meta: n_head_kv        = 16
0.00.065.730 I llm_load_print_meta: n_rot            = 32
0.00.065.731 I llm_load_print_meta: n_swa            = 0
0.00.065.732 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.733 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.733 I llm_load_print_meta: n_gqa            = 1
0.00.065.734 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.735 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.736 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.736 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.736 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.738 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.738 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.738 I llm_load_print_meta: n_ff             = 8192
0.00.065.739 I llm_load_print_meta: n_expert         = 0
0.00.065.739 I llm_load_print_meta: n_expert_used    = 0
0.00.065.739 I llm_load_print_meta: causal attn      = 1
0.00.065.739 I llm_load_print_meta: pooling type     = 0
0.00.065.739 I llm_load_print_meta: rope type        = 2
0.00.065.740 I llm_load_print_meta: rope scaling     = linear
0.00.065.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.741 I llm_load_print_meta: freq_scale_train = 1
0.00.065.741 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.741 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.742 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.742 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.742 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.742 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.742 I llm_load_print_meta: model type       = 1.4B
0.00.065.756 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.756 I llm_load_print_meta: model params     = 1.41 B
0.00.065.757 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.757 I llm_load_print_meta: general.name     = 1.4B
0.00.065.757 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.757 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.757 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.758 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.759 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.759 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.759 I llm_load_print_meta: max token length = 1024
0.00.068.181 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.181 I llm_load_tensors: offloading output layer to GPU
0.00.068.181 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.192 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.194 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.177 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.178 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.178 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.179 I llama_new_context_with_model: n_batch       = 2048
0.00.069.179 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.179 I llama_new_context_with_model: flash_attn    = 0
0.00.069.179 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.180 I llama_new_context_with_model: freq_scale    = 1
0.00.069.181 I ggml_metal_init: allocating
0.00.069.189 I ggml_metal_init: found device: Apple M4
0.00.069.192 I ggml_metal_init: picking default device: Apple M4
0.00.070.007 I ggml_metal_init: using embedded metal library
0.00.072.752 I ggml_metal_init: GPU name:   Apple M4
0.00.072.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.754 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.754 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.755 I ggml_metal_init: simdgroup reduction   = true
0.00.072.755 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.755 I ggml_metal_init: has bfloat            = true
0.00.072.755 I ggml_metal_init: use bfloat            = true
0.00.072.755 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.756 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.207 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.233 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.242 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.267 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.425 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.428 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.428 I llama_new_context_with_model: graph nodes  = 967
0.00.111.429 I llama_new_context_with_model: graph splits = 2
0.00.111.433 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.111.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.560.216 I main: llama threadpool init, n_threads = 4
0.01.560.261 I 
0.01.560.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.560.297 I 
0.01.560.528 I sampler seed: 1234
0.01.560.534 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.560.577 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.560.579 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.560.579 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.672.318 I llama_perf_sampler_print:    sampling time =       1.60 ms /    71 runs   (    0.02 ms per token, 44236.76 tokens per second)
0.02.672.319 I llama_perf_context_print:        load time =    1550.60 ms
0.02.672.319 I llama_perf_context_print: prompt eval time =      50.05 ms /     7 tokens (    7.15 ms per token,   139.86 tokens per second)
0.02.672.320 I llama_perf_context_print:        eval time =    1058.95 ms /    63 runs   (   16.81 ms per token,    59.49 tokens per second)
0.02.672.321 I llama_perf_context_print:       total time =    1112.11 ms /    70 tokens
0.02.672.557 I ggml_metal_free: deallocating

real	0m2.690s
user	0m0.118s
sys	0m0.224s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.014.513 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.640 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.644 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.646 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.647 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.649 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.649 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.650 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.650 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.650 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.651 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.653 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.691 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.692 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.693 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.693 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.693 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.694 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.694 I llama_model_loader: - type  f32:  194 tensors
0.00.040.694 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.695 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.690 I llm_load_vocab: special tokens cache size = 25
0.00.074.947 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.951 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.951 I llm_load_print_meta: arch             = gptneox
0.00.074.952 I llm_load_print_meta: vocab type       = BPE
0.00.074.952 I llm_load_print_meta: n_vocab          = 50304
0.00.074.952 I llm_load_print_meta: n_merges         = 50009
0.00.074.953 I llm_load_print_meta: vocab_only       = 0
0.00.074.953 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.953 I llm_load_print_meta: n_embd           = 2048
0.00.074.953 I llm_load_print_meta: n_layer          = 24
0.00.074.957 I llm_load_print_meta: n_head           = 16
0.00.074.957 I llm_load_print_meta: n_head_kv        = 16
0.00.074.958 I llm_load_print_meta: n_rot            = 32
0.00.074.958 I llm_load_print_meta: n_swa            = 0
0.00.074.958 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.958 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.959 I llm_load_print_meta: n_gqa            = 1
0.00.074.960 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.961 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.961 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.961 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.962 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.964 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.964 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.965 I llm_load_print_meta: n_ff             = 8192
0.00.074.965 I llm_load_print_meta: n_expert         = 0
0.00.074.965 I llm_load_print_meta: n_expert_used    = 0
0.00.074.966 I llm_load_print_meta: causal attn      = 1
0.00.074.968 I llm_load_print_meta: pooling type     = 0
0.00.074.968 I llm_load_print_meta: rope type        = 2
0.00.074.968 I llm_load_print_meta: rope scaling     = linear
0.00.074.968 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.969 I llm_load_print_meta: freq_scale_train = 1
0.00.074.969 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.969 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.969 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.969 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.970 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.970 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.970 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.970 I llm_load_print_meta: model type       = 1.4B
0.00.074.982 I llm_load_print_meta: model ftype      = Q4_0
0.00.074.982 I llm_load_print_meta: model params     = 1.41 B
0.00.074.983 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.074.983 I llm_load_print_meta: general.name     = 1.4B
0.00.074.983 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.984 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.984 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.985 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.985 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.074.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.986 I llm_load_print_meta: max token length = 1024
0.00.076.997 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.998 I llm_load_tensors: offloading output layer to GPU
0.00.076.998 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.008 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.077.010 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.078.169 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.170 I llama_new_context_with_model: n_ctx         = 2048
0.00.078.170 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.078.170 I llama_new_context_with_model: n_batch       = 2048
0.00.078.171 I llama_new_context_with_model: n_ubatch      = 512
0.00.078.171 I llama_new_context_with_model: flash_attn    = 0
0.00.078.171 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.172 I llama_new_context_with_model: freq_scale    = 1
0.00.078.172 I ggml_metal_init: allocating
0.00.078.182 I ggml_metal_init: found device: Apple M4
0.00.078.185 I ggml_metal_init: picking default device: Apple M4
0.00.078.884 I ggml_metal_init: using embedded metal library
0.00.081.875 I ggml_metal_init: GPU name:   Apple M4
0.00.081.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.878 I ggml_metal_init: simdgroup reduction   = true
0.00.081.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.879 I ggml_metal_init: has bfloat            = true
0.00.081.880 I ggml_metal_init: use bfloat            = true
0.00.081.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.119.222 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.231 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.249 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.311 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.312 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.312 I llama_new_context_with_model: graph nodes  = 967
0.00.120.313 I llama_new_context_with_model: graph splits = 2
0.00.120.315 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.456 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.894.422 I main: llama threadpool init, n_threads = 4
0.00.894.459 I 
0.00.894.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.894.485 I 
0.00.894.723 I sampler seed: 1234
0.00.894.728 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.894.740 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.894.740 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.894.740 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.574.790 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.574.791 I llama_perf_context_print:        load time =     879.90 ms
0.01.574.792 I llama_perf_context_print: prompt eval time =      45.21 ms /     7 tokens (    6.46 ms per token,   154.84 tokens per second)
0.01.574.792 I llama_perf_context_print:        eval time =     631.70 ms /    63 runs   (   10.03 ms per token,    99.73 tokens per second)
0.01.574.796 I llama_perf_context_print:       total time =     680.37 ms /    70 tokens
0.01.575.025 I ggml_metal_free: deallocating

real	0m1.592s
user	0m0.125s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.814 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.108 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.113 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.114 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.115 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.115 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.116 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.120 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.120 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.120 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.120 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.121 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.133 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.136 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.122 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.123 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.124 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.124 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.124 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.125 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.125 I llama_model_loader: - type  f32:  194 tensors
0.00.032.126 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.126 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.976 I llm_load_vocab: special tokens cache size = 25
0.00.064.948 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.951 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.952 I llm_load_print_meta: arch             = gptneox
0.00.064.952 I llm_load_print_meta: vocab type       = BPE
0.00.064.953 I llm_load_print_meta: n_vocab          = 50304
0.00.064.953 I llm_load_print_meta: n_merges         = 50009
0.00.064.953 I llm_load_print_meta: vocab_only       = 0
0.00.064.953 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.953 I llm_load_print_meta: n_embd           = 2048
0.00.064.953 I llm_load_print_meta: n_layer          = 24
0.00.064.956 I llm_load_print_meta: n_head           = 16
0.00.064.957 I llm_load_print_meta: n_head_kv        = 16
0.00.064.957 I llm_load_print_meta: n_rot            = 32
0.00.064.957 I llm_load_print_meta: n_swa            = 0
0.00.064.958 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.958 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.958 I llm_load_print_meta: n_gqa            = 1
0.00.064.959 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.961 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.962 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.962 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.962 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.963 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.963 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.964 I llm_load_print_meta: n_ff             = 8192
0.00.064.964 I llm_load_print_meta: n_expert         = 0
0.00.064.964 I llm_load_print_meta: n_expert_used    = 0
0.00.064.966 I llm_load_print_meta: causal attn      = 1
0.00.064.967 I llm_load_print_meta: pooling type     = 0
0.00.064.967 I llm_load_print_meta: rope type        = 2
0.00.064.967 I llm_load_print_meta: rope scaling     = linear
0.00.064.968 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.968 I llm_load_print_meta: freq_scale_train = 1
0.00.064.968 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.968 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.968 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.969 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.969 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.969 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.969 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.969 I llm_load_print_meta: model type       = 1.4B
0.00.064.982 I llm_load_print_meta: model ftype      = Q4_1
0.00.064.982 I llm_load_print_meta: model params     = 1.41 B
0.00.064.982 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.064.983 I llm_load_print_meta: general.name     = 1.4B
0.00.064.983 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.984 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.984 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.984 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.985 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.985 I llm_load_print_meta: max token length = 1024
0.00.067.280 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.280 I llm_load_tensors: offloading output layer to GPU
0.00.067.281 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.291 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.067.292 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.068.281 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.282 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.282 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.282 I llama_new_context_with_model: n_batch       = 2048
0.00.068.283 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.283 I llama_new_context_with_model: flash_attn    = 0
0.00.068.283 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.284 I llama_new_context_with_model: freq_scale    = 1
0.00.068.284 I ggml_metal_init: allocating
0.00.068.291 I ggml_metal_init: found device: Apple M4
0.00.068.293 I ggml_metal_init: picking default device: Apple M4
0.00.068.919 I ggml_metal_init: using embedded metal library
0.00.071.478 I ggml_metal_init: GPU name:   Apple M4
0.00.071.480 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.481 I ggml_metal_init: simdgroup reduction   = true
0.00.071.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.481 I ggml_metal_init: has bfloat            = true
0.00.071.482 I ggml_metal_init: use bfloat            = true
0.00.071.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.483 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.661 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.804 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.812 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.829 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.891 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.893 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.893 I llama_new_context_with_model: graph nodes  = 967
0.00.106.893 I llama_new_context_with_model: graph splits = 2
0.00.106.896 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.051 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.443 I main: llama threadpool init, n_threads = 4
0.00.784.483 I 
0.00.784.502 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.502 I 
0.00.784.738 I sampler seed: 1234
0.00.784.743 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.784 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.784 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.784 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.517.375 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64311.59 tokens per second)
0.01.517.376 I llama_perf_context_print:        load time =     775.62 ms
0.01.517.377 I llama_perf_context_print: prompt eval time =      43.79 ms /     7 tokens (    6.26 ms per token,   159.87 tokens per second)
0.01.517.378 I llama_perf_context_print:        eval time =     685.99 ms /    63 runs   (   10.89 ms per token,    91.84 tokens per second)
0.01.517.378 I llama_perf_context_print:       total time =     732.94 ms /    70 tokens
0.01.517.605 I ggml_metal_free: deallocating

real	0m1.541s
user	0m0.119s
sys	0m0.145s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.710 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.476 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.480 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.481 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.488 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.488 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.489 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.489 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.492 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.492 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.539 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.701 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.907 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.034.908 I llama_model_loader: - type  f32:  194 tensors
0.00.034.908 I llama_model_loader: - type q5_0:   97 tensors
0.00.034.908 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.407 I llm_load_vocab: special tokens cache size = 25
0.00.063.361 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.364 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.364 I llm_load_print_meta: arch             = gptneox
0.00.063.365 I llm_load_print_meta: vocab type       = BPE
0.00.063.365 I llm_load_print_meta: n_vocab          = 50304
0.00.063.365 I llm_load_print_meta: n_merges         = 50009
0.00.063.365 I llm_load_print_meta: vocab_only       = 0
0.00.063.365 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.365 I llm_load_print_meta: n_embd           = 2048
0.00.063.366 I llm_load_print_meta: n_layer          = 24
0.00.063.368 I llm_load_print_meta: n_head           = 16
0.00.063.369 I llm_load_print_meta: n_head_kv        = 16
0.00.063.369 I llm_load_print_meta: n_rot            = 32
0.00.063.369 I llm_load_print_meta: n_swa            = 0
0.00.063.369 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.369 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.370 I llm_load_print_meta: n_gqa            = 1
0.00.063.371 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.372 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.372 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.373 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.373 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.375 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.375 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.376 I llm_load_print_meta: n_ff             = 8192
0.00.063.376 I llm_load_print_meta: n_expert         = 0
0.00.063.376 I llm_load_print_meta: n_expert_used    = 0
0.00.063.378 I llm_load_print_meta: causal attn      = 1
0.00.063.378 I llm_load_print_meta: pooling type     = 0
0.00.063.378 I llm_load_print_meta: rope type        = 2
0.00.063.378 I llm_load_print_meta: rope scaling     = linear
0.00.063.379 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.379 I llm_load_print_meta: freq_scale_train = 1
0.00.063.379 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.379 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.379 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.380 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.380 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.380 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.380 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.380 I llm_load_print_meta: model type       = 1.4B
0.00.063.392 I llm_load_print_meta: model ftype      = Q5_0
0.00.063.392 I llm_load_print_meta: model params     = 1.41 B
0.00.063.393 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.063.393 I llm_load_print_meta: general.name     = 1.4B
0.00.063.393 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.393 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.394 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.394 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.394 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.394 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.395 I llm_load_print_meta: max token length = 1024
0.00.065.441 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.441 I llm_load_tensors: offloading output layer to GPU
0.00.065.441 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.452 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.065.453 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.066.357 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.357 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.358 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.358 I llama_new_context_with_model: n_batch       = 2048
0.00.066.358 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.358 I llama_new_context_with_model: flash_attn    = 0
0.00.066.358 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.359 I llama_new_context_with_model: freq_scale    = 1
0.00.066.359 I ggml_metal_init: allocating
0.00.066.362 I ggml_metal_init: found device: Apple M4
0.00.066.364 I ggml_metal_init: picking default device: Apple M4
0.00.066.996 I ggml_metal_init: using embedded metal library
0.00.069.506 I ggml_metal_init: GPU name:   Apple M4
0.00.069.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.508 I ggml_metal_init: simdgroup reduction   = true
0.00.069.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.509 I ggml_metal_init: has bfloat            = true
0.00.069.509 I ggml_metal_init: use bfloat            = true
0.00.069.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.250 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.609 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.615 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.634 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.681 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.683 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.683 I llama_new_context_with_model: graph nodes  = 967
0.00.101.683 I llama_new_context_with_model: graph splits = 2
0.00.101.686 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.101.825 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.824.807 I main: llama threadpool init, n_threads = 4
0.00.824.845 I 
0.00.824.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.824.868 I 
0.00.825.096 I sampler seed: 1234
0.00.825.101 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.825.112 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.825.112 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.825.112 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.617.781 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.01.617.781 I llama_perf_context_print:        load time =     815.09 ms
0.01.617.782 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.33 tokens per second)
0.01.617.783 I llama_perf_context_print:        eval time =     746.43 ms /    63 runs   (   11.85 ms per token,    84.40 tokens per second)
0.01.617.783 I llama_perf_context_print:       total time =     792.98 ms /    70 tokens
0.01.617.984 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.113s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.971 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.939 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.021.943 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.949 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.949 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.950 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.950 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.951 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.951 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.951 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.953 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.954 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.016 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.017 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.018 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.018 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.032.019 I llama_model_loader: - type  f32:  194 tensors
0.00.032.019 I llama_model_loader: - type q5_1:   97 tensors
0.00.032.019 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.693 I llm_load_vocab: special tokens cache size = 25
0.00.064.212 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.215 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.215 I llm_load_print_meta: arch             = gptneox
0.00.064.215 I llm_load_print_meta: vocab type       = BPE
0.00.064.216 I llm_load_print_meta: n_vocab          = 50304
0.00.064.216 I llm_load_print_meta: n_merges         = 50009
0.00.064.216 I llm_load_print_meta: vocab_only       = 0
0.00.064.216 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.216 I llm_load_print_meta: n_embd           = 2048
0.00.064.216 I llm_load_print_meta: n_layer          = 24
0.00.064.220 I llm_load_print_meta: n_head           = 16
0.00.064.220 I llm_load_print_meta: n_head_kv        = 16
0.00.064.221 I llm_load_print_meta: n_rot            = 32
0.00.064.221 I llm_load_print_meta: n_swa            = 0
0.00.064.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.222 I llm_load_print_meta: n_gqa            = 1
0.00.064.222 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.223 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.223 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.225 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.225 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.226 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.227 I llm_load_print_meta: n_ff             = 8192
0.00.064.227 I llm_load_print_meta: n_expert         = 0
0.00.064.227 I llm_load_print_meta: n_expert_used    = 0
0.00.064.227 I llm_load_print_meta: causal attn      = 1
0.00.064.227 I llm_load_print_meta: pooling type     = 0
0.00.064.227 I llm_load_print_meta: rope type        = 2
0.00.064.228 I llm_load_print_meta: rope scaling     = linear
0.00.064.228 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.228 I llm_load_print_meta: freq_scale_train = 1
0.00.064.229 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.229 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.229 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.229 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.229 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.229 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.229 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.230 I llm_load_print_meta: model type       = 1.4B
0.00.064.241 I llm_load_print_meta: model ftype      = Q5_1
0.00.064.242 I llm_load_print_meta: model params     = 1.41 B
0.00.064.242 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.064.242 I llm_load_print_meta: general.name     = 1.4B
0.00.064.243 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.243 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.243 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.243 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.243 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.244 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.244 I llm_load_print_meta: max token length = 1024
0.00.066.439 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.439 I llm_load_tensors: offloading output layer to GPU
0.00.066.440 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.451 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.066.452 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.067.407 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.408 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.408 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.408 I llama_new_context_with_model: n_batch       = 2048
0.00.067.408 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.408 I llama_new_context_with_model: flash_attn    = 0
0.00.067.409 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.409 I llama_new_context_with_model: freq_scale    = 1
0.00.067.410 I ggml_metal_init: allocating
0.00.067.412 I ggml_metal_init: found device: Apple M4
0.00.067.414 I ggml_metal_init: picking default device: Apple M4
0.00.068.074 I ggml_metal_init: using embedded metal library
0.00.070.796 I ggml_metal_init: GPU name:   Apple M4
0.00.070.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.798 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.799 I ggml_metal_init: simdgroup reduction   = true
0.00.070.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.799 I ggml_metal_init: has bfloat            = true
0.00.070.799 I ggml_metal_init: use bfloat            = true
0.00.070.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.018 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.130 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.135 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.154 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.212 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.213 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.214 I llama_new_context_with_model: graph nodes  = 967
0.00.104.214 I llama_new_context_with_model: graph splits = 2
0.00.104.217 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.365 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.365 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.834.425 I main: llama threadpool init, n_threads = 4
0.00.834.463 I 
0.00.834.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.834.482 I 
0.00.834.706 I sampler seed: 1234
0.00.834.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.834.752 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.834.769 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.834.769 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.671.997 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.671.998 I llama_perf_context_print:        load time =     824.45 ms
0.01.671.999 I llama_perf_context_print: prompt eval time =      42.27 ms /     7 tokens (    6.04 ms per token,   165.62 tokens per second)
0.01.672.000 I llama_perf_context_print:        eval time =     791.85 ms /    63 runs   (   12.57 ms per token,    79.56 tokens per second)
0.01.672.000 I llama_perf_context_print:       total time =     837.58 ms /    70 tokens
0.01.672.232 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.118s
sys	0m0.171s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.515 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.913 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.918 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.920 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.922 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.925 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.925 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.926 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.929 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.930 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.072 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.193 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.505 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.506 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.506 I llama_model_loader: - type  f32:  194 tensors
0.00.026.507 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.507 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.507 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.444 I llm_load_vocab: special tokens cache size = 25
0.00.053.288 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.290 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.291 I llm_load_print_meta: arch             = gptneox
0.00.053.291 I llm_load_print_meta: vocab type       = BPE
0.00.053.292 I llm_load_print_meta: n_vocab          = 50304
0.00.053.292 I llm_load_print_meta: n_merges         = 50009
0.00.053.292 I llm_load_print_meta: vocab_only       = 0
0.00.053.292 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.292 I llm_load_print_meta: n_embd           = 2048
0.00.053.292 I llm_load_print_meta: n_layer          = 24
0.00.053.295 I llm_load_print_meta: n_head           = 16
0.00.053.296 I llm_load_print_meta: n_head_kv        = 16
0.00.053.296 I llm_load_print_meta: n_rot            = 32
0.00.053.297 I llm_load_print_meta: n_swa            = 0
0.00.053.297 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.298 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.298 I llm_load_print_meta: n_gqa            = 1
0.00.053.299 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.300 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.301 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.301 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.301 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.303 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.303 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.304 I llm_load_print_meta: n_ff             = 8192
0.00.053.304 I llm_load_print_meta: n_expert         = 0
0.00.053.306 I llm_load_print_meta: n_expert_used    = 0
0.00.053.307 I llm_load_print_meta: causal attn      = 1
0.00.053.307 I llm_load_print_meta: pooling type     = 0
0.00.053.307 I llm_load_print_meta: rope type        = 2
0.00.053.308 I llm_load_print_meta: rope scaling     = linear
0.00.053.308 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.308 I llm_load_print_meta: freq_scale_train = 1
0.00.053.309 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.309 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.309 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.309 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.309 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.309 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.309 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.311 I llm_load_print_meta: model type       = 1.4B
0.00.053.323 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.323 I llm_load_print_meta: model params     = 1.41 B
0.00.053.324 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.324 I llm_load_print_meta: general.name     = 1.4B
0.00.053.324 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.324 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.324 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.324 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.325 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.325 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.325 I llm_load_print_meta: max token length = 1024
0.00.055.301 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.302 I llm_load_tensors: offloading output layer to GPU
0.00.055.302 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.312 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.313 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.241 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.241 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.241 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.242 I llama_new_context_with_model: n_batch       = 2048
0.00.056.242 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.242 I llama_new_context_with_model: flash_attn    = 0
0.00.056.242 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.243 I llama_new_context_with_model: freq_scale    = 1
0.00.056.243 I ggml_metal_init: allocating
0.00.056.247 I ggml_metal_init: found device: Apple M4
0.00.056.249 I ggml_metal_init: picking default device: Apple M4
0.00.056.870 I ggml_metal_init: using embedded metal library
0.00.059.197 I ggml_metal_init: GPU name:   Apple M4
0.00.059.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.199 I ggml_metal_init: simdgroup reduction   = true
0.00.059.199 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.201 I ggml_metal_init: has bfloat            = true
0.00.059.201 I ggml_metal_init: use bfloat            = true
0.00.059.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.143 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.233 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.242 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.263 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.385 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.386 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.387 I llama_new_context_with_model: graph nodes  = 967
0.00.090.387 I llama_new_context_with_model: graph splits = 2
0.00.090.389 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.549 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.492.891 I main: llama threadpool init, n_threads = 4
0.00.492.935 I 
0.00.492.968 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.492.968 I 
0.00.493.195 I sampler seed: 1234
0.00.493.199 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.493.219 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.493.220 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.493.220 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.173.040 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.173.041 I llama_perf_context_print:        load time =     482.37 ms
0.01.173.042 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.66 tokens per second)
0.01.173.042 I llama_perf_context_print:        eval time =     641.10 ms /    63 runs   (   10.18 ms per token,    98.27 tokens per second)
0.01.173.043 I llama_perf_context_print:       total time =     680.15 ms /    70 tokens
0.01.173.260 I ggml_metal_free: deallocating

real	0m1.193s
user	0m0.112s
sys	0m0.117s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.278 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.440 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.446 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.449 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.450 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.878 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.046 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.399 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.400 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.401 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.401 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.032.402 I llama_model_loader: - type  f32:  194 tensors
0.00.032.402 I llama_model_loader: - type q3_K:   25 tensors
0.00.032.402 I llama_model_loader: - type q4_K:   71 tensors
0.00.032.403 I llama_model_loader: - type q5_K:    1 tensors
0.00.032.403 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.774 I llm_load_vocab: special tokens cache size = 25
0.00.061.857 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.860 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.860 I llm_load_print_meta: arch             = gptneox
0.00.061.861 I llm_load_print_meta: vocab type       = BPE
0.00.061.861 I llm_load_print_meta: n_vocab          = 50304
0.00.061.861 I llm_load_print_meta: n_merges         = 50009
0.00.061.861 I llm_load_print_meta: vocab_only       = 0
0.00.061.861 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.862 I llm_load_print_meta: n_embd           = 2048
0.00.061.862 I llm_load_print_meta: n_layer          = 24
0.00.061.864 I llm_load_print_meta: n_head           = 16
0.00.061.865 I llm_load_print_meta: n_head_kv        = 16
0.00.061.865 I llm_load_print_meta: n_rot            = 32
0.00.061.865 I llm_load_print_meta: n_swa            = 0
0.00.061.865 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.865 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.867 I llm_load_print_meta: n_gqa            = 1
0.00.061.868 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.870 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.872 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.872 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.872 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.872 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.873 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.873 I llm_load_print_meta: n_ff             = 8192
0.00.061.873 I llm_load_print_meta: n_expert         = 0
0.00.061.874 I llm_load_print_meta: n_expert_used    = 0
0.00.061.874 I llm_load_print_meta: causal attn      = 1
0.00.061.874 I llm_load_print_meta: pooling type     = 0
0.00.061.874 I llm_load_print_meta: rope type        = 2
0.00.061.874 I llm_load_print_meta: rope scaling     = linear
0.00.061.874 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.875 I llm_load_print_meta: freq_scale_train = 1
0.00.061.875 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.875 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.875 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.875 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.875 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.875 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.877 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.877 I llm_load_print_meta: model type       = 1.4B
0.00.061.889 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.061.889 I llm_load_print_meta: model params     = 1.41 B
0.00.061.890 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.061.890 I llm_load_print_meta: general.name     = 1.4B
0.00.061.891 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.891 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.891 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.891 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.891 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.892 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.892 I llm_load_print_meta: max token length = 1024
0.00.063.950 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.950 I llm_load_tensors: offloading output layer to GPU
0.00.063.951 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.961 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.063.962 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.064.825 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.826 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.826 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.826 I llama_new_context_with_model: n_batch       = 2048
0.00.064.827 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.827 I llama_new_context_with_model: flash_attn    = 0
0.00.064.829 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.829 I llama_new_context_with_model: freq_scale    = 1
0.00.064.829 I ggml_metal_init: allocating
0.00.064.832 I ggml_metal_init: found device: Apple M4
0.00.064.834 I ggml_metal_init: picking default device: Apple M4
0.00.065.445 I ggml_metal_init: using embedded metal library
0.00.067.764 I ggml_metal_init: GPU name:   Apple M4
0.00.067.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.766 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.766 I ggml_metal_init: simdgroup reduction   = true
0.00.067.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.766 I ggml_metal_init: has bfloat            = true
0.00.067.767 I ggml_metal_init: use bfloat            = true
0.00.067.767 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.840 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.302 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.319 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.409 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.410 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.411 I llama_new_context_with_model: graph nodes  = 967
0.00.099.411 I llama_new_context_with_model: graph splits = 2
0.00.099.414 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.429 I main: llama threadpool init, n_threads = 4
0.00.558.472 I 
0.00.558.496 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.558.496 I 
0.00.558.739 I sampler seed: 1234
0.00.558.743 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.558.755 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.558.755 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.558.755 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.307.216 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.307.216 I llama_perf_context_print:        load time =     549.14 ms
0.01.307.217 I llama_perf_context_print: prompt eval time =      40.47 ms /     7 tokens (    5.78 ms per token,   172.97 tokens per second)
0.01.307.218 I llama_perf_context_print:        eval time =     705.03 ms /    63 runs   (   11.19 ms per token,    89.36 tokens per second)
0.01.307.218 I llama_perf_context_print:       total time =     748.79 ms /    70 tokens
0.01.307.460 I ggml_metal_free: deallocating

real	0m1.332s
user	0m0.115s
sys	0m0.129s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.172 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.181 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.182 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.184 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.185 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.186 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.440 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.443 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.446 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.448 I llama_model_loader: - type  f32:  194 tensors
0.00.025.448 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.448 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.449 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.186 I llm_load_vocab: special tokens cache size = 25
0.00.053.184 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.190 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.190 I llm_load_print_meta: arch             = gptneox
0.00.053.191 I llm_load_print_meta: vocab type       = BPE
0.00.053.191 I llm_load_print_meta: n_vocab          = 50304
0.00.053.191 I llm_load_print_meta: n_merges         = 50009
0.00.053.191 I llm_load_print_meta: vocab_only       = 0
0.00.053.197 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.197 I llm_load_print_meta: n_embd           = 2048
0.00.053.198 I llm_load_print_meta: n_layer          = 24
0.00.053.201 I llm_load_print_meta: n_head           = 16
0.00.053.202 I llm_load_print_meta: n_head_kv        = 16
0.00.053.203 I llm_load_print_meta: n_rot            = 32
0.00.053.204 I llm_load_print_meta: n_swa            = 0
0.00.053.204 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.204 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.205 I llm_load_print_meta: n_gqa            = 1
0.00.053.205 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.206 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.207 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.207 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.207 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.207 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.207 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.208 I llm_load_print_meta: n_ff             = 8192
0.00.053.208 I llm_load_print_meta: n_expert         = 0
0.00.053.208 I llm_load_print_meta: n_expert_used    = 0
0.00.053.208 I llm_load_print_meta: causal attn      = 1
0.00.053.209 I llm_load_print_meta: pooling type     = 0
0.00.053.209 I llm_load_print_meta: rope type        = 2
0.00.053.209 I llm_load_print_meta: rope scaling     = linear
0.00.053.209 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.209 I llm_load_print_meta: freq_scale_train = 1
0.00.053.210 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.210 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.210 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.210 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.210 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.210 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.210 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.211 I llm_load_print_meta: model type       = 1.4B
0.00.053.226 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.226 I llm_load_print_meta: model params     = 1.41 B
0.00.053.226 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.227 I llm_load_print_meta: general.name     = 1.4B
0.00.053.227 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.227 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.227 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.229 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.229 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.229 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.229 I llm_load_print_meta: max token length = 1024
0.00.055.188 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.188 I llm_load_tensors: offloading output layer to GPU
0.00.055.188 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.199 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.201 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.138 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.139 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.139 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.140 I llama_new_context_with_model: n_batch       = 2048
0.00.056.140 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.140 I llama_new_context_with_model: flash_attn    = 0
0.00.056.140 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.141 I llama_new_context_with_model: freq_scale    = 1
0.00.056.141 I ggml_metal_init: allocating
0.00.056.146 I ggml_metal_init: found device: Apple M4
0.00.056.148 I ggml_metal_init: picking default device: Apple M4
0.00.056.818 I ggml_metal_init: using embedded metal library
0.00.059.255 I ggml_metal_init: GPU name:   Apple M4
0.00.059.258 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.258 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.259 I ggml_metal_init: simdgroup reduction   = true
0.00.059.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.259 I ggml_metal_init: has bfloat            = true
0.00.059.259 I ggml_metal_init: use bfloat            = true
0.00.059.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.443 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.224 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.235 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.255 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.227 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.228 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.229 I llama_new_context_with_model: graph nodes  = 967
0.00.091.229 I llama_new_context_with_model: graph splits = 2
0.00.091.232 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.373 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.374 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.771 I main: llama threadpool init, n_threads = 4
0.00.608.813 I 
0.00.608.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.834 I 
0.00.609.003 I sampler seed: 1234
0.00.609.008 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.018 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.018 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.018 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.368.118 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.01.368.119 I llama_perf_context_print:        load time =     599.02 ms
0.01.368.120 I llama_perf_context_print: prompt eval time =      47.08 ms /     7 tokens (    6.73 ms per token,   148.67 tokens per second)
0.01.368.120 I llama_perf_context_print:        eval time =     708.85 ms /    63 runs   (   11.25 ms per token,    88.88 tokens per second)
0.01.368.121 I llama_perf_context_print:       total time =     759.35 ms /    70 tokens
0.01.368.354 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.113s
sys	0m0.133s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.765 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.298 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.304 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.307 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.307 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.307 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.310 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.310 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.313 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.313 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.313 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.397 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.379 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.380 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.381 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.381 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.382 I llama_model_loader: - type  f32:  194 tensors
0.00.024.382 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.382 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.575 I llm_load_vocab: special tokens cache size = 25
0.00.050.486 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.489 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.489 I llm_load_print_meta: arch             = gptneox
0.00.050.490 I llm_load_print_meta: vocab type       = BPE
0.00.050.490 I llm_load_print_meta: n_vocab          = 50304
0.00.050.490 I llm_load_print_meta: n_merges         = 50009
0.00.050.490 I llm_load_print_meta: vocab_only       = 0
0.00.050.490 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.491 I llm_load_print_meta: n_embd           = 2048
0.00.050.491 I llm_load_print_meta: n_layer          = 24
0.00.050.494 I llm_load_print_meta: n_head           = 16
0.00.050.495 I llm_load_print_meta: n_head_kv        = 16
0.00.050.495 I llm_load_print_meta: n_rot            = 32
0.00.050.495 I llm_load_print_meta: n_swa            = 0
0.00.050.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.495 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.496 I llm_load_print_meta: n_gqa            = 1
0.00.050.497 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.497 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.498 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.498 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.498 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.500 I llm_load_print_meta: n_ff             = 8192
0.00.050.500 I llm_load_print_meta: n_expert         = 0
0.00.050.501 I llm_load_print_meta: n_expert_used    = 0
0.00.050.503 I llm_load_print_meta: causal attn      = 1
0.00.050.504 I llm_load_print_meta: pooling type     = 0
0.00.050.504 I llm_load_print_meta: rope type        = 2
0.00.050.504 I llm_load_print_meta: rope scaling     = linear
0.00.050.505 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.505 I llm_load_print_meta: freq_scale_train = 1
0.00.050.505 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.505 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.506 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.506 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.506 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.506 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.506 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.506 I llm_load_print_meta: model type       = 1.4B
0.00.050.518 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.519 I llm_load_print_meta: model params     = 1.41 B
0.00.050.519 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.519 I llm_load_print_meta: general.name     = 1.4B
0.00.050.520 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.520 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.520 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.520 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.521 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.521 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.521 I llm_load_print_meta: max token length = 1024
0.00.052.494 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.494 I llm_load_tensors: offloading output layer to GPU
0.00.052.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.505 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.506 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.396 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.396 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.397 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.397 I llama_new_context_with_model: n_batch       = 2048
0.00.053.397 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.397 I llama_new_context_with_model: flash_attn    = 0
0.00.053.397 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.398 I llama_new_context_with_model: freq_scale    = 1
0.00.053.398 I ggml_metal_init: allocating
0.00.053.402 I ggml_metal_init: found device: Apple M4
0.00.053.404 I ggml_metal_init: picking default device: Apple M4
0.00.054.011 I ggml_metal_init: using embedded metal library
0.00.056.282 I ggml_metal_init: GPU name:   Apple M4
0.00.056.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.286 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.286 I ggml_metal_init: simdgroup reduction   = true
0.00.056.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.287 I ggml_metal_init: has bfloat            = true
0.00.056.287 I ggml_metal_init: use bfloat            = true
0.00.056.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.288 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.944 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.769 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.777 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.800 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.842 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.844 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.844 I llama_new_context_with_model: graph nodes  = 967
0.00.086.844 I llama_new_context_with_model: graph splits = 2
0.00.086.847 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.989 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.593 I main: llama threadpool init, n_threads = 4
0.00.732.650 I 
0.00.732.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.687 I 
0.00.733.012 I sampler seed: 1234
0.00.733.017 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.733.030 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.733.030 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.733.030 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.577.174 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.577.175 I llama_perf_context_print:        load time =     723.82 ms
0.01.577.175 I llama_perf_context_print: prompt eval time =      51.34 ms /     7 tokens (    7.33 ms per token,   136.34 tokens per second)
0.01.577.176 I llama_perf_context_print:        eval time =     789.81 ms /    63 runs   (   12.54 ms per token,    79.77 tokens per second)
0.01.577.177 I llama_perf_context_print:       total time =     844.59 ms /    70 tokens
0.01.577.391 I ggml_metal_free: deallocating

real	0m1.596s
user	0m0.111s
sys	0m0.178s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.387 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.206 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.213 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.213 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.218 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.221 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.221 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.214 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.343 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.281 I llama_model_loader: - type  f32:  194 tensors
0.00.026.281 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.550 I llm_load_vocab: special tokens cache size = 25
0.00.052.410 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.413 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.414 I llm_load_print_meta: arch             = gptneox
0.00.052.414 I llm_load_print_meta: vocab type       = BPE
0.00.052.414 I llm_load_print_meta: n_vocab          = 50304
0.00.052.415 I llm_load_print_meta: n_merges         = 50009
0.00.052.415 I llm_load_print_meta: vocab_only       = 0
0.00.052.415 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.415 I llm_load_print_meta: n_embd           = 2048
0.00.052.415 I llm_load_print_meta: n_layer          = 24
0.00.052.418 I llm_load_print_meta: n_head           = 16
0.00.052.419 I llm_load_print_meta: n_head_kv        = 16
0.00.052.419 I llm_load_print_meta: n_rot            = 32
0.00.052.422 I llm_load_print_meta: n_swa            = 0
0.00.052.422 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.422 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.423 I llm_load_print_meta: n_gqa            = 1
0.00.052.423 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.424 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.424 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.426 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.426 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.426 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.426 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.427 I llm_load_print_meta: n_ff             = 8192
0.00.052.427 I llm_load_print_meta: n_expert         = 0
0.00.052.427 I llm_load_print_meta: n_expert_used    = 0
0.00.052.428 I llm_load_print_meta: causal attn      = 1
0.00.052.429 I llm_load_print_meta: pooling type     = 0
0.00.052.431 I llm_load_print_meta: rope type        = 2
0.00.052.431 I llm_load_print_meta: rope scaling     = linear
0.00.052.431 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.432 I llm_load_print_meta: freq_scale_train = 1
0.00.052.432 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.432 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.432 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.432 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.432 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.433 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.433 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.437 I llm_load_print_meta: model type       = 1.4B
0.00.052.449 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.449 I llm_load_print_meta: model params     = 1.41 B
0.00.052.449 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.449 I llm_load_print_meta: general.name     = 1.4B
0.00.052.450 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.450 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.450 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.450 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.450 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.451 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.451 I llm_load_print_meta: max token length = 1024
0.00.054.536 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.536 I llm_load_tensors: offloading output layer to GPU
0.00.054.536 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.547 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.548 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.454 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.454 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.455 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.455 I llama_new_context_with_model: n_batch       = 2048
0.00.055.455 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.455 I llama_new_context_with_model: flash_attn    = 0
0.00.055.456 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.456 I llama_new_context_with_model: freq_scale    = 1
0.00.055.457 I ggml_metal_init: allocating
0.00.055.465 I ggml_metal_init: found device: Apple M4
0.00.055.467 I ggml_metal_init: picking default device: Apple M4
0.00.056.070 I ggml_metal_init: using embedded metal library
0.00.058.437 I ggml_metal_init: GPU name:   Apple M4
0.00.058.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.440 I ggml_metal_init: simdgroup reduction   = true
0.00.058.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.441 I ggml_metal_init: has bfloat            = true
0.00.058.442 I ggml_metal_init: use bfloat            = true
0.00.058.442 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.045 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.620 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.626 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.644 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.719 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.721 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.721 I llama_new_context_with_model: graph nodes  = 967
0.00.088.722 I llama_new_context_with_model: graph splits = 2
0.00.088.724 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.846 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.559 I main: llama threadpool init, n_threads = 4
0.00.741.595 I 
0.00.741.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.616 I 
0.00.741.851 I sampler seed: 1234
0.00.741.855 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.897 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.898 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.898 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.624.018 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.624.019 I llama_perf_context_print:        load time =     731.17 ms
0.01.624.019 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.49 tokens per second)
0.01.624.020 I llama_perf_context_print:        eval time =     824.61 ms /    63 runs   (   13.09 ms per token,    76.40 tokens per second)
0.01.624.024 I llama_perf_context_print:       total time =     882.46 ms /    70 tokens
0.01.624.256 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.476 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.663 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.056 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.077 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.092 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.093 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.093 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.720 I llama_model_loader: - type  f32:  194 tensors
0.00.053.720 I llama_model_loader: - type  f16:   98 tensors
0.00.084.280 I llm_load_vocab: special tokens cache size = 25
0.00.091.072 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.075 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.076 I llm_load_print_meta: arch             = gptneox
0.00.091.076 I llm_load_print_meta: vocab type       = BPE
0.00.091.076 I llm_load_print_meta: n_vocab          = 50304
0.00.091.076 I llm_load_print_meta: n_merges         = 50009
0.00.091.076 I llm_load_print_meta: vocab_only       = 0
0.00.091.077 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.077 I llm_load_print_meta: n_embd           = 2048
0.00.091.077 I llm_load_print_meta: n_layer          = 24
0.00.091.080 I llm_load_print_meta: n_head           = 16
0.00.091.080 I llm_load_print_meta: n_head_kv        = 16
0.00.091.080 I llm_load_print_meta: n_rot            = 32
0.00.091.081 I llm_load_print_meta: n_swa            = 0
0.00.091.082 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.082 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.083 I llm_load_print_meta: n_gqa            = 1
0.00.091.083 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.084 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.084 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.085 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.085 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.085 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.085 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.086 I llm_load_print_meta: n_ff             = 8192
0.00.091.086 I llm_load_print_meta: n_expert         = 0
0.00.091.086 I llm_load_print_meta: n_expert_used    = 0
0.00.091.086 I llm_load_print_meta: causal attn      = 1
0.00.091.086 I llm_load_print_meta: pooling type     = 0
0.00.091.087 I llm_load_print_meta: rope type        = 2
0.00.091.087 I llm_load_print_meta: rope scaling     = linear
0.00.091.089 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.089 I llm_load_print_meta: freq_scale_train = 1
0.00.091.089 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.090 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.090 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.090 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.090 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.090 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.090 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.090 I llm_load_print_meta: model type       = 1.4B
0.00.091.102 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.102 I llm_load_print_meta: model params     = 1.41 B
0.00.091.103 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.105 I llm_load_print_meta: general.name     = 1.4B
0.00.091.105 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.105 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.105 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.106 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.106 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.106 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.106 I llm_load_print_meta: max token length = 1024
0.00.093.624 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.624 I llm_load_tensors: offloading output layer to GPU
0.00.093.624 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.634 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.636 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.588 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.589 I llama_new_context_with_model: n_ctx         = 128
0.00.094.589 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.589 I llama_new_context_with_model: n_batch       = 128
0.00.094.589 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.589 I llama_new_context_with_model: flash_attn    = 0
0.00.094.590 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.590 I llama_new_context_with_model: freq_scale    = 1
0.00.094.591 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.591 I ggml_metal_init: allocating
0.00.094.600 I ggml_metal_init: found device: Apple M4
0.00.094.602 I ggml_metal_init: picking default device: Apple M4
0.00.095.238 I ggml_metal_init: using embedded metal library
0.00.097.932 I ggml_metal_init: GPU name:   Apple M4
0.00.097.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.934 I ggml_metal_init: simdgroup reduction   = true
0.00.097.935 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.935 I ggml_metal_init: has bfloat            = true
0.00.097.935 I ggml_metal_init: use bfloat            = true
0.00.097.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.197 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.484 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.486 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.502 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.445 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.446 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.446 I llama_new_context_with_model: graph nodes  = 967
0.00.110.447 I llama_new_context_with_model: graph splits = 2
0.00.110.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.198.317 I 
0.01.198.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.198.445 I perplexity: tokenizing the input ..
0.01.211.557 I perplexity: tokenization took 13.107 ms
0.01.211.567 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.332.599 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.334.248 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.334.271 I llama_perf_context_print:        load time =    1175.63 ms
0.01.334.272 I llama_perf_context_print: prompt eval time =     120.36 ms /   128 tokens (    0.94 ms per token,  1063.45 tokens per second)
0.01.334.276 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.334.277 I llama_perf_context_print:       total time =     135.96 ms /   129 tokens
0.01.335.014 I ggml_metal_free: deallocating

real	0m1.526s
user	0m0.125s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.318 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.721 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.190 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.194 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.195 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.197 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.197 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.197 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.358 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.054 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.054 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.055 I llama_model_loader: - type  f32:  194 tensors
0.00.036.056 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.652 I llm_load_vocab: special tokens cache size = 25
0.00.068.790 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.793 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.793 I llm_load_print_meta: arch             = gptneox
0.00.068.794 I llm_load_print_meta: vocab type       = BPE
0.00.068.794 I llm_load_print_meta: n_vocab          = 50304
0.00.068.794 I llm_load_print_meta: n_merges         = 50009
0.00.068.794 I llm_load_print_meta: vocab_only       = 0
0.00.068.794 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.794 I llm_load_print_meta: n_embd           = 2048
0.00.068.794 I llm_load_print_meta: n_layer          = 24
0.00.068.798 I llm_load_print_meta: n_head           = 16
0.00.068.799 I llm_load_print_meta: n_head_kv        = 16
0.00.068.799 I llm_load_print_meta: n_rot            = 32
0.00.068.799 I llm_load_print_meta: n_swa            = 0
0.00.068.799 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.799 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.800 I llm_load_print_meta: n_gqa            = 1
0.00.068.801 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.801 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.802 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.803 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.803 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.804 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.804 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.805 I llm_load_print_meta: n_ff             = 8192
0.00.068.805 I llm_load_print_meta: n_expert         = 0
0.00.068.805 I llm_load_print_meta: n_expert_used    = 0
0.00.068.805 I llm_load_print_meta: causal attn      = 1
0.00.068.805 I llm_load_print_meta: pooling type     = 0
0.00.068.806 I llm_load_print_meta: rope type        = 2
0.00.068.808 I llm_load_print_meta: rope scaling     = linear
0.00.068.808 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.808 I llm_load_print_meta: freq_scale_train = 1
0.00.068.809 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.809 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.809 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.809 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.809 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.809 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.809 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.810 I llm_load_print_meta: model type       = 1.4B
0.00.068.822 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.823 I llm_load_print_meta: model params     = 1.41 B
0.00.068.823 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.823 I llm_load_print_meta: general.name     = 1.4B
0.00.068.824 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.824 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.824 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.824 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.825 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.825 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.825 I llm_load_print_meta: max token length = 1024
0.00.071.245 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.245 I llm_load_tensors: offloading output layer to GPU
0.00.071.245 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.257 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.258 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.210 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.211 I llama_new_context_with_model: n_ctx         = 128
0.00.072.211 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.072.212 I llama_new_context_with_model: n_batch       = 128
0.00.072.212 I llama_new_context_with_model: n_ubatch      = 128
0.00.072.212 I llama_new_context_with_model: flash_attn    = 0
0.00.072.212 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.212 I llama_new_context_with_model: freq_scale    = 1
0.00.072.213 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.213 I ggml_metal_init: allocating
0.00.072.216 I ggml_metal_init: found device: Apple M4
0.00.072.218 I ggml_metal_init: picking default device: Apple M4
0.00.072.923 I ggml_metal_init: using embedded metal library
0.00.075.886 I ggml_metal_init: GPU name:   Apple M4
0.00.075.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.889 I ggml_metal_init: simdgroup reduction   = true
0.00.075.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.889 I ggml_metal_init: has bfloat            = true
0.00.075.889 I ggml_metal_init: use bfloat            = true
0.00.075.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.926 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.361 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.364 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.381 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.420 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.421 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.421 I llama_new_context_with_model: graph nodes  = 967
0.00.088.422 I llama_new_context_with_model: graph splits = 2
0.00.088.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.726 I 
0.00.833.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.766 I perplexity: tokenizing the input ..
0.00.840.714 I perplexity: tokenization took 6.946 ms
0.00.840.719 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.965.327 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.966.596 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.966.613 I llama_perf_context_print:        load time =     821.00 ms
0.00.966.615 I llama_perf_context_print: prompt eval time =     124.38 ms /   128 tokens (    0.97 ms per token,  1029.15 tokens per second)
0.00.966.617 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.966.617 I llama_perf_context_print:       total time =     132.89 ms /   129 tokens
0.00.966.989 I ggml_metal_free: deallocating

real	0m0.987s
user	0m0.096s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.267 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.301 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.230 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.240 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.241 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.244 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.246 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.247 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.247 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.248 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.317 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.383 I llama_model_loader: - type  f32:  194 tensors
0.00.024.384 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.384 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.447 I llm_load_vocab: special tokens cache size = 25
0.00.050.327 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.330 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.330 I llm_load_print_meta: arch             = gptneox
0.00.050.330 I llm_load_print_meta: vocab type       = BPE
0.00.050.331 I llm_load_print_meta: n_vocab          = 50304
0.00.050.331 I llm_load_print_meta: n_merges         = 50009
0.00.050.331 I llm_load_print_meta: vocab_only       = 0
0.00.050.331 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.331 I llm_load_print_meta: n_embd           = 2048
0.00.050.331 I llm_load_print_meta: n_layer          = 24
0.00.050.334 I llm_load_print_meta: n_head           = 16
0.00.050.337 I llm_load_print_meta: n_head_kv        = 16
0.00.050.337 I llm_load_print_meta: n_rot            = 32
0.00.050.337 I llm_load_print_meta: n_swa            = 0
0.00.050.337 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.337 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.338 I llm_load_print_meta: n_gqa            = 1
0.00.050.339 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.340 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.340 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.340 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.340 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.341 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.341 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.342 I llm_load_print_meta: n_ff             = 8192
0.00.050.343 I llm_load_print_meta: n_expert         = 0
0.00.050.343 I llm_load_print_meta: n_expert_used    = 0
0.00.050.343 I llm_load_print_meta: causal attn      = 1
0.00.050.343 I llm_load_print_meta: pooling type     = 0
0.00.050.343 I llm_load_print_meta: rope type        = 2
0.00.050.345 I llm_load_print_meta: rope scaling     = linear
0.00.050.345 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.345 I llm_load_print_meta: freq_scale_train = 1
0.00.050.346 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.346 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.346 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.346 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.346 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.346 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.348 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.348 I llm_load_print_meta: model type       = 1.4B
0.00.050.360 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.360 I llm_load_print_meta: model params     = 1.41 B
0.00.050.361 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.361 I llm_load_print_meta: general.name     = 1.4B
0.00.050.361 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.361 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.361 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.361 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.362 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.362 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.362 I llm_load_print_meta: max token length = 1024
0.00.052.312 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.313 I llm_load_tensors: offloading output layer to GPU
0.00.052.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.323 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.324 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.264 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.265 I llama_new_context_with_model: n_ctx         = 128
0.00.053.266 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.266 I llama_new_context_with_model: n_batch       = 128
0.00.053.266 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.266 I llama_new_context_with_model: flash_attn    = 0
0.00.053.266 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.267 I llama_new_context_with_model: freq_scale    = 1
0.00.053.267 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.268 I ggml_metal_init: allocating
0.00.053.270 I ggml_metal_init: found device: Apple M4
0.00.053.272 I ggml_metal_init: picking default device: Apple M4
0.00.053.845 I ggml_metal_init: using embedded metal library
0.00.056.136 I ggml_metal_init: GPU name:   Apple M4
0.00.056.138 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.139 I ggml_metal_init: simdgroup reduction   = true
0.00.056.139 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.139 I ggml_metal_init: has bfloat            = true
0.00.056.139 I ggml_metal_init: use bfloat            = true
0.00.056.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.187 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.481 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.484 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.500 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.378 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.379 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.379 I llama_new_context_with_model: graph nodes  = 967
0.00.068.379 I llama_new_context_with_model: graph splits = 2
0.00.068.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.381 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.911 I 
0.00.604.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.958 I perplexity: tokenizing the input ..
0.00.612.459 I perplexity: tokenization took 7.499 ms
0.00.612.462 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.735.274 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.736.409 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.736.432 I llama_perf_context_print:        load time =     595.60 ms
0.00.736.433 I llama_perf_context_print: prompt eval time =     122.58 ms /   128 tokens (    0.96 ms per token,  1044.22 tokens per second)
0.00.736.434 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.736.434 I llama_perf_context_print:       total time =     131.52 ms /   129 tokens
0.00.736.922 I ggml_metal_free: deallocating

real	0m0.752s
user	0m0.078s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.771 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.591 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.507 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.448 I llama_model_loader: - type  f32:  194 tensors
0.00.023.449 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.449 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.557 I llm_load_vocab: special tokens cache size = 25
0.00.049.591 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.593 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.594 I llm_load_print_meta: arch             = gptneox
0.00.049.594 I llm_load_print_meta: vocab type       = BPE
0.00.049.594 I llm_load_print_meta: n_vocab          = 50304
0.00.049.595 I llm_load_print_meta: n_merges         = 50009
0.00.049.595 I llm_load_print_meta: vocab_only       = 0
0.00.049.595 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.595 I llm_load_print_meta: n_embd           = 2048
0.00.049.595 I llm_load_print_meta: n_layer          = 24
0.00.049.598 I llm_load_print_meta: n_head           = 16
0.00.049.599 I llm_load_print_meta: n_head_kv        = 16
0.00.049.599 I llm_load_print_meta: n_rot            = 32
0.00.049.599 I llm_load_print_meta: n_swa            = 0
0.00.049.599 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.599 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.600 I llm_load_print_meta: n_gqa            = 1
0.00.049.601 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.602 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.602 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.603 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.603 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.603 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.603 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.604 I llm_load_print_meta: n_ff             = 8192
0.00.049.604 I llm_load_print_meta: n_expert         = 0
0.00.049.604 I llm_load_print_meta: n_expert_used    = 0
0.00.049.604 I llm_load_print_meta: causal attn      = 1
0.00.049.604 I llm_load_print_meta: pooling type     = 0
0.00.049.604 I llm_load_print_meta: rope type        = 2
0.00.049.607 I llm_load_print_meta: rope scaling     = linear
0.00.049.607 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.607 I llm_load_print_meta: freq_scale_train = 1
0.00.049.613 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.615 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.615 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.616 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.616 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.616 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.616 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.616 I llm_load_print_meta: model type       = 1.4B
0.00.049.629 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.630 I llm_load_print_meta: model params     = 1.41 B
0.00.049.630 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.630 I llm_load_print_meta: general.name     = 1.4B
0.00.049.630 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.631 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.631 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.631 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.631 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.631 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.632 I llm_load_print_meta: max token length = 1024
0.00.051.583 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.584 I llm_load_tensors: offloading output layer to GPU
0.00.051.584 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.594 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.596 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.483 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.484 I llama_new_context_with_model: n_ctx         = 128
0.00.052.484 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.485 I llama_new_context_with_model: n_batch       = 128
0.00.052.485 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.485 I llama_new_context_with_model: flash_attn    = 0
0.00.052.485 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.486 I llama_new_context_with_model: freq_scale    = 1
0.00.052.486 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.487 I ggml_metal_init: allocating
0.00.052.493 I ggml_metal_init: found device: Apple M4
0.00.052.495 I ggml_metal_init: picking default device: Apple M4
0.00.053.089 I ggml_metal_init: using embedded metal library
0.00.055.406 I ggml_metal_init: GPU name:   Apple M4
0.00.055.407 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.408 I ggml_metal_init: simdgroup reduction   = true
0.00.055.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.408 I ggml_metal_init: has bfloat            = true
0.00.055.408 I ggml_metal_init: use bfloat            = true
0.00.055.409 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.030 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.361 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.363 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.376 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.265 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.266 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.266 I llama_new_context_with_model: graph nodes  = 967
0.00.067.266 I llama_new_context_with_model: graph splits = 2
0.00.067.267 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.268 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.089 I 
0.00.646.119 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.132 I perplexity: tokenizing the input ..
0.00.653.721 I perplexity: tokenization took 7.588 ms
0.00.653.724 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.417 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.777.069 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.777.080 I llama_perf_context_print:        load time =     637.31 ms
0.00.777.081 I llama_perf_context_print: prompt eval time =     121.46 ms /   128 tokens (    0.95 ms per token,  1053.86 tokens per second)
0.00.777.081 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.082 I llama_perf_context_print:       total time =     130.99 ms /   129 tokens
0.00.777.478 I ggml_metal_free: deallocating

real	0m0.791s
user	0m0.078s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.707 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.354 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.358 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.364 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.365 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.367 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.370 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.372 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.373 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.373 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.880 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.886 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.886 I llama_model_loader: - type  f32:  194 tensors
0.00.024.887 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.887 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.199 I llm_load_vocab: special tokens cache size = 25
0.00.052.194 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.198 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.198 I llm_load_print_meta: arch             = gptneox
0.00.052.199 I llm_load_print_meta: vocab type       = BPE
0.00.052.199 I llm_load_print_meta: n_vocab          = 50304
0.00.052.199 I llm_load_print_meta: n_merges         = 50009
0.00.052.199 I llm_load_print_meta: vocab_only       = 0
0.00.052.200 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.200 I llm_load_print_meta: n_embd           = 2048
0.00.052.200 I llm_load_print_meta: n_layer          = 24
0.00.052.204 I llm_load_print_meta: n_head           = 16
0.00.052.205 I llm_load_print_meta: n_head_kv        = 16
0.00.052.205 I llm_load_print_meta: n_rot            = 32
0.00.052.205 I llm_load_print_meta: n_swa            = 0
0.00.052.205 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.205 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.206 I llm_load_print_meta: n_gqa            = 1
0.00.052.207 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.209 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.210 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.210 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.210 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.211 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.211 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.211 I llm_load_print_meta: n_ff             = 8192
0.00.052.212 I llm_load_print_meta: n_expert         = 0
0.00.052.213 I llm_load_print_meta: n_expert_used    = 0
0.00.052.213 I llm_load_print_meta: causal attn      = 1
0.00.052.213 I llm_load_print_meta: pooling type     = 0
0.00.052.213 I llm_load_print_meta: rope type        = 2
0.00.052.213 I llm_load_print_meta: rope scaling     = linear
0.00.052.213 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.216 I llm_load_print_meta: freq_scale_train = 1
0.00.052.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.216 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.216 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.216 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.216 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.217 I llm_load_print_meta: model type       = 1.4B
0.00.052.229 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.230 I llm_load_print_meta: model params     = 1.41 B
0.00.052.230 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.230 I llm_load_print_meta: general.name     = 1.4B
0.00.052.231 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.231 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.231 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.231 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.231 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.232 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.232 I llm_load_print_meta: max token length = 1024
0.00.054.316 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.316 I llm_load_tensors: offloading output layer to GPU
0.00.054.317 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.328 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.329 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.232 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.233 I llama_new_context_with_model: n_ctx         = 128
0.00.055.233 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.233 I llama_new_context_with_model: n_batch       = 128
0.00.055.233 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.234 I llama_new_context_with_model: flash_attn    = 0
0.00.055.234 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.234 I llama_new_context_with_model: freq_scale    = 1
0.00.055.235 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.236 I ggml_metal_init: allocating
0.00.055.241 I ggml_metal_init: found device: Apple M4
0.00.055.243 I ggml_metal_init: picking default device: Apple M4
0.00.055.835 I ggml_metal_init: using embedded metal library
0.00.058.175 I ggml_metal_init: GPU name:   Apple M4
0.00.058.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.177 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.178 I ggml_metal_init: simdgroup reduction   = true
0.00.058.178 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.178 I ggml_metal_init: has bfloat            = true
0.00.058.178 I ggml_metal_init: use bfloat            = true
0.00.058.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.372 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.666 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.670 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.687 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.573 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.574 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.574 I llama_new_context_with_model: graph nodes  = 967
0.00.070.574 I llama_new_context_with_model: graph splits = 2
0.00.070.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.576 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.866 I 
0.00.647.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.914 I perplexity: tokenizing the input ..
0.00.655.446 I perplexity: tokenization took 7.53 ms
0.00.655.450 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.803 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.791.325 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.791.339 I llama_perf_context_print:        load time =     638.15 ms
0.00.791.340 I llama_perf_context_print: prompt eval time =     134.12 ms /   128 tokens (    1.05 ms per token,   954.38 tokens per second)
0.00.791.340 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.341 I llama_perf_context_print:       total time =     143.47 ms /   129 tokens
0.00.791.737 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.080s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.081 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.183 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.187 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.189 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.190 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.191 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.347 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.467 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.470 I llama_model_loader: - type  f32:  194 tensors
0.00.024.470 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.471 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.353 I llm_load_vocab: special tokens cache size = 25
0.00.051.493 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.497 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.498 I llm_load_print_meta: arch             = gptneox
0.00.051.498 I llm_load_print_meta: vocab type       = BPE
0.00.051.500 I llm_load_print_meta: n_vocab          = 50304
0.00.051.500 I llm_load_print_meta: n_merges         = 50009
0.00.051.500 I llm_load_print_meta: vocab_only       = 0
0.00.051.501 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.501 I llm_load_print_meta: n_embd           = 2048
0.00.051.501 I llm_load_print_meta: n_layer          = 24
0.00.051.505 I llm_load_print_meta: n_head           = 16
0.00.051.505 I llm_load_print_meta: n_head_kv        = 16
0.00.051.505 I llm_load_print_meta: n_rot            = 32
0.00.051.505 I llm_load_print_meta: n_swa            = 0
0.00.051.506 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.506 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.506 I llm_load_print_meta: n_gqa            = 1
0.00.051.507 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.508 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.508 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.508 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.508 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.509 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.509 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.510 I llm_load_print_meta: n_ff             = 8192
0.00.051.510 I llm_load_print_meta: n_expert         = 0
0.00.051.510 I llm_load_print_meta: n_expert_used    = 0
0.00.051.510 I llm_load_print_meta: causal attn      = 1
0.00.051.511 I llm_load_print_meta: pooling type     = 0
0.00.051.511 I llm_load_print_meta: rope type        = 2
0.00.051.511 I llm_load_print_meta: rope scaling     = linear
0.00.051.511 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.512 I llm_load_print_meta: freq_scale_train = 1
0.00.051.512 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.512 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.512 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.512 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.512 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.512 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.513 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.513 I llm_load_print_meta: model type       = 1.4B
0.00.051.525 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.526 I llm_load_print_meta: model params     = 1.41 B
0.00.051.526 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.526 I llm_load_print_meta: general.name     = 1.4B
0.00.051.526 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.527 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.528 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.529 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.529 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.529 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.529 I llm_load_print_meta: max token length = 1024
0.00.053.494 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.495 I llm_load_tensors: offloading output layer to GPU
0.00.053.495 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.506 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.507 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.459 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.460 I llama_new_context_with_model: n_ctx         = 128
0.00.054.460 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.461 I llama_new_context_with_model: n_batch       = 128
0.00.054.461 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.461 I llama_new_context_with_model: flash_attn    = 0
0.00.054.461 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.461 I llama_new_context_with_model: freq_scale    = 1
0.00.054.462 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.462 I ggml_metal_init: allocating
0.00.054.466 I ggml_metal_init: found device: Apple M4
0.00.054.468 I ggml_metal_init: picking default device: Apple M4
0.00.055.097 I ggml_metal_init: using embedded metal library
0.00.057.598 I ggml_metal_init: GPU name:   Apple M4
0.00.057.599 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.600 I ggml_metal_init: simdgroup reduction   = true
0.00.057.600 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.622 I ggml_metal_init: has bfloat            = true
0.00.057.623 I ggml_metal_init: use bfloat            = true
0.00.057.623 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.624 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.989 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.265 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.277 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.300 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.241 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.242 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.243 I llama_new_context_with_model: graph nodes  = 967
0.00.069.243 I llama_new_context_with_model: graph splits = 2
0.00.069.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.353 I 
0.00.711.403 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.416 I perplexity: tokenizing the input ..
0.00.718.919 I perplexity: tokenization took 7.501 ms
0.00.718.924 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.062 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.854.490 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.854.508 I llama_perf_context_print:        load time =     702.26 ms
0.00.854.509 I llama_perf_context_print: prompt eval time =     133.91 ms /   128 tokens (    1.05 ms per token,   955.89 tokens per second)
0.00.854.510 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.510 I llama_perf_context_print:       total time =     143.16 ms /   129 tokens
0.00.854.885 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.079s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.175 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.892 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.904 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.904 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.905 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.906 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.906 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.906 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.907 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.907 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.910 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.911 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.554 I llama_model_loader: - type  f32:  194 tensors
0.00.024.554 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.555 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.680 I llm_load_vocab: special tokens cache size = 25
0.00.051.631 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.634 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.634 I llm_load_print_meta: arch             = gptneox
0.00.051.634 I llm_load_print_meta: vocab type       = BPE
0.00.051.635 I llm_load_print_meta: n_vocab          = 50304
0.00.051.635 I llm_load_print_meta: n_merges         = 50009
0.00.051.635 I llm_load_print_meta: vocab_only       = 0
0.00.051.635 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.635 I llm_load_print_meta: n_embd           = 2048
0.00.051.636 I llm_load_print_meta: n_layer          = 24
0.00.051.639 I llm_load_print_meta: n_head           = 16
0.00.051.640 I llm_load_print_meta: n_head_kv        = 16
0.00.051.640 I llm_load_print_meta: n_rot            = 32
0.00.051.640 I llm_load_print_meta: n_swa            = 0
0.00.051.640 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.640 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.641 I llm_load_print_meta: n_gqa            = 1
0.00.051.642 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.643 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.643 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.644 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.644 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.644 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.644 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.645 I llm_load_print_meta: n_ff             = 8192
0.00.051.645 I llm_load_print_meta: n_expert         = 0
0.00.051.645 I llm_load_print_meta: n_expert_used    = 0
0.00.051.645 I llm_load_print_meta: causal attn      = 1
0.00.051.646 I llm_load_print_meta: pooling type     = 0
0.00.051.646 I llm_load_print_meta: rope type        = 2
0.00.051.646 I llm_load_print_meta: rope scaling     = linear
0.00.051.649 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.649 I llm_load_print_meta: freq_scale_train = 1
0.00.051.649 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.649 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.649 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.650 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.650 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.650 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.650 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.650 I llm_load_print_meta: model type       = 1.4B
0.00.051.662 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.662 I llm_load_print_meta: model params     = 1.41 B
0.00.051.663 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.663 I llm_load_print_meta: general.name     = 1.4B
0.00.051.664 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.664 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.665 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.665 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.665 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.665 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.666 I llm_load_print_meta: max token length = 1024
0.00.053.614 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.614 I llm_load_tensors: offloading output layer to GPU
0.00.053.614 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.625 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.626 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.559 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.560 I llama_new_context_with_model: n_ctx         = 128
0.00.054.560 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.561 I llama_new_context_with_model: n_batch       = 128
0.00.054.561 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.561 I llama_new_context_with_model: flash_attn    = 0
0.00.054.561 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.561 I llama_new_context_with_model: freq_scale    = 1
0.00.054.562 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.562 I ggml_metal_init: allocating
0.00.054.566 I ggml_metal_init: found device: Apple M4
0.00.054.568 I ggml_metal_init: picking default device: Apple M4
0.00.055.143 I ggml_metal_init: using embedded metal library
0.00.057.590 I ggml_metal_init: GPU name:   Apple M4
0.00.057.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.593 I ggml_metal_init: simdgroup reduction   = true
0.00.057.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.593 I ggml_metal_init: has bfloat            = true
0.00.057.593 I ggml_metal_init: use bfloat            = true
0.00.057.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.855 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.146 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.149 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.165 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.160 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.161 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.161 I llama_new_context_with_model: graph nodes  = 967
0.00.070.161 I llama_new_context_with_model: graph splits = 2
0.00.070.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.949 I 
0.00.387.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.000 I perplexity: tokenizing the input ..
0.00.395.642 I perplexity: tokenization took 7.64 ms
0.00.395.646 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.065 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.529.219 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.233 I llama_perf_context_print:        load time =     378.77 ms
0.00.529.234 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.29 tokens per second)
0.00.529.235 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.235 I llama_perf_context_print:       total time =     141.28 ms /   129 tokens
0.00.529.678 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.081s
sys	0m0.070s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.725 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.646 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.651 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.657 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.658 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.660 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.660 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.660 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.661 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.664 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.843 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.917 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.918 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.918 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.918 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.919 I llama_model_loader: - type  f32:  194 tensors
0.00.023.919 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.920 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.920 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.834 I llm_load_vocab: special tokens cache size = 25
0.00.050.719 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.721 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.721 I llm_load_print_meta: arch             = gptneox
0.00.050.722 I llm_load_print_meta: vocab type       = BPE
0.00.050.722 I llm_load_print_meta: n_vocab          = 50304
0.00.050.722 I llm_load_print_meta: n_merges         = 50009
0.00.050.722 I llm_load_print_meta: vocab_only       = 0
0.00.050.723 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.723 I llm_load_print_meta: n_embd           = 2048
0.00.050.723 I llm_load_print_meta: n_layer          = 24
0.00.050.726 I llm_load_print_meta: n_head           = 16
0.00.050.727 I llm_load_print_meta: n_head_kv        = 16
0.00.050.727 I llm_load_print_meta: n_rot            = 32
0.00.050.727 I llm_load_print_meta: n_swa            = 0
0.00.050.728 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.728 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.729 I llm_load_print_meta: n_gqa            = 1
0.00.050.729 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.730 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.731 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.731 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.731 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.731 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.731 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.732 I llm_load_print_meta: n_ff             = 8192
0.00.050.733 I llm_load_print_meta: n_expert         = 0
0.00.050.733 I llm_load_print_meta: n_expert_used    = 0
0.00.050.734 I llm_load_print_meta: causal attn      = 1
0.00.050.734 I llm_load_print_meta: pooling type     = 0
0.00.050.734 I llm_load_print_meta: rope type        = 2
0.00.050.734 I llm_load_print_meta: rope scaling     = linear
0.00.050.735 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.735 I llm_load_print_meta: freq_scale_train = 1
0.00.050.736 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.737 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.737 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.737 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.737 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.737 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.739 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.739 I llm_load_print_meta: model type       = 1.4B
0.00.050.751 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.752 I llm_load_print_meta: model params     = 1.41 B
0.00.050.752 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.752 I llm_load_print_meta: general.name     = 1.4B
0.00.050.753 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.753 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.753 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.753 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.753 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.754 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.754 I llm_load_print_meta: max token length = 1024
0.00.052.706 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.706 I llm_load_tensors: offloading output layer to GPU
0.00.052.706 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.717 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.718 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.600 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.601 I llama_new_context_with_model: n_ctx         = 128
0.00.053.601 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.601 I llama_new_context_with_model: n_batch       = 128
0.00.053.601 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.601 I llama_new_context_with_model: flash_attn    = 0
0.00.053.602 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.602 I llama_new_context_with_model: freq_scale    = 1
0.00.053.602 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.603 I ggml_metal_init: allocating
0.00.053.606 I ggml_metal_init: found device: Apple M4
0.00.053.608 I ggml_metal_init: picking default device: Apple M4
0.00.054.172 I ggml_metal_init: using embedded metal library
0.00.056.521 I ggml_metal_init: GPU name:   Apple M4
0.00.056.523 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.524 I ggml_metal_init: simdgroup reduction   = true
0.00.056.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.524 I ggml_metal_init: has bfloat            = true
0.00.056.524 I ggml_metal_init: use bfloat            = true
0.00.056.524 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.525 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.317 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.613 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.617 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.631 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.576 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.577 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.578 I llama_new_context_with_model: graph nodes  = 967
0.00.068.578 I llama_new_context_with_model: graph splits = 2
0.00.068.579 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.781 I 
0.00.478.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.879 I perplexity: tokenizing the input ..
0.00.486.540 I perplexity: tokenization took 7.66 ms
0.00.486.544 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.010 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.620.247 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.620.267 I llama_perf_context_print:        load time =     470.05 ms
0.00.620.268 I llama_perf_context_print: prompt eval time =     132.23 ms /   128 tokens (    1.03 ms per token,   968.01 tokens per second)
0.00.620.269 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.620.269 I llama_perf_context_print:       total time =     141.49 ms /   129 tokens
0.00.620.819 I ggml_metal_free: deallocating

real	0m0.635s
user	0m0.079s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.887 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.868 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.872 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.875 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.875 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.876 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.877 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.878 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.878 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.880 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.881 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.881 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.050 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.211 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.213 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.214 I llama_model_loader: - type  f32:  194 tensors
0.00.024.214 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.215 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.215 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.260 I llm_load_vocab: special tokens cache size = 25
0.00.051.126 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.130 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.130 I llm_load_print_meta: arch             = gptneox
0.00.051.130 I llm_load_print_meta: vocab type       = BPE
0.00.051.131 I llm_load_print_meta: n_vocab          = 50304
0.00.051.131 I llm_load_print_meta: n_merges         = 50009
0.00.051.131 I llm_load_print_meta: vocab_only       = 0
0.00.051.131 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.131 I llm_load_print_meta: n_embd           = 2048
0.00.051.132 I llm_load_print_meta: n_layer          = 24
0.00.051.134 I llm_load_print_meta: n_head           = 16
0.00.051.135 I llm_load_print_meta: n_head_kv        = 16
0.00.051.135 I llm_load_print_meta: n_rot            = 32
0.00.051.135 I llm_load_print_meta: n_swa            = 0
0.00.051.136 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.136 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.136 I llm_load_print_meta: n_gqa            = 1
0.00.051.137 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.138 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.138 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.141 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.141 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.141 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.141 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.142 I llm_load_print_meta: n_ff             = 8192
0.00.051.142 I llm_load_print_meta: n_expert         = 0
0.00.051.142 I llm_load_print_meta: n_expert_used    = 0
0.00.051.143 I llm_load_print_meta: causal attn      = 1
0.00.051.143 I llm_load_print_meta: pooling type     = 0
0.00.051.143 I llm_load_print_meta: rope type        = 2
0.00.051.143 I llm_load_print_meta: rope scaling     = linear
0.00.051.144 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.144 I llm_load_print_meta: freq_scale_train = 1
0.00.051.145 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.145 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.145 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.145 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.145 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.147 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.147 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.148 I llm_load_print_meta: model type       = 1.4B
0.00.051.160 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.160 I llm_load_print_meta: model params     = 1.41 B
0.00.051.162 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.162 I llm_load_print_meta: general.name     = 1.4B
0.00.051.163 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.163 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.163 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.163 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.163 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.163 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.164 I llm_load_print_meta: max token length = 1024
0.00.053.175 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.175 I llm_load_tensors: offloading output layer to GPU
0.00.053.175 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.186 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.187 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.081 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.082 I llama_new_context_with_model: n_ctx         = 128
0.00.054.082 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.082 I llama_new_context_with_model: n_batch       = 128
0.00.054.082 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.082 I llama_new_context_with_model: flash_attn    = 0
0.00.054.083 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.083 I llama_new_context_with_model: freq_scale    = 1
0.00.054.083 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.084 I ggml_metal_init: allocating
0.00.054.090 I ggml_metal_init: found device: Apple M4
0.00.054.092 I ggml_metal_init: picking default device: Apple M4
0.00.054.650 I ggml_metal_init: using embedded metal library
0.00.056.963 I ggml_metal_init: GPU name:   Apple M4
0.00.056.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.965 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.966 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.966 I ggml_metal_init: simdgroup reduction   = true
0.00.056.966 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.966 I ggml_metal_init: has bfloat            = true
0.00.056.966 I ggml_metal_init: use bfloat            = true
0.00.056.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.970 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.248 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.507 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.513 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.527 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.400 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.401 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.402 I llama_new_context_with_model: graph nodes  = 967
0.00.068.402 I llama_new_context_with_model: graph splits = 2
0.00.068.403 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.546.621 I 
0.00.546.675 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.546.700 I perplexity: tokenizing the input ..
0.00.554.342 I perplexity: tokenization took 7.64 ms
0.00.554.346 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.688.522 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.689.699 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.689.716 I llama_perf_context_print:        load time =     537.73 ms
0.00.689.717 I llama_perf_context_print: prompt eval time =     133.95 ms /   128 tokens (    1.05 ms per token,   955.57 tokens per second)
0.00.689.718 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.718 I llama_perf_context_print:       total time =     143.10 ms /   129 tokens
0.00.690.242 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.079s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.779 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.668 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.627 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.629 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.629 I llama_model_loader: - type  f32:  194 tensors
0.00.024.630 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.630 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.669 I llm_load_vocab: special tokens cache size = 25
0.00.051.660 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.663 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.664 I llm_load_print_meta: arch             = gptneox
0.00.051.664 I llm_load_print_meta: vocab type       = BPE
0.00.051.664 I llm_load_print_meta: n_vocab          = 50304
0.00.051.665 I llm_load_print_meta: n_merges         = 50009
0.00.051.665 I llm_load_print_meta: vocab_only       = 0
0.00.051.665 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.665 I llm_load_print_meta: n_embd           = 2048
0.00.051.665 I llm_load_print_meta: n_layer          = 24
0.00.051.668 I llm_load_print_meta: n_head           = 16
0.00.051.671 I llm_load_print_meta: n_head_kv        = 16
0.00.051.671 I llm_load_print_meta: n_rot            = 32
0.00.051.671 I llm_load_print_meta: n_swa            = 0
0.00.051.671 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.671 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.672 I llm_load_print_meta: n_gqa            = 1
0.00.051.673 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.674 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.674 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.675 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.675 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.675 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.675 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.676 I llm_load_print_meta: n_ff             = 8192
0.00.051.676 I llm_load_print_meta: n_expert         = 0
0.00.051.676 I llm_load_print_meta: n_expert_used    = 0
0.00.051.676 I llm_load_print_meta: causal attn      = 1
0.00.051.677 I llm_load_print_meta: pooling type     = 0
0.00.051.677 I llm_load_print_meta: rope type        = 2
0.00.051.677 I llm_load_print_meta: rope scaling     = linear
0.00.051.677 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.678 I llm_load_print_meta: freq_scale_train = 1
0.00.051.678 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.678 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.678 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.678 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.678 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.679 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.679 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.681 I llm_load_print_meta: model type       = 1.4B
0.00.051.693 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.694 I llm_load_print_meta: model params     = 1.41 B
0.00.051.694 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.694 I llm_load_print_meta: general.name     = 1.4B
0.00.051.694 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.695 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.696 I llm_load_print_meta: max token length = 1024
0.00.053.760 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.760 I llm_load_tensors: offloading output layer to GPU
0.00.053.760 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.771 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.772 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.667 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.668 I llama_new_context_with_model: n_ctx         = 128
0.00.054.668 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.668 I llama_new_context_with_model: n_batch       = 128
0.00.054.668 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.668 I llama_new_context_with_model: flash_attn    = 0
0.00.054.669 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.669 I llama_new_context_with_model: freq_scale    = 1
0.00.054.669 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.670 I ggml_metal_init: allocating
0.00.054.677 I ggml_metal_init: found device: Apple M4
0.00.054.679 I ggml_metal_init: picking default device: Apple M4
0.00.055.221 I ggml_metal_init: using embedded metal library
0.00.057.547 I ggml_metal_init: GPU name:   Apple M4
0.00.057.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.550 I ggml_metal_init: simdgroup reduction   = true
0.00.057.550 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.550 I ggml_metal_init: has bfloat            = true
0.00.057.550 I ggml_metal_init: use bfloat            = true
0.00.057.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.837 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.082 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.084 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.097 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.882 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.883 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.883 I llama_new_context_with_model: graph nodes  = 967
0.00.068.883 I llama_new_context_with_model: graph splits = 2
0.00.068.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.018 I 
0.00.656.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.102 I perplexity: tokenizing the input ..
0.00.663.419 I perplexity: tokenization took 7.313 ms
0.00.663.422 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.092 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.805.252 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.805.276 I llama_perf_context_print:        load time =     646.23 ms
0.00.805.277 I llama_perf_context_print: prompt eval time =     140.44 ms /   128 tokens (    1.10 ms per token,   911.41 tokens per second)
0.00.805.278 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.279 I llama_perf_context_print:       total time =     149.26 ms /   129 tokens
0.00.805.756 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.079s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.506 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.511 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.514 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.514 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.515 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.515 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.516 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.523 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.871 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.874 I llama_model_loader: - type  f32:  194 tensors
0.00.023.874 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.721 I llm_load_vocab: special tokens cache size = 25
0.00.049.558 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.560 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.561 I llm_load_print_meta: arch             = gptneox
0.00.049.561 I llm_load_print_meta: vocab type       = BPE
0.00.049.561 I llm_load_print_meta: n_vocab          = 50304
0.00.049.562 I llm_load_print_meta: n_merges         = 50009
0.00.049.562 I llm_load_print_meta: vocab_only       = 0
0.00.049.562 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.562 I llm_load_print_meta: n_embd           = 2048
0.00.049.562 I llm_load_print_meta: n_layer          = 24
0.00.049.565 I llm_load_print_meta: n_head           = 16
0.00.049.566 I llm_load_print_meta: n_head_kv        = 16
0.00.049.566 I llm_load_print_meta: n_rot            = 32
0.00.049.566 I llm_load_print_meta: n_swa            = 0
0.00.049.567 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.567 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.567 I llm_load_print_meta: n_gqa            = 1
0.00.049.568 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.569 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.570 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.570 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.570 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.570 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.570 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.573 I llm_load_print_meta: n_ff             = 8192
0.00.049.573 I llm_load_print_meta: n_expert         = 0
0.00.049.573 I llm_load_print_meta: n_expert_used    = 0
0.00.049.573 I llm_load_print_meta: causal attn      = 1
0.00.049.574 I llm_load_print_meta: pooling type     = 0
0.00.049.574 I llm_load_print_meta: rope type        = 2
0.00.049.574 I llm_load_print_meta: rope scaling     = linear
0.00.049.576 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.576 I llm_load_print_meta: freq_scale_train = 1
0.00.049.576 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.576 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.577 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.577 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.577 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.577 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.577 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.577 I llm_load_print_meta: model type       = 1.4B
0.00.049.589 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.589 I llm_load_print_meta: model params     = 1.41 B
0.00.049.590 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.590 I llm_load_print_meta: general.name     = 1.4B
0.00.049.590 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.590 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.590 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.591 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.591 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.591 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.591 I llm_load_print_meta: max token length = 1024
0.00.051.590 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.590 I llm_load_tensors: offloading output layer to GPU
0.00.051.590 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.601 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.602 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.487 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.488 I llama_new_context_with_model: n_ctx         = 128
0.00.052.488 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.488 I llama_new_context_with_model: n_batch       = 128
0.00.052.489 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.489 I llama_new_context_with_model: flash_attn    = 0
0.00.052.489 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.489 I llama_new_context_with_model: freq_scale    = 1
0.00.052.490 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.490 I ggml_metal_init: allocating
0.00.052.496 I ggml_metal_init: found device: Apple M4
0.00.052.498 I ggml_metal_init: picking default device: Apple M4
0.00.053.036 I ggml_metal_init: using embedded metal library
0.00.055.314 I ggml_metal_init: GPU name:   Apple M4
0.00.055.316 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.316 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.316 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.316 I ggml_metal_init: simdgroup reduction   = true
0.00.055.317 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.317 I ggml_metal_init: has bfloat            = true
0.00.055.317 I ggml_metal_init: use bfloat            = true
0.00.055.317 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.318 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.821 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.338 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.342 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.356 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.226 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.227 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.227 I llama_new_context_with_model: graph nodes  = 967
0.00.067.228 I llama_new_context_with_model: graph splits = 2
0.00.067.229 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.229 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.684 I 
0.00.432.715 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.432.728 I perplexity: tokenizing the input ..
0.00.440.347 I perplexity: tokenization took 7.618 ms
0.00.440.350 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.580.316 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.581.503 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.581.522 I llama_perf_context_print:        load time =     423.88 ms
0.00.581.523 I llama_perf_context_print: prompt eval time =     139.74 ms /   128 tokens (    1.09 ms per token,   915.99 tokens per second)
0.00.581.524 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.581.524 I llama_perf_context_print:       total time =     148.84 ms /   129 tokens
0.00.582.021 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.078s
sys	0m0.087s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.354 I build: 4410 (f03c717a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.767 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.579 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.596 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.600 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.601 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.601 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.605 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.605 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.606 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.606 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.614 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.614 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.706 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.706 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.707 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.707 I llama_model_loader: - type  f32:  194 tensors
0.00.054.708 I llama_model_loader: - type  f16:   98 tensors
0.00.082.234 I llm_load_vocab: special tokens cache size = 25
0.00.088.649 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.652 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.652 I llm_load_print_meta: arch             = gptneox
0.00.088.652 I llm_load_print_meta: vocab type       = BPE
0.00.088.652 I llm_load_print_meta: n_vocab          = 50304
0.00.088.652 I llm_load_print_meta: n_merges         = 50009
0.00.088.653 I llm_load_print_meta: vocab_only       = 0
0.00.088.653 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.653 I llm_load_print_meta: n_embd           = 2048
0.00.088.653 I llm_load_print_meta: n_layer          = 24
0.00.088.656 I llm_load_print_meta: n_head           = 16
0.00.088.657 I llm_load_print_meta: n_head_kv        = 16
0.00.088.657 I llm_load_print_meta: n_rot            = 32
0.00.088.658 I llm_load_print_meta: n_swa            = 0
0.00.088.658 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.659 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.659 I llm_load_print_meta: n_gqa            = 1
0.00.088.660 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.660 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.661 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.661 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.661 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.661 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.662 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.662 I llm_load_print_meta: n_ff             = 8192
0.00.088.662 I llm_load_print_meta: n_expert         = 0
0.00.088.664 I llm_load_print_meta: n_expert_used    = 0
0.00.088.664 I llm_load_print_meta: causal attn      = 1
0.00.088.664 I llm_load_print_meta: pooling type     = 0
0.00.088.664 I llm_load_print_meta: rope type        = 2
0.00.088.664 I llm_load_print_meta: rope scaling     = linear
0.00.088.665 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.665 I llm_load_print_meta: freq_scale_train = 1
0.00.088.665 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.665 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.666 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.666 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.666 I llm_load_print_meta: model type       = 1.4B
0.00.088.678 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.679 I llm_load_print_meta: model params     = 1.41 B
0.00.088.680 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.680 I llm_load_print_meta: general.name     = 1.4B
0.00.088.681 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.681 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.681 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.681 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.682 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.682 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.682 I llm_load_print_meta: max token length = 1024
0.00.091.213 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.213 I llm_load_tensors: offloading output layer to GPU
0.00.091.213 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.224 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.225 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.206 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.207 I llama_new_context_with_model: n_ctx         = 128
0.00.092.207 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.207 I llama_new_context_with_model: n_batch       = 128
0.00.092.207 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.207 I llama_new_context_with_model: flash_attn    = 0
0.00.092.208 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.208 I llama_new_context_with_model: freq_scale    = 1
0.00.092.209 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.209 I ggml_metal_init: allocating
0.00.092.214 I ggml_metal_init: found device: Apple M4
0.00.092.218 I ggml_metal_init: picking default device: Apple M4
0.00.092.823 I ggml_metal_init: using embedded metal library
0.00.095.357 I ggml_metal_init: GPU name:   Apple M4
0.00.095.359 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.359 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.360 I ggml_metal_init: simdgroup reduction   = true
0.00.095.360 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.360 I ggml_metal_init: has bfloat            = true
0.00.095.360 I ggml_metal_init: use bfloat            = true
0.00.095.361 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.361 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.436 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.975 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.977 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.990 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.849 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.850 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.850 I llama_new_context_with_model: graph nodes  = 967
0.00.106.851 I llama_new_context_with_model: graph splits = 2
0.00.106.852 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.852 I 
0.00.106.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.877 I compute_imatrix: tokenizing the input ..
0.00.113.546 I compute_imatrix: tokenization took 6.669 ms
0.00.113.548 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.574.327 I compute_imatrix: 1.46 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.576.745 I llama_perf_context_print:        load time =    1551.56 ms
0.01.576.749 I llama_perf_context_print: prompt eval time =    1460.15 ms /   128 tokens (   11.41 ms per token,    87.66 tokens per second)
0.01.576.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.576.750 I llama_perf_context_print:       total time =    1553.97 ms /   129 tokens
0.01.577.298 I ggml_metal_free: deallocating

real	0m1.774s
user	0m0.166s
sys	0m0.230s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4410 (f03c717a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121c0a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121c0ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121c0b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121c0b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121c0bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121c0c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121c0c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121c0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121c0d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121c0d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121c0de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121c0e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121c0ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121c0f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121c0fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121c10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121c10c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121c11340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121c11a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121c12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121c12950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121c13070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121c13790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121c14030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121c14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121c14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121c15020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121c15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121c161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121c16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121c16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121c16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121c17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121c179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121c17c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121c18120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121c185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121c18a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121c18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121c193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121c19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121c19ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121c1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121c1a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121c1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121c1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121c1b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121c1be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121c1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121c1ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121c1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121c1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121c1dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121c1e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121c1ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121c1ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121c1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121c1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121c1fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121c20470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121c20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121c20bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121c21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121c21510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121c219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121c21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121c222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121c22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121c22c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121c230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121c23570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121c23a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121c23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121c24400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121c24950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121c24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121c253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121c25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121c25e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121c263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121c26930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121c26e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121c273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121c27920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121c27e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121c283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121c28910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121c28e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121c293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121c29900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121c29e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121c2a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121c2a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121c2ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121c2b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121c2b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121c2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121c1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121c2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121c2ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121c2cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121c2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121c2da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121c2df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121c2e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121c2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121c2ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121c2f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121c2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121c2ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121c304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121c30a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121c30f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121c31400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121c318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121c31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121c321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121c32680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121c32b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121c32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121c33460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121c33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121c33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121c34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121c346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121c34b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121c35020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121c354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121c35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121c35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121c362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121c36740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121c36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121c37080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121c37520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121c379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121c37e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121c38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121c387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121c38c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121c390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121c39580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121c39a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121c39ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121c3a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121c3a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121c3aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121c3b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121c3b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121c3ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121c3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121c3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121c3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121c3cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121c3d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121c3d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121c3dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121c3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121c3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121c3e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121c3ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121c3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121c3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121c3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121c3ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121c40480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121c40920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121c40dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121c41260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121c41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121c41ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121c42040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121c424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121c42980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121c42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121c432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121c43760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121c43c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121c440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121c44540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121c449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121c44e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121c45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121c457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121c45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121c46100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121c465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121c46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121c46ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121c47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121c47820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121c47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121c48160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121c486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121c48c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121c49150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121c496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121c49960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121c49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121c4a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121c4ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121c4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121c4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121c4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121c4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121c4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121c4cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121c4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121c4d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121c4dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121c4e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121c4e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121c4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121c4f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121c4f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121c4ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121c50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121c509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121c50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121c51450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121c519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121c51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121c52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121c52990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121c52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121c53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121c53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121c53ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121c54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121c54970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121c54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121c55410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121c55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121c55eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121c56400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121c56950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121c56ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121c573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121c57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121c57e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121c583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121c58930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121c58e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121c593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121c59920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121c59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121c5a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121c5a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121c5ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121c5b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121c5b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121c5be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121c5c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121c5c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121c5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121c5d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121c5d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121c5de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121c5e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121c5e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121c5ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121c5f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121c5f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121c5fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121c60360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121c608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121c60e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121c612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121c61740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121c61be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121c62080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121c62520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121c629c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121c62e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121c63300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121c637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121c63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121c640e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121c64580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121c64a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121c64ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121c65360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121c658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121c65fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121c666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121c66e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121c67530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121c677f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121c67fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121c682a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121c688b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10df04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10df04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10df05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10df05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10df05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10df06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10df065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10df06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10df06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10df07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10df07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10df07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10df08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10df09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10df09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10df0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10df0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10df0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10df0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10df0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10df0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10df0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10df0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10df0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10df0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10df0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10df0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10df0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10df0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10df0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10df0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10df0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10df10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10df10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10df108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10df10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10df11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10df11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10df11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10df11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10df12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10df127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10df12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10df130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10df13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10df13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10df13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10df14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10df146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10df14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10df14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10df15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10df15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10df15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10df16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10df165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10df16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121b04300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121b081a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121b08460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121b088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121b08d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121b091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121b09620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121b09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121b09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121b0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121b0a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121b0ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121b0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121b0b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121b0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121b0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121b0c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121b0c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121b0cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121b0cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121b0d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121b0d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121b0dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121b0e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121b0e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121b0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121b0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121b0f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121b0f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121b0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121b100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121b10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121b10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121b10df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121b11260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121b116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121b11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121b11fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121b12420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121b12890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121b12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121b13170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121b135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121b13a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121b13ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121b14330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121b147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121b14c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121b15080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121b154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121b15960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121b15dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121b16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121b166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121b16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121b16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121b17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121b17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121b17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121b18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121b185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121b18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121b18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121b19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121b19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121b19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121b1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121b1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121b1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121b1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121b1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121b1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121b1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121b1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121b1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121b1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121b1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121b1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121b1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121b1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121b1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121b1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121b1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121b1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121b1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121b1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121b1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121b1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121b20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121b20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121b20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121b20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121b213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121b21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121b21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121b22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121b22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121b229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121b22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121b232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121b23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121b23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121b24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121b24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121b24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121b24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121b251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121b25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121b25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121b266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121b269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121b26c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121b270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121b27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121b279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121b27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121b282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121b28710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121b28b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121b28ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121b29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121b298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121b29d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121b2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121b2a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121b2aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121b2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121b2b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121b2b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121b2bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121b2c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121b2c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121b2c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121b2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121b2d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121b2d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121b2db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121b2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121b2e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121b2e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121b2ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121b2f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121b2f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121b2fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121b2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121b30440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121b30950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121b30dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121b31230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121b316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121b31b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121b32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121b32540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121b330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121b33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121b33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121b33ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121b344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121b34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121b35030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121b355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121b35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121b36170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121b36730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121b36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121b372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121b37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121b37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121b383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121b389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121b38f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121b39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121b39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121b3a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121b3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121b3ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121b3b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121b3b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121b3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121b3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121b3c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121b3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121b3d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121b3da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121b3dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121b3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121b3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121b3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121b3f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121b3fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121b40270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121b40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121b40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121b413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121b41970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121b41f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121b424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121b42ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121b43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121b43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121b43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121b441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121b44770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121b44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121b452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121b458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121b45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121b46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121b469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121b46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121b47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121b47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121b47f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121b48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121b48970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121b48e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121b49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121b49870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121b49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121b4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121b4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121b4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121b4b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121b4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121b4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121b4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121b4ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121b4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121b4d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121b4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121b4e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121b4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121b4ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121b4f360 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121b3d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121b3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121b38c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121b36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121b45b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121b43330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121b410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121b3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121b36fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121b34770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121b397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121b3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121b3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121b3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121b44a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121b37570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121b3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121b3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121b40af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121b3dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121b39230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121b438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121b3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121b341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121b466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121b3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121b43eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121b39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121b3c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121b40530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121b37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121b421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121b33630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121b369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121b44ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121b427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121b3e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121b47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121b358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121b46cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121b34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121b455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121b3f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121b41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121b44470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121b42d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121b3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121b06160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121b4e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121b4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121b4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121b4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121b50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121b50540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121b50800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121b50ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121b50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121b51040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121b51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121b515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121b51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121b51b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121b51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121b520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121b52380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121b52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121b52900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121b52bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121b52e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121b53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121b53400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121b536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121b53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121b53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121b53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121b541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121b54480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121b54740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121b54a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121b54cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121b54f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121b55240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121b55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121b557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121b55a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121b55d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121b56000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121b562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121b56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121b56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121b56b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121b56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121b57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121b57340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121b57600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121b578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121b57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121b57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121b58100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121b583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121b58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121b58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121b58c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121b58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121b59180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121b59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121b59700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121b599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121b59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121b59f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121b5a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121b5a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121b5a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121b5aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121b5ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121b5afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121b5b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121b5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121b5b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121b5bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121b5bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121b5c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121b5c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121b5c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121b5c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121b5cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121b5ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121b5d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121b5d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121b5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121b5d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121b5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121b5de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121b5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121b5e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121b5e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121b5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121b5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121b5ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121b5f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121b5f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121b5f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121b5fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121b5fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121b5ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121b60240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121b60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121b607c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121b60a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121b60d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121b61000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121b612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121b61580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121b61840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121b61b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121b61dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121b62080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121b62340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121b62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121b628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121b62b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121b62e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121b63100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121b633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121b637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121b63a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121b63f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121b64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121b64980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121b64e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121b65380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121b65880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121b65d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121b66280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121b66780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121b66c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121b67180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121b67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121b67b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121b68080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121b68580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121b68a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121b68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121b69480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121b69980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121b69e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121b6a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121b6a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121b6ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121b6b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121b6b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121b6bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121b6c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121b6c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121b6cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121b6d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121b6d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121b6dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121b6e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121b6e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121b6ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121b6f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121b6fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121b6fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121b704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121b70ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121b712d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121b71770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121b71c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121b720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121b72860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121b72db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121b73300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121b73850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121b73da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121b742f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121b74840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121b74d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121b752e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121b75830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121b75d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121b762d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121b76820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121b76d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121b772c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121b77810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121b77d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121b782b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121b78800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121b78d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121b792a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121b797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121b79d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121b7a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121b7a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121b7ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121b7b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121b7b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121b7bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121b7c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121b7c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121b7cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121b7d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121b7d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121b7dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121b7e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121b7e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121b7ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121b7f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121b7f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121b7fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121b80230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121b80780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121b80cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121b81220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121b81770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121b81cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121b82210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121b82760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121b82cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121b83200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121b83750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121b83ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121b841f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121b84740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121b84c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121b851e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121b85680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121b85b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121b85fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121b86460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121b86900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121b86da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121b87240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121b876e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121b87b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121b88020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121b884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121b88960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121b88e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121b892a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121b89740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121b89c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121b8a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121b8aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121b8b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121b8b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121b8bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121b8c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121b8c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121b8cc90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.795s
user	0m0.296s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4410 (f03c717a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ff102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ff109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ff10fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ff11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ff11b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ff120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ff12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ff12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ff131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ff136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ff13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ff140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ff14be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ff15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ff15ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ff162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ff169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ff17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ff17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ff17ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ff18710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ff18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ff19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ff19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ff1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ff1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ff1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ff1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ff1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ff1c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ff1c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ff1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ff1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ff1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ff1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ff1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ff1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ff1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ff1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ff1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ff1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ff1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ff1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ff203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ff206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ff20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ff212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ff21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ff221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ff22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ff22e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ff23420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ff23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ff24040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ff24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ff24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ff25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ff25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ff25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ff26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ff264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ff26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ff26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ff272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ff27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ff27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ff280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ff28550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ff289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ff28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ff29330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ff297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ff29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ff2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ff2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ff2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ff2b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ff2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ff2bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ff2c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ff2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ff2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ff2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ff2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ff2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ff2e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ff2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ff2ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ff2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ff2f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ff2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ff30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ff306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ff30c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ff31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ff316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ff31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ff218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ff32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ff32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ff32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ff332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ff33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ff33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ff342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ff347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ff34d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ff35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ff357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ff35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ff36280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ff367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ff36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ff371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ff37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ff37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ff37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ff38440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ff388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ff38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ff39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ff396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ff39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ff3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ff3a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ff3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ff3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ff3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ff3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ff3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ff3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ff3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ff3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ff3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ff3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ff3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ff3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ff3e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ff3e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ff3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ff3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ff3f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ff3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ff3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ff40120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ff405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ff40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ff40f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ff413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ff41840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ff41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ff42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ff42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ff42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ff42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ff43400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ff438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ff43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ff441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ff44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ff44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ff44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ff45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ff45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ff45da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ff46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ff466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ff46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ff47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ff474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ff47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ff47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ff482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ff48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ff48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ff49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ff49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ff499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ff49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ff4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ff4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ff4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ff4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ff4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ff4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ff4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ff4c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ff4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ff4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ff4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ff4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ff4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ff4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ff4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ff4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ff4ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ff4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ff4f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ff4fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ff50340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ff50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ff51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ff515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ff518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ff51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ff524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ff52cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ff53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ff535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ff53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ff54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ff54790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ff54ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ff55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ff55780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ff55cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ff56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ff56770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ff56cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ff57210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ff57760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ff57cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ff58200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ff58750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ff58ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ff591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ff59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ff59c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ff5a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ff5a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ff5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ff5b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ff5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ff5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ff5c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ff5c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ff5cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ff5d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ff5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ff5dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ff5e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ff5e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ff5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ff5f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ff5f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ff5fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ff60180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ff606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ff60c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ff61170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ff616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ff61c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ff62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ff626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ff62c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ff63150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ff636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ff63bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ff64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ff64690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ff64be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ff65130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ff65680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ff65bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ff66120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ff66670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ff66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ff67060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ff67500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ff679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ff67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ff682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ff68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ff68c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ff690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ff69560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ff69a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ff69ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ff6a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ff6a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ff6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ff6b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ff6b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ff6bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ff6c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ff6cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ff6d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ff6d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ff6dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ff6e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ff6e670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ff04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ff04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ff053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ff05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ff05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ff06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ff06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ff069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ff06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ff073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ff07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ff07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ff089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ff091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ff099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ff0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ff0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ff0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ff0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ff0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ff0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ff0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ff0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ff0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ff0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ff0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ff0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ff0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ff0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ff0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ff0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ff0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ff10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ff10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ff109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ff10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ff11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ff11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ff11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ff11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ff12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ff128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ff12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ff131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ff13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ff13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ff13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ff14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ff147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ff14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ff150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ff15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ff15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ff15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ff16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ff166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ff16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ff17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ff175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ff17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ff17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ff18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ff18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ff18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ff19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ff194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ff19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ff19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ff1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ff1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ff1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ff1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ff1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ff1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ff1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ff1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ff1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ff1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ff1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ff1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ff1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ff1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ff1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ff1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ff1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ff1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ff1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ff1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ff1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ff1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ff203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ff20830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ff20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ff21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ff21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ff219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ff21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ff222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ff22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ff22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ff23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ff23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ff23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ff23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ff241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ff24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ff24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ff24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ff253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ff25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ff25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ff260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ff26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ff269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ff26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ff272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ff27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ff27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ff28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ff28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ff288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ff28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ff291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ff29630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ff29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ff29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ff2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ff2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ff2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ff2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ff2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ff2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ff2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ff2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ff2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ff2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ff2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ff2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ff2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ff2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ff2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ff2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ff2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ff2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ff2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ff2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ff2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ff300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ff30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ff30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ff30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ff31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ff316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ff31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ff31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ff32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ff328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ff32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ff33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ff335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ff33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ff33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ff34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ff347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ff34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ff35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ff35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ff35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ff36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ff366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ff36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ff36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ff37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ff37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ff37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ff38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ff385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ff38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ff38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ff39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ff39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ff39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ff3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ff3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ff3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ff3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ff3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ff3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ff3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ff3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ff3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ff3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ff3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ff3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ff3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ff3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ff3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ff3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ff3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ff3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ff3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ff3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ff3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ff3ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ff40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ff40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ff40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ff410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ff41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ff41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ff42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ff42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ff42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ff434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ff43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ff44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ff44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ff44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ff45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ff45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ff45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ff462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ff46880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ff46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ff47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ff479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ff47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ff48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ff48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ff490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ff49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ff49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ff4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ff4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ff4ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ff4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ff4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ff4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ff4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ff4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ff4d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ff4d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ff4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ff4e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ff4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ff4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ff4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ff4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ff4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ff503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ff50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ff50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ff51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ff51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ff52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ff52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ff52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ff531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ff53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ff53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ff54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ff548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ff54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ff55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ff55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ff55fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ff56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ff56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ff57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ff57540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ff57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ff57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ff58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ff58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ff58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ff59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ff59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ff59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ff5a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ff5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ff5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ff5b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ff5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ff5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ff5c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ff5ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ff5d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ff5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ff5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ff5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ff5e930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1210046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1210058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1210065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121006ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121007340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121007a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121008530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121008ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1210094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121009c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12100a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12100aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12100b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12100b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12100c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12100c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12100cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12100d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12100dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12100dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12100e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12100e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12100eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12100efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12100f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12100f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12100fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121010080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1210104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121010960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121010dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121011240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1210116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121011b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121011f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121012400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121012870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121012ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121013150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1210135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121013a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121013ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121014310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121014780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121014bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121015060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1210154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121015940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121015db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121016220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121016790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121016c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121017100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121017570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1210179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121017e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1210182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121018730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121018ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121019010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121019480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1210198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121019d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12101a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12101a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12101aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12101af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12101b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12101b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12101bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12101c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12101c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12101c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12101ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12101d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12101d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12101db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12101dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12101e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12101e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12101ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12101f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12101f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12101fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12101ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121020370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1210207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121020c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1210210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121021530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1210219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121021e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121022280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1210226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121022b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121022fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121023440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121023cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121023f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121024400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121024870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121024ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121025150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1210255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121025a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121025ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121026310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121026780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121026bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121027060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1210274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121027940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121027db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121028220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121028690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121028b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121028f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1210293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121029850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121029cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12102a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12102a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12102aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12102ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12102b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12102b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12102bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12102c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12102c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12102c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12102cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12102d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12102d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12102dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12102df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12102e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12102e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12102eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12102f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12102f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12102f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12102fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1210302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121030740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121030bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121031020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121031490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121031900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121031d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1210321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121032650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121032ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121032f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1210333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121033810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121033c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1210340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121034560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1210349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121034e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1210352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121035720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121035b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121036000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121036470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1210368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121036d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1210371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121037630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121037aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121037f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121038380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1210387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121038c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1210390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121039540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1210399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121039e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12103a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12103a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12103ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12103afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12103b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12103b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12103bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12103c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12103c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12103ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12103cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12103d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12103d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12103dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12103e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12103e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12103e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12103ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12103f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12103f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12103fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12103ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121040430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1210408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121040d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121041180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121041d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121042280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1210426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121042b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121042fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121043440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1210438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121043d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121044600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121044a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121044ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121045350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1210457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121045c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1210460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121046510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121046980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121046df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121047260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1210476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121047b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121047fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121048420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121048890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121048d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121049170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1210495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121049a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121049ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12104a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12104a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12104ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12104b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12104b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12104b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12104bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12104c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12104c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12104cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12104cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12104d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12104d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12104dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12104e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12104e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12104ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12104eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12104f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12104f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12104fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121050060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1210504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121050940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121050db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121051220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121051690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121051b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121051f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1210523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121052850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121052cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121053130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1210535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121053a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121053e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1210542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121054760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121054bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121055040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1210554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121055920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121056390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121056ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1210571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1210578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121057bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121058020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121058620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121058c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.918s
user	0m0.245s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
