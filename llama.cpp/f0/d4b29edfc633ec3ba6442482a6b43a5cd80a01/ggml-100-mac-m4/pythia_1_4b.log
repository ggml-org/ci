Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.572s
user	0m0.900s
sys	0m1.229s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Built target llava
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-log
[ 49%] Built target test-sampling
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Linking CXX executable ../bin/test-rope
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-quantize-perf
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Built target llama-imatrix
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-infill
[ 77%] Built target llama-gritlm
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-cli
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-parallel
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-passkey
[ 85%] Generating index.html.gz.hpp
[ 85%] Built target llama-perplexity
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-quantize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Built target llama-run
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-tts
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.049s
user	0m6.238s
sys	0m9.332s

main: quantize time =  5252.01 ms
main:    total time =  5252.01 ms

main: quantize time =  1899.65 ms
main:    total time =  1899.65 ms

main: quantize time =  2977.41 ms
main:    total time =  2977.41 ms

main: quantize time =  3497.80 ms
main:    total time =  3497.80 ms

main: quantize time =  5563.52 ms
main:    total time =  5563.52 ms

main: quantize time =  5230.05 ms
main:    total time =  5230.06 ms

main: quantize time =  5707.95 ms
main:    total time =  5707.95 ms

main: quantize time =  6764.81 ms
main:    total time =  6764.81 ms

main: quantize time =  5809.83 ms
main:    total time =  5809.83 ms

main: quantize time =  4720.01 ms
main:    total time =  4720.01 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.111 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.250 I main: llama backend init
0.00.000.257 I main: load the model and apply lora adapter, if any
0.00.051.217 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.063.550 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.564 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.578 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.578 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.070.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.072.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.463 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.464 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.464 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.466 I llama_model_loader: - type  f32:  194 tensors
0.00.079.466 I llama_model_loader: - type  f16:   98 tensors
0.00.079.469 I print_info: file format = GGUF V3 (latest)
0.00.079.470 I print_info: file type   = all F32 (guessed)
0.00.079.474 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.097.336 I load: special tokens cache size = 25
0.00.107.104 I load: token to piece cache size = 0.2984 MB
0.00.107.109 I print_info: arch             = gptneox
0.00.107.109 I print_info: vocab_only       = 0
0.00.107.110 I print_info: n_ctx_train      = 2048
0.00.107.110 I print_info: n_embd           = 2048
0.00.107.110 I print_info: n_layer          = 24
0.00.107.117 I print_info: n_head           = 16
0.00.107.118 I print_info: n_head_kv        = 16
0.00.107.118 I print_info: n_rot            = 32
0.00.107.118 I print_info: n_swa            = 0
0.00.107.118 I print_info: n_embd_head_k    = 128
0.00.107.118 I print_info: n_embd_head_v    = 128
0.00.107.119 I print_info: n_gqa            = 1
0.00.107.120 I print_info: n_embd_k_gqa     = 2048
0.00.107.121 I print_info: n_embd_v_gqa     = 2048
0.00.107.122 I print_info: f_norm_eps       = 1.0e-05
0.00.107.122 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.107.123 I print_info: f_clamp_kqv      = 0.0e+00
0.00.107.123 I print_info: f_max_alibi_bias = 0.0e+00
0.00.107.123 I print_info: f_logit_scale    = 0.0e+00
0.00.107.124 I print_info: n_ff             = 8192
0.00.107.124 I print_info: n_expert         = 0
0.00.107.124 I print_info: n_expert_used    = 0
0.00.107.125 I print_info: causal attn      = 1
0.00.107.125 I print_info: pooling type     = 0
0.00.107.125 I print_info: rope type        = 2
0.00.107.125 I print_info: rope scaling     = linear
0.00.107.129 I print_info: freq_base_train  = 10000.0
0.00.107.129 I print_info: freq_scale_train = 1
0.00.107.129 I print_info: n_ctx_orig_yarn  = 2048
0.00.107.130 I print_info: rope_finetuned   = unknown
0.00.107.130 I print_info: ssm_d_conv       = 0
0.00.107.130 I print_info: ssm_d_inner      = 0
0.00.107.130 I print_info: ssm_d_state      = 0
0.00.107.130 I print_info: ssm_dt_rank      = 0
0.00.107.130 I print_info: ssm_dt_b_c_rms   = 0
0.00.107.131 I print_info: model type       = 1.4B
0.00.107.131 I print_info: model params     = 1.41 B
0.00.107.131 I print_info: general.name     = 1.4B
0.00.107.132 I print_info: vocab type       = BPE
0.00.107.132 I print_info: n_vocab          = 50304
0.00.107.138 I print_info: n_merges         = 50009
0.00.107.139 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.107.139 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.107.139 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.107.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.107.140 I print_info: LF token         = 128 'Ä'
0.00.107.141 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.107.141 I print_info: max token length = 1024
0.00.154.704 I load_tensors: offloading 24 repeating layers to GPU
0.00.154.708 I load_tensors: offloading output layer to GPU
0.00.154.708 I load_tensors: offloaded 25/25 layers to GPU
0.00.154.735 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.154.736 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.155.329 I llama_init_from_model: n_seq_max     = 1
0.00.155.330 I llama_init_from_model: n_ctx         = 2048
0.00.155.330 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.155.330 I llama_init_from_model: n_batch       = 2048
0.00.155.331 I llama_init_from_model: n_ubatch      = 512
0.00.155.331 I llama_init_from_model: flash_attn    = 0
0.00.155.331 I llama_init_from_model: freq_base     = 10000.0
0.00.155.331 I llama_init_from_model: freq_scale    = 1
0.00.155.332 I ggml_metal_init: allocating
0.00.155.358 I ggml_metal_init: found device: Apple M4
0.00.155.364 I ggml_metal_init: picking default device: Apple M4
0.00.155.973 I ggml_metal_init: using embedded metal library
0.00.166.968 I ggml_metal_init: GPU name:   Apple M4
0.00.166.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.166.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.166.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.166.970 I ggml_metal_init: simdgroup reduction   = true
0.00.166.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.166.971 I ggml_metal_init: has residency sets    = true
0.00.166.971 I ggml_metal_init: has bfloat            = true
0.00.166.971 I ggml_metal_init: use bfloat            = true
0.00.166.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.166.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.190.005 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.216.464 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.216.471 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.216.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.220.012 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.220.014 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.220.015 I llama_init_from_model: graph nodes  = 967
0.00.220.015 I llama_init_from_model: graph splits = 2
0.00.220.020 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.220.147 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.220.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.287.362 I main: llama threadpool init, n_threads = 4
0.00.287.404 I 
0.00.287.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.287.438 I 
0.00.287.614 I sampler seed: 1234
0.00.287.619 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.287.643 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.287.644 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.287.644 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.133.047 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.02.133.048 I llama_perf_context_print:        load time =     235.13 ms
0.02.133.049 I llama_perf_context_print: prompt eval time =      44.16 ms /     7 tokens (    6.31 ms per token,   158.50 tokens per second)
0.02.133.050 I llama_perf_context_print:        eval time =    1798.31 ms /    63 runs   (   28.54 ms per token,    35.03 tokens per second)
0.02.133.050 I llama_perf_context_print:       total time =    1846.69 ms /    70 tokens
0.02.133.321 I ggml_metal_free: deallocating

real	0m2.449s
user	0m0.132s
sys	0m0.139s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.009.481 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.780 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.797 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.648 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.649 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.649 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.649 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.650 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.650 I llama_model_loader: - type  f32:  194 tensors
0.00.031.651 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.652 I print_info: file format = GGUF V3 (latest)
0.00.031.652 I print_info: file type   = Q8_0
0.00.031.653 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.039.982 I load: special tokens cache size = 25
0.00.045.951 I load: token to piece cache size = 0.2984 MB
0.00.045.955 I print_info: arch             = gptneox
0.00.045.955 I print_info: vocab_only       = 0
0.00.045.956 I print_info: n_ctx_train      = 2048
0.00.045.957 I print_info: n_embd           = 2048
0.00.045.958 I print_info: n_layer          = 24
0.00.045.962 I print_info: n_head           = 16
0.00.045.963 I print_info: n_head_kv        = 16
0.00.045.964 I print_info: n_rot            = 32
0.00.045.964 I print_info: n_swa            = 0
0.00.045.964 I print_info: n_embd_head_k    = 128
0.00.045.964 I print_info: n_embd_head_v    = 128
0.00.045.965 I print_info: n_gqa            = 1
0.00.045.967 I print_info: n_embd_k_gqa     = 2048
0.00.045.967 I print_info: n_embd_v_gqa     = 2048
0.00.045.968 I print_info: f_norm_eps       = 1.0e-05
0.00.045.968 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.969 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.969 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.969 I print_info: f_logit_scale    = 0.0e+00
0.00.045.970 I print_info: n_ff             = 8192
0.00.045.970 I print_info: n_expert         = 0
0.00.045.970 I print_info: n_expert_used    = 0
0.00.045.970 I print_info: causal attn      = 1
0.00.045.970 I print_info: pooling type     = 0
0.00.045.971 I print_info: rope type        = 2
0.00.045.971 I print_info: rope scaling     = linear
0.00.045.971 I print_info: freq_base_train  = 10000.0
0.00.045.972 I print_info: freq_scale_train = 1
0.00.045.973 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.974 I print_info: rope_finetuned   = unknown
0.00.045.974 I print_info: ssm_d_conv       = 0
0.00.045.974 I print_info: ssm_d_inner      = 0
0.00.045.974 I print_info: ssm_d_state      = 0
0.00.045.974 I print_info: ssm_dt_rank      = 0
0.00.045.974 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.974 I print_info: model type       = 1.4B
0.00.045.975 I print_info: model params     = 1.41 B
0.00.045.975 I print_info: general.name     = 1.4B
0.00.045.976 I print_info: vocab type       = BPE
0.00.045.976 I print_info: n_vocab          = 50304
0.00.045.976 I print_info: n_merges         = 50009
0.00.045.976 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.976 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.977 I print_info: LF token         = 128 'Ä'
0.00.045.977 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.977 I print_info: max token length = 1024
0.01.091.316 I load_tensors: offloading 24 repeating layers to GPU
0.01.091.319 I load_tensors: offloading output layer to GPU
0.01.091.320 I load_tensors: offloaded 25/25 layers to GPU
0.01.091.341 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.091.342 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.092.606 I llama_init_from_model: n_seq_max     = 1
0.01.092.608 I llama_init_from_model: n_ctx         = 2048
0.01.092.609 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.092.609 I llama_init_from_model: n_batch       = 2048
0.01.092.610 I llama_init_from_model: n_ubatch      = 512
0.01.092.610 I llama_init_from_model: flash_attn    = 0
0.01.092.611 I llama_init_from_model: freq_base     = 10000.0
0.01.092.611 I llama_init_from_model: freq_scale    = 1
0.01.092.612 I ggml_metal_init: allocating
0.01.092.639 I ggml_metal_init: found device: Apple M4
0.01.092.649 I ggml_metal_init: picking default device: Apple M4
0.01.093.920 I ggml_metal_init: using embedded metal library
0.01.099.236 I ggml_metal_init: GPU name:   Apple M4
0.01.099.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.099.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.099.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.099.241 I ggml_metal_init: simdgroup reduction   = true
0.01.099.241 I ggml_metal_init: simdgroup matrix mul. = true
0.01.099.242 I ggml_metal_init: has residency sets    = true
0.01.099.242 I ggml_metal_init: has bfloat            = true
0.01.099.242 I ggml_metal_init: use bfloat            = true
0.01.099.243 I ggml_metal_init: hasUnifiedMemory      = true
0.01.099.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.114.954 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.163.176 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.163.183 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.163.208 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.168.159 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.168.160 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.168.161 I llama_init_from_model: graph nodes  = 967
0.01.168.161 I llama_init_from_model: graph splits = 2
0.01.168.166 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.168.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.168.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.222.271 I main: llama threadpool init, n_threads = 4
0.01.222.316 I 
0.01.222.341 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.222.343 I 
0.01.222.516 I sampler seed: 1234
0.01.222.521 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.222.563 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.222.566 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.222.566 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.312.938 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.02.312.938 I llama_perf_context_print:        load time =    1211.91 ms
0.02.312.939 I llama_perf_context_print: prompt eval time =      49.20 ms /     7 tokens (    7.03 ms per token,   142.29 tokens per second)
0.02.312.940 I llama_perf_context_print:        eval time =    1038.33 ms /    63 runs   (   16.48 ms per token,    60.67 tokens per second)
0.02.312.940 I llama_perf_context_print:       total time =    1091.54 ms /    70 tokens
0.02.313.192 I ggml_metal_free: deallocating

real	0m2.332s
user	0m0.107s
sys	0m0.256s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.769 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.569 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.580 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.581 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.581 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.582 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.582 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.582 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.584 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.584 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.585 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.466 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.501 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.309 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.310 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.310 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.311 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.311 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.311 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.312 I llama_model_loader: - type  f32:  194 tensors
0.00.028.312 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.313 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.314 I print_info: file format = GGUF V3 (latest)
0.00.028.314 I print_info: file type   = Q4_0
0.00.028.315 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.754 I load: special tokens cache size = 25
0.00.042.682 I load: token to piece cache size = 0.2984 MB
0.00.042.685 I print_info: arch             = gptneox
0.00.042.685 I print_info: vocab_only       = 0
0.00.042.686 I print_info: n_ctx_train      = 2048
0.00.042.686 I print_info: n_embd           = 2048
0.00.042.686 I print_info: n_layer          = 24
0.00.042.691 I print_info: n_head           = 16
0.00.042.692 I print_info: n_head_kv        = 16
0.00.042.692 I print_info: n_rot            = 32
0.00.042.692 I print_info: n_swa            = 0
0.00.042.692 I print_info: n_embd_head_k    = 128
0.00.042.692 I print_info: n_embd_head_v    = 128
0.00.042.693 I print_info: n_gqa            = 1
0.00.042.694 I print_info: n_embd_k_gqa     = 2048
0.00.042.695 I print_info: n_embd_v_gqa     = 2048
0.00.042.695 I print_info: f_norm_eps       = 1.0e-05
0.00.042.696 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.696 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.696 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.696 I print_info: f_logit_scale    = 0.0e+00
0.00.042.697 I print_info: n_ff             = 8192
0.00.042.697 I print_info: n_expert         = 0
0.00.042.698 I print_info: n_expert_used    = 0
0.00.042.698 I print_info: causal attn      = 1
0.00.042.698 I print_info: pooling type     = 0
0.00.042.698 I print_info: rope type        = 2
0.00.042.698 I print_info: rope scaling     = linear
0.00.042.699 I print_info: freq_base_train  = 10000.0
0.00.042.699 I print_info: freq_scale_train = 1
0.00.042.699 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.699 I print_info: rope_finetuned   = unknown
0.00.042.699 I print_info: ssm_d_conv       = 0
0.00.042.700 I print_info: ssm_d_inner      = 0
0.00.042.700 I print_info: ssm_d_state      = 0
0.00.042.700 I print_info: ssm_dt_rank      = 0
0.00.042.700 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.700 I print_info: model type       = 1.4B
0.00.042.701 I print_info: model params     = 1.41 B
0.00.042.701 I print_info: general.name     = 1.4B
0.00.042.702 I print_info: vocab type       = BPE
0.00.042.702 I print_info: n_vocab          = 50304
0.00.042.702 I print_info: n_merges         = 50009
0.00.042.702 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.702 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.703 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.703 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.703 I print_info: LF token         = 128 'Ä'
0.00.042.704 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.704 I print_info: max token length = 1024
0.00.579.327 I load_tensors: offloading 24 repeating layers to GPU
0.00.579.343 I load_tensors: offloading output layer to GPU
0.00.579.343 I load_tensors: offloaded 25/25 layers to GPU
0.00.579.381 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.579.382 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.580.769 I llama_init_from_model: n_seq_max     = 1
0.00.580.775 I llama_init_from_model: n_ctx         = 2048
0.00.580.775 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.580.775 I llama_init_from_model: n_batch       = 2048
0.00.580.776 I llama_init_from_model: n_ubatch      = 512
0.00.580.776 I llama_init_from_model: flash_attn    = 0
0.00.580.778 I llama_init_from_model: freq_base     = 10000.0
0.00.580.779 I llama_init_from_model: freq_scale    = 1
0.00.580.781 I ggml_metal_init: allocating
0.00.580.849 I ggml_metal_init: found device: Apple M4
0.00.580.864 I ggml_metal_init: picking default device: Apple M4
0.00.582.681 I ggml_metal_init: using embedded metal library
0.00.588.200 I ggml_metal_init: GPU name:   Apple M4
0.00.588.215 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.588.216 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.588.217 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.588.217 I ggml_metal_init: simdgroup reduction   = true
0.00.588.218 I ggml_metal_init: simdgroup matrix mul. = true
0.00.588.218 I ggml_metal_init: has residency sets    = true
0.00.588.218 I ggml_metal_init: has bfloat            = true
0.00.588.218 I ggml_metal_init: use bfloat            = true
0.00.588.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.588.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.608.606 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.656.948 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.656.955 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.656.977 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.661.972 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.661.974 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.661.974 I llama_init_from_model: graph nodes  = 967
0.00.661.974 I llama_init_from_model: graph splits = 2
0.00.661.980 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.662.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.662.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.985 I main: llama threadpool init, n_threads = 4
0.00.719.026 I 
0.00.719.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.049 I 
0.00.719.229 I sampler seed: 1234
0.00.719.234 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.270 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.272 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.272 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.400.771 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.400.772 I llama_perf_context_print:        load time =     706.32 ms
0.01.400.774 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.57 tokens per second)
0.01.400.775 I llama_perf_context_print:        eval time =     629.48 ms /    63 runs   (    9.99 ms per token,   100.08 tokens per second)
0.01.400.775 I llama_perf_context_print:       total time =     682.68 ms /    70 tokens
0.01.400.993 I ggml_metal_free: deallocating

real	0m1.421s
user	0m0.113s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.616 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.337 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.339 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.343 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.343 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.344 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.344 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.347 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.350 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.351 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.118 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.092 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.824 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.824 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.824 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.825 I llama_model_loader: - type  f32:  194 tensors
0.00.024.826 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.826 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.827 I print_info: file format = GGUF V3 (latest)
0.00.024.827 I print_info: file type   = Q4_1
0.00.024.828 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.636 I load: special tokens cache size = 25
0.00.038.294 I load: token to piece cache size = 0.2984 MB
0.00.038.297 I print_info: arch             = gptneox
0.00.038.297 I print_info: vocab_only       = 0
0.00.038.297 I print_info: n_ctx_train      = 2048
0.00.038.298 I print_info: n_embd           = 2048
0.00.038.298 I print_info: n_layer          = 24
0.00.038.300 I print_info: n_head           = 16
0.00.038.301 I print_info: n_head_kv        = 16
0.00.038.301 I print_info: n_rot            = 32
0.00.038.301 I print_info: n_swa            = 0
0.00.038.301 I print_info: n_embd_head_k    = 128
0.00.038.301 I print_info: n_embd_head_v    = 128
0.00.038.302 I print_info: n_gqa            = 1
0.00.038.303 I print_info: n_embd_k_gqa     = 2048
0.00.038.304 I print_info: n_embd_v_gqa     = 2048
0.00.038.304 I print_info: f_norm_eps       = 1.0e-05
0.00.038.305 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.305 I print_info: f_logit_scale    = 0.0e+00
0.00.038.306 I print_info: n_ff             = 8192
0.00.038.306 I print_info: n_expert         = 0
0.00.038.306 I print_info: n_expert_used    = 0
0.00.038.306 I print_info: causal attn      = 1
0.00.038.306 I print_info: pooling type     = 0
0.00.038.308 I print_info: rope type        = 2
0.00.038.309 I print_info: rope scaling     = linear
0.00.038.310 I print_info: freq_base_train  = 10000.0
0.00.038.310 I print_info: freq_scale_train = 1
0.00.038.310 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.311 I print_info: rope_finetuned   = unknown
0.00.038.311 I print_info: ssm_d_conv       = 0
0.00.038.311 I print_info: ssm_d_inner      = 0
0.00.038.311 I print_info: ssm_d_state      = 0
0.00.038.311 I print_info: ssm_dt_rank      = 0
0.00.038.312 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.312 I print_info: model type       = 1.4B
0.00.038.312 I print_info: model params     = 1.41 B
0.00.038.312 I print_info: general.name     = 1.4B
0.00.038.313 I print_info: vocab type       = BPE
0.00.038.313 I print_info: n_vocab          = 50304
0.00.038.313 I print_info: n_merges         = 50009
0.00.038.315 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.315 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.315 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.315 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.316 I print_info: LF token         = 128 'Ä'
0.00.038.316 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.316 I print_info: max token length = 1024
0.00.651.661 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.673 I load_tensors: offloading output layer to GPU
0.00.651.674 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.707 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.651.708 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.653.254 I llama_init_from_model: n_seq_max     = 1
0.00.653.259 I llama_init_from_model: n_ctx         = 2048
0.00.653.260 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.653.260 I llama_init_from_model: n_batch       = 2048
0.00.653.261 I llama_init_from_model: n_ubatch      = 512
0.00.653.261 I llama_init_from_model: flash_attn    = 0
0.00.653.263 I llama_init_from_model: freq_base     = 10000.0
0.00.653.264 I llama_init_from_model: freq_scale    = 1
0.00.653.269 I ggml_metal_init: allocating
0.00.653.320 I ggml_metal_init: found device: Apple M4
0.00.653.333 I ggml_metal_init: picking default device: Apple M4
0.00.655.111 I ggml_metal_init: using embedded metal library
0.00.661.746 I ggml_metal_init: GPU name:   Apple M4
0.00.661.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.754 I ggml_metal_init: simdgroup reduction   = true
0.00.661.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.754 I ggml_metal_init: has residency sets    = true
0.00.661.755 I ggml_metal_init: has bfloat            = true
0.00.661.755 I ggml_metal_init: use bfloat            = true
0.00.661.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.679.909 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.475 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.733.482 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.513 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.208 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.210 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.210 I llama_init_from_model: graph nodes  = 967
0.00.738.211 I llama_init_from_model: graph splits = 2
0.00.738.218 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.987 I main: llama threadpool init, n_threads = 4
0.00.794.026 I 
0.00.794.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.054 I 
0.00.794.184 I sampler seed: 1234
0.00.794.190 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.199 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.200 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.200 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.534.207 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.534.208 I llama_perf_context_print:        load time =     784.51 ms
0.01.534.209 I llama_perf_context_print: prompt eval time =      49.11 ms /     7 tokens (    7.02 ms per token,   142.54 tokens per second)
0.01.534.210 I llama_perf_context_print:        eval time =     688.55 ms /    63 runs   (   10.93 ms per token,    91.50 tokens per second)
0.01.534.210 I llama_perf_context_print:       total time =     741.08 ms /    70 tokens
0.01.534.522 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.109s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.270 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.978 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.983 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.986 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.986 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.987 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.989 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.990 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.992 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.993 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.835 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.886 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.650 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.651 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.651 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.652 I llama_model_loader: - type  f32:  194 tensors
0.00.026.652 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.652 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.653 I print_info: file format = GGUF V3 (latest)
0.00.026.654 I print_info: file type   = Q5_0
0.00.026.654 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.786 I load: special tokens cache size = 25
0.00.040.799 I load: token to piece cache size = 0.2984 MB
0.00.040.803 I print_info: arch             = gptneox
0.00.040.803 I print_info: vocab_only       = 0
0.00.040.803 I print_info: n_ctx_train      = 2048
0.00.040.803 I print_info: n_embd           = 2048
0.00.040.804 I print_info: n_layer          = 24
0.00.040.806 I print_info: n_head           = 16
0.00.040.807 I print_info: n_head_kv        = 16
0.00.040.807 I print_info: n_rot            = 32
0.00.040.808 I print_info: n_swa            = 0
0.00.040.808 I print_info: n_embd_head_k    = 128
0.00.040.808 I print_info: n_embd_head_v    = 128
0.00.040.809 I print_info: n_gqa            = 1
0.00.040.809 I print_info: n_embd_k_gqa     = 2048
0.00.040.810 I print_info: n_embd_v_gqa     = 2048
0.00.040.811 I print_info: f_norm_eps       = 1.0e-05
0.00.040.811 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.813 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.813 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.813 I print_info: f_logit_scale    = 0.0e+00
0.00.040.814 I print_info: n_ff             = 8192
0.00.040.814 I print_info: n_expert         = 0
0.00.040.814 I print_info: n_expert_used    = 0
0.00.040.814 I print_info: causal attn      = 1
0.00.040.814 I print_info: pooling type     = 0
0.00.040.815 I print_info: rope type        = 2
0.00.040.815 I print_info: rope scaling     = linear
0.00.040.815 I print_info: freq_base_train  = 10000.0
0.00.040.816 I print_info: freq_scale_train = 1
0.00.040.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.816 I print_info: rope_finetuned   = unknown
0.00.040.816 I print_info: ssm_d_conv       = 0
0.00.040.816 I print_info: ssm_d_inner      = 0
0.00.040.816 I print_info: ssm_d_state      = 0
0.00.040.817 I print_info: ssm_dt_rank      = 0
0.00.040.817 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.819 I print_info: model type       = 1.4B
0.00.040.819 I print_info: model params     = 1.41 B
0.00.040.819 I print_info: general.name     = 1.4B
0.00.040.820 I print_info: vocab type       = BPE
0.00.040.820 I print_info: n_vocab          = 50304
0.00.040.820 I print_info: n_merges         = 50009
0.00.040.820 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.820 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.820 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.821 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.821 I print_info: LF token         = 128 'Ä'
0.00.040.821 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.821 I print_info: max token length = 1024
0.00.644.671 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.682 I load_tensors: offloading output layer to GPU
0.00.644.682 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.715 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.644.717 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.646.372 I llama_init_from_model: n_seq_max     = 1
0.00.646.376 I llama_init_from_model: n_ctx         = 2048
0.00.646.377 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.646.377 I llama_init_from_model: n_batch       = 2048
0.00.646.377 I llama_init_from_model: n_ubatch      = 512
0.00.646.378 I llama_init_from_model: flash_attn    = 0
0.00.646.379 I llama_init_from_model: freq_base     = 10000.0
0.00.646.380 I llama_init_from_model: freq_scale    = 1
0.00.646.383 I ggml_metal_init: allocating
0.00.646.444 I ggml_metal_init: found device: Apple M4
0.00.646.458 I ggml_metal_init: picking default device: Apple M4
0.00.648.142 I ggml_metal_init: using embedded metal library
0.00.654.841 I ggml_metal_init: GPU name:   Apple M4
0.00.654.846 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.654.847 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.654.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.654.849 I ggml_metal_init: simdgroup reduction   = true
0.00.654.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.654.850 I ggml_metal_init: has residency sets    = true
0.00.654.850 I ggml_metal_init: has bfloat            = true
0.00.654.850 I ggml_metal_init: use bfloat            = true
0.00.654.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.654.861 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.960 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.761 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.724.767 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.792 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.749 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.729.751 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.729.751 I llama_init_from_model: graph nodes  = 967
0.00.729.752 I llama_init_from_model: graph splits = 2
0.00.729.757 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.729.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.882 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.294 I main: llama threadpool init, n_threads = 4
0.00.786.341 I 
0.00.786.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.367 I 
0.00.786.538 I sampler seed: 1234
0.00.786.543 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.588 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.592 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.592 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.576.087 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.576.088 I llama_perf_context_print:        load time =     775.14 ms
0.01.576.089 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.38 tokens per second)
0.01.576.089 I llama_perf_context_print:        eval time =     743.51 ms /    63 runs   (   11.80 ms per token,    84.73 tokens per second)
0.01.576.090 I llama_perf_context_print:       total time =     790.68 ms /    70 tokens
0.01.576.391 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.860 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.216 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.224 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.225 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.226 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.227 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.228 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.232 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.853 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.854 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.854 I llama_model_loader: - type  f32:  194 tensors
0.00.025.854 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.854 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.855 I print_info: file format = GGUF V3 (latest)
0.00.025.855 I print_info: file type   = Q5_1
0.00.025.856 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.710 I load: special tokens cache size = 25
0.00.039.646 I load: token to piece cache size = 0.2984 MB
0.00.039.650 I print_info: arch             = gptneox
0.00.039.650 I print_info: vocab_only       = 0
0.00.039.650 I print_info: n_ctx_train      = 2048
0.00.039.650 I print_info: n_embd           = 2048
0.00.039.650 I print_info: n_layer          = 24
0.00.039.653 I print_info: n_head           = 16
0.00.039.654 I print_info: n_head_kv        = 16
0.00.039.654 I print_info: n_rot            = 32
0.00.039.655 I print_info: n_swa            = 0
0.00.039.655 I print_info: n_embd_head_k    = 128
0.00.039.655 I print_info: n_embd_head_v    = 128
0.00.039.656 I print_info: n_gqa            = 1
0.00.039.656 I print_info: n_embd_k_gqa     = 2048
0.00.039.658 I print_info: n_embd_v_gqa     = 2048
0.00.039.659 I print_info: f_norm_eps       = 1.0e-05
0.00.039.659 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.659 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.669 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.670 I print_info: f_logit_scale    = 0.0e+00
0.00.039.675 I print_info: n_ff             = 8192
0.00.039.675 I print_info: n_expert         = 0
0.00.039.675 I print_info: n_expert_used    = 0
0.00.039.675 I print_info: causal attn      = 1
0.00.039.677 I print_info: pooling type     = 0
0.00.039.679 I print_info: rope type        = 2
0.00.039.679 I print_info: rope scaling     = linear
0.00.039.680 I print_info: freq_base_train  = 10000.0
0.00.039.680 I print_info: freq_scale_train = 1
0.00.039.680 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.681 I print_info: rope_finetuned   = unknown
0.00.039.681 I print_info: ssm_d_conv       = 0
0.00.039.681 I print_info: ssm_d_inner      = 0
0.00.039.681 I print_info: ssm_d_state      = 0
0.00.039.681 I print_info: ssm_dt_rank      = 0
0.00.039.681 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.682 I print_info: model type       = 1.4B
0.00.039.682 I print_info: model params     = 1.41 B
0.00.039.682 I print_info: general.name     = 1.4B
0.00.039.683 I print_info: vocab type       = BPE
0.00.039.683 I print_info: n_vocab          = 50304
0.00.039.683 I print_info: n_merges         = 50009
0.00.039.684 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: LF token         = 128 'Ä'
0.00.039.685 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: max token length = 1024
0.00.611.131 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.145 I load_tensors: offloading output layer to GPU
0.00.611.145 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.176 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.611.181 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.612.457 I llama_init_from_model: n_seq_max     = 1
0.00.612.469 I llama_init_from_model: n_ctx         = 2048
0.00.612.470 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.612.470 I llama_init_from_model: n_batch       = 2048
0.00.612.470 I llama_init_from_model: n_ubatch      = 512
0.00.612.471 I llama_init_from_model: flash_attn    = 0
0.00.612.473 I llama_init_from_model: freq_base     = 10000.0
0.00.612.473 I llama_init_from_model: freq_scale    = 1
0.00.612.479 I ggml_metal_init: allocating
0.00.612.561 I ggml_metal_init: found device: Apple M4
0.00.612.575 I ggml_metal_init: picking default device: Apple M4
0.00.614.257 I ggml_metal_init: using embedded metal library
0.00.619.669 I ggml_metal_init: GPU name:   Apple M4
0.00.619.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.682 I ggml_metal_init: simdgroup reduction   = true
0.00.619.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.682 I ggml_metal_init: has residency sets    = true
0.00.619.682 I ggml_metal_init: has bfloat            = true
0.00.619.682 I ggml_metal_init: use bfloat            = true
0.00.619.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.625 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.317 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.664.325 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.664.355 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.313 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.669.315 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.669.315 I llama_init_from_model: graph nodes  = 967
0.00.669.316 I llama_init_from_model: graph splits = 2
0.00.669.322 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.669.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.669.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.180 I main: llama threadpool init, n_threads = 4
0.00.724.225 I 
0.00.724.254 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.256 I 
0.00.724.405 I sampler seed: 1234
0.00.724.410 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.724.431 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.724.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.724.432 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.562.737 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51079.14 tokens per second)
0.01.562.738 I llama_perf_context_print:        load time =     713.44 ms
0.01.562.739 I llama_perf_context_print: prompt eval time =      41.95 ms /     7 tokens (    5.99 ms per token,   166.89 tokens per second)
0.01.562.740 I llama_perf_context_print:        eval time =     793.36 ms /    63 runs   (   12.59 ms per token,    79.41 tokens per second)
0.01.562.740 I llama_perf_context_print:       total time =     839.44 ms /    70 tokens
0.01.563.016 I ggml_metal_free: deallocating

real	0m1.581s
user	0m0.103s
sys	0m0.180s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.816 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.501 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.506 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.133 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.134 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.134 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.135 I llama_model_loader: - type  f32:  194 tensors
0.00.024.135 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.136 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.136 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.137 I print_info: file format = GGUF V3 (latest)
0.00.024.137 I print_info: file type   = Q2_K - Medium
0.00.024.138 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.270 I load: special tokens cache size = 25
0.00.038.175 I load: token to piece cache size = 0.2984 MB
0.00.038.179 I print_info: arch             = gptneox
0.00.038.179 I print_info: vocab_only       = 0
0.00.038.179 I print_info: n_ctx_train      = 2048
0.00.038.179 I print_info: n_embd           = 2048
0.00.038.180 I print_info: n_layer          = 24
0.00.038.182 I print_info: n_head           = 16
0.00.038.183 I print_info: n_head_kv        = 16
0.00.038.183 I print_info: n_rot            = 32
0.00.038.184 I print_info: n_swa            = 0
0.00.038.184 I print_info: n_embd_head_k    = 128
0.00.038.184 I print_info: n_embd_head_v    = 128
0.00.038.186 I print_info: n_gqa            = 1
0.00.038.187 I print_info: n_embd_k_gqa     = 2048
0.00.038.187 I print_info: n_embd_v_gqa     = 2048
0.00.038.188 I print_info: f_norm_eps       = 1.0e-05
0.00.038.189 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.189 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.189 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.189 I print_info: f_logit_scale    = 0.0e+00
0.00.038.190 I print_info: n_ff             = 8192
0.00.038.190 I print_info: n_expert         = 0
0.00.038.190 I print_info: n_expert_used    = 0
0.00.038.190 I print_info: causal attn      = 1
0.00.038.190 I print_info: pooling type     = 0
0.00.038.191 I print_info: rope type        = 2
0.00.038.191 I print_info: rope scaling     = linear
0.00.038.193 I print_info: freq_base_train  = 10000.0
0.00.038.193 I print_info: freq_scale_train = 1
0.00.038.194 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.194 I print_info: rope_finetuned   = unknown
0.00.038.194 I print_info: ssm_d_conv       = 0
0.00.038.194 I print_info: ssm_d_inner      = 0
0.00.038.194 I print_info: ssm_d_state      = 0
0.00.038.194 I print_info: ssm_dt_rank      = 0
0.00.038.194 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.195 I print_info: model type       = 1.4B
0.00.038.195 I print_info: model params     = 1.41 B
0.00.038.195 I print_info: general.name     = 1.4B
0.00.038.196 I print_info: vocab type       = BPE
0.00.038.196 I print_info: n_vocab          = 50304
0.00.038.196 I print_info: n_merges         = 50009
0.00.038.197 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.201 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.201 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.201 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.202 I print_info: LF token         = 128 'Ä'
0.00.038.202 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.202 I print_info: max token length = 1024
0.00.345.949 I load_tensors: offloading 24 repeating layers to GPU
0.00.345.966 I load_tensors: offloading output layer to GPU
0.00.345.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.346.003 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.346.004 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.347.502 I llama_init_from_model: n_seq_max     = 1
0.00.347.508 I llama_init_from_model: n_ctx         = 2048
0.00.347.508 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.347.509 I llama_init_from_model: n_batch       = 2048
0.00.347.509 I llama_init_from_model: n_ubatch      = 512
0.00.347.510 I llama_init_from_model: flash_attn    = 0
0.00.347.512 I llama_init_from_model: freq_base     = 10000.0
0.00.347.512 I llama_init_from_model: freq_scale    = 1
0.00.347.515 I ggml_metal_init: allocating
0.00.347.630 I ggml_metal_init: found device: Apple M4
0.00.347.644 I ggml_metal_init: picking default device: Apple M4
0.00.349.465 I ggml_metal_init: using embedded metal library
0.00.355.216 I ggml_metal_init: GPU name:   Apple M4
0.00.355.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.355.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.355.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.355.234 I ggml_metal_init: simdgroup reduction   = true
0.00.355.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.355.234 I ggml_metal_init: has residency sets    = true
0.00.355.235 I ggml_metal_init: has bfloat            = true
0.00.355.235 I ggml_metal_init: use bfloat            = true
0.00.355.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.355.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.376.309 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.432.993 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.433.001 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.433.026 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.436.986 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.436.989 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.436.989 I llama_init_from_model: graph nodes  = 967
0.00.436.989 I llama_init_from_model: graph splits = 2
0.00.436.996 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.437.119 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.437.120 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.980 I main: llama threadpool init, n_threads = 4
0.00.494.028 I 
0.00.494.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.053 I 
0.00.494.226 I sampler seed: 1234
0.00.494.231 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.494.274 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.494.279 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.494.279 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.169.527 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56483.69 tokens per second)
0.01.169.528 I llama_perf_context_print:        load time =     484.29 ms
0.01.169.528 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.45 tokens per second)
0.01.169.529 I llama_perf_context_print:        eval time =     636.67 ms /    63 runs   (   10.11 ms per token,    98.95 tokens per second)
0.01.169.529 I llama_perf_context_print:       total time =     676.42 ms /    70 tokens
0.01.169.792 I ggml_metal_free: deallocating

real	0m1.185s
user	0m0.111s
sys	0m0.171s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.649 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.420 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.425 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.427 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.428 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.428 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.428 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.431 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.432 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.432 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.433 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.433 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.433 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.440 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.289 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.097 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.098 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.098 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.099 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.099 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.099 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.100 I llama_model_loader: - type  f32:  194 tensors
0.00.026.100 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.100 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.101 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.101 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.102 I print_info: file format = GGUF V3 (latest)
0.00.026.102 I print_info: file type   = Q3_K - Medium
0.00.026.104 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.298 I load: special tokens cache size = 25
0.00.040.206 I load: token to piece cache size = 0.2984 MB
0.00.040.208 I print_info: arch             = gptneox
0.00.040.209 I print_info: vocab_only       = 0
0.00.040.209 I print_info: n_ctx_train      = 2048
0.00.040.209 I print_info: n_embd           = 2048
0.00.040.209 I print_info: n_layer          = 24
0.00.040.212 I print_info: n_head           = 16
0.00.040.213 I print_info: n_head_kv        = 16
0.00.040.213 I print_info: n_rot            = 32
0.00.040.213 I print_info: n_swa            = 0
0.00.040.213 I print_info: n_embd_head_k    = 128
0.00.040.213 I print_info: n_embd_head_v    = 128
0.00.040.214 I print_info: n_gqa            = 1
0.00.040.215 I print_info: n_embd_k_gqa     = 2048
0.00.040.216 I print_info: n_embd_v_gqa     = 2048
0.00.040.216 I print_info: f_norm_eps       = 1.0e-05
0.00.040.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.217 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.217 I print_info: f_logit_scale    = 0.0e+00
0.00.040.218 I print_info: n_ff             = 8192
0.00.040.218 I print_info: n_expert         = 0
0.00.040.218 I print_info: n_expert_used    = 0
0.00.040.220 I print_info: causal attn      = 1
0.00.040.221 I print_info: pooling type     = 0
0.00.040.222 I print_info: rope type        = 2
0.00.040.222 I print_info: rope scaling     = linear
0.00.040.222 I print_info: freq_base_train  = 10000.0
0.00.040.223 I print_info: freq_scale_train = 1
0.00.040.223 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.223 I print_info: rope_finetuned   = unknown
0.00.040.223 I print_info: ssm_d_conv       = 0
0.00.040.224 I print_info: ssm_d_inner      = 0
0.00.040.227 I print_info: ssm_d_state      = 0
0.00.040.227 I print_info: ssm_dt_rank      = 0
0.00.040.228 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.228 I print_info: model type       = 1.4B
0.00.040.228 I print_info: model params     = 1.41 B
0.00.040.228 I print_info: general.name     = 1.4B
0.00.040.229 I print_info: vocab type       = BPE
0.00.040.229 I print_info: n_vocab          = 50304
0.00.040.229 I print_info: n_merges         = 50009
0.00.040.230 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.230 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.230 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.230 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.230 I print_info: LF token         = 128 'Ä'
0.00.040.231 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.231 I print_info: max token length = 1024
0.00.445.321 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.337 I load_tensors: offloading output layer to GPU
0.00.445.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.372 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.373 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.446.935 I llama_init_from_model: n_seq_max     = 1
0.00.446.940 I llama_init_from_model: n_ctx         = 2048
0.00.446.941 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.446.941 I llama_init_from_model: n_batch       = 2048
0.00.446.941 I llama_init_from_model: n_ubatch      = 512
0.00.446.942 I llama_init_from_model: flash_attn    = 0
0.00.446.949 I llama_init_from_model: freq_base     = 10000.0
0.00.446.958 I llama_init_from_model: freq_scale    = 1
0.00.446.961 I ggml_metal_init: allocating
0.00.447.038 I ggml_metal_init: found device: Apple M4
0.00.447.051 I ggml_metal_init: picking default device: Apple M4
0.00.448.835 I ggml_metal_init: using embedded metal library
0.00.454.516 I ggml_metal_init: GPU name:   Apple M4
0.00.454.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.532 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.532 I ggml_metal_init: simdgroup reduction   = true
0.00.454.533 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.533 I ggml_metal_init: has residency sets    = true
0.00.454.533 I ggml_metal_init: has bfloat            = true
0.00.454.534 I ggml_metal_init: use bfloat            = true
0.00.454.538 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.474.630 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.529.486 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.529.493 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.529.515 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.533.820 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.533.822 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.533.823 I llama_init_from_model: graph nodes  = 967
0.00.533.823 I llama_init_from_model: graph splits = 2
0.00.533.827 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.533.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.533.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.115 I main: llama threadpool init, n_threads = 4
0.00.589.167 I 
0.00.589.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.196 I 
0.00.589.350 I sampler seed: 1234
0.00.589.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.589.398 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.589.402 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.589.402 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.340.441 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.340.442 I llama_perf_context_print:        load time =     578.57 ms
0.01.340.443 I llama_perf_context_print: prompt eval time =      50.13 ms /     7 tokens (    7.16 ms per token,   139.63 tokens per second)
0.01.340.444 I llama_perf_context_print:        eval time =     698.02 ms /    63 runs   (   11.08 ms per token,    90.26 tokens per second)
0.01.340.445 I llama_perf_context_print:       total time =     752.22 ms /    70 tokens
0.01.340.651 I ggml_metal_free: deallocating

real	0m1.358s
user	0m0.111s
sys	0m0.186s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.552 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.557 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.563 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.563 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.564 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.565 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.565 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.566 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.566 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.567 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.569 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.569 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.263 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.263 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.263 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.264 I llama_model_loader: - type  f32:  194 tensors
0.00.026.264 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.264 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.265 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.265 I print_info: file format = GGUF V3 (latest)
0.00.026.266 I print_info: file type   = Q4_K - Medium
0.00.026.266 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.093 I load: special tokens cache size = 25
0.00.040.027 I load: token to piece cache size = 0.2984 MB
0.00.040.031 I print_info: arch             = gptneox
0.00.040.031 I print_info: vocab_only       = 0
0.00.040.031 I print_info: n_ctx_train      = 2048
0.00.040.031 I print_info: n_embd           = 2048
0.00.040.032 I print_info: n_layer          = 24
0.00.040.034 I print_info: n_head           = 16
0.00.040.035 I print_info: n_head_kv        = 16
0.00.040.035 I print_info: n_rot            = 32
0.00.040.036 I print_info: n_swa            = 0
0.00.040.036 I print_info: n_embd_head_k    = 128
0.00.040.036 I print_info: n_embd_head_v    = 128
0.00.040.037 I print_info: n_gqa            = 1
0.00.040.038 I print_info: n_embd_k_gqa     = 2048
0.00.040.038 I print_info: n_embd_v_gqa     = 2048
0.00.040.039 I print_info: f_norm_eps       = 1.0e-05
0.00.040.042 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.042 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.042 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.042 I print_info: f_logit_scale    = 0.0e+00
0.00.040.043 I print_info: n_ff             = 8192
0.00.040.043 I print_info: n_expert         = 0
0.00.040.043 I print_info: n_expert_used    = 0
0.00.040.043 I print_info: causal attn      = 1
0.00.040.043 I print_info: pooling type     = 0
0.00.040.044 I print_info: rope type        = 2
0.00.040.044 I print_info: rope scaling     = linear
0.00.040.044 I print_info: freq_base_train  = 10000.0
0.00.040.045 I print_info: freq_scale_train = 1
0.00.040.045 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.045 I print_info: rope_finetuned   = unknown
0.00.040.045 I print_info: ssm_d_conv       = 0
0.00.040.045 I print_info: ssm_d_inner      = 0
0.00.040.045 I print_info: ssm_d_state      = 0
0.00.040.046 I print_info: ssm_dt_rank      = 0
0.00.040.046 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.046 I print_info: model type       = 1.4B
0.00.040.046 I print_info: model params     = 1.41 B
0.00.040.047 I print_info: general.name     = 1.4B
0.00.040.047 I print_info: vocab type       = BPE
0.00.040.047 I print_info: n_vocab          = 50304
0.00.040.048 I print_info: n_merges         = 50009
0.00.040.048 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.048 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.048 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.048 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.049 I print_info: LF token         = 128 'Ä'
0.00.040.049 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.049 I print_info: max token length = 1024
0.00.511.994 I load_tensors: offloading 24 repeating layers to GPU
0.00.512.007 I load_tensors: offloading output layer to GPU
0.00.512.008 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.040 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.512.046 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.513.359 I llama_init_from_model: n_seq_max     = 1
0.00.513.364 I llama_init_from_model: n_ctx         = 2048
0.00.513.364 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.513.365 I llama_init_from_model: n_batch       = 2048
0.00.513.365 I llama_init_from_model: n_ubatch      = 512
0.00.513.365 I llama_init_from_model: flash_attn    = 0
0.00.513.368 I llama_init_from_model: freq_base     = 10000.0
0.00.513.369 I llama_init_from_model: freq_scale    = 1
0.00.513.374 I ggml_metal_init: allocating
0.00.513.451 I ggml_metal_init: found device: Apple M4
0.00.513.465 I ggml_metal_init: picking default device: Apple M4
0.00.515.249 I ggml_metal_init: using embedded metal library
0.00.521.840 I ggml_metal_init: GPU name:   Apple M4
0.00.521.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.847 I ggml_metal_init: simdgroup reduction   = true
0.00.521.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.848 I ggml_metal_init: has residency sets    = true
0.00.521.848 I ggml_metal_init: has bfloat            = true
0.00.521.848 I ggml_metal_init: use bfloat            = true
0.00.521.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.858 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.539.246 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.595.526 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.595.533 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.595.556 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.599.940 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.599.942 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.599.942 I llama_init_from_model: graph nodes  = 967
0.00.599.942 I llama_init_from_model: graph splits = 2
0.00.599.949 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.600.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.600.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.170 I main: llama threadpool init, n_threads = 4
0.00.658.215 I 
0.00.658.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.238 I 
0.00.658.387 I sampler seed: 1234
0.00.658.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.658.416 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.658.417 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.658.417 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.426.096 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.426.096 I llama_perf_context_print:        load time =     647.27 ms
0.01.426.097 I llama_perf_context_print: prompt eval time =      57.20 ms /     7 tokens (    8.17 ms per token,   122.38 tokens per second)
0.01.426.098 I llama_perf_context_print:        eval time =     707.51 ms /    63 runs   (   11.23 ms per token,    89.04 tokens per second)
0.01.426.102 I llama_perf_context_print:       total time =     768.80 ms /    70 tokens
0.01.426.331 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.109s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.749 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.841 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.846 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.848 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.848 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.849 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.849 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.849 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.850 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.851 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.851 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.851 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.852 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.852 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.855 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.855 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.856 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.345 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.347 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.347 I llama_model_loader: - type  f32:  194 tensors
0.00.024.347 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.348 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.348 I print_info: file format = GGUF V3 (latest)
0.00.024.349 I print_info: file type   = Q5_K - Medium
0.00.024.350 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.488 I load: special tokens cache size = 25
0.00.038.341 I load: token to piece cache size = 0.2984 MB
0.00.038.345 I print_info: arch             = gptneox
0.00.038.345 I print_info: vocab_only       = 0
0.00.038.345 I print_info: n_ctx_train      = 2048
0.00.038.345 I print_info: n_embd           = 2048
0.00.038.346 I print_info: n_layer          = 24
0.00.038.348 I print_info: n_head           = 16
0.00.038.349 I print_info: n_head_kv        = 16
0.00.038.349 I print_info: n_rot            = 32
0.00.038.349 I print_info: n_swa            = 0
0.00.038.350 I print_info: n_embd_head_k    = 128
0.00.038.350 I print_info: n_embd_head_v    = 128
0.00.038.351 I print_info: n_gqa            = 1
0.00.038.351 I print_info: n_embd_k_gqa     = 2048
0.00.038.352 I print_info: n_embd_v_gqa     = 2048
0.00.038.353 I print_info: f_norm_eps       = 1.0e-05
0.00.038.353 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.355 I print_info: f_logit_scale    = 0.0e+00
0.00.038.356 I print_info: n_ff             = 8192
0.00.038.356 I print_info: n_expert         = 0
0.00.038.356 I print_info: n_expert_used    = 0
0.00.038.356 I print_info: causal attn      = 1
0.00.038.356 I print_info: pooling type     = 0
0.00.038.358 I print_info: rope type        = 2
0.00.038.360 I print_info: rope scaling     = linear
0.00.038.360 I print_info: freq_base_train  = 10000.0
0.00.038.361 I print_info: freq_scale_train = 1
0.00.038.361 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.361 I print_info: rope_finetuned   = unknown
0.00.038.361 I print_info: ssm_d_conv       = 0
0.00.038.361 I print_info: ssm_d_inner      = 0
0.00.038.361 I print_info: ssm_d_state      = 0
0.00.038.362 I print_info: ssm_dt_rank      = 0
0.00.038.362 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.362 I print_info: model type       = 1.4B
0.00.038.362 I print_info: model params     = 1.41 B
0.00.038.362 I print_info: general.name     = 1.4B
0.00.038.368 I print_info: vocab type       = BPE
0.00.038.368 I print_info: n_vocab          = 50304
0.00.038.368 I print_info: n_merges         = 50009
0.00.038.368 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.368 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.369 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.369 I print_info: LF token         = 128 'Ä'
0.00.038.369 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.369 I print_info: max token length = 1024
0.00.584.355 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.364 I load_tensors: offloading output layer to GPU
0.00.584.365 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.399 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.584.400 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.585.930 I llama_init_from_model: n_seq_max     = 1
0.00.585.934 I llama_init_from_model: n_ctx         = 2048
0.00.585.934 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.585.934 I llama_init_from_model: n_batch       = 2048
0.00.585.935 I llama_init_from_model: n_ubatch      = 512
0.00.585.935 I llama_init_from_model: flash_attn    = 0
0.00.585.937 I llama_init_from_model: freq_base     = 10000.0
0.00.585.937 I llama_init_from_model: freq_scale    = 1
0.00.585.942 I ggml_metal_init: allocating
0.00.585.961 I ggml_metal_init: found device: Apple M4
0.00.585.970 I ggml_metal_init: picking default device: Apple M4
0.00.587.414 I ggml_metal_init: using embedded metal library
0.00.593.589 I ggml_metal_init: GPU name:   Apple M4
0.00.593.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.594 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.594 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.595 I ggml_metal_init: simdgroup reduction   = true
0.00.593.595 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.596 I ggml_metal_init: has residency sets    = true
0.00.593.596 I ggml_metal_init: has bfloat            = true
0.00.593.596 I ggml_metal_init: use bfloat            = true
0.00.593.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.610.656 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.499 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.662.505 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.662.529 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.401 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.667.404 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.667.404 I llama_init_from_model: graph nodes  = 967
0.00.667.404 I llama_init_from_model: graph splits = 2
0.00.667.411 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.667.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.667.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.222 I main: llama threadpool init, n_threads = 4
0.00.734.265 I 
0.00.734.291 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.292 I 
0.00.734.466 I sampler seed: 1234
0.00.734.471 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.530 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.533 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.533 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.585.399 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55038.76 tokens per second)
0.01.585.399 I llama_perf_context_print:        load time =     724.57 ms
0.01.585.400 I llama_perf_context_print: prompt eval time =      55.71 ms /     7 tokens (    7.96 ms per token,   125.66 tokens per second)
0.01.585.401 I llama_perf_context_print:        eval time =     792.27 ms /    63 runs   (   12.58 ms per token,    79.52 tokens per second)
0.01.585.402 I llama_perf_context_print:       total time =     852.08 ms /    70 tokens
0.01.585.652 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.108s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.102 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.739 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.745 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.239 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.240 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.240 I llama_model_loader: - type  f32:  194 tensors
0.00.025.240 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.241 I print_info: file format = GGUF V3 (latest)
0.00.025.241 I print_info: file type   = Q6_K
0.00.025.242 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.018 I load: special tokens cache size = 25
0.00.038.979 I load: token to piece cache size = 0.2984 MB
0.00.038.982 I print_info: arch             = gptneox
0.00.038.982 I print_info: vocab_only       = 0
0.00.038.982 I print_info: n_ctx_train      = 2048
0.00.038.982 I print_info: n_embd           = 2048
0.00.038.982 I print_info: n_layer          = 24
0.00.038.985 I print_info: n_head           = 16
0.00.038.986 I print_info: n_head_kv        = 16
0.00.038.986 I print_info: n_rot            = 32
0.00.038.986 I print_info: n_swa            = 0
0.00.038.986 I print_info: n_embd_head_k    = 128
0.00.038.986 I print_info: n_embd_head_v    = 128
0.00.038.987 I print_info: n_gqa            = 1
0.00.038.988 I print_info: n_embd_k_gqa     = 2048
0.00.038.989 I print_info: n_embd_v_gqa     = 2048
0.00.038.989 I print_info: f_norm_eps       = 1.0e-05
0.00.038.990 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.990 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.990 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.990 I print_info: f_logit_scale    = 0.0e+00
0.00.038.991 I print_info: n_ff             = 8192
0.00.038.992 I print_info: n_expert         = 0
0.00.038.993 I print_info: n_expert_used    = 0
0.00.038.993 I print_info: causal attn      = 1
0.00.038.993 I print_info: pooling type     = 0
0.00.038.993 I print_info: rope type        = 2
0.00.038.993 I print_info: rope scaling     = linear
0.00.038.994 I print_info: freq_base_train  = 10000.0
0.00.038.994 I print_info: freq_scale_train = 1
0.00.038.995 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.996 I print_info: rope_finetuned   = unknown
0.00.038.996 I print_info: ssm_d_conv       = 0
0.00.038.996 I print_info: ssm_d_inner      = 0
0.00.038.996 I print_info: ssm_d_state      = 0
0.00.038.996 I print_info: ssm_dt_rank      = 0
0.00.038.996 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.997 I print_info: model type       = 1.4B
0.00.038.997 I print_info: model params     = 1.41 B
0.00.038.997 I print_info: general.name     = 1.4B
0.00.038.998 I print_info: vocab type       = BPE
0.00.038.998 I print_info: n_vocab          = 50304
0.00.038.998 I print_info: n_merges         = 50009
0.00.038.998 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.998 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.999 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.999 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.999 I print_info: LF token         = 128 'Ä'
0.00.038.999 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.999 I print_info: max token length = 1024
0.00.648.263 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.267 I load_tensors: offloading output layer to GPU
0.00.648.268 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.296 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.648.298 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.649.647 I llama_init_from_model: n_seq_max     = 1
0.00.649.649 I llama_init_from_model: n_ctx         = 2048
0.00.649.654 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.649.654 I llama_init_from_model: n_batch       = 2048
0.00.649.655 I llama_init_from_model: n_ubatch      = 512
0.00.649.655 I llama_init_from_model: flash_attn    = 0
0.00.649.659 I llama_init_from_model: freq_base     = 10000.0
0.00.649.659 I llama_init_from_model: freq_scale    = 1
0.00.649.660 I ggml_metal_init: allocating
0.00.649.720 I ggml_metal_init: found device: Apple M4
0.00.649.735 I ggml_metal_init: picking default device: Apple M4
0.00.651.224 I ggml_metal_init: using embedded metal library
0.00.657.372 I ggml_metal_init: GPU name:   Apple M4
0.00.657.375 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.376 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.377 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.378 I ggml_metal_init: simdgroup reduction   = true
0.00.657.378 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.379 I ggml_metal_init: has residency sets    = true
0.00.657.379 I ggml_metal_init: has bfloat            = true
0.00.657.379 I ggml_metal_init: use bfloat            = true
0.00.657.380 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.448 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.270 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.725.278 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.725.303 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.794 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.729.797 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.729.797 I llama_init_from_model: graph nodes  = 967
0.00.729.797 I llama_init_from_model: graph splits = 2
0.00.729.803 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.729.931 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.569 I main: llama threadpool init, n_threads = 4
0.00.799.614 I 
0.00.799.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.640 I 
0.00.799.805 I sampler seed: 1234
0.00.799.810 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.820 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.821 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.821 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.681.363 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.681.364 I llama_perf_context_print:        load time =     788.60 ms
0.01.681.364 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.60 tokens per second)
0.01.681.365 I llama_perf_context_print:        eval time =     824.07 ms /    63 runs   (   13.08 ms per token,    76.45 tokens per second)
0.01.681.365 I llama_perf_context_print:       total time =     882.66 ms /    70 tokens
0.01.681.595 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.108s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.795 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.915 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.100 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.114 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.116 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.116 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.117 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.118 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.119 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.130 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.130 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.131 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.132 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.136 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.137 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.885 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.885 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.885 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.890 I llama_model_loader: - type  f32:  194 tensors
0.00.047.891 I llama_model_loader: - type  f16:   98 tensors
0.00.047.891 I print_info: file format = GGUF V3 (latest)
0.00.047.892 I print_info: file type   = all F32 (guessed)
0.00.047.893 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.056.094 I load: special tokens cache size = 25
0.00.061.951 I load: token to piece cache size = 0.2984 MB
0.00.061.956 I print_info: arch             = gptneox
0.00.061.956 I print_info: vocab_only       = 0
0.00.061.956 I print_info: n_ctx_train      = 2048
0.00.061.956 I print_info: n_embd           = 2048
0.00.061.957 I print_info: n_layer          = 24
0.00.061.961 I print_info: n_head           = 16
0.00.061.961 I print_info: n_head_kv        = 16
0.00.061.962 I print_info: n_rot            = 32
0.00.061.962 I print_info: n_swa            = 0
0.00.061.962 I print_info: n_embd_head_k    = 128
0.00.061.962 I print_info: n_embd_head_v    = 128
0.00.061.963 I print_info: n_gqa            = 1
0.00.061.964 I print_info: n_embd_k_gqa     = 2048
0.00.061.965 I print_info: n_embd_v_gqa     = 2048
0.00.061.965 I print_info: f_norm_eps       = 1.0e-05
0.00.061.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.966 I print_info: f_logit_scale    = 0.0e+00
0.00.061.967 I print_info: n_ff             = 8192
0.00.061.967 I print_info: n_expert         = 0
0.00.061.967 I print_info: n_expert_used    = 0
0.00.061.967 I print_info: causal attn      = 1
0.00.061.967 I print_info: pooling type     = 0
0.00.061.967 I print_info: rope type        = 2
0.00.061.968 I print_info: rope scaling     = linear
0.00.061.968 I print_info: freq_base_train  = 10000.0
0.00.061.968 I print_info: freq_scale_train = 1
0.00.061.968 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.968 I print_info: rope_finetuned   = unknown
0.00.061.969 I print_info: ssm_d_conv       = 0
0.00.061.969 I print_info: ssm_d_inner      = 0
0.00.061.969 I print_info: ssm_d_state      = 0
0.00.061.969 I print_info: ssm_dt_rank      = 0
0.00.061.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.969 I print_info: model type       = 1.4B
0.00.061.970 I print_info: model params     = 1.41 B
0.00.061.970 I print_info: general.name     = 1.4B
0.00.061.971 I print_info: vocab type       = BPE
0.00.061.971 I print_info: n_vocab          = 50304
0.00.061.972 I print_info: n_merges         = 50009
0.00.061.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.973 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.973 I print_info: LF token         = 128 'Ä'
0.00.061.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.973 I print_info: max token length = 1024
0.01.444.552 I load_tensors: offloading 24 repeating layers to GPU
0.01.444.555 I load_tensors: offloading output layer to GPU
0.01.444.556 I load_tensors: offloaded 25/25 layers to GPU
0.01.444.573 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.444.573 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.444.991 I llama_init_from_model: n_seq_max     = 1
0.01.444.992 I llama_init_from_model: n_ctx         = 128
0.01.444.992 I llama_init_from_model: n_ctx_per_seq = 128
0.01.444.992 I llama_init_from_model: n_batch       = 128
0.01.444.992 I llama_init_from_model: n_ubatch      = 128
0.01.444.992 I llama_init_from_model: flash_attn    = 0
0.01.444.994 I llama_init_from_model: freq_base     = 10000.0
0.01.444.995 I llama_init_from_model: freq_scale    = 1
0.01.444.995 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.444.997 I ggml_metal_init: allocating
0.01.445.042 I ggml_metal_init: found device: Apple M4
0.01.445.049 I ggml_metal_init: picking default device: Apple M4
0.01.445.679 I ggml_metal_init: using embedded metal library
0.01.448.164 I ggml_metal_init: GPU name:   Apple M4
0.01.448.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.448.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.448.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.448.167 I ggml_metal_init: simdgroup reduction   = true
0.01.448.167 I ggml_metal_init: simdgroup matrix mul. = true
0.01.448.167 I ggml_metal_init: has residency sets    = true
0.01.448.167 I ggml_metal_init: has bfloat            = true
0.01.448.168 I ggml_metal_init: use bfloat            = true
0.01.448.168 I ggml_metal_init: hasUnifiedMemory      = true
0.01.448.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.457.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.459.574 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.459.577 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.459.593 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.461.164 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.461.165 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.461.166 I llama_init_from_model: graph nodes  = 967
0.01.461.166 I llama_init_from_model: graph splits = 2
0.01.461.167 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.461.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.495.666 I 
0.01.495.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.495.724 I perplexity: tokenizing the input ..
0.01.499.624 I perplexity: tokenization took 3.898 ms
0.01.499.644 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.618.654 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.620.082 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.620.104 I llama_perf_context_print:        load time =    1474.74 ms
0.01.620.105 I llama_perf_context_print: prompt eval time =     118.78 ms /   128 tokens (    0.93 ms per token,  1077.64 tokens per second)
0.01.620.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.620.106 I llama_perf_context_print:       total time =     124.44 ms /   129 tokens
0.01.620.476 I ggml_metal_free: deallocating

real	0m1.828s
user	0m0.080s
sys	0m0.240s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.531 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.542 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.543 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.544 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.544 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.545 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.545 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.545 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.547 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.547 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.461 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.284 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.285 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.285 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.286 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.286 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.286 I llama_model_loader: - type  f32:  194 tensors
0.00.027.287 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.287 I print_info: file format = GGUF V3 (latest)
0.00.027.288 I print_info: file type   = Q8_0
0.00.027.288 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.351 I load: special tokens cache size = 25
0.00.041.357 I load: token to piece cache size = 0.2984 MB
0.00.041.360 I print_info: arch             = gptneox
0.00.041.361 I print_info: vocab_only       = 0
0.00.041.361 I print_info: n_ctx_train      = 2048
0.00.041.362 I print_info: n_embd           = 2048
0.00.041.363 I print_info: n_layer          = 24
0.00.041.366 I print_info: n_head           = 16
0.00.041.367 I print_info: n_head_kv        = 16
0.00.041.367 I print_info: n_rot            = 32
0.00.041.367 I print_info: n_swa            = 0
0.00.041.368 I print_info: n_embd_head_k    = 128
0.00.041.369 I print_info: n_embd_head_v    = 128
0.00.041.370 I print_info: n_gqa            = 1
0.00.041.370 I print_info: n_embd_k_gqa     = 2048
0.00.041.371 I print_info: n_embd_v_gqa     = 2048
0.00.041.371 I print_info: f_norm_eps       = 1.0e-05
0.00.041.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.372 I print_info: f_logit_scale    = 0.0e+00
0.00.041.373 I print_info: n_ff             = 8192
0.00.041.373 I print_info: n_expert         = 0
0.00.041.373 I print_info: n_expert_used    = 0
0.00.041.373 I print_info: causal attn      = 1
0.00.041.377 I print_info: pooling type     = 0
0.00.041.377 I print_info: rope type        = 2
0.00.041.377 I print_info: rope scaling     = linear
0.00.041.377 I print_info: freq_base_train  = 10000.0
0.00.041.378 I print_info: freq_scale_train = 1
0.00.041.378 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.378 I print_info: rope_finetuned   = unknown
0.00.041.379 I print_info: ssm_d_conv       = 0
0.00.041.379 I print_info: ssm_d_inner      = 0
0.00.041.380 I print_info: ssm_d_state      = 0
0.00.041.380 I print_info: ssm_dt_rank      = 0
0.00.041.380 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.380 I print_info: model type       = 1.4B
0.00.041.380 I print_info: model params     = 1.41 B
0.00.041.381 I print_info: general.name     = 1.4B
0.00.041.381 I print_info: vocab type       = BPE
0.00.041.381 I print_info: n_vocab          = 50304
0.00.041.381 I print_info: n_merges         = 50009
0.00.041.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.382 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.382 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.382 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.382 I print_info: LF token         = 128 'Ä'
0.00.041.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.382 I print_info: max token length = 1024
0.00.882.847 I load_tensors: offloading 24 repeating layers to GPU
0.00.882.853 I load_tensors: offloading output layer to GPU
0.00.882.854 I load_tensors: offloaded 25/25 layers to GPU
0.00.882.887 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.882.889 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.884.207 I llama_init_from_model: n_seq_max     = 1
0.00.884.210 I llama_init_from_model: n_ctx         = 128
0.00.884.211 I llama_init_from_model: n_ctx_per_seq = 128
0.00.884.211 I llama_init_from_model: n_batch       = 128
0.00.884.211 I llama_init_from_model: n_ubatch      = 128
0.00.884.212 I llama_init_from_model: flash_attn    = 0
0.00.884.213 I llama_init_from_model: freq_base     = 10000.0
0.00.884.213 I llama_init_from_model: freq_scale    = 1
0.00.884.214 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.884.215 I ggml_metal_init: allocating
0.00.884.294 I ggml_metal_init: found device: Apple M4
0.00.884.306 I ggml_metal_init: picking default device: Apple M4
0.00.885.588 I ggml_metal_init: using embedded metal library
0.00.890.901 I ggml_metal_init: GPU name:   Apple M4
0.00.890.904 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.890.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.890.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.890.906 I ggml_metal_init: simdgroup reduction   = true
0.00.890.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.890.907 I ggml_metal_init: has residency sets    = true
0.00.890.907 I ggml_metal_init: has bfloat            = true
0.00.890.907 I ggml_metal_init: use bfloat            = true
0.00.890.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.890.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.905.572 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.908.838 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.908.841 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.908.866 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.912.106 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.912.108 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.912.108 I llama_init_from_model: graph nodes  = 967
0.00.912.108 I llama_init_from_model: graph splits = 2
0.00.912.111 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.912.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.937.085 I 
0.00.937.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.937.145 I perplexity: tokenizing the input ..
0.00.944.264 I perplexity: tokenization took 7.117 ms
0.00.944.283 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.082.493 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.083.902 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.083.919 I llama_perf_context_print:        load time =     925.88 ms
0.01.083.920 I llama_perf_context_print: prompt eval time =     137.33 ms /   128 tokens (    1.07 ms per token,   932.03 tokens per second)
0.01.083.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.083.921 I llama_perf_context_print:       total time =     146.84 ms /   129 tokens
0.01.084.239 I ggml_metal_free: deallocating

real	0m1.098s
user	0m0.075s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.121 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.605 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.610 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.611 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.611 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.611 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.612 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.612 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.613 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.613 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.613 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.617 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.617 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.589 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.408 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.408 I llama_model_loader: - type  f32:  194 tensors
0.00.027.408 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.409 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.409 I print_info: file format = GGUF V3 (latest)
0.00.027.410 I print_info: file type   = Q4_0
0.00.027.411 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.037.402 I load: special tokens cache size = 25
0.00.044.204 I load: token to piece cache size = 0.2984 MB
0.00.044.209 I print_info: arch             = gptneox
0.00.044.209 I print_info: vocab_only       = 0
0.00.044.210 I print_info: n_ctx_train      = 2048
0.00.044.210 I print_info: n_embd           = 2048
0.00.044.210 I print_info: n_layer          = 24
0.00.044.218 I print_info: n_head           = 16
0.00.044.219 I print_info: n_head_kv        = 16
0.00.044.219 I print_info: n_rot            = 32
0.00.044.220 I print_info: n_swa            = 0
0.00.044.220 I print_info: n_embd_head_k    = 128
0.00.044.220 I print_info: n_embd_head_v    = 128
0.00.044.221 I print_info: n_gqa            = 1
0.00.044.222 I print_info: n_embd_k_gqa     = 2048
0.00.044.223 I print_info: n_embd_v_gqa     = 2048
0.00.044.223 I print_info: f_norm_eps       = 1.0e-05
0.00.044.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.224 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.224 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.226 I print_info: f_logit_scale    = 0.0e+00
0.00.044.228 I print_info: n_ff             = 8192
0.00.044.230 I print_info: n_expert         = 0
0.00.044.230 I print_info: n_expert_used    = 0
0.00.044.230 I print_info: causal attn      = 1
0.00.044.230 I print_info: pooling type     = 0
0.00.044.230 I print_info: rope type        = 2
0.00.044.230 I print_info: rope scaling     = linear
0.00.044.231 I print_info: freq_base_train  = 10000.0
0.00.044.231 I print_info: freq_scale_train = 1
0.00.044.231 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.232 I print_info: rope_finetuned   = unknown
0.00.044.232 I print_info: ssm_d_conv       = 0
0.00.044.232 I print_info: ssm_d_inner      = 0
0.00.044.232 I print_info: ssm_d_state      = 0
0.00.044.232 I print_info: ssm_dt_rank      = 0
0.00.044.232 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.232 I print_info: model type       = 1.4B
0.00.044.236 I print_info: model params     = 1.41 B
0.00.044.236 I print_info: general.name     = 1.4B
0.00.044.237 I print_info: vocab type       = BPE
0.00.044.237 I print_info: n_vocab          = 50304
0.00.044.237 I print_info: n_merges         = 50009
0.00.044.238 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.238 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.238 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.238 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.240 I print_info: LF token         = 128 'Ä'
0.00.044.240 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.240 I print_info: max token length = 1024
0.00.589.558 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.569 I load_tensors: offloading output layer to GPU
0.00.589.570 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.605 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.589.609 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.591.029 I llama_init_from_model: n_seq_max     = 1
0.00.591.033 I llama_init_from_model: n_ctx         = 128
0.00.591.033 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.038 I llama_init_from_model: n_batch       = 128
0.00.591.038 I llama_init_from_model: n_ubatch      = 128
0.00.591.039 I llama_init_from_model: flash_attn    = 0
0.00.591.040 I llama_init_from_model: freq_base     = 10000.0
0.00.591.046 I llama_init_from_model: freq_scale    = 1
0.00.591.046 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.049 I ggml_metal_init: allocating
0.00.591.134 I ggml_metal_init: found device: Apple M4
0.00.591.148 I ggml_metal_init: picking default device: Apple M4
0.00.592.992 I ggml_metal_init: using embedded metal library
0.00.599.074 I ggml_metal_init: GPU name:   Apple M4
0.00.599.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.080 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.081 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.082 I ggml_metal_init: simdgroup reduction   = true
0.00.599.082 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.082 I ggml_metal_init: has residency sets    = true
0.00.599.082 I ggml_metal_init: has bfloat            = true
0.00.599.083 I ggml_metal_init: use bfloat            = true
0.00.599.084 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.086 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.568 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.622.146 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.622.149 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.622.176 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.557 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.625.559 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.625.560 I llama_init_from_model: graph nodes  = 967
0.00.625.560 I llama_init_from_model: graph splits = 2
0.00.625.563 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.564 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.923 I 
0.00.653.000 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.021 I perplexity: tokenizing the input ..
0.00.660.483 I perplexity: tokenization took 7.459 ms
0.00.660.505 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.085 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.796.412 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.796.426 I llama_perf_context_print:        load time =     642.79 ms
0.00.796.427 I llama_perf_context_print: prompt eval time =     133.70 ms /   128 tokens (    1.04 ms per token,   957.35 tokens per second)
0.00.796.428 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.428 I llama_perf_context_print:       total time =     143.51 ms /   129 tokens
0.00.796.779 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.084s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.861 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.158 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.162 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.164 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.169 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.170 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.171 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.172 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.174 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.148 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.209 I llama_model_loader: - type  f32:  194 tensors
0.00.037.209 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.209 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.210 I print_info: file format = GGUF V3 (latest)
0.00.037.210 I print_info: file type   = Q4_1
0.00.037.211 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.852 I load: special tokens cache size = 25
0.00.052.597 I load: token to piece cache size = 0.2984 MB
0.00.052.600 I print_info: arch             = gptneox
0.00.052.600 I print_info: vocab_only       = 0
0.00.052.600 I print_info: n_ctx_train      = 2048
0.00.052.601 I print_info: n_embd           = 2048
0.00.052.601 I print_info: n_layer          = 24
0.00.052.604 I print_info: n_head           = 16
0.00.052.605 I print_info: n_head_kv        = 16
0.00.052.605 I print_info: n_rot            = 32
0.00.052.605 I print_info: n_swa            = 0
0.00.052.605 I print_info: n_embd_head_k    = 128
0.00.052.607 I print_info: n_embd_head_v    = 128
0.00.052.608 I print_info: n_gqa            = 1
0.00.052.608 I print_info: n_embd_k_gqa     = 2048
0.00.052.609 I print_info: n_embd_v_gqa     = 2048
0.00.052.610 I print_info: f_norm_eps       = 1.0e-05
0.00.052.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.610 I print_info: f_logit_scale    = 0.0e+00
0.00.052.611 I print_info: n_ff             = 8192
0.00.052.611 I print_info: n_expert         = 0
0.00.052.611 I print_info: n_expert_used    = 0
0.00.052.611 I print_info: causal attn      = 1
0.00.052.611 I print_info: pooling type     = 0
0.00.052.612 I print_info: rope type        = 2
0.00.052.613 I print_info: rope scaling     = linear
0.00.052.613 I print_info: freq_base_train  = 10000.0
0.00.052.613 I print_info: freq_scale_train = 1
0.00.052.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.617 I print_info: rope_finetuned   = unknown
0.00.052.617 I print_info: ssm_d_conv       = 0
0.00.052.617 I print_info: ssm_d_inner      = 0
0.00.052.617 I print_info: ssm_d_state      = 0
0.00.052.617 I print_info: ssm_dt_rank      = 0
0.00.052.617 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.618 I print_info: model type       = 1.4B
0.00.052.618 I print_info: model params     = 1.41 B
0.00.052.618 I print_info: general.name     = 1.4B
0.00.052.618 I print_info: vocab type       = BPE
0.00.052.619 I print_info: n_vocab          = 50304
0.00.052.619 I print_info: n_merges         = 50009
0.00.052.619 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.619 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.620 I print_info: LF token         = 128 'Ä'
0.00.052.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.621 I print_info: max token length = 1024
0.00.681.980 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.996 I load_tensors: offloading output layer to GPU
0.00.681.997 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.029 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.682.030 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.683.396 I llama_init_from_model: n_seq_max     = 1
0.00.683.401 I llama_init_from_model: n_ctx         = 128
0.00.683.401 I llama_init_from_model: n_ctx_per_seq = 128
0.00.683.402 I llama_init_from_model: n_batch       = 128
0.00.683.403 I llama_init_from_model: n_ubatch      = 128
0.00.683.403 I llama_init_from_model: flash_attn    = 0
0.00.683.406 I llama_init_from_model: freq_base     = 10000.0
0.00.683.406 I llama_init_from_model: freq_scale    = 1
0.00.683.407 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.683.409 I ggml_metal_init: allocating
0.00.683.484 I ggml_metal_init: found device: Apple M4
0.00.683.498 I ggml_metal_init: picking default device: Apple M4
0.00.685.162 I ggml_metal_init: using embedded metal library
0.00.691.665 I ggml_metal_init: GPU name:   Apple M4
0.00.691.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.673 I ggml_metal_init: simdgroup reduction   = true
0.00.691.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.674 I ggml_metal_init: has residency sets    = true
0.00.691.674 I ggml_metal_init: has bfloat            = true
0.00.691.674 I ggml_metal_init: use bfloat            = true
0.00.691.675 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.428 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.118 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.713.122 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.713.149 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.716.553 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.716.555 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.716.555 I llama_init_from_model: graph nodes  = 967
0.00.716.556 I llama_init_from_model: graph splits = 2
0.00.716.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.716.559 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.810 I 
0.00.741.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.903 I perplexity: tokenizing the input ..
0.00.749.531 I perplexity: tokenization took 7.625 ms
0.00.749.553 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.111 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.887.437 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.887.453 I llama_perf_context_print:        load time =     720.94 ms
0.00.887.455 I llama_perf_context_print: prompt eval time =     135.69 ms /   128 tokens (    1.06 ms per token,   943.36 tokens per second)
0.00.887.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.887.456 I llama_perf_context_print:       total time =     145.65 ms /   129 tokens
0.00.887.843 I ggml_metal_free: deallocating

real	0m0.902s
user	0m0.082s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.774 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.022.781 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.786 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.786 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.787 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.788 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.788 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.789 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.789 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.789 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.790 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.790 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.792 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.830 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.939 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.939 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.940 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.940 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.940 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.031.941 I llama_model_loader: - type  f32:  194 tensors
0.00.031.941 I llama_model_loader: - type q5_0:   97 tensors
0.00.031.941 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.942 I print_info: file format = GGUF V3 (latest)
0.00.031.942 I print_info: file type   = Q5_0
0.00.031.943 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.040.697 I load: special tokens cache size = 25
0.00.047.912 I load: token to piece cache size = 0.2984 MB
0.00.047.915 I print_info: arch             = gptneox
0.00.047.916 I print_info: vocab_only       = 0
0.00.047.916 I print_info: n_ctx_train      = 2048
0.00.047.916 I print_info: n_embd           = 2048
0.00.047.916 I print_info: n_layer          = 24
0.00.047.919 I print_info: n_head           = 16
0.00.047.920 I print_info: n_head_kv        = 16
0.00.047.921 I print_info: n_rot            = 32
0.00.047.921 I print_info: n_swa            = 0
0.00.047.921 I print_info: n_embd_head_k    = 128
0.00.047.921 I print_info: n_embd_head_v    = 128
0.00.047.922 I print_info: n_gqa            = 1
0.00.047.923 I print_info: n_embd_k_gqa     = 2048
0.00.047.924 I print_info: n_embd_v_gqa     = 2048
0.00.047.924 I print_info: f_norm_eps       = 1.0e-05
0.00.047.925 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.925 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.925 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.925 I print_info: f_logit_scale    = 0.0e+00
0.00.047.926 I print_info: n_ff             = 8192
0.00.047.926 I print_info: n_expert         = 0
0.00.047.926 I print_info: n_expert_used    = 0
0.00.047.927 I print_info: causal attn      = 1
0.00.047.927 I print_info: pooling type     = 0
0.00.047.927 I print_info: rope type        = 2
0.00.047.927 I print_info: rope scaling     = linear
0.00.047.928 I print_info: freq_base_train  = 10000.0
0.00.047.928 I print_info: freq_scale_train = 1
0.00.047.928 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.930 I print_info: rope_finetuned   = unknown
0.00.047.931 I print_info: ssm_d_conv       = 0
0.00.047.931 I print_info: ssm_d_inner      = 0
0.00.047.931 I print_info: ssm_d_state      = 0
0.00.047.931 I print_info: ssm_dt_rank      = 0
0.00.047.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.931 I print_info: model type       = 1.4B
0.00.047.932 I print_info: model params     = 1.41 B
0.00.047.932 I print_info: general.name     = 1.4B
0.00.047.932 I print_info: vocab type       = BPE
0.00.047.933 I print_info: n_vocab          = 50304
0.00.047.933 I print_info: n_merges         = 50009
0.00.047.933 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.934 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.934 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.938 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.938 I print_info: LF token         = 128 'Ä'
0.00.047.942 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.942 I print_info: max token length = 1024
0.00.813.541 I load_tensors: offloading 24 repeating layers to GPU
0.00.813.551 I load_tensors: offloading output layer to GPU
0.00.813.552 I load_tensors: offloaded 25/25 layers to GPU
0.00.813.584 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.813.585 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.815.059 I llama_init_from_model: n_seq_max     = 1
0.00.815.064 I llama_init_from_model: n_ctx         = 128
0.00.815.064 I llama_init_from_model: n_ctx_per_seq = 128
0.00.815.069 I llama_init_from_model: n_batch       = 128
0.00.815.069 I llama_init_from_model: n_ubatch      = 128
0.00.815.070 I llama_init_from_model: flash_attn    = 0
0.00.815.076 I llama_init_from_model: freq_base     = 10000.0
0.00.815.077 I llama_init_from_model: freq_scale    = 1
0.00.815.077 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.815.079 I ggml_metal_init: allocating
0.00.815.159 I ggml_metal_init: found device: Apple M4
0.00.815.173 I ggml_metal_init: picking default device: Apple M4
0.00.816.921 I ggml_metal_init: using embedded metal library
0.00.823.712 I ggml_metal_init: GPU name:   Apple M4
0.00.823.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.823.718 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.823.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.823.720 I ggml_metal_init: simdgroup reduction   = true
0.00.823.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.823.720 I ggml_metal_init: has residency sets    = true
0.00.823.720 I ggml_metal_init: has bfloat            = true
0.00.823.721 I ggml_metal_init: use bfloat            = true
0.00.823.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.823.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.840.858 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.844.291 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.844.297 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.844.329 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.847.771 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.847.773 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.847.773 I llama_init_from_model: graph nodes  = 967
0.00.847.774 I llama_init_from_model: graph splits = 2
0.00.847.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.847.777 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.990 I 
0.00.880.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.880.095 I perplexity: tokenizing the input ..
0.00.887.321 I perplexity: tokenization took 7.224 ms
0.00.887.339 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.035.979 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.01.037.331 I Final estimate: PPL = 10.0972 +/- 3.20136

0.01.037.351 I llama_perf_context_print:        load time =     871.07 ms
0.01.037.352 I llama_perf_context_print: prompt eval time =     148.06 ms /   128 tokens (    1.16 ms per token,   864.54 tokens per second)
0.01.037.353 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.037.353 I llama_perf_context_print:       total time =     157.37 ms /   129 tokens
0.01.037.758 I ggml_metal_free: deallocating

real	0m1.060s
user	0m0.082s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.626 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.030.239 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.245 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.246 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.246 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.246 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.247 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.248 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.248 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.249 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.249 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.250 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.250 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.681 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.247 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.248 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.249 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.249 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.249 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.250 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.040.250 I llama_model_loader: - type  f32:  194 tensors
0.00.040.250 I llama_model_loader: - type q5_1:   97 tensors
0.00.040.251 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.251 I print_info: file format = GGUF V3 (latest)
0.00.040.252 I print_info: file type   = Q5_1
0.00.040.253 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.050.366 I load: special tokens cache size = 25
0.00.057.761 I load: token to piece cache size = 0.2984 MB
0.00.057.764 I print_info: arch             = gptneox
0.00.057.765 I print_info: vocab_only       = 0
0.00.057.765 I print_info: n_ctx_train      = 2048
0.00.057.765 I print_info: n_embd           = 2048
0.00.057.765 I print_info: n_layer          = 24
0.00.057.768 I print_info: n_head           = 16
0.00.057.769 I print_info: n_head_kv        = 16
0.00.057.769 I print_info: n_rot            = 32
0.00.057.769 I print_info: n_swa            = 0
0.00.057.769 I print_info: n_embd_head_k    = 128
0.00.057.770 I print_info: n_embd_head_v    = 128
0.00.057.770 I print_info: n_gqa            = 1
0.00.057.771 I print_info: n_embd_k_gqa     = 2048
0.00.057.772 I print_info: n_embd_v_gqa     = 2048
0.00.057.773 I print_info: f_norm_eps       = 1.0e-05
0.00.057.773 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.773 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.773 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.774 I print_info: f_logit_scale    = 0.0e+00
0.00.057.774 I print_info: n_ff             = 8192
0.00.057.774 I print_info: n_expert         = 0
0.00.057.777 I print_info: n_expert_used    = 0
0.00.057.777 I print_info: causal attn      = 1
0.00.057.777 I print_info: pooling type     = 0
0.00.057.778 I print_info: rope type        = 2
0.00.057.778 I print_info: rope scaling     = linear
0.00.057.778 I print_info: freq_base_train  = 10000.0
0.00.057.779 I print_info: freq_scale_train = 1
0.00.057.779 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.779 I print_info: rope_finetuned   = unknown
0.00.057.779 I print_info: ssm_d_conv       = 0
0.00.057.779 I print_info: ssm_d_inner      = 0
0.00.057.781 I print_info: ssm_d_state      = 0
0.00.057.781 I print_info: ssm_dt_rank      = 0
0.00.057.781 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.782 I print_info: model type       = 1.4B
0.00.057.782 I print_info: model params     = 1.41 B
0.00.057.782 I print_info: general.name     = 1.4B
0.00.057.783 I print_info: vocab type       = BPE
0.00.057.783 I print_info: n_vocab          = 50304
0.00.057.783 I print_info: n_merges         = 50009
0.00.057.783 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.784 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.784 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.784 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.784 I print_info: LF token         = 128 'Ä'
0.00.057.785 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.785 I print_info: max token length = 1024
0.00.620.927 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.941 I load_tensors: offloading output layer to GPU
0.00.620.942 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.976 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.620.977 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.622.487 I llama_init_from_model: n_seq_max     = 1
0.00.622.493 I llama_init_from_model: n_ctx         = 128
0.00.622.493 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.494 I llama_init_from_model: n_batch       = 128
0.00.622.495 I llama_init_from_model: n_ubatch      = 128
0.00.622.495 I llama_init_from_model: flash_attn    = 0
0.00.622.497 I llama_init_from_model: freq_base     = 10000.0
0.00.622.497 I llama_init_from_model: freq_scale    = 1
0.00.622.498 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.506 I ggml_metal_init: allocating
0.00.622.578 I ggml_metal_init: found device: Apple M4
0.00.622.593 I ggml_metal_init: picking default device: Apple M4
0.00.624.192 I ggml_metal_init: using embedded metal library
0.00.630.728 I ggml_metal_init: GPU name:   Apple M4
0.00.630.732 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.733 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.734 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.734 I ggml_metal_init: simdgroup reduction   = true
0.00.630.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.735 I ggml_metal_init: has residency sets    = true
0.00.630.735 I ggml_metal_init: has bfloat            = true
0.00.630.735 I ggml_metal_init: use bfloat            = true
0.00.630.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.244 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.782 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.651.789 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.817 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.655.076 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.655.078 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.655.078 I llama_init_from_model: graph nodes  = 967
0.00.655.078 I llama_init_from_model: graph splits = 2
0.00.655.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.655.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.294 I 
0.00.683.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.403 I perplexity: tokenizing the input ..
0.00.690.880 I perplexity: tokenization took 7.473 ms
0.00.690.901 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.833 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.828.184 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.828.203 I llama_perf_context_print:        load time =     666.66 ms
0.00.828.204 I llama_perf_context_print: prompt eval time =     134.99 ms /   128 tokens (    1.05 ms per token,   948.24 tokens per second)
0.00.828.205 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.206 I llama_perf_context_print:       total time =     144.92 ms /   129 tokens
0.00.828.614 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.085s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.253 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.613 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.021.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.147 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.149 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.149 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.149 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.150 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.150 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.030.150 I llama_model_loader: - type  f32:  194 tensors
0.00.030.151 I llama_model_loader: - type q2_K:   49 tensors
0.00.030.151 I llama_model_loader: - type q3_K:   48 tensors
0.00.030.151 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.152 I print_info: file format = GGUF V3 (latest)
0.00.030.152 I print_info: file type   = Q2_K - Medium
0.00.030.153 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.038.329 I load: special tokens cache size = 25
0.00.044.235 I load: token to piece cache size = 0.2984 MB
0.00.044.238 I print_info: arch             = gptneox
0.00.044.239 I print_info: vocab_only       = 0
0.00.044.239 I print_info: n_ctx_train      = 2048
0.00.044.239 I print_info: n_embd           = 2048
0.00.044.239 I print_info: n_layer          = 24
0.00.044.242 I print_info: n_head           = 16
0.00.044.243 I print_info: n_head_kv        = 16
0.00.044.243 I print_info: n_rot            = 32
0.00.044.244 I print_info: n_swa            = 0
0.00.044.244 I print_info: n_embd_head_k    = 128
0.00.044.244 I print_info: n_embd_head_v    = 128
0.00.044.245 I print_info: n_gqa            = 1
0.00.044.248 I print_info: n_embd_k_gqa     = 2048
0.00.044.249 I print_info: n_embd_v_gqa     = 2048
0.00.044.249 I print_info: f_norm_eps       = 1.0e-05
0.00.044.250 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.250 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.256 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.257 I print_info: f_logit_scale    = 0.0e+00
0.00.044.262 I print_info: n_ff             = 8192
0.00.044.264 I print_info: n_expert         = 0
0.00.044.264 I print_info: n_expert_used    = 0
0.00.044.264 I print_info: causal attn      = 1
0.00.044.264 I print_info: pooling type     = 0
0.00.044.264 I print_info: rope type        = 2
0.00.044.264 I print_info: rope scaling     = linear
0.00.044.265 I print_info: freq_base_train  = 10000.0
0.00.044.265 I print_info: freq_scale_train = 1
0.00.044.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.265 I print_info: rope_finetuned   = unknown
0.00.044.266 I print_info: ssm_d_conv       = 0
0.00.044.266 I print_info: ssm_d_inner      = 0
0.00.044.267 I print_info: ssm_d_state      = 0
0.00.044.267 I print_info: ssm_dt_rank      = 0
0.00.044.267 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.267 I print_info: model type       = 1.4B
0.00.044.268 I print_info: model params     = 1.41 B
0.00.044.268 I print_info: general.name     = 1.4B
0.00.044.268 I print_info: vocab type       = BPE
0.00.044.269 I print_info: n_vocab          = 50304
0.00.044.269 I print_info: n_merges         = 50009
0.00.044.269 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.270 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.270 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.270 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.271 I print_info: LF token         = 128 'Ä'
0.00.044.271 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.271 I print_info: max token length = 1024
0.00.377.597 I load_tensors: offloading 24 repeating layers to GPU
0.00.377.611 I load_tensors: offloading output layer to GPU
0.00.377.611 I load_tensors: offloaded 25/25 layers to GPU
0.00.377.646 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.377.647 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.379.197 I llama_init_from_model: n_seq_max     = 1
0.00.379.203 I llama_init_from_model: n_ctx         = 128
0.00.379.203 I llama_init_from_model: n_ctx_per_seq = 128
0.00.379.204 I llama_init_from_model: n_batch       = 128
0.00.379.204 I llama_init_from_model: n_ubatch      = 128
0.00.379.204 I llama_init_from_model: flash_attn    = 0
0.00.379.207 I llama_init_from_model: freq_base     = 10000.0
0.00.379.207 I llama_init_from_model: freq_scale    = 1
0.00.379.208 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.379.220 I ggml_metal_init: allocating
0.00.379.285 I ggml_metal_init: found device: Apple M4
0.00.379.299 I ggml_metal_init: picking default device: Apple M4
0.00.381.015 I ggml_metal_init: using embedded metal library
0.00.386.573 I ggml_metal_init: GPU name:   Apple M4
0.00.386.590 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.386.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.386.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.386.593 I ggml_metal_init: simdgroup reduction   = true
0.00.386.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.386.593 I ggml_metal_init: has residency sets    = true
0.00.386.593 I ggml_metal_init: has bfloat            = true
0.00.386.594 I ggml_metal_init: use bfloat            = true
0.00.386.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.386.600 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.408.182 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.411.770 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.411.777 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.411.831 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.415.201 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.415.203 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.415.203 I llama_init_from_model: graph nodes  = 967
0.00.415.204 I llama_init_from_model: graph splits = 2
0.00.415.207 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.415.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.442.124 I 
0.00.442.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.442.221 I perplexity: tokenizing the input ..
0.00.449.484 I perplexity: tokenization took 7.258 ms
0.00.449.507 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.583.321 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.584.757 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.584.779 I llama_perf_context_print:        load time =     430.86 ms
0.00.584.780 I llama_perf_context_print: prompt eval time =     132.85 ms /   128 tokens (    1.04 ms per token,   963.51 tokens per second)
0.00.584.781 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.584.781 I llama_perf_context_print:       total time =     142.66 ms /   129 tokens
0.00.585.164 I ggml_metal_free: deallocating

real	0m0.600s
user	0m0.081s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.528 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.858 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.862 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.865 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.865 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.866 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.867 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.867 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.867 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.868 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.808 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.809 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.809 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.031.809 I llama_model_loader: - type  f32:  194 tensors
0.00.031.810 I llama_model_loader: - type q3_K:   25 tensors
0.00.031.810 I llama_model_loader: - type q4_K:   71 tensors
0.00.031.810 I llama_model_loader: - type q5_K:    1 tensors
0.00.031.810 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.811 I print_info: file format = GGUF V3 (latest)
0.00.031.811 I print_info: file type   = Q3_K - Medium
0.00.031.812 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.040.788 I load: special tokens cache size = 25
0.00.047.806 I load: token to piece cache size = 0.2984 MB
0.00.047.809 I print_info: arch             = gptneox
0.00.047.810 I print_info: vocab_only       = 0
0.00.047.810 I print_info: n_ctx_train      = 2048
0.00.047.810 I print_info: n_embd           = 2048
0.00.047.810 I print_info: n_layer          = 24
0.00.047.813 I print_info: n_head           = 16
0.00.047.814 I print_info: n_head_kv        = 16
0.00.047.815 I print_info: n_rot            = 32
0.00.047.815 I print_info: n_swa            = 0
0.00.047.815 I print_info: n_embd_head_k    = 128
0.00.047.815 I print_info: n_embd_head_v    = 128
0.00.047.816 I print_info: n_gqa            = 1
0.00.047.817 I print_info: n_embd_k_gqa     = 2048
0.00.047.818 I print_info: n_embd_v_gqa     = 2048
0.00.047.818 I print_info: f_norm_eps       = 1.0e-05
0.00.047.819 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.819 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.819 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.819 I print_info: f_logit_scale    = 0.0e+00
0.00.047.820 I print_info: n_ff             = 8192
0.00.047.820 I print_info: n_expert         = 0
0.00.047.820 I print_info: n_expert_used    = 0
0.00.047.821 I print_info: causal attn      = 1
0.00.047.821 I print_info: pooling type     = 0
0.00.047.821 I print_info: rope type        = 2
0.00.047.821 I print_info: rope scaling     = linear
0.00.047.821 I print_info: freq_base_train  = 10000.0
0.00.047.822 I print_info: freq_scale_train = 1
0.00.047.822 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.822 I print_info: rope_finetuned   = unknown
0.00.047.822 I print_info: ssm_d_conv       = 0
0.00.047.823 I print_info: ssm_d_inner      = 0
0.00.047.823 I print_info: ssm_d_state      = 0
0.00.047.823 I print_info: ssm_dt_rank      = 0
0.00.047.823 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.823 I print_info: model type       = 1.4B
0.00.047.824 I print_info: model params     = 1.41 B
0.00.047.824 I print_info: general.name     = 1.4B
0.00.047.824 I print_info: vocab type       = BPE
0.00.047.824 I print_info: n_vocab          = 50304
0.00.047.825 I print_info: n_merges         = 50009
0.00.047.825 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.825 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.825 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.825 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.826 I print_info: LF token         = 128 'Ä'
0.00.047.826 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.826 I print_info: max token length = 1024
0.00.698.758 I load_tensors: offloading 24 repeating layers to GPU
0.00.698.774 I load_tensors: offloading output layer to GPU
0.00.698.775 I load_tensors: offloaded 25/25 layers to GPU
0.00.698.811 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.698.813 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.700.388 I llama_init_from_model: n_seq_max     = 1
0.00.700.398 I llama_init_from_model: n_ctx         = 128
0.00.700.399 I llama_init_from_model: n_ctx_per_seq = 128
0.00.700.399 I llama_init_from_model: n_batch       = 128
0.00.700.400 I llama_init_from_model: n_ubatch      = 128
0.00.700.400 I llama_init_from_model: flash_attn    = 0
0.00.700.406 I llama_init_from_model: freq_base     = 10000.0
0.00.700.407 I llama_init_from_model: freq_scale    = 1
0.00.700.407 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.700.409 I ggml_metal_init: allocating
0.00.700.503 I ggml_metal_init: found device: Apple M4
0.00.700.516 I ggml_metal_init: picking default device: Apple M4
0.00.702.297 I ggml_metal_init: using embedded metal library
0.00.708.057 I ggml_metal_init: GPU name:   Apple M4
0.00.708.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.708.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.708.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.708.074 I ggml_metal_init: simdgroup reduction   = true
0.00.708.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.708.074 I ggml_metal_init: has residency sets    = true
0.00.708.075 I ggml_metal_init: has bfloat            = true
0.00.708.075 I ggml_metal_init: use bfloat            = true
0.00.708.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.708.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.727.907 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.490 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.731.497 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.731.530 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.874 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.734.876 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.734.876 I llama_init_from_model: graph nodes  = 967
0.00.734.877 I llama_init_from_model: graph splits = 2
0.00.734.880 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.734.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.224 I 
0.00.765.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.326 I perplexity: tokenizing the input ..
0.00.772.444 I perplexity: tokenization took 7.114 ms
0.00.772.468 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.918.370 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.919.702 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.919.715 I llama_perf_context_print:        load time =     755.69 ms
0.00.919.716 I llama_perf_context_print: prompt eval time =     144.95 ms /   128 tokens (    1.13 ms per token,   883.07 tokens per second)
0.00.919.717 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.919.717 I llama_perf_context_print:       total time =     154.50 ms /   129 tokens
0.00.920.106 I ggml_metal_free: deallocating

real	0m0.938s
user	0m0.083s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.199 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.029.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.585 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.586 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.586 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.587 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.590 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.591 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.555 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.714 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.617 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.040.619 I llama_model_loader: - type  f32:  194 tensors
0.00.040.620 I llama_model_loader: - type q4_K:   61 tensors
0.00.040.620 I llama_model_loader: - type q5_K:   24 tensors
0.00.040.620 I llama_model_loader: - type q6_K:   13 tensors
0.00.040.621 I print_info: file format = GGUF V3 (latest)
0.00.040.621 I print_info: file type   = Q4_K - Medium
0.00.040.622 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.051.680 I load: special tokens cache size = 25
0.00.060.788 I load: token to piece cache size = 0.2984 MB
0.00.060.792 I print_info: arch             = gptneox
0.00.060.793 I print_info: vocab_only       = 0
0.00.060.793 I print_info: n_ctx_train      = 2048
0.00.060.793 I print_info: n_embd           = 2048
0.00.060.793 I print_info: n_layer          = 24
0.00.060.796 I print_info: n_head           = 16
0.00.060.798 I print_info: n_head_kv        = 16
0.00.060.798 I print_info: n_rot            = 32
0.00.060.798 I print_info: n_swa            = 0
0.00.060.798 I print_info: n_embd_head_k    = 128
0.00.060.801 I print_info: n_embd_head_v    = 128
0.00.060.801 I print_info: n_gqa            = 1
0.00.060.803 I print_info: n_embd_k_gqa     = 2048
0.00.060.803 I print_info: n_embd_v_gqa     = 2048
0.00.060.804 I print_info: f_norm_eps       = 1.0e-05
0.00.060.804 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.805 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.805 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.805 I print_info: f_logit_scale    = 0.0e+00
0.00.060.806 I print_info: n_ff             = 8192
0.00.060.806 I print_info: n_expert         = 0
0.00.060.806 I print_info: n_expert_used    = 0
0.00.060.807 I print_info: causal attn      = 1
0.00.060.807 I print_info: pooling type     = 0
0.00.060.807 I print_info: rope type        = 2
0.00.060.807 I print_info: rope scaling     = linear
0.00.060.808 I print_info: freq_base_train  = 10000.0
0.00.060.815 I print_info: freq_scale_train = 1
0.00.060.815 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.815 I print_info: rope_finetuned   = unknown
0.00.060.816 I print_info: ssm_d_conv       = 0
0.00.060.816 I print_info: ssm_d_inner      = 0
0.00.060.816 I print_info: ssm_d_state      = 0
0.00.060.816 I print_info: ssm_dt_rank      = 0
0.00.060.816 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.818 I print_info: model type       = 1.4B
0.00.060.818 I print_info: model params     = 1.41 B
0.00.060.818 I print_info: general.name     = 1.4B
0.00.060.819 I print_info: vocab type       = BPE
0.00.060.819 I print_info: n_vocab          = 50304
0.00.060.820 I print_info: n_merges         = 50009
0.00.060.820 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.821 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.821 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.821 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.822 I print_info: LF token         = 128 'Ä'
0.00.060.822 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.822 I print_info: max token length = 1024
0.00.542.201 I load_tensors: offloading 24 repeating layers to GPU
0.00.542.215 I load_tensors: offloading output layer to GPU
0.00.542.216 I load_tensors: offloaded 25/25 layers to GPU
0.00.542.253 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.542.254 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.543.810 I llama_init_from_model: n_seq_max     = 1
0.00.543.815 I llama_init_from_model: n_ctx         = 128
0.00.543.816 I llama_init_from_model: n_ctx_per_seq = 128
0.00.543.816 I llama_init_from_model: n_batch       = 128
0.00.543.817 I llama_init_from_model: n_ubatch      = 128
0.00.543.817 I llama_init_from_model: flash_attn    = 0
0.00.543.819 I llama_init_from_model: freq_base     = 10000.0
0.00.543.820 I llama_init_from_model: freq_scale    = 1
0.00.543.820 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.543.827 I ggml_metal_init: allocating
0.00.543.886 I ggml_metal_init: found device: Apple M4
0.00.543.900 I ggml_metal_init: picking default device: Apple M4
0.00.545.660 I ggml_metal_init: using embedded metal library
0.00.552.412 I ggml_metal_init: GPU name:   Apple M4
0.00.552.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.552.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.552.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.552.419 I ggml_metal_init: simdgroup reduction   = true
0.00.552.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.552.419 I ggml_metal_init: has residency sets    = true
0.00.552.419 I ggml_metal_init: has bfloat            = true
0.00.552.420 I ggml_metal_init: use bfloat            = true
0.00.552.420 I ggml_metal_init: hasUnifiedMemory      = true
0.00.552.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.738 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.574.245 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.574.249 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.574.294 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.577.486 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.577.488 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.577.489 I llama_init_from_model: graph nodes  = 967
0.00.577.489 I llama_init_from_model: graph splits = 2
0.00.577.492 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.577.492 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.941 I 
0.00.609.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.050 I perplexity: tokenizing the input ..
0.00.616.404 I perplexity: tokenization took 7.35 ms
0.00.616.428 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.419 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.764.772 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.764.785 I llama_perf_context_print:        load time =     592.74 ms
0.00.764.786 I llama_perf_context_print: prompt eval time =     146.03 ms /   128 tokens (    1.14 ms per token,   876.54 tokens per second)
0.00.764.786 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.787 I llama_perf_context_print:       total time =     155.85 ms /   129 tokens
0.00.765.159 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.090s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.102 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.107 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.108 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.108 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.108 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.109 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.110 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.110 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.110 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.113 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.115 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.116 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.870 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.698 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.698 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.699 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.699 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.699 I llama_model_loader: - type  f32:  194 tensors
0.00.028.700 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.700 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.700 I print_info: file format = GGUF V3 (latest)
0.00.028.701 I print_info: file type   = Q5_K - Medium
0.00.028.703 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.036.871 I load: special tokens cache size = 25
0.00.042.869 I load: token to piece cache size = 0.2984 MB
0.00.042.872 I print_info: arch             = gptneox
0.00.042.872 I print_info: vocab_only       = 0
0.00.042.872 I print_info: n_ctx_train      = 2048
0.00.042.873 I print_info: n_embd           = 2048
0.00.042.873 I print_info: n_layer          = 24
0.00.042.876 I print_info: n_head           = 16
0.00.042.877 I print_info: n_head_kv        = 16
0.00.042.879 I print_info: n_rot            = 32
0.00.042.879 I print_info: n_swa            = 0
0.00.042.879 I print_info: n_embd_head_k    = 128
0.00.042.879 I print_info: n_embd_head_v    = 128
0.00.042.880 I print_info: n_gqa            = 1
0.00.042.881 I print_info: n_embd_k_gqa     = 2048
0.00.042.881 I print_info: n_embd_v_gqa     = 2048
0.00.042.882 I print_info: f_norm_eps       = 1.0e-05
0.00.042.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.883 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.883 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.887 I print_info: f_logit_scale    = 0.0e+00
0.00.042.887 I print_info: n_ff             = 8192
0.00.042.889 I print_info: n_expert         = 0
0.00.042.889 I print_info: n_expert_used    = 0
0.00.042.889 I print_info: causal attn      = 1
0.00.042.889 I print_info: pooling type     = 0
0.00.042.889 I print_info: rope type        = 2
0.00.042.890 I print_info: rope scaling     = linear
0.00.042.890 I print_info: freq_base_train  = 10000.0
0.00.042.890 I print_info: freq_scale_train = 1
0.00.042.890 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.890 I print_info: rope_finetuned   = unknown
0.00.042.891 I print_info: ssm_d_conv       = 0
0.00.042.891 I print_info: ssm_d_inner      = 0
0.00.042.891 I print_info: ssm_d_state      = 0
0.00.042.891 I print_info: ssm_dt_rank      = 0
0.00.042.891 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.894 I print_info: model type       = 1.4B
0.00.042.896 I print_info: model params     = 1.41 B
0.00.042.896 I print_info: general.name     = 1.4B
0.00.042.896 I print_info: vocab type       = BPE
0.00.042.897 I print_info: n_vocab          = 50304
0.00.042.897 I print_info: n_merges         = 50009
0.00.042.897 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.897 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.897 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.897 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.897 I print_info: LF token         = 128 'Ä'
0.00.042.900 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.900 I print_info: max token length = 1024
0.00.641.845 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.857 I load_tensors: offloading output layer to GPU
0.00.641.858 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.886 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.641.888 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.643.240 I llama_init_from_model: n_seq_max     = 1
0.00.643.246 I llama_init_from_model: n_ctx         = 128
0.00.643.247 I llama_init_from_model: n_ctx_per_seq = 128
0.00.643.247 I llama_init_from_model: n_batch       = 128
0.00.643.248 I llama_init_from_model: n_ubatch      = 128
0.00.643.248 I llama_init_from_model: flash_attn    = 0
0.00.643.250 I llama_init_from_model: freq_base     = 10000.0
0.00.643.251 I llama_init_from_model: freq_scale    = 1
0.00.643.251 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.643.253 I ggml_metal_init: allocating
0.00.643.315 I ggml_metal_init: found device: Apple M4
0.00.643.327 I ggml_metal_init: picking default device: Apple M4
0.00.645.017 I ggml_metal_init: using embedded metal library
0.00.650.816 I ggml_metal_init: GPU name:   Apple M4
0.00.650.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.825 I ggml_metal_init: simdgroup reduction   = true
0.00.650.825 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.826 I ggml_metal_init: has residency sets    = true
0.00.650.826 I ggml_metal_init: has bfloat            = true
0.00.650.826 I ggml_metal_init: use bfloat            = true
0.00.650.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.800 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.378 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.673.385 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.423 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.660 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.661 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.662 I llama_init_from_model: graph nodes  = 967
0.00.676.662 I llama_init_from_model: graph splits = 2
0.00.676.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.917 I 
0.00.709.006 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.027 I perplexity: tokenizing the input ..
0.00.715.840 I perplexity: tokenization took 6.81 ms
0.00.715.860 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.114 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.858.452 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.858.469 I llama_perf_context_print:        load time =     700.06 ms
0.00.858.470 I llama_perf_context_print: prompt eval time =     140.71 ms /   128 tokens (    1.10 ms per token,   909.69 tokens per second)
0.00.858.471 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.858.471 I llama_perf_context_print:       total time =     149.56 ms /   129 tokens
0.00.858.864 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.079s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.530 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.013 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.024 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.026 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.027 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.028 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.028 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.028 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.029 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.033 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.037 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.673 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.675 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.675 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.675 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.676 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.676 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.676 I llama_model_loader: - type  f32:  194 tensors
0.00.025.677 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.677 I print_info: file format = GGUF V3 (latest)
0.00.025.678 I print_info: file type   = Q6_K
0.00.025.678 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.835 I load: special tokens cache size = 25
0.00.039.857 I load: token to piece cache size = 0.2984 MB
0.00.039.860 I print_info: arch             = gptneox
0.00.039.861 I print_info: vocab_only       = 0
0.00.039.861 I print_info: n_ctx_train      = 2048
0.00.039.861 I print_info: n_embd           = 2048
0.00.039.861 I print_info: n_layer          = 24
0.00.039.864 I print_info: n_head           = 16
0.00.039.865 I print_info: n_head_kv        = 16
0.00.039.865 I print_info: n_rot            = 32
0.00.039.866 I print_info: n_swa            = 0
0.00.039.867 I print_info: n_embd_head_k    = 128
0.00.039.867 I print_info: n_embd_head_v    = 128
0.00.039.868 I print_info: n_gqa            = 1
0.00.039.868 I print_info: n_embd_k_gqa     = 2048
0.00.039.869 I print_info: n_embd_v_gqa     = 2048
0.00.039.870 I print_info: f_norm_eps       = 1.0e-05
0.00.039.870 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.870 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.870 I print_info: f_logit_scale    = 0.0e+00
0.00.039.871 I print_info: n_ff             = 8192
0.00.039.871 I print_info: n_expert         = 0
0.00.039.873 I print_info: n_expert_used    = 0
0.00.039.873 I print_info: causal attn      = 1
0.00.039.873 I print_info: pooling type     = 0
0.00.039.873 I print_info: rope type        = 2
0.00.039.873 I print_info: rope scaling     = linear
0.00.039.874 I print_info: freq_base_train  = 10000.0
0.00.039.874 I print_info: freq_scale_train = 1
0.00.039.874 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.875 I print_info: rope_finetuned   = unknown
0.00.039.875 I print_info: ssm_d_conv       = 0
0.00.039.875 I print_info: ssm_d_inner      = 0
0.00.039.875 I print_info: ssm_d_state      = 0
0.00.039.875 I print_info: ssm_dt_rank      = 0
0.00.039.875 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.875 I print_info: model type       = 1.4B
0.00.039.876 I print_info: model params     = 1.41 B
0.00.039.876 I print_info: general.name     = 1.4B
0.00.039.877 I print_info: vocab type       = BPE
0.00.039.877 I print_info: n_vocab          = 50304
0.00.039.877 I print_info: n_merges         = 50009
0.00.039.877 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.877 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.878 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.878 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.878 I print_info: LF token         = 128 'Ä'
0.00.039.879 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.879 I print_info: max token length = 1024
0.00.615.728 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.743 I load_tensors: offloading output layer to GPU
0.00.615.744 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.779 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.615.781 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.617.324 I llama_init_from_model: n_seq_max     = 1
0.00.617.327 I llama_init_from_model: n_ctx         = 128
0.00.617.328 I llama_init_from_model: n_ctx_per_seq = 128
0.00.617.328 I llama_init_from_model: n_batch       = 128
0.00.617.328 I llama_init_from_model: n_ubatch      = 128
0.00.617.329 I llama_init_from_model: flash_attn    = 0
0.00.617.330 I llama_init_from_model: freq_base     = 10000.0
0.00.617.331 I llama_init_from_model: freq_scale    = 1
0.00.617.332 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.617.333 I ggml_metal_init: allocating
0.00.617.357 I ggml_metal_init: found device: Apple M4
0.00.617.367 I ggml_metal_init: picking default device: Apple M4
0.00.618.732 I ggml_metal_init: using embedded metal library
0.00.625.006 I ggml_metal_init: GPU name:   Apple M4
0.00.625.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.012 I ggml_metal_init: simdgroup reduction   = true
0.00.625.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.013 I ggml_metal_init: has residency sets    = true
0.00.625.013 I ggml_metal_init: has bfloat            = true
0.00.625.013 I ggml_metal_init: use bfloat            = true
0.00.625.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.018 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.634 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.644.987 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.644.996 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.645.023 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.648.145 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.648.147 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.648.148 I llama_init_from_model: graph nodes  = 967
0.00.648.148 I llama_init_from_model: graph splits = 2
0.00.648.151 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.648.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.534 I 
0.00.680.608 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.628 I perplexity: tokenizing the input ..
0.00.687.651 I perplexity: tokenization took 7.022 ms
0.00.687.666 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.455 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.828.788 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.828.801 I llama_perf_context_print:        load time =     670.00 ms
0.00.828.802 I llama_perf_context_print: prompt eval time =     139.50 ms /   128 tokens (    1.09 ms per token,   917.54 tokens per second)
0.00.828.802 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.803 I llama_perf_context_print:       total time =     148.27 ms /   129 tokens
0.00.829.181 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.077s
sys	0m0.147s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.237 I build: 4585 (f0d4b29e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.603 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.028 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.047 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.050 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.050 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.051 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.113 I llama_model_loader: - type  f32:  194 tensors
0.00.057.113 I llama_model_loader: - type  f16:   98 tensors
0.00.057.121 I print_info: file format = GGUF V3 (latest)
0.00.057.122 I print_info: file type   = all F32 (guessed)
0.00.057.124 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.071.308 I load: special tokens cache size = 25
0.00.079.556 I load: token to piece cache size = 0.2984 MB
0.00.079.559 I print_info: arch             = gptneox
0.00.079.560 I print_info: vocab_only       = 0
0.00.079.560 I print_info: n_ctx_train      = 2048
0.00.079.560 I print_info: n_embd           = 2048
0.00.079.560 I print_info: n_layer          = 24
0.00.079.564 I print_info: n_head           = 16
0.00.079.565 I print_info: n_head_kv        = 16
0.00.079.565 I print_info: n_rot            = 32
0.00.079.565 I print_info: n_swa            = 0
0.00.079.565 I print_info: n_embd_head_k    = 128
0.00.079.566 I print_info: n_embd_head_v    = 128
0.00.079.567 I print_info: n_gqa            = 1
0.00.079.567 I print_info: n_embd_k_gqa     = 2048
0.00.079.568 I print_info: n_embd_v_gqa     = 2048
0.00.079.569 I print_info: f_norm_eps       = 1.0e-05
0.00.079.569 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.570 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.570 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.572 I print_info: f_logit_scale    = 0.0e+00
0.00.079.573 I print_info: n_ff             = 8192
0.00.079.573 I print_info: n_expert         = 0
0.00.079.573 I print_info: n_expert_used    = 0
0.00.079.574 I print_info: causal attn      = 1
0.00.079.574 I print_info: pooling type     = 0
0.00.079.574 I print_info: rope type        = 2
0.00.079.574 I print_info: rope scaling     = linear
0.00.079.575 I print_info: freq_base_train  = 10000.0
0.00.079.575 I print_info: freq_scale_train = 1
0.00.079.575 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.575 I print_info: rope_finetuned   = unknown
0.00.079.576 I print_info: ssm_d_conv       = 0
0.00.079.576 I print_info: ssm_d_inner      = 0
0.00.079.576 I print_info: ssm_d_state      = 0
0.00.079.576 I print_info: ssm_dt_rank      = 0
0.00.079.576 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.576 I print_info: model type       = 1.4B
0.00.079.577 I print_info: model params     = 1.41 B
0.00.079.577 I print_info: general.name     = 1.4B
0.00.079.577 I print_info: vocab type       = BPE
0.00.079.578 I print_info: n_vocab          = 50304
0.00.079.578 I print_info: n_merges         = 50009
0.00.079.582 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.582 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.582 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.582 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.583 I print_info: LF token         = 128 'Ä'
0.00.079.583 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.583 I print_info: max token length = 1024
0.01.360.315 I load_tensors: offloading 24 repeating layers to GPU
0.01.360.319 I load_tensors: offloading output layer to GPU
0.01.360.319 I load_tensors: offloaded 25/25 layers to GPU
0.01.360.345 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.360.346 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.361.542 I llama_init_from_model: n_seq_max     = 1
0.01.361.544 I llama_init_from_model: n_ctx         = 128
0.01.361.544 I llama_init_from_model: n_ctx_per_seq = 128
0.01.361.544 I llama_init_from_model: n_batch       = 128
0.01.361.545 I llama_init_from_model: n_ubatch      = 128
0.01.361.547 I llama_init_from_model: flash_attn    = 0
0.01.361.548 I llama_init_from_model: freq_base     = 10000.0
0.01.361.548 I llama_init_from_model: freq_scale    = 1
0.01.361.549 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.361.549 I ggml_metal_init: allocating
0.01.361.629 I ggml_metal_init: found device: Apple M4
0.01.361.636 I ggml_metal_init: picking default device: Apple M4
0.01.362.810 I ggml_metal_init: using embedded metal library
0.01.366.779 I ggml_metal_init: GPU name:   Apple M4
0.01.366.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.366.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.366.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.366.783 I ggml_metal_init: simdgroup reduction   = true
0.01.366.783 I ggml_metal_init: simdgroup matrix mul. = true
0.01.366.783 I ggml_metal_init: has residency sets    = true
0.01.366.783 I ggml_metal_init: has bfloat            = true
0.01.366.783 I ggml_metal_init: use bfloat            = true
0.01.366.784 I ggml_metal_init: hasUnifiedMemory      = true
0.01.366.785 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.377.459 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.379.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.379.191 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.379.207 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.380.764 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.380.766 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.380.766 I llama_init_from_model: graph nodes  = 967
0.01.380.766 I llama_init_from_model: graph splits = 2
0.01.380.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.380.768 I 
0.01.380.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.380.808 I compute_imatrix: tokenizing the input ..
0.01.384.934 I compute_imatrix: tokenization took 4.125 ms
0.01.384.936 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.650.267 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.652.604 I llama_perf_context_print:        load time =    1626.66 ms
0.01.652.605 I llama_perf_context_print: prompt eval time =     263.58 ms /   128 tokens (    2.06 ms per token,   485.62 tokens per second)
0.01.652.606 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.652.606 I llama_perf_context_print:       total time =    1628.99 ms /   129 tokens
0.01.653.098 I ggml_metal_free: deallocating

real	0m1.905s
user	0m0.128s
sys	0m0.263s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4585 (f0d4b29e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119607ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1196085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119608ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119609150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119609700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119609cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11960a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11960a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11960adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11960b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11960b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11960bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11960c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11960cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11960d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11960dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11960e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11960ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11960f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11960fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x119610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x119610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x119611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1196119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x119612110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1196123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1196129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119613650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119613b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119613e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1196142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1196145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119614e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119615380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119615640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119615ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119615f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119616420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1196168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119616d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119617200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1196176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119617b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1196182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1196188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119618ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1196197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11961a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11961aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11961b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11961b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11961bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11961c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11961c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11961cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11961d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11961d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11961de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11961e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11961e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11961ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11961eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11961f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11961f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11961fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119620150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1196205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x119620f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1196213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x119621870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x119621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x119622310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119622860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119622db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x119623300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119623850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119623da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1196242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119624d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1196252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119625830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119625d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1196262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119626820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119626d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1196272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119627810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119627d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1196282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119628800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1196292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1196297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1196194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11962a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11962a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11962aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11962b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11962b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11962bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11962c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11962c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11962ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11962d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11962d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11962de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11962e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11962e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11962edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11962f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11962f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11962fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119630040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1196304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119630e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1196312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x119631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x119631c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1196320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x119632540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1196329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x119632e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1196337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x119633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x119634100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1196345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119635380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119635820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119636160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x119636600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119636f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1196373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119637880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119637d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1196381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119638660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119638b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119639440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1196398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119639d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11963a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11963a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11963ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11963b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11963b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11963b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11963bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11963c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11963c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11963cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11963d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11963d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11963d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11963de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11963e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11963e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11963ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11963f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11963f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11963fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11963fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x119640340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1196407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119640c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119641120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1196415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119641a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x119641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1196423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119642840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119642ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119643180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x119643620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119643f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x119644400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1196448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x119644d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1196451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119645680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119646070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1196465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119646b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119647060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119647320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119647930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x119648550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119648d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1196491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1196494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119649ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11964a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11964a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11964ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11964b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11964b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11964be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11964c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11964c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11964ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11964d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11964d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11964de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11964e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11964e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11964ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11964f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11964f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11964fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119650350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1196508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119651890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119652330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119652880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119652dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119653870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119654860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x119655300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119655da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1196562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119656840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119656d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1196572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119657830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119657d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1196582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119658d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1196592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119659810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119659d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11965a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11965a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11965ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11965b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11965b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11965bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11965c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11965c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11965cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11965d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11965d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11965dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11965e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11965e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11965ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11965f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11965f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11965fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11965fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119660380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119660820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119660cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x119661160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119661600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x119661aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119661f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1196623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119662880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119662d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119663270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119663990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1196640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1196647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119664ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1196651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1196659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119665c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119666270 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.709.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c606bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c607040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c6074b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c607920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c607d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c608200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c608670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c608ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c608f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c6093c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c609830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c609ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c60aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c60b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c60b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c60c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c60c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c60cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c60d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c60de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c60e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c60ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c60f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c60faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c6101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c610480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c610740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c610bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c611020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c611990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c611ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c612310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c6125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c612a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c612eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c613410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c613910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c613e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c614310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c614810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c615210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c615710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c615c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c616080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c6164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c616dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c617240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c6176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c617b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c617f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c618400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c619040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c6194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c6197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c619db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c61a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c61aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c61aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c61b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c61b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c61bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c61c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c61c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c61caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c61cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c61d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c61d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c61dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c61e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c61e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c61ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c61f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c61f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c61fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c6201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c6206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c620c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c621190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c6216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c621c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c622180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c6226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c622c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c623170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c6236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c623c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c624160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c6246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c624c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c625150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c6256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c625bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c626140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c626690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c626be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c627130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c627bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c628120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c628670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c628bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c629110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c629660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c629bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c62a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c62a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c62aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c62b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c62b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c62bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c62bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c62c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c62c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c62cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c62d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c62d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c62db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c62dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c62e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c62e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c62edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c62f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c62f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c62fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c630040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c6304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c630e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c6312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c631c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c6320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c632540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c6329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c632e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c6337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c634100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c6345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c635380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c635820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c636160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c636600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c636f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c6373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c637880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c637d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c6381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c638660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c638b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c639440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c6398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c639d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c63a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c63a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c63ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c63b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c63b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c63b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c63bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c63c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c63c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c63cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c63d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c63d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c63d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c63de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c63e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c63e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c63ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c63f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c63f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c63fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c63fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c640340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c6407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c640c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c641120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c6415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c641a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c6423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c642840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c642d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c6432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c643830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c643d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c644040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c644650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c644c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c645270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c645a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c645f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c6461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c6467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c646de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c6475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c647a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c647f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c6483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c648b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c6490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c649600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c649b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c64a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c64a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c64ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c64b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c64b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c64bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c64c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c64c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c64cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c64d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c64d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c64db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c64e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c64e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c64eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c64f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c64f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c64faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c650040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c650590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c650ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c651030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c651580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c651ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c652020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c652570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c652ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c653010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c653560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c653ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c654000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c654550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c654aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c654ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c655540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c655a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c655fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c656530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c656a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c656fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c657520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c657a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c657fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c658510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c658a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c658fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c659500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c659a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c659fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c65a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c65aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c65af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c65b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c65b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c65be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c65c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c65c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c65cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c65d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c65d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c65d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c65de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c65e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c65e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c65ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c65f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c65f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c65fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c65ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c6606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c660dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c6614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c661c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c661ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c6626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c662980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c662f90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c7044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c7056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c7063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c7078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c7083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c70a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c70a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c70b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c70b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c70bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c70c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c70cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c70d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c70db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c70de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c70e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c70e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c70e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c70ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c70f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c70f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c70fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c70ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c7107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c7110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c7119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c7138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c7141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c7157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c7160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c7185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c71a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c71a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c71a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c71adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c71b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c71b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c71bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c71bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c71c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c71ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c71d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c71d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c71da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c71de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c71e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c71ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c71f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c71f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c71f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c71fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c7213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c7229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c7232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c723e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c7258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c7261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c726ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c727360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c7277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c727c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c7280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c728520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c7296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c72a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c72a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c72ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c72b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c72b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c72ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c72bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c72c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c72c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c72cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c72d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c72d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c72d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c72dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c72e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c72e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c72eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c72efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c72f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c72f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c72fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c7305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c730a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c7324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c732950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c7336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c7343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c734860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c7355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c7374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c738210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c738f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c7393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c73a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c73a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c73aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c73ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c73b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c73b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c73bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c73c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c73c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c73c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c73cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c73d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c73d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c73dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c73df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c73e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c73e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c73ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c73f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c73f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c73f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c73fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c7402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c741b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c742110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c7429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c742e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c7432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c743740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c744020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c7451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c745650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c7463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c746810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c7470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c7479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c747e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c7482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c749470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c7498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c749d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c74a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c74a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c74aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c74af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c74b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c74b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c74bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c74c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c74c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c74c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c74ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c74d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c74d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c74db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c74dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c74e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c74e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c74ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c74f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c74f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c74fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c74fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c750360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c7507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c750c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c7510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c751990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c751e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c7526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c752b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c752fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c7538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c753d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c7545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c754a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c754ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c7557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c756940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c757060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c757780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c757eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c7584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c758ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.784s
user	0m0.280s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4585 (f0d4b29e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1449077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144907ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144908470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144908a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144908fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144909580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144909b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14490a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14490a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14490ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14490b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14490b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14490c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14490c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14490d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14490d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14490deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14490e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14490ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14490f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14490fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144910300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144910a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1449112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1449119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144911ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1449122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144912f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144913460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144913720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144913bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144913e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144914710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144914c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144914f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1449153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144915850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144915cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144916190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144916630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144916ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144916f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144917410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1449178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144917b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144918180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144918790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1449190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1449196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144919cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14491a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14491a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14491af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14491b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14491bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14491c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14491c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14491c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14491cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14491d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14491d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14491de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14491e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14491e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14491ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14491f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14491f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14491fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14491fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144920360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144920800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144920ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144921140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144921690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144921be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144922130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144922680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144922bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144923120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144923670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144923bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144924110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144924660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144924bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144925100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144925650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144925ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1449260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144926640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144926b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1449270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144927630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144927b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1449280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144928620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144928b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1449290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144918da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144929530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144929ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14492a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14492a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14492acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14492b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14492b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14492bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14492c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14492c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14492ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14492d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14492d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14492dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14492e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14492e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14492eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14492efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14492f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14492f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14492fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144930250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1449306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144930b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144931030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1449314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144931970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144931e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1449322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144932750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144932bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144933090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144933530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1449339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144933e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144934310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1449347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144934c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1449350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144935590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144935a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144935ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144936370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144936810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144936cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144937150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1449375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144937a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144937f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1449383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144938870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144938d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1449391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144939650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144939af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144939f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14493a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14493a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14493ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14493b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14493b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14493bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14493bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14493c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14493c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14493cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14493d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14493d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14493dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14493e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14493e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14493e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14493ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14493f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14493f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14493fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1449400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144940550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1449409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144940e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144941330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1449417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144941c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144942110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1449425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144942a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144942ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144943390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144943830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144943cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144944170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144944610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144944ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144944f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1449453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144945940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144945e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1449463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144946930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144946bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144947200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144947810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144947e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144948610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144948ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144948d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144949380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144949990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14494a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14494a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14494aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14494af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14494b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14494bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14494c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14494c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14494cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14494d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14494d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14494dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14494e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14494e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14494ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14494f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14494f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14494fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144950170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1449506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144950c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144951160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1449516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144951c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144952150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1449526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144952bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144953140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144953690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144953be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144954130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144954680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144954bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144955120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144955670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144955bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144956110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144956660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144956bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144957100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144957650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144957ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1449580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144958640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144958b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1449590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144959630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144959b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14495a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14495a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14495ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14495b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14495b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14495bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14495c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14495c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14495cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14495d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14495d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14495db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14495e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14495e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14495e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14495ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14495f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14495f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14495fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1449600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144960590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144960a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144960ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144961370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144961810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144961cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144962150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1449625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144962b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144963260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144963980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1449640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1449647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144964a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144965270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144965530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144965b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.879 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.887 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142e075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142e078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142e07d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142e08180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142e085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142e08a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142e08ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142e09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142e097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142e09c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142e0a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142e0a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142e0b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142e0ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142e0c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142e0c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142e0d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142e0d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142e0df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142e0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142e0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142e0f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142e0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142e102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142e109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142e10ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142e10f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142e113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142e11840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142e11cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142e12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142e12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142e12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142e12d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142e131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142e13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142e13ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142e13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142e143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142e14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142e14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142e15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142e15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142e159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142e15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142e162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142e16730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142e16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142e17010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142e17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142e178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142e17d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142e181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142e18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142e18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142e18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142e19490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142e19990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142e19e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142e1a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142e1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142e1ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142e1afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142e1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142e1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142e1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142e1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142e1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142e1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142e1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142e1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142e1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142e1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142e1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142e1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142e1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142e1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142e1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142e1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142e1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142e1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142e20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142e20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142e20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142e21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142e215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142e21a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142e21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142e22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142e22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142e22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142e23070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142e234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142e23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142e23dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142e24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142e246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142e24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142e24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142e253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142e25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142e25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142e26140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142e265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142e26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142e26e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142e27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142e27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142e27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142e28050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142e284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142e28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142e28da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142e29210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142e29680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142e29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142e29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142e2a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142e2a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142e2acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142e2b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142e2b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142e2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142e2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142e2c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142e2c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142e2cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142e2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142e2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142e2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142e2dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142e2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142e2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142e2ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142e2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142e2f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142e2f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142e2fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142e30100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142e30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142e309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142e30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142e312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142e31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142e31ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142e32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142e32480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142e328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142e32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142e331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142e33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142e33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142e33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142e34390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142e34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142e34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142e350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142e35550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142e359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142e35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142e362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142e36710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142e36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142e36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142e37460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142e378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142e38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104604080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1046044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104604960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104604dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104605240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1046056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104605b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104605f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104606400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104606870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104606ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104607150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1046075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104607a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104607ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104608310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104608780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104608bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104609060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1046094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104609940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104609db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10460a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10460a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10460ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10460af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10460b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10460b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10460bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10460c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10460c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10460ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10460ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10460d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10460d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10460dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10460e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10460e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10460e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10460ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10460f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10460f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10460fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1046106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104610970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104610c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1046110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104611510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104611980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104611df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104612260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1046126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104612b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104612fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104613420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104613d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104614170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1046145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104614a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104614ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104615330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1046157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104615c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104616080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1046164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104616dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104617240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1046176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104617b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104617f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104618400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104618ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104619150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1046195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104619a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104619ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10461a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10461a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10461abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10461b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10461b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10461b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10461bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10461c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10461c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10461cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10461cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10461d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10461d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10461dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10461e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10461e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10461ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10461ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10461f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10461f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10461fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104620040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1046204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104620920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104620d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104621670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104621ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104621f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1046223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104622830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104622ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104623110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104623580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1046239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104623e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1046242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104624d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104625460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104625b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1046262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104626560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1046269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104626fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1046275e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104627cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104627f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104628680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104628940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104628c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104628ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104629180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104629440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104629700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1046299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104629c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104629f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10462a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10462aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10462b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10462b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10462b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10462be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10462c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10462cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10462d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10462d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10462db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10462e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10462e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10462eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10462eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10462f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10462f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10462f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10462f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10462fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10462fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1046300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1046303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104630660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104630920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104630ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104631160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1046316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1046319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104631c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104631f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1046321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1046324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104632760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104632a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104632ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104632fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104633260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104633520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1046337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104633aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104633d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104634020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1046342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1046345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104634860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104634b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104634de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1046350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104635360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104635620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1046358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104635ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104635e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104636120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1046363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1046366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x104636c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104636ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1046371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x104637460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104637720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1046379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x104637ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104637f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104638220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1046384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1046387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104638a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104638d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104638fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1046392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104639560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104639820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104639ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104639da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10463a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10463a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10463a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10463a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10463ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10463ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10463b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10463b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10463b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10463b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10463bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10463bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10463c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10463c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10463c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10463c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10463cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10463cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10463d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10463d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10463d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10463da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10463dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10463dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10463e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10463e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10463e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10463ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10463eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10463f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10463f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10463fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10463feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104640320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104640790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104640c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1046414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104641950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104642230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1046426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104642f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1046433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104643860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104643cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104644140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1046445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104644a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104644e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104645300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104645770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104645be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104646050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1046464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104646930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104646da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104647210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104647680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104647af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104647f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1046483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104648840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104648cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104649120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104649590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104649a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104649e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10464a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10464a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10464abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10464b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10464b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10464b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10464bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10464c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10464c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10464cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10464cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10464d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10464d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10464dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10464e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10464e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10464e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10464ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10464f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10464f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10464fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104650010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104650480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1046508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104650d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1046511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104651640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104651ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104651f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104652800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1046530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104653550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1046539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104653e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1046542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104654710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104654ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104655460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1046558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104655e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104656320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104656790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104656c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104657070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1046574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104657a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104657f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104658a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104658d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104659300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1046598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10465a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10465aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10465afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10465b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10465bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10465c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10465c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10465cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10465d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10465d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10465ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10465e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10465e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10465ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10465f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10465fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104660040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104660600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104660bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104661180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104661740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104661d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1046622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104662880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104662e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104663400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1046639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104663f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104664540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104664b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1046650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104665680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104665c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104666200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1046667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104666d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x104667900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104667ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104668480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104668a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104669000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1046695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104669b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10466a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10466a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10466acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10466b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10466b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10466be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10466c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10466c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10466cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10466d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10466d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10466de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10466e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10466e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10466ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10466f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10466f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10466fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104670140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104670640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104670b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104671040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104671540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104671a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104672450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104672b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104673290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1046739b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104673c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104674460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104674720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104674d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.975s
user	0m0.240s
sys	0m0.195s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
