Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:42 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:104 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.615s
user	0m0.701s
sys	0m0.960s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Built target sha256
[  6%] Built target xxhash
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target build_info
[  6%] Built target sha1
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 11%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX shared library libllama.dylib
[ 21%] Built target llama-gguf
[ 21%] Built target llama-gguf-hash
[ 21%] Built target llama
[ 21%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 21%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 21%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 21%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 22%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Linking C executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 27%] Linking CXX executable ../../bin/llama-run
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Built target llava
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llama-run
[ 31%] Built target llama-simple
[ 31%] Built target test-c
[ 31%] Built target llama-quantize-stats
[ 31%] Linking CXX static library libllava_static.a
[ 31%] Built target llama-simple-chat
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target common
[ 32%] Built target llava_static
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 34%] Built target llava_shared
[ 35%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-sampling
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Linking CXX executable ../bin/test-log
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-chat-template
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-grammar-parser
[ 47%] Built target test-log
[ 47%] Built target test-arg-parser
[ 47%] Built target test-chat-template
[ 48%] Built target test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 51%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Built target test-llama-grammar
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Linking CXX executable ../../bin/llama-batched
[ 62%] Built target test-barrier
[ 62%] Built target test-rope
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-quantize-perf
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-json-schema-to-grammar
[ 62%] Built target llama-batched-bench
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target llama-batched
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Built target llama-infill
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-embedding
[ 71%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-lookup
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-bench
[ 72%] Built target llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Generating completion.js.hpp
[ 83%] Built target llama-passkey
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-cli
[ 83%] Built target llama-perplexity
[ 84%] Generating deps_daisyui.min.css.hpp
[ 84%] Generating deps_markdown-it.js.hpp
[ 84%] Built target llama-quantize
[ 85%] Generating deps_tailwindcss.js.hpp
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Built target llama-retrieval
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-speculative-simple
[ 94%] Generating deps_vue.esm-browser.js.hpp
[ 94%] Built target llama-speculative
[ 94%] Built target llama-save-load-state
[ 94%] Built target llama-tokenize
[ 94%] Generating index.html.hpp
[ 94%] Built target llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-export-lora
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.691s
user	0m5.668s
sys	0m8.161s

main: quantize time =  2997.61 ms
main:    total time =  2997.61 ms

main: quantize time =  1266.00 ms
main:    total time =  1266.00 ms

main: quantize time =  1333.70 ms
main:    total time =  1333.70 ms

main: quantize time =  1494.09 ms
main:    total time =  1494.09 ms

main: quantize time =  4078.30 ms
main:    total time =  4078.30 ms

main: quantize time =  4993.40 ms
main:    total time =  4993.40 ms

main: quantize time =  5705.06 ms
main:    total time =  5705.06 ms

main: quantize time =  6881.77 ms
main:    total time =  6881.77 ms

main: quantize time =  5876.80 ms
main:    total time =  5876.80 ms

main: quantize time =  4561.25 ms
main:    total time =  4561.25 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.207 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.398 I main: llama backend init
0.00.000.435 I main: load the model and apply lora adapter, if any
0.00.032.345 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.728 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.756 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.757 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.758 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.762 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.763 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.763 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.543 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.543 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.544 I llama_model_loader: - type  f32:  194 tensors
0.00.062.545 I llama_model_loader: - type  f16:   98 tensors
0.00.090.774 I llm_load_vocab: special tokens cache size = 25
0.00.097.478 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.097.481 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.097.481 I llm_load_print_meta: arch             = gptneox
0.00.097.481 I llm_load_print_meta: vocab type       = BPE
0.00.097.481 I llm_load_print_meta: n_vocab          = 50304
0.00.097.482 I llm_load_print_meta: n_merges         = 50009
0.00.097.482 I llm_load_print_meta: vocab_only       = 0
0.00.097.482 I llm_load_print_meta: n_ctx_train      = 2048
0.00.097.482 I llm_load_print_meta: n_embd           = 2048
0.00.097.482 I llm_load_print_meta: n_layer          = 24
0.00.097.485 I llm_load_print_meta: n_head           = 16
0.00.097.486 I llm_load_print_meta: n_head_kv        = 16
0.00.097.486 I llm_load_print_meta: n_rot            = 32
0.00.097.486 I llm_load_print_meta: n_swa            = 0
0.00.097.486 I llm_load_print_meta: n_embd_head_k    = 128
0.00.097.488 I llm_load_print_meta: n_embd_head_v    = 128
0.00.097.489 I llm_load_print_meta: n_gqa            = 1
0.00.097.489 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.097.491 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.097.491 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.097.492 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.097.492 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.097.492 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.097.492 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.097.493 I llm_load_print_meta: n_ff             = 8192
0.00.097.493 I llm_load_print_meta: n_expert         = 0
0.00.097.493 I llm_load_print_meta: n_expert_used    = 0
0.00.097.493 I llm_load_print_meta: causal attn      = 1
0.00.097.493 I llm_load_print_meta: pooling type     = 0
0.00.097.494 I llm_load_print_meta: rope type        = 2
0.00.097.494 I llm_load_print_meta: rope scaling     = linear
0.00.097.495 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.097.495 I llm_load_print_meta: freq_scale_train = 1
0.00.097.496 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.097.496 I llm_load_print_meta: rope_finetuned   = unknown
0.00.097.496 I llm_load_print_meta: ssm_d_conv       = 0
0.00.097.496 I llm_load_print_meta: ssm_d_inner      = 0
0.00.097.496 I llm_load_print_meta: ssm_d_state      = 0
0.00.097.496 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.097.496 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.508 I llm_load_print_meta: model type       = 1.4B
0.00.097.508 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.097.509 I llm_load_print_meta: model params     = 1.41 B
0.00.097.509 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.097.509 I llm_load_print_meta: general.name     = 1.4B
0.00.097.510 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.510 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.510 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.510 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.510 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.097.511 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.511 I llm_load_print_meta: max token length = 1024
0.00.099.944 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.099.944 I llm_load_tensors: offloading output layer to GPU
0.00.099.944 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.099.961 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.099.962 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.100.878 I llama_new_context_with_model: n_seq_max     = 1
0.00.100.879 I llama_new_context_with_model: n_ctx         = 2048
0.00.100.879 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.100.879 I llama_new_context_with_model: n_batch       = 2048
0.00.100.879 I llama_new_context_with_model: n_ubatch      = 512
0.00.100.879 I llama_new_context_with_model: flash_attn    = 0
0.00.100.880 I llama_new_context_with_model: freq_base     = 10000.0
0.00.100.880 I llama_new_context_with_model: freq_scale    = 1
0.00.100.881 I ggml_metal_init: allocating
0.00.100.887 I ggml_metal_init: found device: Apple M4
0.00.100.890 I ggml_metal_init: picking default device: Apple M4
0.00.101.461 I ggml_metal_init: using embedded metal library
0.00.110.378 I ggml_metal_init: GPU name:   Apple M4
0.00.110.380 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.110.380 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.110.380 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.110.381 I ggml_metal_init: simdgroup reduction   = true
0.00.110.381 I ggml_metal_init: simdgroup matrix mul. = true
0.00.110.381 I ggml_metal_init: has bfloat            = true
0.00.110.381 I ggml_metal_init: use bfloat            = true
0.00.110.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.110.382 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.144.903 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.144.909 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.144.929 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.145.864 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.145.865 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.145.866 I llama_new_context_with_model: graph nodes  = 967
0.00.145.866 I llama_new_context_with_model: graph splits = 2
0.00.145.904 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.220.869 I main: llama threadpool init, n_threads = 4
0.00.220.902 I 
0.00.220.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.220.935 I 
0.00.221.004 I sampler seed: 1234
0.00.221.009 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.221.033 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.221.034 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.221.035 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.074.012 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.02.074.012 I llama_perf_context_print:        load time =     188.51 ms
0.02.074.013 I llama_perf_context_print: prompt eval time =      37.53 ms /     7 tokens (    5.36 ms per token,   186.53 tokens per second)
0.02.074.014 I llama_perf_context_print:        eval time =    1812.45 ms /    63 runs   (   28.77 ms per token,    34.76 tokens per second)
0.02.074.014 I llama_perf_context_print:       total time =    1853.14 ms /    70 tokens
0.02.074.194 I ggml_metal_free: deallocating

real	0m2.363s
user	0m0.141s
sys	0m0.096s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.663 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.165 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.179 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.180 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.180 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.182 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.182 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.182 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.182 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.183 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.183 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.184 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.186 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.186 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.186 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.137 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.620 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.622 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.622 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.622 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.623 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.623 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.624 I llama_model_loader: - type  f32:  194 tensors
0.00.037.624 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.289 I llm_load_vocab: special tokens cache size = 25
0.00.068.922 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.926 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.926 I llm_load_print_meta: arch             = gptneox
0.00.068.927 I llm_load_print_meta: vocab type       = BPE
0.00.068.927 I llm_load_print_meta: n_vocab          = 50304
0.00.068.928 I llm_load_print_meta: n_merges         = 50009
0.00.068.930 I llm_load_print_meta: vocab_only       = 0
0.00.068.930 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.930 I llm_load_print_meta: n_embd           = 2048
0.00.068.931 I llm_load_print_meta: n_layer          = 24
0.00.068.935 I llm_load_print_meta: n_head           = 16
0.00.068.935 I llm_load_print_meta: n_head_kv        = 16
0.00.068.936 I llm_load_print_meta: n_rot            = 32
0.00.068.936 I llm_load_print_meta: n_swa            = 0
0.00.068.936 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.938 I llm_load_print_meta: n_gqa            = 1
0.00.068.939 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.939 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.941 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.941 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.941 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.945 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.946 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.948 I llm_load_print_meta: n_ff             = 8192
0.00.068.948 I llm_load_print_meta: n_expert         = 0
0.00.068.948 I llm_load_print_meta: n_expert_used    = 0
0.00.068.948 I llm_load_print_meta: causal attn      = 1
0.00.068.948 I llm_load_print_meta: pooling type     = 0
0.00.068.948 I llm_load_print_meta: rope type        = 2
0.00.068.949 I llm_load_print_meta: rope scaling     = linear
0.00.068.949 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.950 I llm_load_print_meta: freq_scale_train = 1
0.00.068.950 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.950 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.950 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.950 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.950 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.950 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.950 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.964 I llm_load_print_meta: model type       = 1.4B
0.00.068.965 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.965 I llm_load_print_meta: model params     = 1.41 B
0.00.068.965 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.965 I llm_load_print_meta: general.name     = 1.4B
0.00.068.966 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.966 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.966 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.966 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.967 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.967 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.967 I llm_load_print_meta: max token length = 1024
0.00.071.507 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.507 I llm_load_tensors: offloading output layer to GPU
0.00.071.507 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.517 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.519 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.639 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.640 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.640 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.641 I llama_new_context_with_model: n_batch       = 2048
0.00.072.641 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.641 I llama_new_context_with_model: flash_attn    = 0
0.00.072.642 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.642 I llama_new_context_with_model: freq_scale    = 1
0.00.072.642 I ggml_metal_init: allocating
0.00.072.646 I ggml_metal_init: found device: Apple M4
0.00.072.648 I ggml_metal_init: picking default device: Apple M4
0.00.073.396 I ggml_metal_init: using embedded metal library
0.00.075.694 I ggml_metal_init: GPU name:   Apple M4
0.00.075.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.696 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.696 I ggml_metal_init: simdgroup reduction   = true
0.00.075.697 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.697 I ggml_metal_init: has bfloat            = true
0.00.075.697 I ggml_metal_init: use bfloat            = true
0.00.075.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.601 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.611 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.633 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.688 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.690 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.690 I llama_new_context_with_model: graph nodes  = 967
0.00.111.690 I llama_new_context_with_model: graph splits = 2
0.00.111.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.490.893 I main: llama threadpool init, n_threads = 4
0.01.490.955 I 
0.01.491.014 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.491.017 I 
0.01.491.320 I sampler seed: 1234
0.01.491.328 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.491.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.491.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.491.348 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.591.303 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47396.53 tokens per second)
0.02.591.304 I llama_perf_context_print:        load time =    1481.22 ms
0.02.591.304 I llama_perf_context_print: prompt eval time =      42.53 ms /     7 tokens (    6.08 ms per token,   164.59 tokens per second)
0.02.591.306 I llama_perf_context_print:        eval time =    1054.11 ms /    63 runs   (   16.73 ms per token,    59.77 tokens per second)
0.02.591.306 I llama_perf_context_print:       total time =    1100.42 ms /    70 tokens
0.02.591.489 I ggml_metal_free: deallocating

real	0m2.611s
user	0m0.133s
sys	0m0.225s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.879 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.167 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.170 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.170 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.224 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.111 I llama_model_loader: - type  f32:  194 tensors
0.00.026.111 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.112 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.334 I llm_load_vocab: special tokens cache size = 25
0.00.053.508 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.511 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.511 I llm_load_print_meta: arch             = gptneox
0.00.053.512 I llm_load_print_meta: vocab type       = BPE
0.00.053.512 I llm_load_print_meta: n_vocab          = 50304
0.00.053.512 I llm_load_print_meta: n_merges         = 50009
0.00.053.513 I llm_load_print_meta: vocab_only       = 0
0.00.053.513 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.513 I llm_load_print_meta: n_embd           = 2048
0.00.053.513 I llm_load_print_meta: n_layer          = 24
0.00.053.517 I llm_load_print_meta: n_head           = 16
0.00.053.518 I llm_load_print_meta: n_head_kv        = 16
0.00.053.518 I llm_load_print_meta: n_rot            = 32
0.00.053.518 I llm_load_print_meta: n_swa            = 0
0.00.053.519 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.519 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.520 I llm_load_print_meta: n_gqa            = 1
0.00.053.520 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.521 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.522 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.522 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.523 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.523 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.523 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.524 I llm_load_print_meta: n_ff             = 8192
0.00.053.524 I llm_load_print_meta: n_expert         = 0
0.00.053.524 I llm_load_print_meta: n_expert_used    = 0
0.00.053.524 I llm_load_print_meta: causal attn      = 1
0.00.053.525 I llm_load_print_meta: pooling type     = 0
0.00.053.525 I llm_load_print_meta: rope type        = 2
0.00.053.525 I llm_load_print_meta: rope scaling     = linear
0.00.053.525 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.525 I llm_load_print_meta: freq_scale_train = 1
0.00.053.526 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.526 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.526 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.526 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.526 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.527 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.527 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.539 I llm_load_print_meta: model type       = 1.4B
0.00.053.540 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.540 I llm_load_print_meta: model params     = 1.41 B
0.00.053.541 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.541 I llm_load_print_meta: general.name     = 1.4B
0.00.053.544 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.544 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.544 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.544 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.545 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.545 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.545 I llm_load_print_meta: max token length = 1024
0.00.055.799 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.799 I llm_load_tensors: offloading output layer to GPU
0.00.055.799 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.810 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.811 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.815 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.816 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.816 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.816 I llama_new_context_with_model: n_batch       = 2048
0.00.056.816 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.816 I llama_new_context_with_model: flash_attn    = 0
0.00.056.817 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.817 I llama_new_context_with_model: freq_scale    = 1
0.00.056.817 I ggml_metal_init: allocating
0.00.056.821 I ggml_metal_init: found device: Apple M4
0.00.056.823 I ggml_metal_init: picking default device: Apple M4
0.00.057.496 I ggml_metal_init: using embedded metal library
0.00.059.595 I ggml_metal_init: GPU name:   Apple M4
0.00.059.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.598 I ggml_metal_init: simdgroup reduction   = true
0.00.059.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.598 I ggml_metal_init: has bfloat            = true
0.00.059.598 I ggml_metal_init: use bfloat            = true
0.00.059.599 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.099 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.116 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.141 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.336 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.338 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.338 I llama_new_context_with_model: graph nodes  = 967
0.00.093.338 I llama_new_context_with_model: graph splits = 2
0.00.093.363 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.706 I main: llama threadpool init, n_threads = 4
0.00.669.750 I 
0.00.669.778 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.669.779 I 
0.00.669.937 I sampler seed: 1234
0.00.669.942 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.669.958 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.669.959 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.669.959 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.350.391 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.350.392 I llama_perf_context_print:        load time =     658.82 ms
0.01.350.393 I llama_perf_context_print: prompt eval time =      38.30 ms /     7 tokens (    5.47 ms per token,   182.78 tokens per second)
0.01.350.394 I llama_perf_context_print:        eval time =     639.09 ms /    63 runs   (   10.14 ms per token,    98.58 tokens per second)
0.01.350.394 I llama_perf_context_print:       total time =     680.69 ms /    70 tokens
0.01.350.579 I ggml_metal_free: deallocating

real	0m1.368s
user	0m0.110s
sys	0m0.147s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.791 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.285 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.289 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.297 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.301 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.166 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.167 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.167 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.167 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.168 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.168 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.169 I llama_model_loader: - type  f32:  194 tensors
0.00.024.169 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.169 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.547 I llm_load_vocab: special tokens cache size = 25
0.00.050.621 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.624 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.624 I llm_load_print_meta: arch             = gptneox
0.00.050.625 I llm_load_print_meta: vocab type       = BPE
0.00.050.625 I llm_load_print_meta: n_vocab          = 50304
0.00.050.625 I llm_load_print_meta: n_merges         = 50009
0.00.050.625 I llm_load_print_meta: vocab_only       = 0
0.00.050.625 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.626 I llm_load_print_meta: n_embd           = 2048
0.00.050.626 I llm_load_print_meta: n_layer          = 24
0.00.050.629 I llm_load_print_meta: n_head           = 16
0.00.050.629 I llm_load_print_meta: n_head_kv        = 16
0.00.050.630 I llm_load_print_meta: n_rot            = 32
0.00.050.630 I llm_load_print_meta: n_swa            = 0
0.00.050.630 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.630 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.631 I llm_load_print_meta: n_gqa            = 1
0.00.050.632 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.632 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.633 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.633 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.633 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.634 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.634 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.634 I llm_load_print_meta: n_ff             = 8192
0.00.050.635 I llm_load_print_meta: n_expert         = 0
0.00.050.637 I llm_load_print_meta: n_expert_used    = 0
0.00.050.637 I llm_load_print_meta: causal attn      = 1
0.00.050.637 I llm_load_print_meta: pooling type     = 0
0.00.050.638 I llm_load_print_meta: rope type        = 2
0.00.050.638 I llm_load_print_meta: rope scaling     = linear
0.00.050.638 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.638 I llm_load_print_meta: freq_scale_train = 1
0.00.050.640 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.640 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.640 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.640 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.640 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.640 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.641 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.652 I llm_load_print_meta: model type       = 1.4B
0.00.050.653 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.653 I llm_load_print_meta: model params     = 1.41 B
0.00.050.653 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.654 I llm_load_print_meta: general.name     = 1.4B
0.00.050.655 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.655 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.655 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.655 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.656 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.656 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: max token length = 1024
0.00.052.656 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.656 I llm_load_tensors: offloading output layer to GPU
0.00.052.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.666 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.667 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.593 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.594 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.594 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.595 I llama_new_context_with_model: n_batch       = 2048
0.00.053.595 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.595 I llama_new_context_with_model: flash_attn    = 0
0.00.053.595 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.596 I llama_new_context_with_model: freq_scale    = 1
0.00.053.596 I ggml_metal_init: allocating
0.00.053.599 I ggml_metal_init: found device: Apple M4
0.00.053.601 I ggml_metal_init: picking default device: Apple M4
0.00.054.176 I ggml_metal_init: using embedded metal library
0.00.056.127 I ggml_metal_init: GPU name:   Apple M4
0.00.056.129 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.129 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.130 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.130 I ggml_metal_init: simdgroup reduction   = true
0.00.056.130 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.130 I ggml_metal_init: has bfloat            = true
0.00.056.130 I ggml_metal_init: use bfloat            = true
0.00.056.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.131 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.420 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.425 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.443 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.410 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.411 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.411 I llama_new_context_with_model: graph nodes  = 967
0.00.084.411 I llama_new_context_with_model: graph splits = 2
0.00.084.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.992 I main: llama threadpool init, n_threads = 4
0.00.713.028 I 
0.00.713.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.713.057 I 
0.00.713.276 I sampler seed: 1234
0.00.713.280 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.291 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.292 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.292 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.437.913 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60118.54 tokens per second)
0.01.437.914 I llama_perf_context_print:        load time =     704.20 ms
0.01.437.914 I llama_perf_context_print: prompt eval time =      36.08 ms /     7 tokens (    5.15 ms per token,   194.01 tokens per second)
0.01.437.915 I llama_perf_context_print:        eval time =     685.51 ms /    63 runs   (   10.88 ms per token,    91.90 tokens per second)
0.01.437.918 I llama_perf_context_print:       total time =     724.92 ms /    70 tokens
0.01.438.110 I ggml_metal_free: deallocating

real	0m1.456s
user	0m0.109s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.011.575 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.209 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.210 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.210 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.210 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.211 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.923 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.647 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.647 I llama_model_loader: - type  f32:  194 tensors
0.00.026.647 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.648 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.719 I llm_load_vocab: special tokens cache size = 25
0.00.052.896 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.899 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.899 I llm_load_print_meta: arch             = gptneox
0.00.052.900 I llm_load_print_meta: vocab type       = BPE
0.00.052.900 I llm_load_print_meta: n_vocab          = 50304
0.00.052.900 I llm_load_print_meta: n_merges         = 50009
0.00.052.900 I llm_load_print_meta: vocab_only       = 0
0.00.052.901 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.901 I llm_load_print_meta: n_embd           = 2048
0.00.052.901 I llm_load_print_meta: n_layer          = 24
0.00.052.904 I llm_load_print_meta: n_head           = 16
0.00.052.905 I llm_load_print_meta: n_head_kv        = 16
0.00.052.905 I llm_load_print_meta: n_rot            = 32
0.00.052.905 I llm_load_print_meta: n_swa            = 0
0.00.052.906 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.907 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.907 I llm_load_print_meta: n_gqa            = 1
0.00.052.908 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.909 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.909 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.910 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.910 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.910 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.910 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.911 I llm_load_print_meta: n_ff             = 8192
0.00.052.911 I llm_load_print_meta: n_expert         = 0
0.00.052.911 I llm_load_print_meta: n_expert_used    = 0
0.00.052.913 I llm_load_print_meta: causal attn      = 1
0.00.052.915 I llm_load_print_meta: pooling type     = 0
0.00.052.915 I llm_load_print_meta: rope type        = 2
0.00.052.915 I llm_load_print_meta: rope scaling     = linear
0.00.052.916 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.916 I llm_load_print_meta: freq_scale_train = 1
0.00.052.916 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.916 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.917 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.917 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.917 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.917 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.917 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.929 I llm_load_print_meta: model type       = 1.4B
0.00.052.929 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.930 I llm_load_print_meta: model params     = 1.41 B
0.00.052.930 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.930 I llm_load_print_meta: general.name     = 1.4B
0.00.052.931 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.932 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.932 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.932 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.932 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.932 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.933 I llm_load_print_meta: max token length = 1024
0.00.054.873 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.873 I llm_load_tensors: offloading output layer to GPU
0.00.054.873 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.883 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.885 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.792 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.793 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.793 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.793 I llama_new_context_with_model: n_batch       = 2048
0.00.055.793 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.793 I llama_new_context_with_model: flash_attn    = 0
0.00.055.794 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.794 I llama_new_context_with_model: freq_scale    = 1
0.00.055.794 I ggml_metal_init: allocating
0.00.055.799 I ggml_metal_init: found device: Apple M4
0.00.055.802 I ggml_metal_init: picking default device: Apple M4
0.00.056.349 I ggml_metal_init: using embedded metal library
0.00.058.234 I ggml_metal_init: GPU name:   Apple M4
0.00.058.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.236 I ggml_metal_init: simdgroup reduction   = true
0.00.058.236 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.236 I ggml_metal_init: has bfloat            = true
0.00.058.236 I ggml_metal_init: use bfloat            = true
0.00.058.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.239 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.669 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.677 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.695 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.713 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.714 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.714 I llama_new_context_with_model: graph nodes  = 967
0.00.086.714 I llama_new_context_with_model: graph splits = 2
0.00.086.736 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.258 I main: llama threadpool init, n_threads = 4
0.00.778.292 I 
0.00.778.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.778.320 I 
0.00.778.554 I sampler seed: 1234
0.00.778.559 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.569 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.570 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.570 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.562.398 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.562.399 I llama_perf_context_print:        load time =     766.68 ms
0.01.562.399 I llama_perf_context_print: prompt eval time =      36.55 ms /     7 tokens (    5.22 ms per token,   191.51 tokens per second)
0.01.562.400 I llama_perf_context_print:        eval time =     744.35 ms /    63 runs   (   11.82 ms per token,    84.64 tokens per second)
0.01.562.400 I llama_perf_context_print:       total time =     784.14 ms /    70 tokens
0.01.562.594 I ggml_metal_free: deallocating

real	0m1.580s
user	0m0.107s
sys	0m0.163s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.652 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.157 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.158 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.162 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.163 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.163 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.169 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.718 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.719 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.720 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.720 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.720 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.721 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.721 I llama_model_loader: - type  f32:  194 tensors
0.00.023.721 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.722 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.988 I llm_load_vocab: special tokens cache size = 25
0.00.049.971 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.973 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.974 I llm_load_print_meta: arch             = gptneox
0.00.049.974 I llm_load_print_meta: vocab type       = BPE
0.00.049.974 I llm_load_print_meta: n_vocab          = 50304
0.00.049.975 I llm_load_print_meta: n_merges         = 50009
0.00.049.975 I llm_load_print_meta: vocab_only       = 0
0.00.049.975 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.975 I llm_load_print_meta: n_embd           = 2048
0.00.049.975 I llm_load_print_meta: n_layer          = 24
0.00.049.978 I llm_load_print_meta: n_head           = 16
0.00.049.980 I llm_load_print_meta: n_head_kv        = 16
0.00.049.980 I llm_load_print_meta: n_rot            = 32
0.00.049.980 I llm_load_print_meta: n_swa            = 0
0.00.049.980 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.980 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.981 I llm_load_print_meta: n_gqa            = 1
0.00.049.982 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.983 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.983 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.983 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.984 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.984 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.984 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.985 I llm_load_print_meta: n_ff             = 8192
0.00.049.985 I llm_load_print_meta: n_expert         = 0
0.00.049.985 I llm_load_print_meta: n_expert_used    = 0
0.00.049.987 I llm_load_print_meta: causal attn      = 1
0.00.049.989 I llm_load_print_meta: pooling type     = 0
0.00.049.989 I llm_load_print_meta: rope type        = 2
0.00.049.989 I llm_load_print_meta: rope scaling     = linear
0.00.049.990 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.990 I llm_load_print_meta: freq_scale_train = 1
0.00.049.990 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.990 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.991 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.991 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.991 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.991 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.991 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.003 I llm_load_print_meta: model type       = 1.4B
0.00.050.003 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.003 I llm_load_print_meta: model params     = 1.41 B
0.00.050.004 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.004 I llm_load_print_meta: general.name     = 1.4B
0.00.050.004 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.004 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.004 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.005 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.005 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.006 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.006 I llm_load_print_meta: max token length = 1024
0.00.052.043 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.043 I llm_load_tensors: offloading output layer to GPU
0.00.052.044 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.054 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.055 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.028 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.029 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.029 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.029 I llama_new_context_with_model: n_batch       = 2048
0.00.053.030 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.030 I llama_new_context_with_model: flash_attn    = 0
0.00.053.030 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.031 I llama_new_context_with_model: freq_scale    = 1
0.00.053.031 I ggml_metal_init: allocating
0.00.053.038 I ggml_metal_init: found device: Apple M4
0.00.053.040 I ggml_metal_init: picking default device: Apple M4
0.00.053.605 I ggml_metal_init: using embedded metal library
0.00.055.539 I ggml_metal_init: GPU name:   Apple M4
0.00.055.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.541 I ggml_metal_init: simdgroup reduction   = true
0.00.055.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.543 I ggml_metal_init: has bfloat            = true
0.00.055.543 I ggml_metal_init: use bfloat            = true
0.00.055.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.066 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.077 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.097 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.051 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.052 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.052 I llama_new_context_with_model: graph nodes  = 967
0.00.084.052 I llama_new_context_with_model: graph splits = 2
0.00.084.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.465 I main: llama threadpool init, n_threads = 4
0.00.692.511 I 
0.00.692.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.692.549 I 
0.00.692.713 I sampler seed: 1234
0.00.692.718 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.743 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.744 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.744 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.566.495 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.566.496 I llama_perf_context_print:        load time =     683.81 ms
0.01.566.496 I llama_perf_context_print: prompt eval time =      36.71 ms /     7 tokens (    5.24 ms per token,   190.67 tokens per second)
0.01.566.497 I llama_perf_context_print:        eval time =     833.96 ms /    63 runs   (   13.24 ms per token,    75.54 tokens per second)
0.01.566.497 I llama_perf_context_print:       total time =     874.03 ms /    70 tokens
0.01.566.667 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.108s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.868 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.397 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.401 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.404 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.404 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.405 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.405 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.406 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.408 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.410 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.410 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.230 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.232 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.232 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.232 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.233 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.233 I llama_model_loader: - type  f32:  194 tensors
0.00.024.234 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.234 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.234 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.250 I llm_load_vocab: special tokens cache size = 25
0.00.051.500 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.503 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.503 I llm_load_print_meta: arch             = gptneox
0.00.051.504 I llm_load_print_meta: vocab type       = BPE
0.00.051.504 I llm_load_print_meta: n_vocab          = 50304
0.00.051.504 I llm_load_print_meta: n_merges         = 50009
0.00.051.504 I llm_load_print_meta: vocab_only       = 0
0.00.051.504 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.505 I llm_load_print_meta: n_embd           = 2048
0.00.051.505 I llm_load_print_meta: n_layer          = 24
0.00.051.508 I llm_load_print_meta: n_head           = 16
0.00.051.508 I llm_load_print_meta: n_head_kv        = 16
0.00.051.509 I llm_load_print_meta: n_rot            = 32
0.00.051.509 I llm_load_print_meta: n_swa            = 0
0.00.051.509 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.509 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.510 I llm_load_print_meta: n_gqa            = 1
0.00.051.511 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.511 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.512 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.512 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.512 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.512 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.513 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.513 I llm_load_print_meta: n_ff             = 8192
0.00.051.513 I llm_load_print_meta: n_expert         = 0
0.00.051.518 I llm_load_print_meta: n_expert_used    = 0
0.00.051.519 I llm_load_print_meta: causal attn      = 1
0.00.051.519 I llm_load_print_meta: pooling type     = 0
0.00.051.519 I llm_load_print_meta: rope type        = 2
0.00.051.519 I llm_load_print_meta: rope scaling     = linear
0.00.051.519 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.520 I llm_load_print_meta: freq_scale_train = 1
0.00.051.520 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.520 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.520 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.520 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.520 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.520 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.521 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.533 I llm_load_print_meta: model type       = 1.4B
0.00.051.533 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.533 I llm_load_print_meta: model params     = 1.41 B
0.00.051.534 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.534 I llm_load_print_meta: general.name     = 1.4B
0.00.051.534 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.534 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.535 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.535 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.536 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.536 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.536 I llm_load_print_meta: max token length = 1024
0.00.053.483 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.483 I llm_load_tensors: offloading output layer to GPU
0.00.053.483 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.493 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.494 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.519 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.520 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.520 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.521 I llama_new_context_with_model: n_batch       = 2048
0.00.054.521 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.521 I llama_new_context_with_model: flash_attn    = 0
0.00.054.521 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.522 I llama_new_context_with_model: freq_scale    = 1
0.00.054.522 I ggml_metal_init: allocating
0.00.054.525 I ggml_metal_init: found device: Apple M4
0.00.054.527 I ggml_metal_init: picking default device: Apple M4
0.00.055.116 I ggml_metal_init: using embedded metal library
0.00.057.039 I ggml_metal_init: GPU name:   Apple M4
0.00.057.040 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.040 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.041 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.041 I ggml_metal_init: simdgroup reduction   = true
0.00.057.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.042 I ggml_metal_init: has bfloat            = true
0.00.057.043 I ggml_metal_init: use bfloat            = true
0.00.057.043 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.044 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.702 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.710 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.737 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.760 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.762 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.762 I llama_new_context_with_model: graph nodes  = 967
0.00.085.762 I llama_new_context_with_model: graph splits = 2
0.00.085.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.444.127 I main: llama threadpool init, n_threads = 4
0.00.444.164 I 
0.00.444.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.444.192 I 
0.00.444.415 I sampler seed: 1234
0.00.444.420 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.431 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.431 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.431 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.124.397 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61631.94 tokens per second)
0.01.124.398 I llama_perf_context_print:        load time =     434.25 ms
0.01.124.399 I llama_perf_context_print: prompt eval time =      35.71 ms /     7 tokens (    5.10 ms per token,   196.01 tokens per second)
0.01.124.400 I llama_perf_context_print:        eval time =     641.33 ms /    63 runs   (   10.18 ms per token,    98.23 tokens per second)
0.01.124.400 I llama_perf_context_print:       total time =     680.27 ms /    70 tokens
0.01.124.554 I ggml_metal_free: deallocating

real	0m1.143s
user	0m0.108s
sys	0m0.111s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.425 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.843 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.849 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.849 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.850 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.850 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.851 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.852 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.853 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.853 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.854 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.854 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.854 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.857 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.857 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.858 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.709 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.626 I llama_model_loader: - type  f32:  194 tensors
0.00.025.627 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.627 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.627 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.627 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.076 I llm_load_vocab: special tokens cache size = 25
0.00.052.074 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.077 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.077 I llm_load_print_meta: arch             = gptneox
0.00.052.078 I llm_load_print_meta: vocab type       = BPE
0.00.052.078 I llm_load_print_meta: n_vocab          = 50304
0.00.052.078 I llm_load_print_meta: n_merges         = 50009
0.00.052.078 I llm_load_print_meta: vocab_only       = 0
0.00.052.078 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.079 I llm_load_print_meta: n_embd           = 2048
0.00.052.079 I llm_load_print_meta: n_layer          = 24
0.00.052.081 I llm_load_print_meta: n_head           = 16
0.00.052.082 I llm_load_print_meta: n_head_kv        = 16
0.00.052.082 I llm_load_print_meta: n_rot            = 32
0.00.052.082 I llm_load_print_meta: n_swa            = 0
0.00.052.082 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.083 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.083 I llm_load_print_meta: n_gqa            = 1
0.00.052.084 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.085 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.086 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.086 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.086 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.086 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.086 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.087 I llm_load_print_meta: n_ff             = 8192
0.00.052.089 I llm_load_print_meta: n_expert         = 0
0.00.052.090 I llm_load_print_meta: n_expert_used    = 0
0.00.052.091 I llm_load_print_meta: causal attn      = 1
0.00.052.091 I llm_load_print_meta: pooling type     = 0
0.00.052.091 I llm_load_print_meta: rope type        = 2
0.00.052.091 I llm_load_print_meta: rope scaling     = linear
0.00.052.092 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.093 I llm_load_print_meta: freq_scale_train = 1
0.00.052.093 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.093 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.093 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.094 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.094 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.094 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.095 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.108 I llm_load_print_meta: model type       = 1.4B
0.00.052.108 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.108 I llm_load_print_meta: model params     = 1.41 B
0.00.052.109 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.109 I llm_load_print_meta: general.name     = 1.4B
0.00.052.109 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.109 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.110 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.110 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.110 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.110 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.110 I llm_load_print_meta: max token length = 1024
0.00.054.045 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.046 I llm_load_tensors: offloading output layer to GPU
0.00.054.046 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.056 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.057 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.978 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.979 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.979 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.979 I llama_new_context_with_model: n_batch       = 2048
0.00.054.979 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.979 I llama_new_context_with_model: flash_attn    = 0
0.00.054.980 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.980 I llama_new_context_with_model: freq_scale    = 1
0.00.054.980 I ggml_metal_init: allocating
0.00.054.984 I ggml_metal_init: found device: Apple M4
0.00.054.986 I ggml_metal_init: picking default device: Apple M4
0.00.055.551 I ggml_metal_init: using embedded metal library
0.00.057.440 I ggml_metal_init: GPU name:   Apple M4
0.00.057.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.443 I ggml_metal_init: simdgroup reduction   = true
0.00.057.444 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.444 I ggml_metal_init: has bfloat            = true
0.00.057.444 I ggml_metal_init: use bfloat            = true
0.00.057.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.593 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.601 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.622 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.601 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.603 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.603 I llama_new_context_with_model: graph nodes  = 967
0.00.085.603 I llama_new_context_with_model: graph splits = 2
0.00.085.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.379 I main: llama threadpool init, n_threads = 4
0.00.537.417 I 
0.00.537.442 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.537.443 I 
0.00.537.589 I sampler seed: 1234
0.00.537.593 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.537.603 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.537.604 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.537.604 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.282.540 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.01.282.541 I llama_perf_context_print:        load time =     525.95 ms
0.01.282.542 I llama_perf_context_print: prompt eval time =      35.57 ms /     7 tokens (    5.08 ms per token,   196.81 tokens per second)
0.01.282.542 I llama_perf_context_print:        eval time =     706.48 ms /    63 runs   (   11.21 ms per token,    89.17 tokens per second)
0.01.282.543 I llama_perf_context_print:       total time =     745.16 ms /    70 tokens
0.01.282.710 I ggml_metal_free: deallocating

real	0m1.299s
user	0m0.108s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.626 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.943 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.953 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.954 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.959 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.960 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.070 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.036 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.037 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.038 I llama_model_loader: - type  f32:  194 tensors
0.00.025.038 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.039 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.039 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.011 I llm_load_vocab: special tokens cache size = 25
0.00.052.191 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.194 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.194 I llm_load_print_meta: arch             = gptneox
0.00.052.195 I llm_load_print_meta: vocab type       = BPE
0.00.052.195 I llm_load_print_meta: n_vocab          = 50304
0.00.052.195 I llm_load_print_meta: n_merges         = 50009
0.00.052.195 I llm_load_print_meta: vocab_only       = 0
0.00.052.195 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.195 I llm_load_print_meta: n_embd           = 2048
0.00.052.196 I llm_load_print_meta: n_layer          = 24
0.00.052.198 I llm_load_print_meta: n_head           = 16
0.00.052.199 I llm_load_print_meta: n_head_kv        = 16
0.00.052.199 I llm_load_print_meta: n_rot            = 32
0.00.052.199 I llm_load_print_meta: n_swa            = 0
0.00.052.199 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.200 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.200 I llm_load_print_meta: n_gqa            = 1
0.00.052.201 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.202 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.202 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.203 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.207 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.207 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.209 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.210 I llm_load_print_meta: n_ff             = 8192
0.00.052.210 I llm_load_print_meta: n_expert         = 0
0.00.052.210 I llm_load_print_meta: n_expert_used    = 0
0.00.052.210 I llm_load_print_meta: causal attn      = 1
0.00.052.210 I llm_load_print_meta: pooling type     = 0
0.00.052.211 I llm_load_print_meta: rope type        = 2
0.00.052.211 I llm_load_print_meta: rope scaling     = linear
0.00.052.211 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.211 I llm_load_print_meta: freq_scale_train = 1
0.00.052.212 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.212 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.212 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.212 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.212 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.212 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.213 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.225 I llm_load_print_meta: model type       = 1.4B
0.00.052.225 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.225 I llm_load_print_meta: model params     = 1.41 B
0.00.052.226 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.226 I llm_load_print_meta: general.name     = 1.4B
0.00.052.226 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.227 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.227 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.227 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.227 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.228 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.228 I llm_load_print_meta: max token length = 1024
0.00.054.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.189 I llm_load_tensors: offloading output layer to GPU
0.00.054.189 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.199 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.200 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.170 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.171 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.171 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.171 I llama_new_context_with_model: n_batch       = 2048
0.00.055.171 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.172 I llama_new_context_with_model: flash_attn    = 0
0.00.055.172 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.172 I llama_new_context_with_model: freq_scale    = 1
0.00.055.173 I ggml_metal_init: allocating
0.00.055.176 I ggml_metal_init: found device: Apple M4
0.00.055.178 I ggml_metal_init: picking default device: Apple M4
0.00.055.745 I ggml_metal_init: using embedded metal library
0.00.057.690 I ggml_metal_init: GPU name:   Apple M4
0.00.057.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.692 I ggml_metal_init: simdgroup reduction   = true
0.00.057.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.692 I ggml_metal_init: has bfloat            = true
0.00.057.692 I ggml_metal_init: use bfloat            = true
0.00.057.693 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.197 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.204 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.222 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.225 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.226 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.227 I llama_new_context_with_model: graph nodes  = 967
0.00.086.227 I llama_new_context_with_model: graph splits = 2
0.00.086.240 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.287 I main: llama threadpool init, n_threads = 4
0.00.631.320 I 
0.00.631.350 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.631.351 I 
0.00.631.578 I sampler seed: 1234
0.00.631.582 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.631.602 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.631.602 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.631.602 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.384.817 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.384.817 I llama_perf_context_print:        load time =     621.66 ms
0.01.384.818 I llama_perf_context_print: prompt eval time =      40.31 ms /     7 tokens (    5.76 ms per token,   173.66 tokens per second)
0.01.384.818 I llama_perf_context_print:        eval time =     709.96 ms /    63 runs   (   11.27 ms per token,    88.74 tokens per second)
0.01.384.819 I llama_perf_context_print:       total time =     753.53 ms /    70 tokens
0.01.384.997 I ggml_metal_free: deallocating

real	0m1.404s
user	0m0.110s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.472 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.383 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.390 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.391 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.391 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.392 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.392 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.398 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.223 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.031 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.032 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.032 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.033 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.033 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.034 I llama_model_loader: - type  f32:  194 tensors
0.00.024.034 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.034 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.469 I llm_load_vocab: special tokens cache size = 25
0.00.050.602 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.605 I llm_load_print_meta: arch             = gptneox
0.00.050.605 I llm_load_print_meta: vocab type       = BPE
0.00.050.605 I llm_load_print_meta: n_vocab          = 50304
0.00.050.605 I llm_load_print_meta: n_merges         = 50009
0.00.050.606 I llm_load_print_meta: vocab_only       = 0
0.00.050.606 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.606 I llm_load_print_meta: n_embd           = 2048
0.00.050.606 I llm_load_print_meta: n_layer          = 24
0.00.050.608 I llm_load_print_meta: n_head           = 16
0.00.050.609 I llm_load_print_meta: n_head_kv        = 16
0.00.050.609 I llm_load_print_meta: n_rot            = 32
0.00.050.609 I llm_load_print_meta: n_swa            = 0
0.00.050.609 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.611 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.611 I llm_load_print_meta: n_gqa            = 1
0.00.050.612 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.613 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.613 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.614 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.616 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.616 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.616 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.617 I llm_load_print_meta: n_ff             = 8192
0.00.050.617 I llm_load_print_meta: n_expert         = 0
0.00.050.617 I llm_load_print_meta: n_expert_used    = 0
0.00.050.620 I llm_load_print_meta: causal attn      = 1
0.00.050.622 I llm_load_print_meta: pooling type     = 0
0.00.050.622 I llm_load_print_meta: rope type        = 2
0.00.050.622 I llm_load_print_meta: rope scaling     = linear
0.00.050.623 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.623 I llm_load_print_meta: freq_scale_train = 1
0.00.050.623 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.623 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.627 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.628 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.628 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.628 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.628 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.640 I llm_load_print_meta: model type       = 1.4B
0.00.050.640 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.641 I llm_load_print_meta: model params     = 1.41 B
0.00.050.642 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.642 I llm_load_print_meta: general.name     = 1.4B
0.00.050.642 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.642 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.642 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.642 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.643 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.643 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.643 I llm_load_print_meta: max token length = 1024
0.00.052.658 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.658 I llm_load_tensors: offloading output layer to GPU
0.00.052.659 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.668 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.669 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.560 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.561 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.561 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.561 I llama_new_context_with_model: n_batch       = 2048
0.00.053.561 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.561 I llama_new_context_with_model: flash_attn    = 0
0.00.053.562 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.562 I llama_new_context_with_model: freq_scale    = 1
0.00.053.562 I ggml_metal_init: allocating
0.00.053.569 I ggml_metal_init: found device: Apple M4
0.00.053.571 I ggml_metal_init: picking default device: Apple M4
0.00.054.106 I ggml_metal_init: using embedded metal library
0.00.056.066 I ggml_metal_init: GPU name:   Apple M4
0.00.056.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.069 I ggml_metal_init: simdgroup reduction   = true
0.00.056.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.070 I ggml_metal_init: has bfloat            = true
0.00.056.070 I ggml_metal_init: use bfloat            = true
0.00.056.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.180 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.186 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.217 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.093 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.094 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.095 I llama_new_context_with_model: graph nodes  = 967
0.00.083.095 I llama_new_context_with_model: graph splits = 2
0.00.083.107 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.138 I main: llama threadpool init, n_threads = 4
0.00.701.174 I 
0.00.701.206 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.701.206 I 
0.00.701.448 I sampler seed: 1234
0.00.701.453 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.701.464 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.701.466 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.701.466 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.542.935 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.542.935 I llama_perf_context_print:        load time =     692.66 ms
0.01.542.936 I llama_perf_context_print: prompt eval time =      38.63 ms /     7 tokens (    5.52 ms per token,   181.20 tokens per second)
0.01.542.937 I llama_perf_context_print:        eval time =     799.90 ms /    63 runs   (   12.70 ms per token,    78.76 tokens per second)
0.01.542.940 I llama_perf_context_print:       total time =     841.80 ms /    70 tokens
0.01.543.133 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.084 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.637 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.642 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.648 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.648 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.656 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.656 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.657 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.658 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.659 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.547 I llama_model_loader: - type  f32:  194 tensors
0.00.025.548 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.477 I llm_load_vocab: special tokens cache size = 25
0.00.052.579 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.582 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.583 I llm_load_print_meta: arch             = gptneox
0.00.052.583 I llm_load_print_meta: vocab type       = BPE
0.00.052.583 I llm_load_print_meta: n_vocab          = 50304
0.00.052.583 I llm_load_print_meta: n_merges         = 50009
0.00.052.583 I llm_load_print_meta: vocab_only       = 0
0.00.052.584 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.584 I llm_load_print_meta: n_embd           = 2048
0.00.052.584 I llm_load_print_meta: n_layer          = 24
0.00.052.587 I llm_load_print_meta: n_head           = 16
0.00.052.588 I llm_load_print_meta: n_head_kv        = 16
0.00.052.588 I llm_load_print_meta: n_rot            = 32
0.00.052.588 I llm_load_print_meta: n_swa            = 0
0.00.052.588 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.589 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.590 I llm_load_print_meta: n_gqa            = 1
0.00.052.591 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.592 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.592 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.592 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.593 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.593 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.593 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.594 I llm_load_print_meta: n_ff             = 8192
0.00.052.594 I llm_load_print_meta: n_expert         = 0
0.00.052.594 I llm_load_print_meta: n_expert_used    = 0
0.00.052.594 I llm_load_print_meta: causal attn      = 1
0.00.052.594 I llm_load_print_meta: pooling type     = 0
0.00.052.595 I llm_load_print_meta: rope type        = 2
0.00.052.595 I llm_load_print_meta: rope scaling     = linear
0.00.052.595 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.596 I llm_load_print_meta: freq_scale_train = 1
0.00.052.596 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.596 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.598 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.598 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.598 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.598 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.598 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.605 I llm_load_print_meta: model type       = 1.4B
0.00.052.605 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.606 I llm_load_print_meta: model params     = 1.41 B
0.00.052.606 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.606 I llm_load_print_meta: general.name     = 1.4B
0.00.052.607 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.607 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.607 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.607 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.607 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.607 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.608 I llm_load_print_meta: max token length = 1024
0.00.054.382 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.382 I llm_load_tensors: offloading output layer to GPU
0.00.054.383 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.388 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.388 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.349 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.349 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.350 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.350 I llama_new_context_with_model: n_batch       = 2048
0.00.055.350 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.350 I llama_new_context_with_model: flash_attn    = 0
0.00.055.350 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.351 I llama_new_context_with_model: freq_scale    = 1
0.00.055.351 I ggml_metal_init: allocating
0.00.055.356 I ggml_metal_init: found device: Apple M4
0.00.055.359 I ggml_metal_init: picking default device: Apple M4
0.00.055.908 I ggml_metal_init: using embedded metal library
0.00.057.830 I ggml_metal_init: GPU name:   Apple M4
0.00.057.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.832 I ggml_metal_init: simdgroup reduction   = true
0.00.057.832 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.832 I ggml_metal_init: has bfloat            = true
0.00.057.832 I ggml_metal_init: use bfloat            = true
0.00.057.833 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.833 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.651 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.656 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.676 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.670 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.671 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.671 I llama_new_context_with_model: graph nodes  = 967
0.00.085.672 I llama_new_context_with_model: graph splits = 2
0.00.085.684 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.054 I main: llama threadpool init, n_threads = 4
0.00.772.095 I 
0.00.772.135 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.772.136 I 
0.00.772.363 I sampler seed: 1234
0.00.772.369 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.380 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.381 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.381 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.641.313 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.641.313 I llama_perf_context_print:        load time =     761.96 ms
0.01.641.314 I llama_perf_context_print: prompt eval time =      38.46 ms /     7 tokens (    5.49 ms per token,   181.99 tokens per second)
0.01.641.315 I llama_perf_context_print:        eval time =     827.56 ms /    63 runs   (   13.14 ms per token,    76.13 tokens per second)
0.01.641.315 I llama_perf_context_print:       total time =     869.27 ms /    70 tokens
0.01.641.506 I ggml_metal_free: deallocating

real	0m1.660s
user	0m0.109s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.585 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.645 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.180 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.188 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.195 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.197 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.200 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.202 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.203 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.207 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.207 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.208 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.623 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.678 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.969 I llama_model_loader: - type  f32:  194 tensors
0.00.054.970 I llama_model_loader: - type  f16:   98 tensors
0.00.083.681 I llm_load_vocab: special tokens cache size = 25
0.00.090.489 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.492 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.493 I llm_load_print_meta: arch             = gptneox
0.00.090.493 I llm_load_print_meta: vocab type       = BPE
0.00.090.493 I llm_load_print_meta: n_vocab          = 50304
0.00.090.493 I llm_load_print_meta: n_merges         = 50009
0.00.090.493 I llm_load_print_meta: vocab_only       = 0
0.00.090.494 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.494 I llm_load_print_meta: n_embd           = 2048
0.00.090.494 I llm_load_print_meta: n_layer          = 24
0.00.090.497 I llm_load_print_meta: n_head           = 16
0.00.090.497 I llm_load_print_meta: n_head_kv        = 16
0.00.090.498 I llm_load_print_meta: n_rot            = 32
0.00.090.498 I llm_load_print_meta: n_swa            = 0
0.00.090.498 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.498 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.499 I llm_load_print_meta: n_gqa            = 1
0.00.090.500 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.500 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.501 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.501 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.501 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.501 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.501 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.502 I llm_load_print_meta: n_ff             = 8192
0.00.090.504 I llm_load_print_meta: n_expert         = 0
0.00.090.504 I llm_load_print_meta: n_expert_used    = 0
0.00.090.504 I llm_load_print_meta: causal attn      = 1
0.00.090.504 I llm_load_print_meta: pooling type     = 0
0.00.090.504 I llm_load_print_meta: rope type        = 2
0.00.090.505 I llm_load_print_meta: rope scaling     = linear
0.00.090.505 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.505 I llm_load_print_meta: freq_scale_train = 1
0.00.090.505 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.506 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.506 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.506 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.506 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.507 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.509 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.520 I llm_load_print_meta: model type       = 1.4B
0.00.090.521 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.521 I llm_load_print_meta: model params     = 1.41 B
0.00.090.522 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.522 I llm_load_print_meta: general.name     = 1.4B
0.00.090.522 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.522 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.522 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.523 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.523 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.523 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.523 I llm_load_print_meta: max token length = 1024
0.00.092.951 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.951 I llm_load_tensors: offloading output layer to GPU
0.00.092.952 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.961 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.963 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.892 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.892 I llama_new_context_with_model: n_ctx         = 128
0.00.093.893 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.893 I llama_new_context_with_model: n_batch       = 128
0.00.093.893 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.893 I llama_new_context_with_model: flash_attn    = 0
0.00.093.893 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.894 I llama_new_context_with_model: freq_scale    = 1
0.00.093.894 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.894 I ggml_metal_init: allocating
0.00.093.897 I ggml_metal_init: found device: Apple M4
0.00.093.899 I ggml_metal_init: picking default device: Apple M4
0.00.094.483 I ggml_metal_init: using embedded metal library
0.00.096.548 I ggml_metal_init: GPU name:   Apple M4
0.00.096.550 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.551 I ggml_metal_init: simdgroup reduction   = true
0.00.096.551 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.551 I ggml_metal_init: has bfloat            = true
0.00.096.551 I ggml_metal_init: use bfloat            = true
0.00.096.552 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.266 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.270 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.283 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.124 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.125 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.125 I llama_new_context_with_model: graph nodes  = 967
0.00.107.125 I llama_new_context_with_model: graph splits = 2
0.00.107.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.044.601 I 
0.01.044.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.044.697 I perplexity: tokenizing the input ..
0.01.057.169 I perplexity: tokenization took 12.471 ms
0.01.057.207 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.189.783 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.191.549 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.191.584 I llama_perf_context_print:        load time =    1020.94 ms
0.01.191.587 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.27 tokens per second)
0.01.191.588 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.191.588 I llama_perf_context_print:       total time =     146.99 ms /   129 tokens
0.01.192.142 I ggml_metal_free: deallocating

real	0m1.383s
user	0m0.124s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.909 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.737 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.741 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.751 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.752 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.752 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.752 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.839 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.262 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.285 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.287 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.288 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.288 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.288 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.288 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.289 I llama_model_loader: - type  f32:  194 tensors
0.00.030.289 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.160 I llm_load_vocab: special tokens cache size = 25
0.00.060.569 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.572 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.573 I llm_load_print_meta: arch             = gptneox
0.00.060.573 I llm_load_print_meta: vocab type       = BPE
0.00.060.573 I llm_load_print_meta: n_vocab          = 50304
0.00.060.573 I llm_load_print_meta: n_merges         = 50009
0.00.060.574 I llm_load_print_meta: vocab_only       = 0
0.00.060.574 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.574 I llm_load_print_meta: n_embd           = 2048
0.00.060.574 I llm_load_print_meta: n_layer          = 24
0.00.060.578 I llm_load_print_meta: n_head           = 16
0.00.060.579 I llm_load_print_meta: n_head_kv        = 16
0.00.060.579 I llm_load_print_meta: n_rot            = 32
0.00.060.579 I llm_load_print_meta: n_swa            = 0
0.00.060.579 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.579 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.580 I llm_load_print_meta: n_gqa            = 1
0.00.060.581 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.581 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.582 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.582 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.582 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.583 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.583 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.584 I llm_load_print_meta: n_ff             = 8192
0.00.060.584 I llm_load_print_meta: n_expert         = 0
0.00.060.584 I llm_load_print_meta: n_expert_used    = 0
0.00.060.584 I llm_load_print_meta: causal attn      = 1
0.00.060.585 I llm_load_print_meta: pooling type     = 0
0.00.060.585 I llm_load_print_meta: rope type        = 2
0.00.060.586 I llm_load_print_meta: rope scaling     = linear
0.00.060.586 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.586 I llm_load_print_meta: freq_scale_train = 1
0.00.060.586 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.587 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.587 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.587 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.587 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.587 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.588 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.595 I llm_load_print_meta: model type       = 1.4B
0.00.060.595 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.596 I llm_load_print_meta: model params     = 1.41 B
0.00.060.596 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.596 I llm_load_print_meta: general.name     = 1.4B
0.00.060.600 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.600 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.600 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.600 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.601 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.601 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.601 I llm_load_print_meta: max token length = 1024
0.00.062.559 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.559 I llm_load_tensors: offloading output layer to GPU
0.00.062.560 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.565 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.565 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.512 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.513 I llama_new_context_with_model: n_ctx         = 128
0.00.063.513 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.513 I llama_new_context_with_model: n_batch       = 128
0.00.063.513 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.514 I llama_new_context_with_model: flash_attn    = 0
0.00.063.514 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.514 I llama_new_context_with_model: freq_scale    = 1
0.00.063.515 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.515 I ggml_metal_init: allocating
0.00.063.522 I ggml_metal_init: found device: Apple M4
0.00.063.524 I ggml_metal_init: picking default device: Apple M4
0.00.064.114 I ggml_metal_init: using embedded metal library
0.00.066.277 I ggml_metal_init: GPU name:   Apple M4
0.00.066.278 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.279 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.279 I ggml_metal_init: simdgroup reduction   = true
0.00.066.279 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.280 I ggml_metal_init: has bfloat            = true
0.00.066.280 I ggml_metal_init: use bfloat            = true
0.00.066.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.064 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.071 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.089 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.983 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.984 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.985 I llama_new_context_with_model: graph nodes  = 967
0.00.076.985 I llama_new_context_with_model: graph splits = 2
0.00.076.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.927.495 I 
0.00.927.522 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.927.525 I perplexity: tokenizing the input ..
0.00.935.746 I perplexity: tokenization took 8.219 ms
0.00.935.758 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.057.932 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.059.340 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.059.356 I llama_perf_context_print:        load time =     916.58 ms
0.01.059.357 I llama_perf_context_print: prompt eval time =     121.95 ms /   128 tokens (    0.95 ms per token,  1049.62 tokens per second)
0.01.059.357 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.059.358 I llama_perf_context_print:       total time =     131.86 ms /   129 tokens
0.01.059.765 I ggml_metal_free: deallocating

real	0m1.077s
user	0m0.090s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.240 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.830 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.836 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.837 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.837 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.837 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.838 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.838 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.839 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.839 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.840 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.841 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.842 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.842 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.842 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.691 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.563 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.564 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.564 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.564 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.565 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.565 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.566 I llama_model_loader: - type  f32:  194 tensors
0.00.023.566 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.566 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.892 I llm_load_vocab: special tokens cache size = 25
0.00.050.009 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.012 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.012 I llm_load_print_meta: arch             = gptneox
0.00.050.012 I llm_load_print_meta: vocab type       = BPE
0.00.050.013 I llm_load_print_meta: n_vocab          = 50304
0.00.050.013 I llm_load_print_meta: n_merges         = 50009
0.00.050.013 I llm_load_print_meta: vocab_only       = 0
0.00.050.013 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.013 I llm_load_print_meta: n_embd           = 2048
0.00.050.013 I llm_load_print_meta: n_layer          = 24
0.00.050.016 I llm_load_print_meta: n_head           = 16
0.00.050.017 I llm_load_print_meta: n_head_kv        = 16
0.00.050.017 I llm_load_print_meta: n_rot            = 32
0.00.050.017 I llm_load_print_meta: n_swa            = 0
0.00.050.017 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.017 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.018 I llm_load_print_meta: n_gqa            = 1
0.00.050.019 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.020 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.020 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.021 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.021 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.022 I llm_load_print_meta: n_ff             = 8192
0.00.050.022 I llm_load_print_meta: n_expert         = 0
0.00.050.022 I llm_load_print_meta: n_expert_used    = 0
0.00.050.022 I llm_load_print_meta: causal attn      = 1
0.00.050.022 I llm_load_print_meta: pooling type     = 0
0.00.050.022 I llm_load_print_meta: rope type        = 2
0.00.050.023 I llm_load_print_meta: rope scaling     = linear
0.00.050.023 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.024 I llm_load_print_meta: freq_scale_train = 1
0.00.050.024 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.029 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.030 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.030 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.030 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.030 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.030 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.042 I llm_load_print_meta: model type       = 1.4B
0.00.050.042 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.044 I llm_load_print_meta: model params     = 1.41 B
0.00.050.045 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.045 I llm_load_print_meta: general.name     = 1.4B
0.00.050.045 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.045 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.045 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.045 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.046 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.046 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.046 I llm_load_print_meta: max token length = 1024
0.00.051.930 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.930 I llm_load_tensors: offloading output layer to GPU
0.00.051.930 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.940 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.941 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.839 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.840 I llama_new_context_with_model: n_ctx         = 128
0.00.052.840 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.840 I llama_new_context_with_model: n_batch       = 128
0.00.052.840 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.841 I llama_new_context_with_model: flash_attn    = 0
0.00.052.841 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.841 I llama_new_context_with_model: freq_scale    = 1
0.00.052.842 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.842 I ggml_metal_init: allocating
0.00.052.848 I ggml_metal_init: found device: Apple M4
0.00.052.850 I ggml_metal_init: picking default device: Apple M4
0.00.053.395 I ggml_metal_init: using embedded metal library
0.00.055.358 I ggml_metal_init: GPU name:   Apple M4
0.00.055.359 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.360 I ggml_metal_init: simdgroup reduction   = true
0.00.055.360 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.360 I ggml_metal_init: has bfloat            = true
0.00.055.361 I ggml_metal_init: use bfloat            = true
0.00.055.361 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.362 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.540 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.545 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.560 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.459 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.460 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.460 I llama_new_context_with_model: graph nodes  = 967
0.00.065.460 I llama_new_context_with_model: graph splits = 2
0.00.065.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.989 I 
0.00.616.141 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.616.167 I perplexity: tokenizing the input ..
0.00.625.303 I perplexity: tokenization took 9.133 ms
0.00.625.316 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.042 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.749.404 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.749.418 I llama_perf_context_print:        load time =     606.73 ms
0.00.749.419 I llama_perf_context_print: prompt eval time =     122.48 ms /   128 tokens (    0.96 ms per token,  1045.03 tokens per second)
0.00.749.420 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.749.420 I llama_perf_context_print:       total time =     133.44 ms /   129 tokens
0.00.749.945 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.906 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.521 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.522 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.524 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.525 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.525 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.530 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.530 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.530 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.377 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.377 I llama_model_loader: - type  f32:  194 tensors
0.00.023.378 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.378 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.508 I llm_load_vocab: special tokens cache size = 25
0.00.050.700 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.702 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.703 I llm_load_print_meta: arch             = gptneox
0.00.050.703 I llm_load_print_meta: vocab type       = BPE
0.00.050.703 I llm_load_print_meta: n_vocab          = 50304
0.00.050.704 I llm_load_print_meta: n_merges         = 50009
0.00.050.704 I llm_load_print_meta: vocab_only       = 0
0.00.050.704 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.704 I llm_load_print_meta: n_embd           = 2048
0.00.050.704 I llm_load_print_meta: n_layer          = 24
0.00.050.707 I llm_load_print_meta: n_head           = 16
0.00.050.708 I llm_load_print_meta: n_head_kv        = 16
0.00.050.708 I llm_load_print_meta: n_rot            = 32
0.00.050.709 I llm_load_print_meta: n_swa            = 0
0.00.050.710 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.710 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.710 I llm_load_print_meta: n_gqa            = 1
0.00.050.711 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.712 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.713 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.713 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.713 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.713 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.714 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.714 I llm_load_print_meta: n_ff             = 8192
0.00.050.714 I llm_load_print_meta: n_expert         = 0
0.00.050.714 I llm_load_print_meta: n_expert_used    = 0
0.00.050.715 I llm_load_print_meta: causal attn      = 1
0.00.050.715 I llm_load_print_meta: pooling type     = 0
0.00.050.715 I llm_load_print_meta: rope type        = 2
0.00.050.715 I llm_load_print_meta: rope scaling     = linear
0.00.050.716 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.716 I llm_load_print_meta: freq_scale_train = 1
0.00.050.718 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.718 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.718 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.718 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.719 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.719 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.719 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.731 I llm_load_print_meta: model type       = 1.4B
0.00.050.731 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.732 I llm_load_print_meta: model params     = 1.41 B
0.00.050.732 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.732 I llm_load_print_meta: general.name     = 1.4B
0.00.050.733 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.733 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.733 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.733 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.733 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.734 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.734 I llm_load_print_meta: max token length = 1024
0.00.052.691 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.691 I llm_load_tensors: offloading output layer to GPU
0.00.052.691 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.701 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.702 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.644 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.645 I llama_new_context_with_model: n_ctx         = 128
0.00.053.646 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.646 I llama_new_context_with_model: n_batch       = 128
0.00.053.646 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.646 I llama_new_context_with_model: flash_attn    = 0
0.00.053.646 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.647 I llama_new_context_with_model: freq_scale    = 1
0.00.053.647 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.647 I ggml_metal_init: allocating
0.00.053.651 I ggml_metal_init: found device: Apple M4
0.00.053.653 I ggml_metal_init: picking default device: Apple M4
0.00.054.210 I ggml_metal_init: using embedded metal library
0.00.056.172 I ggml_metal_init: GPU name:   Apple M4
0.00.056.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.175 I ggml_metal_init: simdgroup reduction   = true
0.00.056.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.175 I ggml_metal_init: has bfloat            = true
0.00.056.175 I ggml_metal_init: use bfloat            = true
0.00.056.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.504 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.506 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.521 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.453 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.454 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.455 I llama_new_context_with_model: graph nodes  = 967
0.00.066.455 I llama_new_context_with_model: graph splits = 2
0.00.066.467 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.572 I 
0.00.670.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.670.646 I perplexity: tokenizing the input ..
0.00.678.531 I perplexity: tokenization took 7.883 ms
0.00.678.542 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.351 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.802.696 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.802.710 I llama_perf_context_print:        load time =     661.66 ms
0.00.802.711 I llama_perf_context_print: prompt eval time =     122.57 ms /   128 tokens (    0.96 ms per token,  1044.31 tokens per second)
0.00.802.712 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.712 I llama_perf_context_print:       total time =     132.14 ms /   129 tokens
0.00.803.052 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.079s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.468 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.159 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.160 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.162 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.984 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.759 I llama_model_loader: - type  f32:  194 tensors
0.00.023.759 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.868 I llm_load_vocab: special tokens cache size = 25
0.00.049.655 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.657 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.658 I llm_load_print_meta: arch             = gptneox
0.00.049.658 I llm_load_print_meta: vocab type       = BPE
0.00.049.658 I llm_load_print_meta: n_vocab          = 50304
0.00.049.658 I llm_load_print_meta: n_merges         = 50009
0.00.049.659 I llm_load_print_meta: vocab_only       = 0
0.00.049.659 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.659 I llm_load_print_meta: n_embd           = 2048
0.00.049.659 I llm_load_print_meta: n_layer          = 24
0.00.049.662 I llm_load_print_meta: n_head           = 16
0.00.049.663 I llm_load_print_meta: n_head_kv        = 16
0.00.049.663 I llm_load_print_meta: n_rot            = 32
0.00.049.663 I llm_load_print_meta: n_swa            = 0
0.00.049.663 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.663 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.664 I llm_load_print_meta: n_gqa            = 1
0.00.049.665 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.665 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.666 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.668 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.668 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.668 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.668 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.669 I llm_load_print_meta: n_ff             = 8192
0.00.049.669 I llm_load_print_meta: n_expert         = 0
0.00.049.669 I llm_load_print_meta: n_expert_used    = 0
0.00.049.670 I llm_load_print_meta: causal attn      = 1
0.00.049.670 I llm_load_print_meta: pooling type     = 0
0.00.049.670 I llm_load_print_meta: rope type        = 2
0.00.049.670 I llm_load_print_meta: rope scaling     = linear
0.00.049.670 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.671 I llm_load_print_meta: freq_scale_train = 1
0.00.049.671 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.671 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.671 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.671 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.672 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.674 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.674 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.686 I llm_load_print_meta: model type       = 1.4B
0.00.049.686 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.687 I llm_load_print_meta: model params     = 1.41 B
0.00.049.687 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.688 I llm_load_print_meta: general.name     = 1.4B
0.00.049.688 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.688 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.688 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.688 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.689 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.689 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.689 I llm_load_print_meta: max token length = 1024
0.00.051.623 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.623 I llm_load_tensors: offloading output layer to GPU
0.00.051.623 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.633 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.634 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.538 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.539 I llama_new_context_with_model: n_ctx         = 128
0.00.052.539 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.539 I llama_new_context_with_model: n_batch       = 128
0.00.052.540 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.540 I llama_new_context_with_model: flash_attn    = 0
0.00.052.540 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.540 I llama_new_context_with_model: freq_scale    = 1
0.00.052.541 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.541 I ggml_metal_init: allocating
0.00.052.544 I ggml_metal_init: found device: Apple M4
0.00.052.546 I ggml_metal_init: picking default device: Apple M4
0.00.053.071 I ggml_metal_init: using embedded metal library
0.00.054.973 I ggml_metal_init: GPU name:   Apple M4
0.00.054.975 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.975 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.976 I ggml_metal_init: simdgroup reduction   = true
0.00.054.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.976 I ggml_metal_init: has bfloat            = true
0.00.054.976 I ggml_metal_init: use bfloat            = true
0.00.054.977 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.059 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.063 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.077 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.986 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.988 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.988 I llama_new_context_with_model: graph nodes  = 967
0.00.064.988 I llama_new_context_with_model: graph splits = 2
0.00.065.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.600 I 
0.00.719.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.719.645 I perplexity: tokenizing the input ..
0.00.727.625 I perplexity: tokenization took 7.98 ms
0.00.727.637 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.862.868 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.864.206 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.864.238 I llama_perf_context_print:        load time =     710.13 ms
0.00.864.239 I llama_perf_context_print: prompt eval time =     135.00 ms /   128 tokens (    1.05 ms per token,   948.14 tokens per second)
0.00.864.240 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.240 I llama_perf_context_print:       total time =     144.64 ms /   129 tokens
0.00.864.725 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.078s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.659 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.261 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.271 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.271 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.272 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.273 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.274 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.275 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.276 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.277 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.277 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.680 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.681 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.681 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.682 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.682 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.683 I llama_model_loader: - type  f32:  194 tensors
0.00.022.683 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.683 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.774 I llm_load_vocab: special tokens cache size = 25
0.00.048.625 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.628 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.629 I llm_load_print_meta: arch             = gptneox
0.00.048.629 I llm_load_print_meta: vocab type       = BPE
0.00.048.629 I llm_load_print_meta: n_vocab          = 50304
0.00.048.629 I llm_load_print_meta: n_merges         = 50009
0.00.048.630 I llm_load_print_meta: vocab_only       = 0
0.00.048.630 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.630 I llm_load_print_meta: n_embd           = 2048
0.00.048.630 I llm_load_print_meta: n_layer          = 24
0.00.048.632 I llm_load_print_meta: n_head           = 16
0.00.048.633 I llm_load_print_meta: n_head_kv        = 16
0.00.048.634 I llm_load_print_meta: n_rot            = 32
0.00.048.634 I llm_load_print_meta: n_swa            = 0
0.00.048.634 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.635 I llm_load_print_meta: n_gqa            = 1
0.00.048.635 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.636 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.636 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.637 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.637 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.637 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.638 I llm_load_print_meta: n_ff             = 8192
0.00.048.638 I llm_load_print_meta: n_expert         = 0
0.00.048.638 I llm_load_print_meta: n_expert_used    = 0
0.00.048.639 I llm_load_print_meta: causal attn      = 1
0.00.048.639 I llm_load_print_meta: pooling type     = 0
0.00.048.639 I llm_load_print_meta: rope type        = 2
0.00.048.639 I llm_load_print_meta: rope scaling     = linear
0.00.048.639 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.642 I llm_load_print_meta: freq_scale_train = 1
0.00.048.642 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.642 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.642 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.643 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.643 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.643 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.643 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.654 I llm_load_print_meta: model type       = 1.4B
0.00.048.654 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.655 I llm_load_print_meta: model params     = 1.41 B
0.00.048.655 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.656 I llm_load_print_meta: general.name     = 1.4B
0.00.048.656 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.656 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.656 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.656 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.657 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.657 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.657 I llm_load_print_meta: max token length = 1024
0.00.050.203 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.203 I llm_load_tensors: offloading output layer to GPU
0.00.050.204 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.213 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.214 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.014 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.015 I llama_new_context_with_model: n_ctx         = 128
0.00.051.015 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.015 I llama_new_context_with_model: n_batch       = 128
0.00.051.015 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.015 I llama_new_context_with_model: flash_attn    = 0
0.00.051.016 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.016 I llama_new_context_with_model: freq_scale    = 1
0.00.051.016 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.016 I ggml_metal_init: allocating
0.00.051.019 I ggml_metal_init: found device: Apple M4
0.00.051.021 I ggml_metal_init: picking default device: Apple M4
0.00.051.549 I ggml_metal_init: using embedded metal library
0.00.053.461 I ggml_metal_init: GPU name:   Apple M4
0.00.053.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.464 I ggml_metal_init: simdgroup reduction   = true
0.00.053.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.464 I ggml_metal_init: has bfloat            = true
0.00.053.464 I ggml_metal_init: use bfloat            = true
0.00.053.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.654 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.658 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.671 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.564 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.565 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.566 I llama_new_context_with_model: graph nodes  = 967
0.00.063.566 I llama_new_context_with_model: graph splits = 2
0.00.063.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.439 I 
0.00.594.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.594.476 I perplexity: tokenizing the input ..
0.00.602.316 I perplexity: tokenization took 7.837 ms
0.00.602.327 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.817 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.738.196 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.738.214 I llama_perf_context_print:        load time =     585.77 ms
0.00.738.215 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.34 tokens per second)
0.00.738.216 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.738.216 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.738.632 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.077s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.782 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.298 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.299 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.299 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.300 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.301 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.301 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.301 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.302 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.302 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.302 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.303 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.304 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.009 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.010 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.010 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.010 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.011 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.011 I llama_model_loader: - type  f32:  194 tensors
0.00.024.012 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.012 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.012 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.735 I llm_load_vocab: special tokens cache size = 25
0.00.050.956 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.959 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.959 I llm_load_print_meta: arch             = gptneox
0.00.050.959 I llm_load_print_meta: vocab type       = BPE
0.00.050.960 I llm_load_print_meta: n_vocab          = 50304
0.00.050.960 I llm_load_print_meta: n_merges         = 50009
0.00.050.960 I llm_load_print_meta: vocab_only       = 0
0.00.050.960 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.960 I llm_load_print_meta: n_embd           = 2048
0.00.050.961 I llm_load_print_meta: n_layer          = 24
0.00.050.963 I llm_load_print_meta: n_head           = 16
0.00.050.964 I llm_load_print_meta: n_head_kv        = 16
0.00.050.964 I llm_load_print_meta: n_rot            = 32
0.00.050.964 I llm_load_print_meta: n_swa            = 0
0.00.050.964 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.965 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.965 I llm_load_print_meta: n_gqa            = 1
0.00.050.966 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.967 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.967 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.969 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.970 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.970 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.970 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.971 I llm_load_print_meta: n_ff             = 8192
0.00.050.971 I llm_load_print_meta: n_expert         = 0
0.00.050.971 I llm_load_print_meta: n_expert_used    = 0
0.00.050.971 I llm_load_print_meta: causal attn      = 1
0.00.050.971 I llm_load_print_meta: pooling type     = 0
0.00.050.971 I llm_load_print_meta: rope type        = 2
0.00.050.972 I llm_load_print_meta: rope scaling     = linear
0.00.050.972 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.972 I llm_load_print_meta: freq_scale_train = 1
0.00.050.973 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.973 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.973 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.973 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.973 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.973 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.974 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.985 I llm_load_print_meta: model type       = 1.4B
0.00.050.985 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.986 I llm_load_print_meta: model params     = 1.41 B
0.00.050.986 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.986 I llm_load_print_meta: general.name     = 1.4B
0.00.050.987 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.987 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.987 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.987 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.989 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: max token length = 1024
0.00.052.863 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.863 I llm_load_tensors: offloading output layer to GPU
0.00.052.864 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.874 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.875 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.753 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.754 I llama_new_context_with_model: n_ctx         = 128
0.00.053.754 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.754 I llama_new_context_with_model: n_batch       = 128
0.00.053.754 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.754 I llama_new_context_with_model: flash_attn    = 0
0.00.053.755 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.755 I llama_new_context_with_model: freq_scale    = 1
0.00.053.755 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.755 I ggml_metal_init: allocating
0.00.053.758 I ggml_metal_init: found device: Apple M4
0.00.053.760 I ggml_metal_init: picking default device: Apple M4
0.00.054.312 I ggml_metal_init: using embedded metal library
0.00.056.234 I ggml_metal_init: GPU name:   Apple M4
0.00.056.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.236 I ggml_metal_init: simdgroup reduction   = true
0.00.056.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.237 I ggml_metal_init: has bfloat            = true
0.00.056.237 I ggml_metal_init: use bfloat            = true
0.00.056.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.555 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.562 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.581 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.479 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.480 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.480 I llama_new_context_with_model: graph nodes  = 967
0.00.066.481 I llama_new_context_with_model: graph splits = 2
0.00.066.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.397.211 I 
0.00.397.252 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.397.261 I perplexity: tokenizing the input ..
0.00.405.255 I perplexity: tokenization took 7.992 ms
0.00.405.266 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.537.501 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.538.838 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.538.856 I llama_perf_context_print:        load time =     387.42 ms
0.00.538.857 I llama_perf_context_print: prompt eval time =     132.00 ms /   128 tokens (    1.03 ms per token,   969.67 tokens per second)
0.00.538.858 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.538.859 I llama_perf_context_print:       total time =     141.65 ms /   129 tokens
0.00.539.283 I ggml_metal_free: deallocating

real	0m0.556s
user	0m0.079s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.598 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.024 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.032 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.035 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.035 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.036 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.036 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.036 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.037 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.037 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.040 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.040 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.040 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.923 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.012 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.907 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.909 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.910 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.911 I llama_model_loader: - type  f32:  194 tensors
0.00.022.911 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.911 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.911 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.912 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.758 I llm_load_vocab: special tokens cache size = 25
0.00.049.883 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.885 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.885 I llm_load_print_meta: arch             = gptneox
0.00.049.886 I llm_load_print_meta: vocab type       = BPE
0.00.049.886 I llm_load_print_meta: n_vocab          = 50304
0.00.049.886 I llm_load_print_meta: n_merges         = 50009
0.00.049.886 I llm_load_print_meta: vocab_only       = 0
0.00.049.887 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.887 I llm_load_print_meta: n_embd           = 2048
0.00.049.887 I llm_load_print_meta: n_layer          = 24
0.00.049.889 I llm_load_print_meta: n_head           = 16
0.00.049.890 I llm_load_print_meta: n_head_kv        = 16
0.00.049.890 I llm_load_print_meta: n_rot            = 32
0.00.049.891 I llm_load_print_meta: n_swa            = 0
0.00.049.891 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.891 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.892 I llm_load_print_meta: n_gqa            = 1
0.00.049.893 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.894 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.894 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.894 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.895 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.895 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.895 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.896 I llm_load_print_meta: n_ff             = 8192
0.00.049.896 I llm_load_print_meta: n_expert         = 0
0.00.049.898 I llm_load_print_meta: n_expert_used    = 0
0.00.049.898 I llm_load_print_meta: causal attn      = 1
0.00.049.898 I llm_load_print_meta: pooling type     = 0
0.00.049.898 I llm_load_print_meta: rope type        = 2
0.00.049.898 I llm_load_print_meta: rope scaling     = linear
0.00.049.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.899 I llm_load_print_meta: freq_scale_train = 1
0.00.049.899 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.900 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.900 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.911 I llm_load_print_meta: model type       = 1.4B
0.00.049.912 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.912 I llm_load_print_meta: model params     = 1.41 B
0.00.049.913 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.913 I llm_load_print_meta: general.name     = 1.4B
0.00.049.913 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.913 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.913 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.913 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.914 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.914 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: max token length = 1024
0.00.051.458 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.459 I llm_load_tensors: offloading output layer to GPU
0.00.051.459 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.468 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.469 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.271 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.272 I llama_new_context_with_model: n_ctx         = 128
0.00.052.272 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.272 I llama_new_context_with_model: n_batch       = 128
0.00.052.272 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.273 I llama_new_context_with_model: flash_attn    = 0
0.00.052.273 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.273 I llama_new_context_with_model: freq_scale    = 1
0.00.052.274 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.274 I ggml_metal_init: allocating
0.00.052.277 I ggml_metal_init: found device: Apple M4
0.00.052.279 I ggml_metal_init: picking default device: Apple M4
0.00.052.797 I ggml_metal_init: using embedded metal library
0.00.054.716 I ggml_metal_init: GPU name:   Apple M4
0.00.054.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.718 I ggml_metal_init: simdgroup reduction   = true
0.00.054.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.718 I ggml_metal_init: has bfloat            = true
0.00.054.719 I ggml_metal_init: use bfloat            = true
0.00.054.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.720 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.024 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.028 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.051 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.958 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.959 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.960 I llama_new_context_with_model: graph nodes  = 967
0.00.064.960 I llama_new_context_with_model: graph splits = 2
0.00.064.972 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.251 I 
0.00.498.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.498.284 I perplexity: tokenizing the input ..
0.00.506.402 I perplexity: tokenization took 8.117 ms
0.00.506.418 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.638.557 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.639.921 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.639.941 I llama_perf_context_print:        load time =     489.65 ms
0.00.639.942 I llama_perf_context_print: prompt eval time =     131.91 ms /   128 tokens (    1.03 ms per token,   970.34 tokens per second)
0.00.639.943 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.639.943 I llama_perf_context_print:       total time =     141.69 ms /   129 tokens
0.00.640.367 I ggml_metal_free: deallocating

real	0m0.654s
user	0m0.079s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.319 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.034 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.046 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.047 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.050 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.913 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.659 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.660 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.661 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.661 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.662 I llama_model_loader: - type  f32:  194 tensors
0.00.023.663 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.663 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.663 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.732 I llm_load_vocab: special tokens cache size = 25
0.00.049.771 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.773 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.774 I llm_load_print_meta: arch             = gptneox
0.00.049.774 I llm_load_print_meta: vocab type       = BPE
0.00.049.774 I llm_load_print_meta: n_vocab          = 50304
0.00.049.774 I llm_load_print_meta: n_merges         = 50009
0.00.049.775 I llm_load_print_meta: vocab_only       = 0
0.00.049.775 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.775 I llm_load_print_meta: n_embd           = 2048
0.00.049.775 I llm_load_print_meta: n_layer          = 24
0.00.049.778 I llm_load_print_meta: n_head           = 16
0.00.049.779 I llm_load_print_meta: n_head_kv        = 16
0.00.049.779 I llm_load_print_meta: n_rot            = 32
0.00.049.779 I llm_load_print_meta: n_swa            = 0
0.00.049.780 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.780 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.782 I llm_load_print_meta: n_gqa            = 1
0.00.049.783 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.784 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.784 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.785 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.786 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.786 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.786 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.787 I llm_load_print_meta: n_ff             = 8192
0.00.049.787 I llm_load_print_meta: n_expert         = 0
0.00.049.787 I llm_load_print_meta: n_expert_used    = 0
0.00.049.788 I llm_load_print_meta: causal attn      = 1
0.00.049.788 I llm_load_print_meta: pooling type     = 0
0.00.049.788 I llm_load_print_meta: rope type        = 2
0.00.049.788 I llm_load_print_meta: rope scaling     = linear
0.00.049.788 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.789 I llm_load_print_meta: freq_scale_train = 1
0.00.049.789 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.789 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.789 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.789 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.790 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.790 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.790 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.801 I llm_load_print_meta: model type       = 1.4B
0.00.049.801 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.802 I llm_load_print_meta: model params     = 1.41 B
0.00.049.802 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.802 I llm_load_print_meta: general.name     = 1.4B
0.00.049.803 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.803 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.804 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.805 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.805 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.805 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.805 I llm_load_print_meta: max token length = 1024
0.00.051.339 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.339 I llm_load_tensors: offloading output layer to GPU
0.00.051.339 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.349 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.350 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.197 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.198 I llama_new_context_with_model: n_ctx         = 128
0.00.052.198 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.198 I llama_new_context_with_model: n_batch       = 128
0.00.052.198 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.198 I llama_new_context_with_model: flash_attn    = 0
0.00.052.199 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.199 I llama_new_context_with_model: freq_scale    = 1
0.00.052.199 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.200 I ggml_metal_init: allocating
0.00.052.203 I ggml_metal_init: found device: Apple M4
0.00.052.205 I ggml_metal_init: picking default device: Apple M4
0.00.052.725 I ggml_metal_init: using embedded metal library
0.00.054.638 I ggml_metal_init: GPU name:   Apple M4
0.00.054.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.641 I ggml_metal_init: simdgroup reduction   = true
0.00.054.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.641 I ggml_metal_init: has bfloat            = true
0.00.054.641 I ggml_metal_init: use bfloat            = true
0.00.054.641 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.822 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.827 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.843 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.724 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.725 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.725 I llama_new_context_with_model: graph nodes  = 967
0.00.064.726 I llama_new_context_with_model: graph splits = 2
0.00.064.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.271 I 
0.00.576.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.576.308 I perplexity: tokenizing the input ..
0.00.584.397 I perplexity: tokenization took 8.088 ms
0.00.584.408 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.718.470 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.719.840 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.719.856 I llama_perf_context_print:        load time =     566.95 ms
0.00.719.857 I llama_perf_context_print: prompt eval time =     133.83 ms /   128 tokens (    1.05 ms per token,   956.42 tokens per second)
0.00.719.858 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.719.858 I llama_perf_context_print:       total time =     143.59 ms /   129 tokens
0.00.720.292 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.078s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.567 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.414 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.414 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.415 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.416 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.416 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.417 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.417 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.143 I llama_model_loader: - type  f32:  194 tensors
0.00.023.143 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.144 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.771 I llm_load_vocab: special tokens cache size = 25
0.00.049.760 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.762 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.763 I llm_load_print_meta: arch             = gptneox
0.00.049.763 I llm_load_print_meta: vocab type       = BPE
0.00.049.763 I llm_load_print_meta: n_vocab          = 50304
0.00.049.763 I llm_load_print_meta: n_merges         = 50009
0.00.049.764 I llm_load_print_meta: vocab_only       = 0
0.00.049.764 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.764 I llm_load_print_meta: n_embd           = 2048
0.00.049.764 I llm_load_print_meta: n_layer          = 24
0.00.049.767 I llm_load_print_meta: n_head           = 16
0.00.049.768 I llm_load_print_meta: n_head_kv        = 16
0.00.049.768 I llm_load_print_meta: n_rot            = 32
0.00.049.768 I llm_load_print_meta: n_swa            = 0
0.00.049.769 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.769 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.770 I llm_load_print_meta: n_gqa            = 1
0.00.049.770 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.771 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.772 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.772 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.772 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.772 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.773 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.774 I llm_load_print_meta: n_ff             = 8192
0.00.049.774 I llm_load_print_meta: n_expert         = 0
0.00.049.774 I llm_load_print_meta: n_expert_used    = 0
0.00.049.774 I llm_load_print_meta: causal attn      = 1
0.00.049.774 I llm_load_print_meta: pooling type     = 0
0.00.049.775 I llm_load_print_meta: rope type        = 2
0.00.049.775 I llm_load_print_meta: rope scaling     = linear
0.00.049.777 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.777 I llm_load_print_meta: freq_scale_train = 1
0.00.049.777 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.777 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.778 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.778 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.778 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.778 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.778 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.790 I llm_load_print_meta: model type       = 1.4B
0.00.049.790 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.790 I llm_load_print_meta: model params     = 1.41 B
0.00.049.791 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.791 I llm_load_print_meta: general.name     = 1.4B
0.00.049.791 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.791 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.792 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: max token length = 1024
0.00.051.280 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.280 I llm_load_tensors: offloading output layer to GPU
0.00.051.281 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.290 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.291 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.085 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.086 I llama_new_context_with_model: n_ctx         = 128
0.00.052.086 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.086 I llama_new_context_with_model: n_batch       = 128
0.00.052.086 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.086 I llama_new_context_with_model: flash_attn    = 0
0.00.052.087 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.087 I llama_new_context_with_model: freq_scale    = 1
0.00.052.087 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.088 I ggml_metal_init: allocating
0.00.052.091 I ggml_metal_init: found device: Apple M4
0.00.052.093 I ggml_metal_init: picking default device: Apple M4
0.00.052.607 I ggml_metal_init: using embedded metal library
0.00.054.518 I ggml_metal_init: GPU name:   Apple M4
0.00.054.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.520 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.520 I ggml_metal_init: simdgroup reduction   = true
0.00.054.520 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.521 I ggml_metal_init: has bfloat            = true
0.00.054.521 I ggml_metal_init: use bfloat            = true
0.00.054.521 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.523 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.666 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.669 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.690 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.570 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.571 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.572 I llama_new_context_with_model: graph nodes  = 967
0.00.064.572 I llama_new_context_with_model: graph splits = 2
0.00.064.584 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.762 I 
0.00.659.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.659.794 I perplexity: tokenizing the input ..
0.00.667.727 I perplexity: tokenization took 7.932 ms
0.00.667.742 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.847 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.288 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.303 I llama_perf_context_print:        load time =     651.19 ms
0.00.809.304 I llama_perf_context_print: prompt eval time =     139.88 ms /   128 tokens (    1.09 ms per token,   915.08 tokens per second)
0.00.809.304 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.305 I llama_perf_context_print:       total time =     149.54 ms /   129 tokens
0.00.809.650 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.078s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.665 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.504 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.352 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.431 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.216 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.217 I llama_model_loader: - type  f32:  194 tensors
0.00.024.217 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.007 I llm_load_vocab: special tokens cache size = 25
0.00.051.198 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.201 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.202 I llm_load_print_meta: arch             = gptneox
0.00.051.202 I llm_load_print_meta: vocab type       = BPE
0.00.051.202 I llm_load_print_meta: n_vocab          = 50304
0.00.051.202 I llm_load_print_meta: n_merges         = 50009
0.00.051.203 I llm_load_print_meta: vocab_only       = 0
0.00.051.203 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.203 I llm_load_print_meta: n_embd           = 2048
0.00.051.203 I llm_load_print_meta: n_layer          = 24
0.00.051.206 I llm_load_print_meta: n_head           = 16
0.00.051.206 I llm_load_print_meta: n_head_kv        = 16
0.00.051.207 I llm_load_print_meta: n_rot            = 32
0.00.051.207 I llm_load_print_meta: n_swa            = 0
0.00.051.207 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.207 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.208 I llm_load_print_meta: n_gqa            = 1
0.00.051.209 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.209 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.210 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.210 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.210 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.211 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.211 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.211 I llm_load_print_meta: n_ff             = 8192
0.00.051.212 I llm_load_print_meta: n_expert         = 0
0.00.051.212 I llm_load_print_meta: n_expert_used    = 0
0.00.051.212 I llm_load_print_meta: causal attn      = 1
0.00.051.212 I llm_load_print_meta: pooling type     = 0
0.00.051.212 I llm_load_print_meta: rope type        = 2
0.00.051.213 I llm_load_print_meta: rope scaling     = linear
0.00.051.213 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.215 I llm_load_print_meta: freq_scale_train = 1
0.00.051.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.216 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.216 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.216 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.216 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.228 I llm_load_print_meta: model type       = 1.4B
0.00.051.228 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.229 I llm_load_print_meta: model params     = 1.41 B
0.00.051.229 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.229 I llm_load_print_meta: general.name     = 1.4B
0.00.051.229 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.232 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: max token length = 1024
0.00.053.252 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.252 I llm_load_tensors: offloading output layer to GPU
0.00.053.253 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.262 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.264 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.222 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.223 I llama_new_context_with_model: n_ctx         = 128
0.00.054.223 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.223 I llama_new_context_with_model: n_batch       = 128
0.00.054.223 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.223 I llama_new_context_with_model: flash_attn    = 0
0.00.054.224 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.224 I llama_new_context_with_model: freq_scale    = 1
0.00.054.224 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.225 I ggml_metal_init: allocating
0.00.054.228 I ggml_metal_init: found device: Apple M4
0.00.054.229 I ggml_metal_init: picking default device: Apple M4
0.00.054.778 I ggml_metal_init: using embedded metal library
0.00.056.754 I ggml_metal_init: GPU name:   Apple M4
0.00.056.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.756 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.756 I ggml_metal_init: simdgroup reduction   = true
0.00.056.757 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.757 I ggml_metal_init: has bfloat            = true
0.00.056.757 I ggml_metal_init: use bfloat            = true
0.00.056.757 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.186 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.188 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.205 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.099 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.100 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.100 I llama_new_context_with_model: graph nodes  = 967
0.00.067.100 I llama_new_context_with_model: graph splits = 2
0.00.067.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.272.023 I 
0.00.272.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.272.081 I perplexity: tokenizing the input ..
0.00.280.087 I perplexity: tokenization took 8.005 ms
0.00.280.098 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.420.547 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.421.957 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.421.977 I llama_perf_context_print:        load time =     262.35 ms
0.00.421.979 I llama_perf_context_print: prompt eval time =     140.21 ms /   128 tokens (    1.10 ms per token,   912.90 tokens per second)
0.00.421.980 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.421.981 I llama_perf_context_print:       total time =     149.96 ms /   129 tokens
0.00.422.448 I ggml_metal_free: deallocating

real	0m0.438s
user	0m0.079s
sys	0m0.059s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.284 I build: 4222 (f0678c5f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.221 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.812 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.816 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.818 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.819 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.819 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.822 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.601 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.287 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.289 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.289 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.291 I llama_model_loader: - type  f32:  194 tensors
0.00.052.291 I llama_model_loader: - type  f16:   98 tensors
0.00.080.642 I llm_load_vocab: special tokens cache size = 25
0.00.087.299 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.302 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.302 I llm_load_print_meta: arch             = gptneox
0.00.087.303 I llm_load_print_meta: vocab type       = BPE
0.00.087.303 I llm_load_print_meta: n_vocab          = 50304
0.00.087.303 I llm_load_print_meta: n_merges         = 50009
0.00.087.303 I llm_load_print_meta: vocab_only       = 0
0.00.087.303 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.303 I llm_load_print_meta: n_embd           = 2048
0.00.087.304 I llm_load_print_meta: n_layer          = 24
0.00.087.307 I llm_load_print_meta: n_head           = 16
0.00.087.307 I llm_load_print_meta: n_head_kv        = 16
0.00.087.308 I llm_load_print_meta: n_rot            = 32
0.00.087.308 I llm_load_print_meta: n_swa            = 0
0.00.087.308 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.308 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.309 I llm_load_print_meta: n_gqa            = 1
0.00.087.309 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.310 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.310 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.310 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.311 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.311 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.311 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.311 I llm_load_print_meta: n_ff             = 8192
0.00.087.312 I llm_load_print_meta: n_expert         = 0
0.00.087.312 I llm_load_print_meta: n_expert_used    = 0
0.00.087.312 I llm_load_print_meta: causal attn      = 1
0.00.087.312 I llm_load_print_meta: pooling type     = 0
0.00.087.312 I llm_load_print_meta: rope type        = 2
0.00.087.312 I llm_load_print_meta: rope scaling     = linear
0.00.087.313 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.313 I llm_load_print_meta: freq_scale_train = 1
0.00.087.313 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.315 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.315 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.315 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.315 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.315 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.316 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.327 I llm_load_print_meta: model type       = 1.4B
0.00.087.328 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.328 I llm_load_print_meta: model params     = 1.41 B
0.00.087.328 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.329 I llm_load_print_meta: general.name     = 1.4B
0.00.087.329 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.329 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.329 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.329 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.330 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.330 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.330 I llm_load_print_meta: max token length = 1024
0.00.089.834 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.834 I llm_load_tensors: offloading output layer to GPU
0.00.089.834 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.845 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.846 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.933 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.934 I llama_new_context_with_model: n_ctx         = 128
0.00.090.934 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.934 I llama_new_context_with_model: n_batch       = 128
0.00.090.934 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.934 I llama_new_context_with_model: flash_attn    = 0
0.00.090.935 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.935 I llama_new_context_with_model: freq_scale    = 1
0.00.090.935 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.936 I ggml_metal_init: allocating
0.00.090.939 I ggml_metal_init: found device: Apple M4
0.00.090.941 I ggml_metal_init: picking default device: Apple M4
0.00.091.494 I ggml_metal_init: using embedded metal library
0.00.093.545 I ggml_metal_init: GPU name:   Apple M4
0.00.093.547 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.547 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.548 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.548 I ggml_metal_init: simdgroup reduction   = true
0.00.093.548 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.548 I ggml_metal_init: has bfloat            = true
0.00.093.548 I ggml_metal_init: use bfloat            = true
0.00.093.549 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.549 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.538 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.541 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.557 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.482 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.483 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.484 I llama_new_context_with_model: graph nodes  = 967
0.00.103.484 I llama_new_context_with_model: graph splits = 2
0.00.103.496 I 
0.00.103.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.103.536 I compute_imatrix: tokenizing the input ..
0.00.110.681 I compute_imatrix: tokenization took 7.144 ms
0.00.110.683 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.552.406 I compute_imatrix: 1.44 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.555.450 I llama_perf_context_print:        load time =    1529.18 ms
0.01.555.454 I llama_perf_context_print: prompt eval time =    1441.04 ms /   128 tokens (   11.26 ms per token,    88.82 tokens per second)
0.01.555.455 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.555.457 I llama_perf_context_print:       total time =    1532.22 ms /   129 tokens
0.01.556.339 I ggml_metal_free: deallocating

real	0m1.739s
user	0m0.172s
sys	0m0.237s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4222 (f0678c5f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15250ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15250b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15250bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15250c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15250c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15250cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15250d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15250d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15250dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15250e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15250e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15250ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15250f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15250ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152510750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152510e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152511590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152511cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1525123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152512ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1525132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1525139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152514100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1525149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1525150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152515380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152515990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152516600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152516b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152516e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1525172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152517560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152517df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152518330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1525185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152518a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152518f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1525193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152519870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152519d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15251a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15251a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15251aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15251af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15251b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15251b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15251be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15251c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15251cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15251d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15251d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15251dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15251e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15251ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15251f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15251f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15251fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15251ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1525205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152520de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1525210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152521540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1525219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152521e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152522320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1525227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152522c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152523100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1525235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152523a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152523ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152524380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152524820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152524cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152525160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152525600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152525aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152525f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1525263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152526880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152526d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1525271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152527660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152527b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152527fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152528440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1525288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152528d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152529220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1525296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152529b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15252a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15252a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15252a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15252ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15252b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15252b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15251c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15252bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15252c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15252c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15252cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15252cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15252d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15252d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15252ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15252e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15252e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15252ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15252f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15252f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15252f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15252fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1525302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152530770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152530c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1525310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152531550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1525319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152531e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152532330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1525327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152532c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152533110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1525335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152533a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152533ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152534390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152534830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152534cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152535170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152535610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152535ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152535f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1525363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152536890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152536d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1525371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152537670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152537b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152537fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152538450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1525388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152538d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152539230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1525396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152539b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15253a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15253a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15253a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15253adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15253b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15253b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15253bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15253c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15253c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15253cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15253cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15253d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15253db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15253e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15253e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15253ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15253f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15253fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15253feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152540350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152540b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152541050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1525415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152541af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152542040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152542590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152542ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152543030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152543580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152543ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152544020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152544570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152544ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152545010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152545560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152545ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152546000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152546550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152546aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152546ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152547540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152547a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152547fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152548530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152548a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152548fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152549520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152549a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152549fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15254a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15254aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15254afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15254b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15254ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15254bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15254c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15254ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15254cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15254d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15254da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15254df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15254e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15254ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15254ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15254f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15254fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15254ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1525504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152550a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152550f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1525514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1525519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152551f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152552490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1525529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152552f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152553480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152553920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152553dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152554260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152554700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152554ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152555040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1525554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152555980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152555e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1525562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152556760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152556c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1525570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1525575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152557d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152558430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152558b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152559270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152559530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152559b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15255a150 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.135.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152304f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1523053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152305830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152305ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152306110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152306580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1523069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152306e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1523072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152307740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152307bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1523082a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152308dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152309570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152309d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15230a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15230abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15230b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15230ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15230c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15230c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15230cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15230d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15230ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15230e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15230e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15230ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15230eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15230f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15230f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15230fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152310140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1523105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152310870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152310ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152311150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1523115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152311a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152311ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152312310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152312780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152312bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152313060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1523134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152313940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152313db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152314220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152314690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152314b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152314f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1523153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152315850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152315cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152316130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1523165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152316a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152316f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152317480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1523178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152317d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1523181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152318640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152318ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152318f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152319390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152319800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152319c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15231a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15231a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15231a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15231ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15231b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15231b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15231bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15231bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15231c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15231c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15231cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15231d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15231d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15231da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15231df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15231e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15231e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15231ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15231f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15231f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15231f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15231fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152320280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1523206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152320b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152320fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152321440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1523218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152321d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152322190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152322600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152322a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152322ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152323350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1523237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152323c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1523240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152324510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152324980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152324df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152325260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1523256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152325b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152325fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152326420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152326890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152326d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152327170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1523275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152327a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152327ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152328330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1523287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152328c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152329080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1523294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152329960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152329dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15232a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15232a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15232ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15232af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15232b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15232b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15232bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15232c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15232c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15232ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15232cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15232d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15232d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15232dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15232e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15232e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15232e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15232edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15232f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15232f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15232fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15232ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1523303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152330850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152330cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152331130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1523315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152331a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152331e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1523322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152332760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152332bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152333040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1523334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152333920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152333d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152334200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152334670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152334ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152334f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1523353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152335830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1523363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152336680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152336940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152336db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152337220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152337690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152337b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152337f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1523383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152338850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152338cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152339130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1523395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152339a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152339e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15233a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15233a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15233abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15233b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15233b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15233b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15233bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15233c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15233c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15233cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15233cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15233d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15233d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15233dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15233e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15233e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15233e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15233ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15233f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15233f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15233fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152340020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152340490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152340900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152340d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1523411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152341650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152341ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152341f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1523423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152342810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152342c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1523430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152343560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1523439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152343e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1523442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152344720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152344b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152345000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152345470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1523458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152345d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1523461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152346630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152346aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152346f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152347380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1523477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152347c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1523480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152348540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1523489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152348e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152349290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152349700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15234a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15234a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15234b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15234b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15234ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15234bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15234c190 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15250c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15250d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15250c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15250bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15250d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15250dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15250cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15250ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152549f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15254a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15254a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15254aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15254b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15254bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15254c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15254ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15254d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15254d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15254dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15254e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15254ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15254f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15254fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152550420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152550b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152550f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1525513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152551860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152551cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152552140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1525525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152552a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152552e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152553150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1525535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152553a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152553ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152554310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152554780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152554bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152555060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1525554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152555940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152555db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152556220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152556690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152556b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152556f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1525573e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152557850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152557cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152558130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1525585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152558a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152558e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1525592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152559760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152559bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15255a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152518320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152518790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152518c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152519070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1525194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152519950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152519dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15251a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15251a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15251ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15251af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15251b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15251b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15251bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15251c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15251c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15251ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15251ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15251d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15251d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15251dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15251e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15251e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15251e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15251eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15251f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15251f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15251faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15251ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1525203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152520840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152520cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152521120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152521590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152521a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152521e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1525222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152522750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152522bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152523030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1525234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152523910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152523d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1525241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152524660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152524ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152524f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1525253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152525820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152525c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152526100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152526570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1525269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152526e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1525272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152527730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152527ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152528010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152528480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1525288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152528d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1525291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152529640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152529ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152529f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15252a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15252a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15252ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15252b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15252b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15252b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15252be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15252c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15252c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15252cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15252cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15252d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15252d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15252dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15252e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15252e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15252ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15252ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15252f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15252f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15252fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1525300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152530530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1525309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152530e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152531280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1525316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152531b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152531fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152532440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1525328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152532d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152533190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152533600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152533a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152533ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152534350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1525347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152534c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1525350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152535510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152535980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152535df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152536570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1525369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152536e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1525372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152537730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152537ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152538010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152538480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1525388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152538d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1525391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152539640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152539ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152539f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15253a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15253a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15253ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15253b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15253b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15253b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15253be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15253c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15253c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15253cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15253cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15253d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15253d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15253dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15253e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15253e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15253ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15253ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15253f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15253f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15253fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1525400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152540530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1525409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152540e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152541280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1525416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152541b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152541fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152542440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1525428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152542d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152543190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152543600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152543a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152543ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152544350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1525447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152544c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1525450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152545510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152545980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152545df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152546260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1525466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152546b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152546fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152547420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152547890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152547d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152548170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1525485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152548a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152548ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152549330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1525497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152516b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152517220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152517910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15250e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15250e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15250ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15250f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15250f670 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.790s
user	0m0.286s
sys	0m0.303s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4222 (f0678c5f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c710240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c710970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c710f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c7114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c711a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c712030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c7125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c712b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c713140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c713640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c713b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c714040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c714b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c715310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c715b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c716240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c716960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c717080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c7177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c717f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c718690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c718db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c7194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c719d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c71a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c71a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c71ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c71b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c71bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c71c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c71c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c71c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c71d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c71d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c71d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c71de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c71e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c71e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c71ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c71f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c71f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c71fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c71fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c720360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c720620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c720c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c721240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c721b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c722170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c722780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c722d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c7233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c7239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c723fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c7247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c724c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c7250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c7253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c7259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c7261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c726470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c726910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c726db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c727250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c7276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c727b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c728030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c7284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c728970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c728e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c7292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c729750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c729bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c72a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c72a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c72a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c72ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c72b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c72b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c72bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c72c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c72c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c72ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c72ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c72d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c72d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c72dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c72e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c72e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c72ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c72ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c72f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c72f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c72fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c7301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c730650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c730af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c721850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c731140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c7315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c731a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c731f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c7323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c732860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c732d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c7331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c733640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c733ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c734420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c7348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c734d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c735200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c7356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c735b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c735fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c736480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c736920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c736dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c737260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c737700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c737ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c738040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c7384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c738980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c738e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c7392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c739760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c739c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c73a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c73a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c73a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c73ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c73b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c73b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c73bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c73c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c73c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c73ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c73cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c73d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c73d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c73dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c73e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c73e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c73eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c73ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c73f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c73f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c73fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c7401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c740660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c740b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c741050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c7415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c741af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c742040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c742300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c742910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c742f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c743530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c744150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c744940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c744de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c745280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c745720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c745ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c746420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c746970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c746ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c747410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c747960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c747eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c748400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c748950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c748ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c7493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c749940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c749e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c74a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c74a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c74ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c74b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c74b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c74be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c74c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c74c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c74ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c74d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c74d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c74de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c74e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c74e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c74ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c74f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c74f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c74fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c750380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c7508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c750e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c751370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c7518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c751e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c752360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c7528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c752e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c753350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c7538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c753df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c754340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c754890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c754de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c755330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c755880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c755dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c756320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c756870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c756dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c757310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c757860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c757db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c758300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c758850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c758cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c759190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c759630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c759ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c759f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c75a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c75a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c75ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c75b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c75b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c75bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c75bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c75c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c75c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c75d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c75d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c75df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c75e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c75e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c75ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c75f520 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.082.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d804c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d8050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d805540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d8059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d805e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d806290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d806700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d806b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d806fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d807450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d8078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d807f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d808aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d809250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d809a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d80a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d80a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d80afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d80b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d80beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d80c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d80ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d80d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d80db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d80e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d80e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d80e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d80ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d80f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d80f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d80fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d80ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d8103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d810660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d810ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d810f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d8114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d8119a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d811ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d8123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d8128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d812da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d8132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d8137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d8149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d8152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d816490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d816900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d8170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d817570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d817830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d817e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d818630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d818ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d818f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d819410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d8198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d819d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d81a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d81a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d81ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d81afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d81b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d81b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d81bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d81c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d81c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d81cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d81d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d81d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d81d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d81de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d81e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d81e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d81ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d81f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d81f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d81f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d81fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d8207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d820c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d8210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d821590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d821a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d821ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d822370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d822810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d822cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d823150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d8235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d823a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d823f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d8243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d824870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d824d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d8251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d825650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d825af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d825f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d826430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d8268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d826d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d827210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d8276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d827b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d827ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d828930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d828dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d829270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d829710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d829bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d82a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d82a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d82a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d82ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d82b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d82b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d82bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d82c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d82c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d82c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d82ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d82d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d82d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d82dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d82e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d82e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d82ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d82eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d82f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d82f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d82fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d830170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d830610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d830ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d830f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d8313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d831890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d831d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d8321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d832670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d832b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d832fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d833450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d8339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d834440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d834990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d834c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d835260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d835870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d835e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d836490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d836aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d837290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d837730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d837bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d838070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d838820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d838d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d8392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d839810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d839d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d83a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d83a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d83ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d83b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d83b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d83bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d83c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d83c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d83cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d83d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d83d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d83dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d83e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d83e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d83ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d83f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d83f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d83fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d840250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d8407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d840cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d841240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d841790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d841ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d842230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d842780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d842cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d843220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d843770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d843cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d844210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d844760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d844cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d845200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d845750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d845ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d8461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d846740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d846c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d8471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d847730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d847c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d8481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d848720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d848c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d8491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d849710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d849c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d84a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d84a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d84ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d84b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d84b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d84bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d84bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d84c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d84c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d84cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d84d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d84d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d84db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d84dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d84e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d84e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d84edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d84f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d84fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d850150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d850870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d850f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d851250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d851860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d851e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c710f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c712020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c7114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c711a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c710240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c74f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c74f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c74fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c74fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c750310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c750780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c7514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c751c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c752440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c752b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c753220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c753910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c754000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c754980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c755070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c755760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c755e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c756540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c756c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c7570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c757510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c757980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c757df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c758260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c7586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c758b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c758fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c759270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c7596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c759b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c759fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c75a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c75a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c75ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c75b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c75b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c75ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c75bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c75c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c75c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c75cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c75d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c75d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c75d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c75dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c75e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c75e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c75eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c75efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c75f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c71d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c71db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c71dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c71e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c71e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c71ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c71f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c71f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c71fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c71fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c720350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c7207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c720c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c7210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c721510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c721980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c721df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c722260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c7226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c722b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c722fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c723420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c723890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c723d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c724170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c7245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c724a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c724ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c725330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c7257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c725c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c726080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c7264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c726960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c726dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c727240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c7276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c727b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c727f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c728870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c728ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c729150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c7295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c729a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c729ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c72a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c72a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c72abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c72b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c72b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c72b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c72bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c72c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c72c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c72cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c72cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c72d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c72d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c72dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c72e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c72e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c72ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c72ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c72f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c72f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c72fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c730040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c7304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c730920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c730d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c731200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c731670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c731ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c731f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c7323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c732830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c732ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c733110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c733580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c7339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c733e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c7342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c734740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c734bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c735020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c735490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c735900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c735d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c7361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c736650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c736ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c736f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c7373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c737810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c737c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c7380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c738560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c7389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c738e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c7392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c739720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c739b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c73a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c73a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c73a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c73ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c73b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c73b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c73baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c73bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c73c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c73cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c73cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c73d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c73d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c73dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c73e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c73e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c73ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c73ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c73f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c73f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c73fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c740040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c7404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c740920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c740d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c741200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c741670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c741ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c741f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c7423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c742830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c742ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c743110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c743580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c7439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c743e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c7442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c744740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c744bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c745020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c745490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c745900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c745d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c7461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c746650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c746ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c746f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c7473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c747810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c747c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c7480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c748560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c7489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c748e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c7492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c749720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c749b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c74a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c74a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c74a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c74ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c74b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c74b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c74baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c74bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c74c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c74c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c74cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c74d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c74d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c74d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c74de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c74e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c74e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c74eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c71bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c71c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c71c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c71cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c713600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c713cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c7143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c714ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c714f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c7153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c715820 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.918s
user	0m0.239s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
